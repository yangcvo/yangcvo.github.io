<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Yancy&#39;s blog</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Yancy&#39;s blog" type="application/atom+xml">
    
</head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/" class="header__link">Home</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/archives" class="header__link">ARCHIVES</a>
			
				<a href="/about/About" class="header__link">ABOUT</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Yancy&#39;s blog</a></h1>
		<h2 class="header__subtitle">SIMPLICITY IS PREREQUISITE FOR  RELIABILITY</h2>
	</header>

	<main>
		



	<article>
	
		<h1><a href="/2017/08/11/日志分析平台/Elasticsearch/ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述/">ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-08-11</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Log-Analysis-Platform/">Log Analysis Platform</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Elasticsearch/">Elasticsearch</a> <a class="article__tag-link" href="/tags/Kibana/">Kibana</a>
			</span>
		
	</div>

	

	
		<h2 id="ElasticStack-5-x介绍"><a href="#ElasticStack-5-x介绍" class="headerlink" title="ElasticStack 5.x介绍"></a>ElasticStack 5.x介绍</h2><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p><code>ELK=ElasticSearch 官方博客:分布式以及 Elastic</code></p>
<p><img src="http://jolestar.com/images/elasticsearch/scaling-stories.svg" alt=""></p>
<p>Elastic Stack是ELK日志系统的官方称呼，而ELK则是盛名在外的一款开源分布式日志系统，一般说来包括了Elasticsearch、Logstash和Kibana，涵盖了后端日志采集、日志搜索服务和前端数据展示等功能。<br>本文将会对Elastic Stack的安装部署流程进行一系列简单的介绍，并记录下了一些部署过程中遇到的坑及解决方法。</p>
<p>对于一个软件或互联网公司来说，对计算资源和应用进行监控和告警是非常基础的需求。对于大公司或成熟公司，一个高度定制化的监控系统应该已经存在了很长时间并且非常成熟了。而对于一个初创公司或小公司来说，如何利用现有开源工具快速搭建一套日志监控及分析平台是需要探索的事情。</p>
		<p><a class="article__read-more-link" href="/2017/08/11/日志分析平台/Elasticsearch/ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述/">...read more</a></p>
	

	

</article>




	<article>
	
		<h1><a href="/2017/07/04/Bigdata-hadoop/Kafka/kafka性能优化–JVM参数配置优化/">Kafka性能优化–JVM参数配置优化</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-07-04</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<p><img src="https://cdn-images-1.medium.com/max/640/1*kV768OHeCILNIYVCNPUyuQ.jpeg" alt=""></p>
<h3 id="Kafka集群稳定"><a href="#Kafka集群稳定" class="headerlink" title="Kafka集群稳定"></a>Kafka集群稳定</h3><p>GC调优<br>　　调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent，具体参数设置可以参考Google，这里不赘述。</p>
<p>　　Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent 分别设置为 20毫秒和 35。Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入。</p>
		<p><a class="article__read-more-link" href="/2017/07/04/Bigdata-hadoop/Kafka/kafka性能优化–JVM参数配置优化/">...read more</a></p>
	

	

</article>




	<article>
	
		<h1><a href="/2017/06/30/Bigdata-hadoop/Kafka/Kafka日志存储解析与实践数据存储优化/">Kafka日志存储解析与实践数据存储优化</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-30</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h1 id="Kafka日志存储解析与实践数据存储优化"><a href="#Kafka日志存储解析与实践数据存储优化" class="headerlink" title="Kafka日志存储解析与实践数据存储优化"></a>Kafka日志存储解析与实践数据存储优化</h1><p>kafka是一款分布式消息发布和订阅的系统，具有高性能和高吞吐率。 </p>
<h4 id="Kafka的名词解释"><a href="#Kafka的名词解释" class="headerlink" title="Kafka的名词解释"></a>Kafka的名词解释</h4><ul>
<li>1，<code>Broker</code>： 一个单独的kafka机器节点就称为一个broker，多个broker组成的集群，称为kafka集群</li>
<li>2，<code>Topic</code>：类似数据库中的一个表，我们将数据存储在Topic里面，当然这只是逻辑上的，在物理上，一个Topic 可能被多个Broker分区存储，这对用户是透明的，用户只需关注消息的产生于消费即可.</li>
<li>3，<code>Partition</code>：类似分区表，每个Topic可根据设置将数据存储在多个整体有序的Partition中，每个顺序化partition会生成2个文件，一个是index文件一个是log文件，index文件存储索引和偏移量，log文件存储具体的数据.</li>
<li>4，<code>Producer</code>：生产者，向Topic里面发送消息的角色 </li>
<li>5，<code>Consumer</code>：消费者，从Topic里面读取消息的角色 </li>
<li>6，<code>Consumer Group</code>：每个Consumer属于一个特定的消费者组，可为Consumer指定group name，如果不指定默认属于group </li>
</ul>
<p><strong>集群安装略过~</strong></p>
<h4 id="日志存储"><a href="#日志存储" class="headerlink" title="日志存储"></a>日志存储</h4><p>Kafka的data是保存在文件系统中的。Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。</p>
<p>每个topic又可以分成几个不同的partition，每个topic有几个partition是在创建topic时指定的，每个partition存储一部分Message。</p>
<p><code>partition</code>是以文件的形式存储在文件系统中，比如，创建了一个名为<code>kakfa-node1</code>的topic，其有12个partition，那么在Kafka的数据目录中(由配置文件中的log.dirs指定的)中就有这样5个目录: </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]$ ll</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-0</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-1</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-10</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-11</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-2</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-3</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-4</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-5</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-6</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-7</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-8</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-9</div></pre></td></tr></table></figure>
<p>其命名规则为<code>&lt;topic_name&gt;-&lt;partition_id&gt;</code>，里面存储的分别就是这12个<code>partition</code>的数据。<br><code>zookeeper</code>会将分区平均分配创建到不同的<code>broker</code>上，例如</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 tools]$ ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic kakfa-node1</div><div class="line">Topic:kakfa-node1	PartitionCount:12	ReplicationFactor:3	Configs:</div><div class="line">	Topic: kakfa-node1	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3</div><div class="line">	Topic: kakfa-node1	Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</div><div class="line">	Topic: kakfa-node1	Partition: 2	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2</div><div class="line">	Topic: kakfa-node1	Partition: 3	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2</div><div class="line">	Topic: kakfa-node1	Partition: 4	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3</div><div class="line">	Topic: kakfa-node1	Partition: 5	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</div><div class="line">	Topic: kakfa-node1	Partition: 6	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3</div><div class="line">	Topic: kakfa-node1	Partition: 7	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</div><div class="line">	Topic: kakfa-node1	Partition: 8	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2</div><div class="line">	Topic: kakfa-node1	Partition: 9	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2</div><div class="line">	Topic: kakfa-node1	Partition: 10	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3</div><div class="line">	Topic: kakfa-node1	Partition: 11	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</div></pre></td></tr></table></figure>
<p>Isr表示分区创建在哪个broker上。<br>Partition中的每条Message由offset来表示它在这个partition中的偏移量，这个offset不是该Message在partition数据文件中的实际存储位置，而是逻辑上一个值，它唯一确定了partition中的一条Message。因此，可以认为offset是partition中Message的id。partition中的每条Message包含了以下三个属性：</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="title">offset</span></div><div class="line"><span class="type">MessageSize</span></div><div class="line"><span class="class"><span class="keyword">data</span></span></div></pre></td></tr></table></figure>
<p>其中offset为long型，MessageSize为int32，表示data有多大，data为message的具体内容。</p>
<h4 id="Kafka通过分段和索引的方式来提高查询效率"><a href="#Kafka通过分段和索引的方式来提高查询效率" class="headerlink" title="Kafka通过分段和索引的方式来提高查询效率"></a>Kafka通过分段和索引的方式来提高查询效率</h4><ul>
<li>1）分段</li>
</ul>
<p>Kafka解决查询效率的手段之一是将数据文件分段，比如有100条Message，它们的offset是从0到99。假设将数据文件分成5段，第一段为0-19，第二段为20-39，以此类推，每段放在一个单独的数据文件里面，数据文件以该段中最小的offset命名。这样在查找指定offset的Message的时候，用二分查找就可以定位到该Message在哪个段中。</p>
<figure class="highlight tap"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]$ ll /data/tools/kafka_2.10-0.8.2.1/kafka-logs/</div><div class="line">total 548</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-0</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-1</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-10</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-11</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-2</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-3</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-4</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-5</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-6</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-7</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-8</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-9</div></pre></td></tr></table></figure>
<h5 id="2）为数据文件建索引"><a href="#2）为数据文件建索引" class="headerlink" title="2）为数据文件建索引"></a>2）为数据文件建索引</h5><p>数据文件分段使得可以在一个较小的数据文件中查找对应offset的Message了，但是这依然需要顺序扫描才能找到对应<code>offset的Message</code>。为了进一步提高查找的效率，Kafka为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩展名为.index。<br>索引文件中包含若干个索引条目，每个条目表示数据文件中一条Message的索引。索引包含两个部分（均为4个字节的数字），分别为相对<code>offset和position</code>。</p>
<p>相对<code>offset</code>：因为数据文件分段以后，每个数据文件的起始offset不为0，相对offset表示这条Message相对于其所属数据文件中最小的offset的大小。举例，分段后的一个数据文件的offset是从20开始，那么offset为25的Message在index文件中的相对offset就是25-20 = 5。存储相对offset可以减小索引文件占用的空间。</p>
<p><code>position</code>，表示该条<code>Message</code>在数据文件中的绝对位置。只要打开文件并移动文件指针到这个position就可以读取对应的<code>Message</code>了。<br>index文件中并没有为数据文件中的每条Message建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/countly/Bigdata-countly需要迁移/">Bigdata-countly需要迁移</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Countly/">Countly</a>
			</span>
		
	</div>

	

	
		<h4 id="官方安装文档：http-resources-count-ly-docs-installing-countly-server"><a href="#官方安装文档：http-resources-count-ly-docs-installing-countly-server" class="headerlink" title="官方安装文档：http://resources.count.ly/docs/installing-countly-server"></a>官方安装文档：<a href="http://resources.count.ly/docs/installing-countly-server" target="_blank" rel="external">http://resources.count.ly/docs/installing-countly-server</a></h4><p>目前<code>countly</code>需要迁移，所需<code>countly</code>版本于官方提供的安装方案有冲突，所以如下安装：</p>
<p>安装官方<code>countly</code>让其设置所需环境变量及其启动脚本，手动指定安装nojs版本，拷贝原countly文件，具体如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1、<span class="built_in">cd</span> /data <span class="comment">#countly安装在data目录 我看了安装脚本，是当前在哪个目录，安装文件就在哪个目录</span></div><div class="line">wget -qO- http://c.ly/install | bash</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2、rpm -qa | grep -i nodejs | xargs -I&#123;&#125;  yum remove &#123;&#125; -y</div><div class="line">卸载掉官网安装的最新nodejs 然后新建如下yum源，用于安装旧版所需nodejs，也可以到nodejs官网下载所需nodejs</div><div class="line">cat /etc/yum.repos.d/nodesource-el.repo </div><div class="line">[nodesource]</div><div class="line">name=Node.js Packages <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span></div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/<span class="variable">$basearch</span></div><div class="line">failovermethod=priority</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">[nodesource-source]</div><div class="line">name=Node.js <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span> - Source</div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/SRPMS</div><div class="line">failovermethod=priority</div><div class="line">enabled=0</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">gpgcheck=1</div></pre></td></tr></table></figure>
<h3 id="安装老版本所需nodejs"><a href="#安装老版本所需nodejs" class="headerlink" title="安装老版本所需nodejs"></a>安装老版本所需nodejs</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum <span class="keyword">install</span> nodejs</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">3、拷贝源countly文件到/data/countly目录</div><div class="line">修改 /data/countly/api/config.js 和 /data/countly/frontend/express/config.js      </div><div class="line">3001端口和 6001端口监听地址换成 本地私有地址   <span class="comment">#源文件是监听的原来机器的内网地址，不修改的话，服务启动不了。</span></div></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="number">4</span><span class="string">、拷贝mongo数据目录到/data/mongo目录，修改mongo配置文件.</span></div><div class="line"></div><div class="line"><span class="string">cat</span> <span class="string">/etc/mongod.conf</span></div><div class="line"><span class="attr">systemLog:</span></div><div class="line"><span class="attr">       destination:</span> <span class="string">file</span></div><div class="line"><span class="attr">       logAppend:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       path:</span> <span class="string">/data/mongodb/mongod.log</span></div><div class="line"><span class="attr">storage:</span></div><div class="line"><span class="attr">       dbPath:</span> <span class="string">/data/mongo</span></div><div class="line"><span class="attr">       journal:</span></div><div class="line"><span class="attr">             enabled:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       engine:</span> <span class="string">mmapv1</span></div><div class="line"><span class="attr">processManagement:</span></div><div class="line"><span class="attr">       fork:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       pidFilePath:</span> <span class="string">/data/mongodb/mongod.pid</span></div><div class="line"><span class="attr">net:</span></div><div class="line"><span class="attr">       port:</span> <span class="number">27017</span></div><div class="line"><span class="attr">       bindIp:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></div><div class="line"><span class="attr">security:</span></div><div class="line"><span class="attr">       authorization:</span> <span class="string">enabled</span></div><div class="line"><span class="attr">operationProfiling:</span></div><div class="line"><span class="attr">       slowOpThresholdMs:</span> <span class="number">40960</span></div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Bigdatacountly01.jpeg" alt=""></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">5、修改硬盘block：</div><div class="line">     blockdev --setra 256 /dev/mapper/xvdc--vg-xvdc–lv    <span class="comment">##按照mongo提示操作</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">6、修改nginx配置文件  conf.d/default.conf</div><div class="line">      将127.0.0.1修改为本地私有地址</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">7、重启countly   mongodb  nginx</div><div class="line"></div><div class="line">countly restart</div><div class="line"></div><div class="line">/etc/init.d/mongod restart</div><div class="line"></div><div class="line">service nginx restart</div><div class="line">迁移完毕</div></pre></td></tr></table></figure>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/Kafka-node模块实现调用js发送数据/">Bigdata-Kafka-node模块实现调用js发送数据</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<p>mongodb写到kafka 指定topic消费。为了保证数据稳定可靠性。<br>配合大数据在countly 使用开源<code>Kafka-node</code>是一个Node.js客户端 写js程序让countly三台集群分别数据到kafka 做新的topic主题备份。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install kafka-node</div></pre></td></tr></table></figure>
<p>进入kafka-node目录: vim kafka_test.js</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">var kafka = require(<span class="string">'kafka-node'</span>),</div><div class="line">HighLevelProducer = kafka.HighLevelProducer,</div><div class="line">    //Producer = kafka.Producer,</div><div class="line">    client = new kafka.Client(<span class="string">'169.44.62.139:2281,169.44.59.138:2281,169.44.62.137:2281'</span>),</div><div class="line">    //producer = new Producer(client);</div><div class="line">producer = new HighLevelProducer(client);</div><div class="line"></div><div class="line">console.log(<span class="string">'连接kafka中'</span>);</div><div class="line"></div><div class="line">var argv = &#123;</div><div class="line">    topic: <span class="string">"test1"</span></div><div class="line">&#125;;</div><div class="line">var topic = argv.topic || <span class="string">'test1'</span>;</div><div class="line">var p = argv.p || 0;</div><div class="line">var a = argv.a || 0;</div><div class="line">var producer = new HighLevelProducer(client, &#123;</div><div class="line">    requireAcks: 1,</div><div class="line">    partitionerType: 3</div><div class="line">&#125;);</div><div class="line"></div><div class="line">console.log(producer);</div><div class="line"></div><div class="line">producer.on(<span class="string">'ready'</span>, <span class="function"><span class="title">function</span></span>() &#123;</div><div class="line">    var args = &#123;</div><div class="line">        appid: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        createTime: <span class="string">'222-ddd'</span>,</div><div class="line">        toUserName: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        fromUserName: <span class="string">'222-wx238c28839a133d0e'</span></div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    producer.send([&#123;</div><div class="line">        topic: topic,</div><div class="line">        partition: p,</div><div class="line">        messages: [JSON.stringify(args)],</div><div class="line">        attributes: a</div><div class="line">    &#125;], <span class="keyword">function</span>(err, result) &#123;</div><div class="line">        console.log(err || result);</div><div class="line">        process.exit();</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    console.log(args);</div><div class="line">&#125;);</div></pre></td></tr></table></figure>
<h4 id="官网地址：https-www-npmjs-com-package-kafka-node-install-kafka"><a href="#官网地址：https-www-npmjs-com-package-kafka-node-install-kafka" class="headerlink" title="官网地址：https://www.npmjs.com/package/kafka-node#install-kafka"></a>官网地址：<a href="https://www.npmjs.com/package/kafka-node#install-kafka" target="_blank" rel="external">https://www.npmjs.com/package/kafka-node#install-kafka</a></h4>
	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/如何手动更新Kafka中某个Topic的偏移量/">Bigdata-如何手动更新Kafka中某个Topic的偏移量</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h1 id="如何手动更新Kafka中某个Topic的偏移量"><a href="#如何手动更新Kafka中某个Topic的偏移量" class="headerlink" title="如何手动更新Kafka中某个Topic的偏移量"></a>如何手动更新Kafka中某个Topic的偏移量</h1><p>　　我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如iteblog主题分区10的偏移量获取如下：<br>　　<br>　　在有些场景下，这个工具不满足我们的需求，我们需要的是能够手动设置分区的偏移量为任何有意义的值，而不仅仅是earliest或者latest。那咋办？</p>
<p>　　我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如<code>mongotail_lz4</code>主题分区10的偏移量获取如下：
　　</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 2]  get /consumers/ibm_event/offsets/mongotail_lz4/10</div><div class="line">293894</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x6000018c9</div><div class="line">mtime = Wed Jul 26 18:18:27 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 20</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 6</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure>
<p>所以，我们可以通过set命令来设置某个分区的偏移量，如下；</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECT get /consumers/ibm_event/offsets/mongotail_lz4/10 0 </div><div class="line">0</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x60000204c</div><div class="line">mtime = Wed Jul 26 18:37:21 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 21</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 1</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure>
<p>12个分区分别更新过去。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/Bigdata-Kafka三款监控工具比较/">Bigdata-Kafka三款监控工具比较</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<p>在之前的博客中，介绍了<code>Kafka Web Console</code>这个监控工具，在生产环境中使用，运行一段时间后，发现该工具会和Kafka生产者、消费者、ZooKeeper建立大量连接，从而导致网络阻塞。并且这个Bug也在其他使用者中出现过，看来使用开源工具要慎重！该Bug暂未得到修复，不得已，只能研究下其他同类的Kafka监控软件。</p>
<p>通过研究，发现主流的三种kafka监控程序分别为：</p>
<ul>
<li>1、Kafka Web Conslole</li>
<li>2、Kafka Manager</li>
<li>3、KafkaOffsetMonitor</li>
</ul>
<p>现在依次介绍以上三种工具：</p>
<h2 id="1、Kafka-Web-Conslole"><a href="#1、Kafka-Web-Conslole" class="headerlink" title="1、Kafka Web Conslole"></a>1、Kafka Web Conslole</h2><p>使用Kafka Web Console，可以监控：</p>
<ul>
<li>Brokers列表</li>
<li>Kafka 集群中 Topic列表，及对应的Partition、LogSiz e等信息</li>
<li>点击Topic，可以浏览对应的Consumer Groups、Offset、Lag等信息</li>
<li>生产和消费流量图、消息预览…</li>
</ul>
<p>程序运行后，会定时去读取kafka集群分区的日志长度，读取完毕后，连接没有正常释放，一段时间后产生大量的socket连接，导致网络堵塞。</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Kafka_web_console.png" alt=""></figure></p>
<h2 id="2、Kafka-Manager"><a href="#2、Kafka-Manager" class="headerlink" title="2、Kafka Manager"></a>2、Kafka Manager</h2><p>雅虎开源的Kafka集群管理工具:</p>
<ul>
<li>管理几个不同的集群</li>
<li>监控集群的状态(topics, brokers, 副本分布, 分区分布)</li>
<li>产生分区分配(Generate partition assignments)基于集群的当前状态</li>
<li>重新分配分区</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka_manager.png" alt=""></figure></p>
<h2 id="3、KafkaOffsetMonitor"><a href="#3、KafkaOffsetMonitor" class="headerlink" title="3、KafkaOffsetMonitor"></a>3、KafkaOffsetMonitor</h2><ul>
<li>KafkaOffsetMonitor可以实时监控：</li>
<li>Kafka集群状态</li>
<li>Topic、Consumer Group列表</li>
<li>图形化展示topic和consumer之间的关系</li>
<li>图形化展示consumer的Offset、Lag等信息</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafkaoffsetmonitor.png" alt=""></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过使用，个人总结以上三种监控程序的优缺点：</p>
<p><a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">Kafka Web Console</a>：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。</p>
<p><a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="external">Kafka Manager</a>：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。</p>
<p><a href="https://github.com/quantifind/KafkaOffsetMonitor" target="_blank" rel="external">KafkaOffsetMonitor</a>：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。</p>
<p>若只需要监控功能，推荐使用KafkaOffsetMonito，若偏重Kafka集群管理，推荐使用Kafka Manager。</p>
<p>因为都是开源程序，稳定性欠缺。故需先了解清楚目前已存在哪些Bug，多测试一下，避免出现类似于Kafka Web Console的问题。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/28/Bigdata-hadoop/Kafka/开源的Kafka集群管理器(Kafka Manager)/">Bigdata-开源的Kafka集群管理器(Kafka Manager)</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-28</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h2 id="Kafka-Manager"><a href="#Kafka-Manager" class="headerlink" title="Kafka Manager"></a>Kafka Manager</h2><p>A tool for managing <a href="http://kafka.apache.org/" target="_blank" rel="external">Apache Kafka.</a></p>
<h4 id="It-supports-the-following"><a href="#It-supports-the-following" class="headerlink" title="It supports the following :"></a>It supports the following :</h4><ul>
<li>管理多个群集</li>
<li>容易检查集群状态（主题，消费者，偏移量，经纪人，副本分发，分区分配）</li>
<li>运行首选副本选举</li>
<li>使用选项生成分区分配，以选择要使用的代理</li>
<li>运行分区的重新分配（基于生成的分配）</li>
<li>创建可选主题配置的主题（0.8.1.1具有不同于0.8.2+的配置）</li>
<li>删除主题（仅支持0.8.2+，并记住在代理配 置中设置delete.topic.enable = true）</li>
<li>主题列表现在表示标记为删除的主题（仅支持0.8.2+）</li>
<li>批量生成多个主题的分区分配，并选择要使用的代理</li>
<li>批量运行多个主题的分区重新分配</li>
<li>将分区添加到现有主题</li>
<li>更新现有主题的配置</li>
<li>可选地，启用JMX轮询代理级和主题级度量。</li>
<li>可选地筛选出在zookeeper中没有ids / owner /＆offset /目录的消费者。</li>
</ul>
<p>参考开源地址：<a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="external">https://github.com/yahoo/kafka-manager</a></p>
<h4 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h4><p>Kafka 0.8.1.1 or 0.8.2.<em> or 0.9.0.</em> or 0.10.0.*<br>Java 8+</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo wget --header <span class="string">"Cookie: oraclelicense=accept-securebackup-cookie”   http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz</span></div><div class="line"></div><div class="line">sudo vim /etc/profile</div><div class="line">export JAVA_HOME=/home/jollybi/tools/jdk1.8.0_144</div><div class="line">export LASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib:<span class="variable">$JAVA_HOME</span>/bin</div><div class="line">export PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JAVA_HOME</span>/jre/bin:<span class="variable">$TOMCAT_HOME</span>/bin:<span class="variable">$PATH</span></div></pre></td></tr></table></figure>
<h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/yahoo/kafka-manager.git</div><div class="line">./sbt clean dist</div><div class="line"></div><div class="line">[info]   Compilation completed <span class="keyword">in</span> 13.366 s</div><div class="line">model contains 672 documentable templates</div><div class="line">[info] Main Scala API documentation successful.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-javadoc.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-sans-externalized.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info]</div><div class="line">[info] Your package is ready <span class="keyword">in</span> /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip</div><div class="line">[info]</div><div class="line">[success] Total time: 142 s, completed Jul 27, 2017 3:48:35 PM</div><div class="line">完成</div></pre></td></tr></table></figure>
<h4 id="Starting-the-service"><a href="#Starting-the-service" class="headerlink" title="Starting the service"></a>Starting the service</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">解压缩生成的zip文件后，将工作目录更改为可以运行的服务：</div><div class="line"></div><div class="line">unzip /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip</div><div class="line"></div><div class="line"></div><div class="line">修改zk地址和管理员账号和密码：</div><div class="line"></div><div class="line">vim kafka-manager-1.3.3.8/conf/application.conf</div><div class="line"></div><div class="line"><span class="comment">#kafka-manager.zkhosts="kafka-manager-zookeeper:2181"</span></div><div class="line"><span class="comment">#zk集群可以这么配置：</span></div><div class="line">kafka-manager.zkhosts=<span class="string">"kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#根据个人公司这里可以开启true 设置账号和密码</span></div><div class="line">basicAuthentication.enabled=<span class="literal">true</span></div><div class="line">basicAuthentication.username=<span class="string">"admin"</span></div><div class="line">basicAuthentication.password=<span class="string">"admin"</span></div><div class="line"></div><div class="line"></div><div class="line">默认情况下，它将选择端口9000.这是可以覆盖的，配置文件的位置也是如此。例如：</div><div class="line"></div><div class="line">$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080</div><div class="line"></div><div class="line">后台生效：</div><div class="line"></div><div class="line">$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;</div><div class="line"></div><div class="line">再次，如果java不在您的路径中，或者您需要针对不同版本的Java运行，请按如下所示添加-java-home选项：</div><div class="line"></div><div class="line">$ bin/kafka-manager -java-home /usr/<span class="built_in">local</span>/oracle-java-8</div></pre></td></tr></table></figure>
<h4 id="Packaging"><a href="#Packaging" class="headerlink" title="Packaging"></a>Packaging</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">If you<span class="string">'d like to create a Debian or RPM package instead, you can run one of:</span></div><div class="line"></div><div class="line">sbt debian:packageBin</div><div class="line"></div><div class="line">sbt rpm:packageBin</div></pre></td></tr></table></figure>
<h3 id="查看端口："><a href="#查看端口：" class="headerlink" title="查看端口："></a>查看端口：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">[jollybi@kafka1 conf]$ netstat -ntulp | grep 8080</div><div class="line">(Not all processes could be identified, non-owned process info</div><div class="line"> will not be shown, you would have to be root to see it all.)</div><div class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      70517/java</div></pre></td></tr></table></figure>
<h3 id="网站访问kafka-Manger"><a href="#网站访问kafka-Manger" class="headerlink" title="网站访问kafka Manger"></a>网站访问kafka Manger</h3><p>这里我设置了登录账号和密码： admin admin</p>
<p><figure class="figure"><img src="media/15014722977191.jpg" alt=""></figure></p>
<p>创建kafka名字;<br>选择kafka版本号;<br>JMX这个不需要;<br>下面选择默认点击确认即可.</p>
<p><figure class="figure"><img src="media/15014724051756.jpg" alt=""></figure></p>
<blockquote>
<p>(2)kafka 启用 JMX端口</p>
</blockquote>
<p><figure class="figure"><img src="media/15020729297106.jpg" alt=""></figure></p>
<figure class="highlight vbscript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">以如下命令重新启动kafka</div><div class="line"></div><div class="line">JMX_PORT=<span class="number">9999</span> bin/kafka-<span class="built_in">server</span>-start.sh config/<span class="built_in">server</span>.properties</div><div class="line">或者修改kafka-<span class="built_in">server</span>-start.sh 文件，追加JMX_PORT=<span class="string">"9999"</span></div><div class="line"></div><div class="line"> <span class="keyword">if</span> [ <span class="string">"x$KAFKA_HEAP_OPTS"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></div><div class="line">    export KAFKA_HEAP_OPTS=<span class="string">"-Xmx1G -Xms1G"</span></div><div class="line">    export JMX_PORT=<span class="string">"9999"</span></div><div class="line">fi</div><div class="line">然后重新启动kafka</div><div class="line">bin/kafka-<span class="built_in">server</span>-start.sh config/<span class="built_in">server</span>.properties</div><div class="line"></div><div class="line">但是Metrics中数据都是零</div><div class="line">查看 kafka manager 报错，无法连接jxm</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">解决方法 修改每个kafka broker的 kafka_2.11-0.10.1.0/bin/kafka-run-class.sh文件</div><div class="line">​</div><div class="line"><span class="comment"># JMX settings</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$KAFKA_JMX_OPTS</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  KAFKA_JMX_OPTS=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=75.126.5.162"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"></div><div class="line">-Djava.rmi.server.hostname 的值为当前kafka服务器ip</div><div class="line"></div><div class="line">这里说明下集群kafka都需要修改</div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="media/15020745879851.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725443386.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725288425.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725659340.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725864134.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014726361909.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014727525618.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014727932029.jpg" alt=""></figure></p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/28/Bigdata-hadoop/Kafka/Bigdata-开源的Kafka集群管理器(kafka-web-console)/">Bigdata-开源的Kafka集群管理器(kafka-web-console)</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-28</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>源码的地址在:<a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">kafka-web-console</a></p>
<p><code>Kafka Web Console</code>也是用Scala语言编写的<code>Java web</code>程序用于监控<code>Apache Kafka</code>。这个系统的功能和<code>KafkaOffsetMonitor</code>很类似，但是我们从源码角度来看，这款系统实现比<code>KafkaOffsetMonitor</code>要复杂很多，而且编译配置比<code>KafkaOffsetMonitor</code>较麻烦。</p>
<p>　要想运行这套系统我们需要的先行条件为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Play Framework 2.2.x</div><div class="line">Apache Kafka 0.8.x</div><div class="line">Zookeeper 3.3.3 or 3.3.4</div></pre></td></tr></table></figure>
<h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><p>同样，我们从<code>https://github.com/claudemamo/kafka-web-console</code>上面将源码下载下来，然后用<code>sbt</code>进行编译，在编译前我们需要做如下的修改：</p>
<p>Kafka Web控制台需要一个关系数据库。默认情况下，服务器连接到嵌入式H2数据库，不需要数据库安装或配置。请咨询Play！的文档以指定控制台的数据库。支持以下数据库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/claudemamo/kafka-web-console.git</div></pre></td></tr></table></figure>
<ul>
<li>H2（默认）</li>
<li>PostgreSql</li>
<li>Oracle</li>
<li>DB2</li>
<li>MySQL</li>
<li>Apache Derby</li>
<li>Microsoft SQL Server</li>
</ul>
<p>为了方便，我们可以使用Mysql数据库，只要做如下修改即可，找到 <code>conf/application.conf</code>文件，并修改如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">将这个</div><div class="line">db.default.driver=org.h2.Driver</div><div class="line">db.default.url=<span class="string">"jdbc:h2:file:play"</span></div><div class="line"><span class="comment"># db.default.user=sa</span></div><div class="line"><span class="comment"># db.default.password=""</span></div><div class="line"> </div><div class="line"> </div><div class="line">修改成</div><div class="line">db.default.driver=com.mysql.jdbc.Driver</div><div class="line">db.default.url=<span class="string">"jdbc:mysql://localhost:3306/kafkamonitor"</span></div><div class="line">db.default.user=iteblog</div><div class="line">db.default.pass=wyp</div></pre></td></tr></table></figure>
<p>我们还需要修改build.sbt，加入对Mysql的依赖:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="string">"mysql"</span> % <span class="string">"mysql-connector-java"</span> % <span class="string">"5.1.31"</span></div></pre></td></tr></table></figure>
<p>　2、执行<code>conf/evolutions/default/bak</code>目录下面的<code>1.sql、2.sql和3.sql</code>三个文件。需要注意的是，这三个sql文件不能直接运行，有语法错误，需要做一些修改。<br>上面的注意事项弄完之后，我们就可以编译下载过来的源码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> sbt package</span></div></pre></td></tr></table></figure>
<p>编译的过程比较慢，有些依赖包下载速度非常地慢，请耐心等待。<br>　在编译的过程中，可能会出现有些依赖包无法下载，如下错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">[warn] module not found: com.typesafe.play<span class="comment">#sbt-plugin;2.2.1</span></div><div class="line">[warn] ==== typesafe-ivy-releases: tried</div><div class="line">[warn] http://repo.typesafe.com/typesafe/ivy-releases/</div><div class="line">com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== sbt-plugin-releases: tried</div><div class="line">[warn] http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/</div><div class="line">com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== <span class="built_in">local</span>: tried</div><div class="line">[warn] /home/iteblog/.ivy2/<span class="built_in">local</span>/com.typesafe.play/</div><div class="line">sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== Typesafe repository: tried</div><div class="line">[warn] http://repo.typesafe.com/typesafe/releases/com/</div><div class="line">typesafe/play/sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom</div><div class="line">[warn] ==== public: tried</div><div class="line">[warn] http://repo1.maven.org/maven2/com/typesafe/play/</div><div class="line">sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom</div><div class="line">[warn] ::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">==== <span class="built_in">local</span>: tried</div><div class="line"> </div><div class="line">/home/iteblog/.ivy2/<span class="built_in">local</span>/org.scala-sbt/collections/0.13.0/jars/collections.jar</div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">:: FAILED DOWNLOADS ::</div><div class="line"> </div><div class="line">:: ^ see resolution messages <span class="keyword">for</span> details ^ ::</div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">:: org.scala-sbt<span class="comment">#collections;0.13.0!collections.jar</span></div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div></pre></td></tr></table></figure>
<p>我们可以手动地下载相关依赖，并放到类似<code>/home/iteblog/.ivy2/local/org.scala-sbt/collections/0.13.0/jars/</code>目录下面。然后再编译就可以了。</p>
<p>　　最后，我们可以通过下面命令启动<code>Kafka Web Console</code>监控系统：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> sbt run</span></div></pre></td></tr></table></figure>
<p>并可以在<a href="http://localhost:9000" target="_blank" rel="external">http://localhost:9000</a> 查看下面是一张效果图</p>
<p><figure class="figure"><img src="https://www.iteblog.com/pic/topics.png" alt=""></figure></p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/01/Bigdata-hadoop/Zabbix监控Kafka集群 Brokers服务/">Zabbix监控Kafka集群 Brokers服务</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-01</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Kafka/">Kafka</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a> <a class="article__tag-link" href="/tags/Zabbix/">Zabbix</a>
			</span>
		
	</div>

	

	
		<h2 id="Zabbix监控Kafka集群-Brokers服务"><a href="#Zabbix监控Kafka集群-Brokers服务" class="headerlink" title="Zabbix监控Kafka集群 Brokers服务"></a>Zabbix监控Kafka集群 Brokers服务</h2><h4 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h4><p>公司让我处理Hadoop及相关服务监控、报警这里主要讲kafka集群服务。这里我也看了几篇kafka相关文章，好文贴出来：</p>
<ul>
<li><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-1" target="_blank" rel="external">infoq kafka 入门了解</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/index.html" target="_blank" rel="external">kafka 工作原理介绍</a></li>
</ul>
<p>在我们公司主要Kafka 的几个特性非常满足我们的需求：可扩展性、数据分区、低延迟、处理大量不同消费者的能力。</p>
<p>而这里我想帮忙BI团队实现Kafka全面监控。分两点：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1. 监控Kafka Brokers服务</div><div class="line">2. 监控Kafka Lag堆积数</div></pre></td></tr></table></figure>
<p>对于Kafka的监控，已经有现成的开源软件了，在我们公司内部也使用了一段时间，有两种方案。我们公司用第三种方案。<br>一般都会选择两个开源的工具：KafkaOffsetMonitor和kafka-web-console，这两款我都有用过.</p>
<ul>
<li>Kafka三款监控工具比较</li>
</ul>
<p>目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">1、Kafka Web Conslole</div><div class="line">2、Kafka Manager</div><div class="line">3、KafkaOffsetMonitor</div></pre></td></tr></table></figure>
<p><a href="https://github.com/quantifind/KafkaOffsetMonitor" target="_blank" rel="external">KafkaOffsetMonitor</a>：最大的好处就是配置简单，只需要配个zookeeper的地址就能用了，坑爹的地方就是不能自动刷新，手动刷新时耗时较长，而且有时候都刷不出来，另外就是图像用了一段时间就完全显示不了了，不知道大家是不是这样。</p>
<p><a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">kafka-web-console</a>：相比与前者，数据是落地的，因此刷新较快，而且支持在前端自定义zookeeper的地址，还能列出实时的topic里的具体内容。但是搭建比较复杂，而且github上的默认数据库是H2的，像我们一般用mysql的，还得自己转化。另外在用的过程中，我遇到一个问题，在连接kafka的leader失败的时候，会一直重试，其结果就是导致我kafka的那台机子连接数过高，都到2w了，不知道是不是它的一个bug。</p>
<p>具体介绍以及安装在我另外篇博文。这里不详细讲解开源软件，这里用zabbix监控Kafka Brokers服务。</p>
<h4 id="kafka-monitoring-Brokers服务"><a href="#kafka-monitoring-Brokers服务" class="headerlink" title="kafka-monitoring Brokers服务"></a>kafka-monitoring Brokers服务</h4><p>操作系统环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Centos 7.1 </div><div class="line">64G内存</div><div class="line">2T磁盘空间，指定kafka写入数据目录。</div></pre></td></tr></table></figure>
<p>说明下:</p>
<p>监控Brokers是利用Zabbix JMX监控获取数据。</p>
<p>☝️第一步：不用解释前提你在zabbix-server端已经安装过abbix-java-gataway 如果没有安装可以看下面，安装过可以略过第一步。</p>
<p>1.0 安装配置zabbix-java-gataway</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">yum install -y zabbix-java-gataway</div><div class="line">vi /etc/zabbix/zabbix_java_gateway.conf</div><div class="line">START_POLLERS=10 </div><div class="line">Uncoment并设置为StartJavaPollers = 5 更改JavaGateway的 IP = IP_address_java_gateway</div><div class="line">``` </div><div class="line">1.1 重新启动zabbix-server</div><div class="line"></div><div class="line">```bash</div><div class="line">/etc/init.d/zabbix-java-gataway restart</div><div class="line">chkconfig --level 345 zabbix-java-gataway on</div><div class="line">/etc/init.d/zabbix-java-gataway start</div></pre></td></tr></table></figure>
<p>☝️第二步：Kafka配置</p>
<p>这里我们服务器给大数据那边安装在指定用户目录下面：/home/jollybi/tools/kafka_2.10-0.8.2.1</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">su - jollybi 进入相应用户</div><div class="line">vim /home/jollybi/tools/kafka_2.10-0.8.2.1/kafka-run-class.sh</div><div class="line"></div><div class="line">从</div><div class="line"></div><div class="line"><span class="comment"># JMX settings</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$KAFKA_JMX_OPTS</span>"</span> ]; <span class="keyword">then</span></div><div class="line">KAFKA_JMX_OPTS=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -   Dcom.sun.management.jmxremote.ssl=false "</span></div><div class="line"><span class="keyword">fi</span></div><div class="line">至</div><div class="line"></div><div class="line"><span class="comment"># JMX settings</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$KAFKA_JMX_OPTS</span>"</span> ]; <span class="keyword">then</span></div><div class="line">KAFKA_JMX_OPTS=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=12345 -    Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false "</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<p>重启服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/home/jollybi/tools/kafka_2.10-0.8.2.1/bin/kafka-server-stop.sh</div><div class="line">/home/jollybi/tools/kafka_2.10-0.8.2.1/bin/kafka-server-start.sh</div></pre></td></tr></table></figure>
<p>☝️第三步：导入模板登录到您的zabbix网页.</p>
<p>导入模板登录到您的zabbix网页</p>
<p>单击配置 - &gt;模板 - &gt;导入</p>
<p>下载模板 <a href="https://github.com/yangcvo/Zabbix-Monitoring-Kafka" target="_blank" rel="external">zbx_kafka_templates.xml</a> 并上传到zabbix然后将此模板添加到Kafka并在zabbix上配置JMX接口</p>
<p>输入Kafka IP地址和JMX端口如果看到jmx图标，您配置了JMX监控好！</p>
<ul>
<li>查看数据：</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Brokers_01.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Brokers_02.png" alt=""></figure></p>
<ul>
<li>查看效果图：</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Brokers_03.png" alt=""></figure></p>
<p>参考官网监控指标的含义;</p>
<p><a href="http://docs.confluent.io/current/kafka/monitoring.html" target="_blank" rel="external">Servilo Metrics broker Metrics</a><br><a href="http://docs.confluent.io/current/kafka/monitoring.html" target="_blank" rel="external">Producer Metrics Global Request Metrics</a><br><a href="http://docs.confluent.io/current/kafka/monitoring.html" target="_blank" rel="external">Global Connection Metrics</a></p>

	

	

</article>





	<span class="different-posts">📖 <a href="/page/2">more posts</a> 📖</span>



	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<h5>Hi there,</h5>
	<p style="text-align:justify;">my name is Yancy, they know me as Radiant🐑🐑. This blog is about Bi-data and my live.<br> I like 🌳🌳🌳</p>
</div>


	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2017 Yancy | Powered by <a href="http://blog.yancy.cc/">Yancy</a> | Theme <a href="https://github.com/yangcvo">Yancy_GitHub</a></span>
		</div>

	</div>


</footer>



</body>

</html>
