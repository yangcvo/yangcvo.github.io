<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Yancy&#39;s blog</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Yancy&#39;s blog" type="application/atom+xml">
    
</head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/" class="header__link">Home</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/archives" class="header__link">ARCHIVES</a>
			
				<a href="/about/About" class="header__link">ABOUT</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Yancy&#39;s blog</a></h1>
		<h2 class="header__subtitle">SIMPLICITY IS PREREQUISITE FOR  RELIABILITY</h2>
	</header>

	<main>
		



	<article>
	
		<h1><a href="/2017/08/11/日志分析平台/Elasticsearch/ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述/">ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-08-11</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/日志分析平台/">日志分析平台</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Elasticsearch/">Elasticsearch</a> <a class="article__tag-link" href="/tags/Kibana/">Kibana</a>
			</span>
		
	</div>

	

	
		<h2 id="ElasticStack-5-x介绍"><a href="#ElasticStack-5-x介绍" class="headerlink" title="ElasticStack 5.x介绍"></a>ElasticStack 5.x介绍</h2><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p><img src="https://static-www.elastic.co/cn/assets/blt635d8def9c11a400/hero.svg?q=566" alt="ELK=ElasticSearch + Logstash + Kibana"></p>
<p>Elastic Stack是ELK日志系统的官方称呼，而ELK则是盛名在外的一款开源分布式日志系统，一般说来包括了Elasticsearch、Logstash和Kibana，涵盖了后端日志采集、日志搜索服务和前端数据展示等功能。<br>本文将会对Elastic Stack的安装部署流程进行一系列简单的介绍，并记录下了一些部署过程中遇到的坑及解决方法。</p>
<p>对于一个软件或互联网公司来说，对计算资源和应用进行监控和告警是非常基础的需求。对于大公司或成熟公司，一个高度定制化的监控系统应该已经存在了很长时间并且非常成熟了。而对于一个初创公司或小公司来说，如何利用现有开源工具快速搭建一套日志监控及分析平台是需要探索的事情。</p>
		<p><a class="article__read-more-link" href="/2017/08/11/日志分析平台/Elasticsearch/ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述/">...read more</a></p>
	

	

</article>




	<article>
	
		<h1><a href="/2017/07/30/日志分析平台/Elasticsearch/研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？/">研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-07-30</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/日志分析平台/">日志分析平台</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Elasticsearch/">Elasticsearch</a>
			</span>
		
	</div>

	

	
		<p>ES 现在普遍大公司都已经在使用，有的用来做数据存储，也有做性能监控，也有做日志收集，跟大数据结合做日志分析。<br>今天在家无聊研究官网学习了下Elasticsearch5.0.0 功能新增哪些，个人觉得ES也是现在开源日志分析平台比较火的，从14年开始陆续使用频率不断提升。<br>之前也有了解过Graylog  现在比ES 做数据存储分析 不错的是Graylog 比ES性能好。</p>
<p>废话不多说了哈哈</p>
<h3 id="Elasticsearch-5-0-0发布"><a href="#Elasticsearch-5-0-0发布" class="headerlink" title="Elasticsearch 5.0.0发布"></a>Elasticsearch 5.0.0发布</h3><p>2016年10月26日发布 Elasticsearch 5.0.0 今天去下载吧！你知道你想。</p>
<p><a href="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="external">下载Elasticsearch 5.0.0</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.0/release-notes-5.0.0.html" target="_blank" rel="external">Elasticsearch 5.0.0发行说明</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.0/release-notes-5.0.0.html" target="_blank" rel="external">弹性搜索5.0突破变化</a></p>
<p>Elasticsearch 5.0.0对大家有所帮助 这是有史以来最快，最安全，最具弹性，最简单易用的Elasticsearch版本，它具有一系列增强功能和新功能。</p>
<ul>
<li>索引性能</li>
<li>取食节点</li>
<li>无痛脚本</li>
<li>新的数据结构</li>
<li>搜索和聚合</li>
<li>用户友好</li>
<li>弹性</li>
<li>Java REST客户端</li>
<li>迁移助手</li>
</ul>
<blockquote>
<p>下面从官网上面翻译得来的：</p>
</blockquote>
<h3 id="索引性能"><a href="#索引性能" class="headerlink" title="索引性能"></a>索引性能</h3><p>由于包括更好的数字数据结构（参见新数据结构）的一些更改，索引吞吐量在5.0.0中有了显着改善，锁中的争用减少，阻止对同一文档的并发更新，以及减少事务日志时的锁定需求。异步translog fsyncing将特别有利于旋转磁盘的用户，而依赖Elasticsearch自动生成文档ID时，仅追加用例（基于思考时间的事件）具有很大的吞吐量提高。对实时文档GET的支持的内部变化意味着索引缓冲区可用的内存更多，垃圾收集时间少得多。</p>
<p>根据您的用例，您可能会看到在提高吞吐量的25％至80％之间。</p>
<h3 id="Ingest节点"><a href="#Ingest节点" class="headerlink" title="Ingest节点"></a>Ingest节点</h3><p>　　向Elasticsearch添加数据更简单了。Logstash是一个强大的工具，而一些较小的用户只需要过滤器，不需要它所提供的众多路由选项。因此，Elastic将一些最流行的Logstash过滤器（如grok、split）直接在Elasticsearch中实现为处理器。多个处理器可以组合成一个管道，在索引时应用到文档上。</p>
<h3 id="Painless脚本"><a href="#Painless脚本" class="headerlink" title="Painless脚本"></a>Painless脚本</h3><p>　　Elasticsearch中很多地方用到了脚本，而出于安全考虑，脚本在默认情况下是禁用的，这令人相当失望。为此，Elastic开发了一种新的脚本语言Painless。该语言更快、更安全，而且默认是启用的。不仅如此，它的执行速度是Groovy的4倍，而且正在变得更快。Painless已经成为默认脚本语言，而Groovy、Javascript和Python都遭到弃用。要了解有关这门新语言的更多信息，请点击这里。</p>
<h3 id="新数据结构"><a href="#新数据结构" class="headerlink" title="新数据结构"></a>新数据结构</h3><p>　　Lucene 6带来了一个新的Points 数据结构K-D树，用于存储数值型和地理位置字段，彻底改变了数值型值的索引和搜索方式。基准测试表明，Points将查询速度提升了36%，将索引速度提升了71%，而占用的磁盘和内存空间分别减少了66%和85%（参见“在5.0中搜索数值”）。</p>
<h4 id="搜索和聚合"><a href="#搜索和聚合" class="headerlink" title="搜索和聚合"></a>搜索和聚合</h4><p>　　借助即时聚合，Kibana图表生成速度显著提升。Elastic用一年的时间对搜索API进行了重构，Elasticsearch现在可以更巧妙地执行范围查询，只针对已经发生变化的索引重新计算聚合，而不是针对每个查询从头开始重新计算。在搜索方面，默认的相关性计算已经由TF/IDF换成了更先进的BM25。补全建议程序经过了完全重写，将已删除的文档也考虑了进来。</p>
<h3 id="更友好"><a href="#更友好" class="headerlink" title="更友好"></a>更友好</h3><p>　　Elasticsearch 5.0更安全、更易用。他们采用了“尽早提示”的方法。如果出现了问题，则新版本会及早给出提示。例如，Elasticsearch 5.0会严格验证设置。如果它不能识别某项设置的值，就会给出提示和建议。不仅如此，集群和索引设置现在可以通过null进行解除。此外，还有其他的一些改进，例如，rollover和shrink API启用了一种新的模式来管理基于时间的索引，引入新的cluster-allocation-explain API，简化索引创建。</p>
<h3 id="弹性"><a href="#弹性" class="headerlink" title="弹性"></a>弹性</h3><p>　　Elasticsearch分布式模型的每一部分都被分解、重构和简化，提升了可靠性。集群状态更新现在会等待集群中的所有节点确认。如果一个“复制片（replica shard）”被“主片（primary）”标记为失败，则主片会等待“主节点（master）”的响应。索引现在使用数据路径中的UUID，而不是索引名，避免了命名冲突。另外，Elasticsearch现在进行启动检查，确保系统配置没有问题。配置比较麻烦，但如果只是试用，开发人员也可以选择localhost-only模式，避免繁琐的配置。另外，新版本还增加了断路器及其他一些软限制，限制请求使用的内存大小，保护集群免受恶意用户攻击。<br>　　此外，该版本还提供了一个底层的Java REST/HTTP客户端，可以用于监听、日志记录、请求轮询、故障节点重试等。它使用Java 7，将依赖降到了最低，比Transport客户端的依赖冲突少。而在基准测试中，它的性能并不输于Transport客户端。不过，这是一个底层客户端，目前还没有提供任何查询构建器或辅助器。它的输入参数和输出结果都是JSON。<br>　　需要注意的是，该版本引入了许多破坏性更改，好在他们提供了一个迁移辅助插件，可以帮助开发人员从Elasticsearch 2.3.x/2.4.x迁移到Elasticsearch 5.0。如果是从更早的Elasticsearch版本向最新的5.0版本迁移，则请查阅升级文档。</p>
<h3 id="Java-REST客户端"><a href="#Java-REST客户端" class="headerlink" title="Java REST客户端"></a>Java REST客户端</h3><p>经过多年的等待，我们终于发布了一个低级的Java HTTP / REST客户端。它提供了一个简单的HTTP客户端，具有最少的依赖关系，可以处理嗅探，记录，循环请求，并重试节点故障。它使用的REST层历史上比Java API更稳定，这意味着它可以跨越升级使用，甚至可能在主要版本之间进行升级。它与Java 7一起工作，并且具有最小的依赖性，导致比传输客户端更少的依赖冲突。它只是HTTP，因此可以像所有其他HTTP客户端一样进行防火墙/代理。在我们的基准测试中，Java REST客户端 与Transport客户端的功能类似。</p>
<p>请注意，这是一个低级客户端。在这个阶段，我们不提供任何可以在IDE中自动完成的查询构建器或帮助器。它是JSON-in，JSON-out，由你来构建JSON。开发不会停止在这里 - 我们将添加一个API，它将帮助您构建查询并解析响应。您可以按照问题＃19055中的更改进行操作。</p>
<h3 id="迁移助手"><a href="#迁移助手" class="headerlink" title="迁移助手"></a>迁移助手</h3><p>Elasticsearch Migration Helper是一个网站插件，可以帮助从<code>Elasticsearch 2.3.x / 2.4.x</code>迁移到Elasticsearch 5.0。它有三个工具：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">集群检查</div><div class="line">对集群，节点和索引执行一系列检查，并提醒您升级之前需要解决的任何已知问题。</div><div class="line"><span class="selector-tag">Reindex</span>助手</div><div class="line">在<span class="selector-tag">v2</span><span class="selector-class">.0</span><span class="selector-class">.0</span>之前创建的索引需要重新编号，才能在<span class="selector-tag">Elasticsearch</span> 5<span class="selector-class">.x</span>中使用。<span class="selector-tag">reindex</span>帮助器点击一个按钮升级旧索引。</div><div class="line">弃用日志</div><div class="line"><span class="selector-tag">Elasticsearch</span>附带了一个弃用记录器，每当使用不推荐使用的功能时，它将记录消息。此工具可启用或禁用群集上的弃用日志记录。</div></pre></td></tr></table></figure>
<p>官网推荐迁移文档：Instruction for <a href="(https://github.com/elastic/elasticsearch-migration/blob/2.x/README.asciidoc">install the Elasticsearch migration helper. </a>)</p>
<h3 id="Elasticsearch-5-0-0-安装需要哪些要求。"><a href="#Elasticsearch-5-0-0-安装需要哪些要求。" class="headerlink" title="Elasticsearch 5.0.0 安装需要哪些要求。"></a>Elasticsearch 5.0.0 安装需要哪些要求。</h3><p>Elasticsearch需要依赖Java JDK1.8</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/07/04/Bigdata-hadoop/Kafka/kafka性能优化–JVM参数配置优化/">Kafka性能优化–JVM参数配置优化</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-07-04</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h3 id="Kafka-Broker个数决定因素"><a href="#Kafka-Broker个数决定因素" class="headerlink" title="Kafka Broker个数决定因素"></a>Kafka Broker个数决定因素</h3><p>　　磁盘容量：首先考虑的是所需保存的消息所占用的总磁盘容量和每个broker所能提供的磁盘空间。如果Kafka集群需要保留 10 TB数据，单个broker能存储 2 TB，那么我们需要的最小Kafka集群大小5个broker。此外，如果启用副本参数，则对应的存储空间需至少增加一倍（取决于副本参数）。这意味着对应的Kafka集群至少需要 10 个broker。</p>
<p>　　请求量：另外一个要考虑的是Kafka集群处理请求的能力。这主要取决于对Kafka client请求的网络处理能力，特别是，有多个consumer或者网路流量不稳定。如果，高峰时刻，单个broker的网络流量达到80%，这时是撑不住两个consumer的，除非有两个broker。再者，如果启用了副本参数，则需要考虑副本这个额外的consumer。也可以扩展多个broker来减少磁盘的吞吐量和系统内存。</p>
<h3 id="Kafka集群稳定"><a href="#Kafka集群稳定" class="headerlink" title="Kafka集群稳定"></a>Kafka集群稳定</h3><p>GC调优<br>　　调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent，具体参数设置可以参考Google，这里不赘述。</p>
<p>　　Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent 分别设置为 20毫秒和 35。Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入。</p>
<h3 id="主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。"><a href="#主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。" class="headerlink" title="主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。"></a>主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。</h3><h3 id="1、JVM参数配置优化"><a href="#1、JVM参数配置优化" class="headerlink" title="1、JVM参数配置优化"></a>1、JVM参数配置优化</h3><p>如果使用的CMS GC算法，建议JVM Heap不要太大，在4GB以内就可以。JVM太大，导致Major GC或者Full GC产生的“stop the world”时间过长，导致broker和zk之间的session超时，比如重新选举controller节点和提升follow replica为leader replica。<br>JVM也不能过小，否则会导致频繁地触发gc操作，也影响Kafka的吞吐量。另外，需要避免CMS GC过程中的发生promotion failure和concurrent failure问题。CMSInitiatingOccupancyFraction=70可以预防concurrent failure问题，提前出发Major GC。<br>Kafka JVM参数可以直接修改启动脚本<code>bin/kafka-server-start.sh</code><br>中的变量值。下面是一些基本参数，也可以根据实际的gc状况和调试GC需要增加一些相关的参数。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx4G -Xms4G -Xmn2G -XX:PermSize=64m -XX:MaxPermSize=128m  -XX:SurvivorRatio=6  -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly"</span></div></pre></td></tr></table></figure>
<p>需要关注gc日志中的YGC时间以及CMS GC里面的CMS-initial-mark和CMS-remark两个阶段的时间，这些GC过程是“stop the world”方式完成的。</p>
<blockquote>
<p>jdk1.8 优化的话会提示MaxPermSize=128m,PermSize=64m 字面意思是MaxPermSize不需要我们配置了,jdk1.8 版本功能其实已不需要这个优化参数：</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka3 kafka_2.10-0.8.2.1]$ /data/tools/kafka_2.10-0.8.2.1/bin/kafka-server-start.sh /data/tools/kafka_2.10-0.8.2.1/config/server.properties &amp;</div><div class="line">[1] 10312</div><div class="line">[jollybi@kafka3 kafka_2.10-0.8.2.1]$ Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=64m; support was removed <span class="keyword">in</span> 8.0</div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed <span class="keyword">in</span> 8.0</div><div class="line"></div><div class="line">优化参数：</div><div class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx4G -Xms4G -Xmn2G  -XX:SurvivorRatio=6  -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly"</span></div><div class="line"><span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span>  <span class="comment">### Kafka Manager监控监听jmx端口,如果没有可不设置。</span></div></pre></td></tr></table></figure>
<h3 id="2、打开JMX端口"><a href="#2、打开JMX端口" class="headerlink" title="2、打开JMX端口"></a>2、打开JMX端口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">主要是为了通过JMX端口监控Kafka Broker信息。可以在bin/kafka-server-start.sh中打开JMX端口变量。</div><div class="line"><span class="built_in">export</span> JMX_PORT=9999</div></pre></td></tr></table></figure>
<h3 id="3、调整log4j的日志级别"><a href="#3、调整log4j的日志级别" class="headerlink" title="3、调整log4j的日志级别"></a>3、调整log4j的日志级别</h3><p>如果集群中topic和partition数量较大时，因为log4j的日志级别太低，导致进程持续很长的时间在打印日志。日志量巨大，导致很多额外的性能开销。特别是contoller日志级别为trace级别，这点比较坑。<br>Tips 通过JMX端口设置log4j日志级别，不用重启broker节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">设置日志级别：</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController <span class="built_in">set</span>LogLevel=kafka.controller,INFO</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController <span class="built_in">set</span>LogLevel=state.change.logger,INFO</div><div class="line"> </div><div class="line">检查日志级别：</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController getLogLevel=kafka.controller</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController getLogLevel=state.change.logger</div></pre></td></tr></table></figure>
<h3 id="4、性能优化技巧"><a href="#4、性能优化技巧" class="headerlink" title="4、性能优化技巧"></a>4、性能优化技巧</h3><p>4.1、配置合适的partitons数量。</p>
<p>这似乎是kafka新手必问得问题。首先，我们必须理解，partiton是kafka的并行单元。从producer和broker的视角看，向不同的partition写入是完全并行的；而对于consumer，并发数完全取决于partition的数量，即，如果consumer数量大于partition数量，则必有consumer闲置。所以，我们可以认为kafka的吞吐与partition时线性关系。partition的数量要根据吞吐来推断，假定p代表生产者写入单个partition的最大吞吐，c代表消费者从单个partition消费的最大吞吐，我们的目标吞吐是t，那么partition的数量应该是t/p和t/c中较大的那一个。实际情况中，p的影响因素有批处理的规模，压缩算法，确认机制和副本数等，然而，多次benchmark的结果表明，单个partition的最大写入吞吐在10MB/sec左右；c的影响因素是逻辑算法，需要在不同场景下实测得出。</p>
<p>这个结论似乎太书生气和不实用。我们通常建议partition的数量一定要大于等于消费者的数量来实现最大并发。官方曾测试过1万个partition的情况，所以不需要太担心partition过多的问题。下面的知识会有助于读者在生产环境做出最佳的选择：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">a、一个partition就是一个存储kafka-log的目录。</div><div class="line">b、一个partition只能寄宿在一个broker上。</div><div class="line">c、单个partition是可以实现消息的顺序写入的。</div><div class="line">d、单个partition只能被单个消费者进程消费，与该消费者所属于的消费组无关。这样做，有助于实现顺序消费。</div><div class="line">e、单个消费者进程可同时消费多个partition，即partition限制了消费端的并发能力。</div><div class="line">f、partition越多则file和memory消耗越大，要在服务器承受服务器设置。</div><div class="line">g、每个partition信息都存在所有的zk节点中。</div><div class="line">h、partition越多则失败选举耗时越长。</div><div class="line">k、offset是对每个partition而言的，partition越多，查询offset就越耗时。</div><div class="line">i、partition的数量是可以动态增加的（只能加不能减）。</div></pre></td></tr></table></figure>
<p>我们建议的做法是，如果是3个broker的集群，有5个消费者，那么建议partition的数量是15，也就是broker和consumer数量的最小公倍数。当然，也可以是一个大于消费者的broker数量的倍数，比如6或者9，还请读者自行根据实际环境裁定。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/30/Bigdata-hadoop/Kafka/Kafka日志存储解析与实践数据存储优化/">Kafka日志存储解析与实践数据存储优化</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-30</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h1 id="Kafka日志存储解析与实践数据存储优化"><a href="#Kafka日志存储解析与实践数据存储优化" class="headerlink" title="Kafka日志存储解析与实践数据存储优化"></a>Kafka日志存储解析与实践数据存储优化</h1><p>kafka是一款分布式消息发布和订阅的系统，具有高性能和高吞吐率。 </p>
<h4 id="Kafka的名词解释"><a href="#Kafka的名词解释" class="headerlink" title="Kafka的名词解释"></a>Kafka的名词解释</h4><ul>
<li>1，<code>Broker</code>： 一个单独的kafka机器节点就称为一个broker，多个broker组成的集群，称为kafka集群</li>
<li>2，<code>Topic</code>：类似数据库中的一个表，我们将数据存储在Topic里面，当然这只是逻辑上的，在物理上，一个Topic 可能被多个Broker分区存储，这对用户是透明的，用户只需关注消息的产生于消费即可.</li>
<li>3，<code>Partition</code>：类似分区表，每个Topic可根据设置将数据存储在多个整体有序的Partition中，每个顺序化partition会生成2个文件，一个是index文件一个是log文件，index文件存储索引和偏移量，log文件存储具体的数据.</li>
<li>4，<code>Producer</code>：生产者，向Topic里面发送消息的角色 </li>
<li>5，<code>Consumer</code>：消费者，从Topic里面读取消息的角色 </li>
<li>6，<code>Consumer Group</code>：每个Consumer属于一个特定的消费者组，可为Consumer指定group name，如果不指定默认属于group </li>
</ul>
<p><strong>集群安装略过~</strong></p>
<h4 id="日志存储"><a href="#日志存储" class="headerlink" title="日志存储"></a>日志存储</h4><p>Kafka的data是保存在文件系统中的。Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。</p>
<p>每个topic又可以分成几个不同的partition，每个topic有几个partition是在创建topic时指定的，每个partition存储一部分Message。</p>
<p><code>partition</code>是以文件的形式存储在文件系统中，比如，创建了一个名为<code>kakfa-node1</code>的topic，其有12个partition，那么在Kafka的数据目录中(由配置文件中的log.dirs指定的)中就有这样5个目录: </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]$ ll</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-0</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-1</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-10</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-11</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-2</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-3</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-4</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-5</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-6</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-7</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-8</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-9</div></pre></td></tr></table></figure>
<p>其命名规则为<code>&lt;topic_name&gt;-&lt;partition_id&gt;</code>，里面存储的分别就是这12个<code>partition</code>的数据。<br><code>zookeeper</code>会将分区平均分配创建到不同的<code>broker</code>上，例如</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 tools]$ ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic kakfa-node1</div><div class="line">Topic:kakfa-node1	PartitionCount:12	ReplicationFactor:3	Configs:</div><div class="line">	Topic: kakfa-node1	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3</div><div class="line">	Topic: kakfa-node1	Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</div><div class="line">	Topic: kakfa-node1	Partition: 2	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2</div><div class="line">	Topic: kakfa-node1	Partition: 3	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2</div><div class="line">	Topic: kakfa-node1	Partition: 4	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3</div><div class="line">	Topic: kakfa-node1	Partition: 5	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</div><div class="line">	Topic: kakfa-node1	Partition: 6	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3</div><div class="line">	Topic: kakfa-node1	Partition: 7	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</div><div class="line">	Topic: kakfa-node1	Partition: 8	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2</div><div class="line">	Topic: kakfa-node1	Partition: 9	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2</div><div class="line">	Topic: kakfa-node1	Partition: 10	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3</div><div class="line">	Topic: kakfa-node1	Partition: 11	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</div></pre></td></tr></table></figure>
<p>Isr表示分区创建在哪个broker上。<br>Partition中的每条Message由offset来表示它在这个partition中的偏移量，这个offset不是该Message在partition数据文件中的实际存储位置，而是逻辑上一个值，它唯一确定了partition中的一条Message。因此，可以认为offset是partition中Message的id。partition中的每条Message包含了以下三个属性：</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="title">offset</span></div><div class="line"><span class="type">MessageSize</span></div><div class="line"><span class="class"><span class="keyword">data</span></span></div></pre></td></tr></table></figure>
<p>其中offset为long型，MessageSize为int32，表示data有多大，data为message的具体内容。</p>
<h4 id="Kafka通过分段和索引的方式来提高查询效率"><a href="#Kafka通过分段和索引的方式来提高查询效率" class="headerlink" title="Kafka通过分段和索引的方式来提高查询效率"></a>Kafka通过分段和索引的方式来提高查询效率</h4><ul>
<li>1）分段</li>
</ul>
<p>Kafka解决查询效率的手段之一是将数据文件分段，比如有100条Message，它们的offset是从0到99。假设将数据文件分成5段，第一段为0-19，第二段为20-39，以此类推，每段放在一个单独的数据文件里面，数据文件以该段中最小的offset命名。这样在查找指定offset的Message的时候，用二分查找就可以定位到该Message在哪个段中。</p>
<figure class="highlight tap"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]$ ll /data/tools/kafka_2.10-0.8.2.1/kafka-logs/</div><div class="line">total 548</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-0</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-1</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-10</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-11</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-2</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-3</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-4</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-5</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-6</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-7</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-8</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-9</div></pre></td></tr></table></figure>
<h5 id="2）为数据文件建索引"><a href="#2）为数据文件建索引" class="headerlink" title="2）为数据文件建索引"></a>2）为数据文件建索引</h5><p>数据文件分段使得可以在一个较小的数据文件中查找对应offset的Message了，但是这依然需要顺序扫描才能找到对应<code>offset的Message</code>。为了进一步提高查找的效率，Kafka为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩展名为.index。<br>索引文件中包含若干个索引条目，每个条目表示数据文件中一条Message的索引。索引包含两个部分（均为4个字节的数字），分别为相对<code>offset和position</code>。</p>
<p>相对<code>offset</code>：因为数据文件分段以后，每个数据文件的起始offset不为0，相对offset表示这条Message相对于其所属数据文件中最小的offset的大小。举例，分段后的一个数据文件的offset是从20开始，那么offset为25的Message在index文件中的相对offset就是25-20 = 5。存储相对offset可以减小索引文件占用的空间。</p>
<p><code>position</code>，表示该条<code>Message</code>在数据文件中的绝对位置。只要打开文件并移动文件指针到这个position就可以读取对应的<code>Message</code>了。<br>index文件中并没有为数据文件中的每条Message建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/Kafka-node模块实现调用js发送数据/">Bigdata-Kafka-node模块实现调用js发送数据</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<p>mongodb写到kafka 指定topic消费。为了保证数据稳定可靠性。<br>配合大数据在countly 使用开源<code>Kafka-node</code>是一个Node.js客户端 写js程序让countly三台集群分别数据到kafka 做新的topic主题备份。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install kafka-node</div></pre></td></tr></table></figure>
<p>进入kafka-node目录: vim kafka_test.js</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">var kafka = require(<span class="string">'kafka-node'</span>),</div><div class="line">HighLevelProducer = kafka.HighLevelProducer,</div><div class="line">    //Producer = kafka.Producer,</div><div class="line">    client = new kafka.Client(<span class="string">'169.44.62.139:2281,169.44.59.138:2281,169.44.62.137:2281'</span>),</div><div class="line">    //producer = new Producer(client);</div><div class="line">producer = new HighLevelProducer(client);</div><div class="line"></div><div class="line">console.log(<span class="string">'连接kafka中'</span>);</div><div class="line"></div><div class="line">var argv = &#123;</div><div class="line">    topic: <span class="string">"test1"</span></div><div class="line">&#125;;</div><div class="line">var topic = argv.topic || <span class="string">'test1'</span>;</div><div class="line">var p = argv.p || 0;</div><div class="line">var a = argv.a || 0;</div><div class="line">var producer = new HighLevelProducer(client, &#123;</div><div class="line">    requireAcks: 1,</div><div class="line">    partitionerType: 3</div><div class="line">&#125;);</div><div class="line"></div><div class="line">console.log(producer);</div><div class="line"></div><div class="line">producer.on(<span class="string">'ready'</span>, <span class="function"><span class="title">function</span></span>() &#123;</div><div class="line">    var args = &#123;</div><div class="line">        appid: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        createTime: <span class="string">'222-ddd'</span>,</div><div class="line">        toUserName: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        fromUserName: <span class="string">'222-wx238c28839a133d0e'</span></div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    producer.send([&#123;</div><div class="line">        topic: topic,</div><div class="line">        partition: p,</div><div class="line">        messages: [JSON.stringify(args)],</div><div class="line">        attributes: a</div><div class="line">    &#125;], <span class="keyword">function</span>(err, result) &#123;</div><div class="line">        console.log(err || result);</div><div class="line">        process.exit();</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    console.log(args);</div><div class="line">&#125;);</div></pre></td></tr></table></figure>
<h4 id="官网地址：https-www-npmjs-com-package-kafka-node-install-kafka"><a href="#官网地址：https-www-npmjs-com-package-kafka-node-install-kafka" class="headerlink" title="官网地址：https://www.npmjs.com/package/kafka-node#install-kafka"></a>官网地址：<a href="https://www.npmjs.com/package/kafka-node#install-kafka" target="_blank" rel="external">https://www.npmjs.com/package/kafka-node#install-kafka</a></h4>
	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/Bigdata-Kafka三款监控工具比较/">Bigdata-Kafka三款监控工具比较</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<p>在之前的博客中，介绍了<code>Kafka Web Console</code>这个监控工具，在生产环境中使用，运行一段时间后，发现该工具会和Kafka生产者、消费者、ZooKeeper建立大量连接，从而导致网络阻塞。并且这个Bug也在其他使用者中出现过，看来使用开源工具要慎重！该Bug暂未得到修复，不得已，只能研究下其他同类的Kafka监控软件。</p>
<p>通过研究，发现主流的三种kafka监控程序分别为：</p>
<ul>
<li>1、Kafka Web Conslole</li>
<li>2、Kafka Manager</li>
<li>3、KafkaOffsetMonitor</li>
</ul>
<p>现在依次介绍以上三种工具：</p>
<h2 id="1、Kafka-Web-Conslole"><a href="#1、Kafka-Web-Conslole" class="headerlink" title="1、Kafka Web Conslole"></a>1、Kafka Web Conslole</h2><p>使用Kafka Web Console，可以监控：</p>
<ul>
<li>Brokers列表</li>
<li>Kafka 集群中 Topic列表，及对应的Partition、LogSiz e等信息</li>
<li>点击Topic，可以浏览对应的Consumer Groups、Offset、Lag等信息</li>
<li>生产和消费流量图、消息预览…</li>
</ul>
<p>程序运行后，会定时去读取kafka集群分区的日志长度，读取完毕后，连接没有正常释放，一段时间后产生大量的socket连接，导致网络堵塞。</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Kafka_web_console.png" alt=""></figure></p>
<h2 id="2、Kafka-Manager"><a href="#2、Kafka-Manager" class="headerlink" title="2、Kafka Manager"></a>2、Kafka Manager</h2><p>雅虎开源的Kafka集群管理工具:</p>
<ul>
<li>管理几个不同的集群</li>
<li>监控集群的状态(topics, brokers, 副本分布, 分区分布)</li>
<li>产生分区分配(Generate partition assignments)基于集群的当前状态</li>
<li>重新分配分区</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka_manager.png" alt=""></figure></p>
<h2 id="3、KafkaOffsetMonitor"><a href="#3、KafkaOffsetMonitor" class="headerlink" title="3、KafkaOffsetMonitor"></a>3、KafkaOffsetMonitor</h2><ul>
<li>KafkaOffsetMonitor可以实时监控：</li>
<li>Kafka集群状态</li>
<li>Topic、Consumer Group列表</li>
<li>图形化展示topic和consumer之间的关系</li>
<li>图形化展示consumer的Offset、Lag等信息</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafkaoffsetmonitor.png" alt=""></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过使用，个人总结以上三种监控程序的优缺点：</p>
<p><a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">Kafka Web Console</a>：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。</p>
<p><a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="external">Kafka Manager</a>：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。</p>
<p><a href="https://github.com/quantifind/KafkaOffsetMonitor" target="_blank" rel="external">KafkaOffsetMonitor</a>：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。</p>
<p>若只需要监控功能，推荐使用KafkaOffsetMonito，若偏重Kafka集群管理，推荐使用Kafka Manager。</p>
<p>因为都是开源程序，稳定性欠缺。故需先了解清楚目前已存在哪些Bug，多测试一下，避免出现类似于Kafka Web Console的问题。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/countly/Bigdata-countly需要迁移/">Bigdata-countly需要迁移</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Countly/">Countly</a>
			</span>
		
	</div>

	

	
		<h4 id="官方安装文档：http-resources-count-ly-docs-installing-countly-server"><a href="#官方安装文档：http-resources-count-ly-docs-installing-countly-server" class="headerlink" title="官方安装文档：http://resources.count.ly/docs/installing-countly-server"></a>官方安装文档：<a href="http://resources.count.ly/docs/installing-countly-server" target="_blank" rel="external">http://resources.count.ly/docs/installing-countly-server</a></h4><p>目前<code>countly</code>需要迁移，所需<code>countly</code>版本于官方提供的安装方案有冲突，所以如下安装：</p>
<p>安装官方<code>countly</code>让其设置所需环境变量及其启动脚本，手动指定安装nojs版本，拷贝原countly文件，具体如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1、<span class="built_in">cd</span> /data <span class="comment">#countly安装在data目录 我看了安装脚本，是当前在哪个目录，安装文件就在哪个目录</span></div><div class="line">wget -qO- http://c.ly/install | bash</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2、rpm -qa | grep -i nodejs | xargs -I&#123;&#125;  yum remove &#123;&#125; -y</div><div class="line">卸载掉官网安装的最新nodejs 然后新建如下yum源，用于安装旧版所需nodejs，也可以到nodejs官网下载所需nodejs</div><div class="line">cat /etc/yum.repos.d/nodesource-el.repo </div><div class="line">[nodesource]</div><div class="line">name=Node.js Packages <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span></div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/<span class="variable">$basearch</span></div><div class="line">failovermethod=priority</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">[nodesource-source]</div><div class="line">name=Node.js <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span> - Source</div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/SRPMS</div><div class="line">failovermethod=priority</div><div class="line">enabled=0</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">gpgcheck=1</div></pre></td></tr></table></figure>
<h3 id="安装老版本所需nodejs"><a href="#安装老版本所需nodejs" class="headerlink" title="安装老版本所需nodejs"></a>安装老版本所需nodejs</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum <span class="keyword">install</span> nodejs</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">3、拷贝源countly文件到/data/countly目录</div><div class="line">修改 /data/countly/api/config.js 和 /data/countly/frontend/express/config.js      </div><div class="line">3001端口和 6001端口监听地址换成 本地私有地址   <span class="comment">#源文件是监听的原来机器的内网地址，不修改的话，服务启动不了。</span></div></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="number">4</span><span class="string">、拷贝mongo数据目录到/data/mongo目录，修改mongo配置文件.</span></div><div class="line"></div><div class="line"><span class="string">cat</span> <span class="string">/etc/mongod.conf</span></div><div class="line"><span class="attr">systemLog:</span></div><div class="line"><span class="attr">       destination:</span> <span class="string">file</span></div><div class="line"><span class="attr">       logAppend:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       path:</span> <span class="string">/data/mongodb/mongod.log</span></div><div class="line"><span class="attr">storage:</span></div><div class="line"><span class="attr">       dbPath:</span> <span class="string">/data/mongo</span></div><div class="line"><span class="attr">       journal:</span></div><div class="line"><span class="attr">             enabled:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       engine:</span> <span class="string">mmapv1</span></div><div class="line"><span class="attr">processManagement:</span></div><div class="line"><span class="attr">       fork:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       pidFilePath:</span> <span class="string">/data/mongodb/mongod.pid</span></div><div class="line"><span class="attr">net:</span></div><div class="line"><span class="attr">       port:</span> <span class="number">27017</span></div><div class="line"><span class="attr">       bindIp:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></div><div class="line"><span class="attr">security:</span></div><div class="line"><span class="attr">       authorization:</span> <span class="string">enabled</span></div><div class="line"><span class="attr">operationProfiling:</span></div><div class="line"><span class="attr">       slowOpThresholdMs:</span> <span class="number">40960</span></div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Bigdatacountly01.jpeg" alt=""></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">5、修改硬盘block：</div><div class="line">     blockdev --setra 256 /dev/mapper/xvdc--vg-xvdc–lv    <span class="comment">##按照mongo提示操作</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">6、修改nginx配置文件  conf.d/default.conf</div><div class="line">      将127.0.0.1修改为本地私有地址</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">7、重启countly   mongodb  nginx</div><div class="line"></div><div class="line">countly restart</div><div class="line"></div><div class="line">/etc/init.d/mongod restart</div><div class="line"></div><div class="line">service nginx restart</div><div class="line">迁移完毕</div></pre></td></tr></table></figure>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/如何手动更新Kafka中某个Topic的偏移量/">Bigdata-如何手动更新Kafka中某个Topic的偏移量</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h1 id="如何手动更新Kafka中某个Topic的偏移量"><a href="#如何手动更新Kafka中某个Topic的偏移量" class="headerlink" title="如何手动更新Kafka中某个Topic的偏移量"></a>如何手动更新Kafka中某个Topic的偏移量</h1><p>　　我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如iteblog主题分区10的偏移量获取如下：<br>　　<br>　　在有些场景下，这个工具不满足我们的需求，我们需要的是能够手动设置分区的偏移量为任何有意义的值，而不仅仅是earliest或者latest。那咋办？</p>
<p>　　我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如<code>mongotail_lz4</code>主题分区10的偏移量获取如下：
　　</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 2]  get /consumers/ibm_event/offsets/mongotail_lz4/10</div><div class="line">293894</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x6000018c9</div><div class="line">mtime = Wed Jul 26 18:18:27 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 20</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 6</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure>
<p>所以，我们可以通过set命令来设置某个分区的偏移量，如下；</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECT get /consumers/ibm_event/offsets/mongotail_lz4/10 0 </div><div class="line">0</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x60000204c</div><div class="line">mtime = Wed Jul 26 18:37:21 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 21</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 1</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure>
<p>12个分区分别更新过去。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/28/Bigdata-hadoop/Kafka/Bigdata-开源的Kafka集群管理器(kafka-web-console)/">Bigdata-开源的Kafka集群管理器(kafka-web-console)</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-28</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>源码的地址在:<a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">kafka-web-console</a></p>
<p><code>Kafka Web Console</code>也是用Scala语言编写的<code>Java web</code>程序用于监控<code>Apache Kafka</code>。这个系统的功能和<code>KafkaOffsetMonitor</code>很类似，但是我们从源码角度来看，这款系统实现比<code>KafkaOffsetMonitor</code>要复杂很多，而且编译配置比<code>KafkaOffsetMonitor</code>较麻烦。</p>
<p>　要想运行这套系统我们需要的先行条件为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Play Framework 2.2.x</div><div class="line">Apache Kafka 0.8.x</div><div class="line">Zookeeper 3.3.3 or 3.3.4</div></pre></td></tr></table></figure>
<h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><p>同样，我们从<code>https://github.com/claudemamo/kafka-web-console</code>上面将源码下载下来，然后用<code>sbt</code>进行编译，在编译前我们需要做如下的修改：</p>
<p>Kafka Web控制台需要一个关系数据库。默认情况下，服务器连接到嵌入式H2数据库，不需要数据库安装或配置。请咨询Play！的文档以指定控制台的数据库。支持以下数据库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/claudemamo/kafka-web-console.git</div></pre></td></tr></table></figure>
<ul>
<li>H2（默认）</li>
<li>PostgreSql</li>
<li>Oracle</li>
<li>DB2</li>
<li>MySQL</li>
<li>Apache Derby</li>
<li>Microsoft SQL Server</li>
</ul>
<p>为了方便，我们可以使用Mysql数据库，只要做如下修改即可，找到 <code>conf/application.conf</code>文件，并修改如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">将这个</div><div class="line">db.default.driver=org.h2.Driver</div><div class="line">db.default.url=<span class="string">"jdbc:h2:file:play"</span></div><div class="line"><span class="comment"># db.default.user=sa</span></div><div class="line"><span class="comment"># db.default.password=""</span></div><div class="line"> </div><div class="line"> </div><div class="line">修改成</div><div class="line">db.default.driver=com.mysql.jdbc.Driver</div><div class="line">db.default.url=<span class="string">"jdbc:mysql://localhost:3306/kafkamonitor"</span></div><div class="line">db.default.user=iteblog</div><div class="line">db.default.pass=wyp</div></pre></td></tr></table></figure>
<p>我们还需要修改build.sbt，加入对Mysql的依赖:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="string">"mysql"</span> % <span class="string">"mysql-connector-java"</span> % <span class="string">"5.1.31"</span></div></pre></td></tr></table></figure>
<p>　2、执行<code>conf/evolutions/default/bak</code>目录下面的<code>1.sql、2.sql和3.sql</code>三个文件。需要注意的是，这三个sql文件不能直接运行，有语法错误，需要做一些修改。<br>上面的注意事项弄完之后，我们就可以编译下载过来的源码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> sbt package</span></div></pre></td></tr></table></figure>
<p>编译的过程比较慢，有些依赖包下载速度非常地慢，请耐心等待。<br>　在编译的过程中，可能会出现有些依赖包无法下载，如下错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">[warn] module not found: com.typesafe.play<span class="comment">#sbt-plugin;2.2.1</span></div><div class="line">[warn] ==== typesafe-ivy-releases: tried</div><div class="line">[warn] http://repo.typesafe.com/typesafe/ivy-releases/</div><div class="line">com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== sbt-plugin-releases: tried</div><div class="line">[warn] http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/</div><div class="line">com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== <span class="built_in">local</span>: tried</div><div class="line">[warn] /home/iteblog/.ivy2/<span class="built_in">local</span>/com.typesafe.play/</div><div class="line">sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== Typesafe repository: tried</div><div class="line">[warn] http://repo.typesafe.com/typesafe/releases/com/</div><div class="line">typesafe/play/sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom</div><div class="line">[warn] ==== public: tried</div><div class="line">[warn] http://repo1.maven.org/maven2/com/typesafe/play/</div><div class="line">sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom</div><div class="line">[warn] ::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">==== <span class="built_in">local</span>: tried</div><div class="line"> </div><div class="line">/home/iteblog/.ivy2/<span class="built_in">local</span>/org.scala-sbt/collections/0.13.0/jars/collections.jar</div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">:: FAILED DOWNLOADS ::</div><div class="line"> </div><div class="line">:: ^ see resolution messages <span class="keyword">for</span> details ^ ::</div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">:: org.scala-sbt<span class="comment">#collections;0.13.0!collections.jar</span></div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div></pre></td></tr></table></figure>
<p>我们可以手动地下载相关依赖，并放到类似<code>/home/iteblog/.ivy2/local/org.scala-sbt/collections/0.13.0/jars/</code>目录下面。然后再编译就可以了。</p>
<p>　　最后，我们可以通过下面命令启动<code>Kafka Web Console</code>监控系统：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> sbt run</span></div></pre></td></tr></table></figure>
<p>并可以在<a href="http://localhost:9000" target="_blank" rel="external">http://localhost:9000</a> 查看下面是一张效果图</p>
<p><figure class="figure"><img src="https://www.iteblog.com/pic/topics.png" alt=""></figure></p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/28/Bigdata-hadoop/Kafka/开源的Kafka集群管理器(Kafka Manager)/">Bigdata-开源的Kafka集群管理器(Kafka Manager)</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-28</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h2 id="Kafka-Manager"><a href="#Kafka-Manager" class="headerlink" title="Kafka Manager"></a>Kafka Manager</h2><p>A tool for managing <a href="http://kafka.apache.org/" target="_blank" rel="external">Apache Kafka.</a></p>
<h4 id="It-supports-the-following"><a href="#It-supports-the-following" class="headerlink" title="It supports the following :"></a>It supports the following :</h4><ul>
<li>管理多个群集</li>
<li>容易检查集群状态（主题，消费者，偏移量，经纪人，副本分发，分区分配）</li>
<li>运行首选副本选举</li>
<li>使用选项生成分区分配，以选择要使用的代理</li>
<li>运行分区的重新分配（基于生成的分配）</li>
<li>创建可选主题配置的主题（0.8.1.1具有不同于0.8.2+的配置）</li>
<li>删除主题（仅支持0.8.2+，并记住在代理配 置中设置delete.topic.enable = true）</li>
<li>主题列表现在表示标记为删除的主题（仅支持0.8.2+）</li>
<li>批量生成多个主题的分区分配，并选择要使用的代理</li>
<li>批量运行多个主题的分区重新分配</li>
<li>将分区添加到现有主题</li>
<li>更新现有主题的配置</li>
<li>可选地，启用JMX轮询代理级和主题级度量。</li>
<li>可选地筛选出在zookeeper中没有ids / owner /＆offset /目录的消费者。</li>
</ul>
<p>参考开源地址：<a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="external">https://github.com/yahoo/kafka-manager</a></p>
<h4 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h4><p>Kafka 0.8.1.1 or 0.8.2.<em> or 0.9.0.</em> or 0.10.0.*<br>Java 8+</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo wget --header <span class="string">"Cookie: oraclelicense=accept-securebackup-cookie”   http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz</span></div><div class="line"></div><div class="line">sudo vim /etc/profile</div><div class="line">export JAVA_HOME=/home/jollybi/tools/jdk1.8.0_144</div><div class="line">export LASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib:<span class="variable">$JAVA_HOME</span>/bin</div><div class="line">export PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JAVA_HOME</span>/jre/bin:<span class="variable">$TOMCAT_HOME</span>/bin:<span class="variable">$PATH</span></div></pre></td></tr></table></figure>
<h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/yahoo/kafka-manager.git</div><div class="line">./sbt clean dist</div><div class="line"></div><div class="line">[info]   Compilation completed <span class="keyword">in</span> 13.366 s</div><div class="line">model contains 672 documentable templates</div><div class="line">[info] Main Scala API documentation successful.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-javadoc.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-sans-externalized.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info]</div><div class="line">[info] Your package is ready <span class="keyword">in</span> /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip</div><div class="line">[info]</div><div class="line">[success] Total time: 142 s, completed Jul 27, 2017 3:48:35 PM</div><div class="line">完成</div></pre></td></tr></table></figure>
<h4 id="Starting-the-service"><a href="#Starting-the-service" class="headerlink" title="Starting the service"></a>Starting the service</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">解压缩生成的zip文件后，将工作目录更改为可以运行的服务：</div><div class="line"></div><div class="line">unzip /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip</div><div class="line"></div><div class="line"></div><div class="line">修改zk地址和管理员账号和密码：</div><div class="line"></div><div class="line">vim kafka-manager-1.3.3.8/conf/application.conf</div><div class="line"></div><div class="line"><span class="comment">#kafka-manager.zkhosts="kafka-manager-zookeeper:2181"</span></div><div class="line"><span class="comment">#zk集群可以这么配置：</span></div><div class="line">kafka-manager.zkhosts=<span class="string">"kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#根据个人公司这里可以开启true 设置账号和密码</span></div><div class="line">basicAuthentication.enabled=<span class="literal">true</span></div><div class="line">basicAuthentication.username=<span class="string">"admin"</span></div><div class="line">basicAuthentication.password=<span class="string">"admin"</span></div><div class="line"></div><div class="line"></div><div class="line">默认情况下，它将选择端口9000.这是可以覆盖的，配置文件的位置也是如此。例如：</div><div class="line"></div><div class="line">$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080</div><div class="line"></div><div class="line">后台生效：</div><div class="line"></div><div class="line">$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;</div><div class="line"></div><div class="line">再次，如果java不在您的路径中，或者您需要针对不同版本的Java运行，请按如下所示添加-java-home选项：</div><div class="line"></div><div class="line">$ bin/kafka-manager -java-home /usr/<span class="built_in">local</span>/oracle-java-8</div></pre></td></tr></table></figure>
<h4 id="Packaging"><a href="#Packaging" class="headerlink" title="Packaging"></a>Packaging</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">If you<span class="string">'d like to create a Debian or RPM package instead, you can run one of:</span></div><div class="line"></div><div class="line">sbt debian:packageBin</div><div class="line"></div><div class="line">sbt rpm:packageBin</div></pre></td></tr></table></figure>
<h3 id="查看端口："><a href="#查看端口：" class="headerlink" title="查看端口："></a>查看端口：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">[jollybi@kafka1 conf]$ netstat -ntulp | grep 8080</div><div class="line">(Not all processes could be identified, non-owned process info</div><div class="line"> will not be shown, you would have to be root to see it all.)</div><div class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      70517/java</div></pre></td></tr></table></figure>
<h3 id="网站访问kafka-Manger"><a href="#网站访问kafka-Manger" class="headerlink" title="网站访问kafka Manger"></a>网站访问kafka Manger</h3><p>这里我设置了登录账号和密码： admin admin</p>
<p><figure class="figure"><img src="media/15014722977191.jpg" alt=""></figure></p>
<p>创建kafka名字;<br>选择kafka版本号;<br>JMX这个不需要;<br>下面选择默认点击确认即可.</p>
<p><figure class="figure"><img src="media/15014724051756.jpg" alt=""></figure></p>
<blockquote>
<p>(2)kafka 启用 JMX端口</p>
</blockquote>
<p><figure class="figure"><img src="media/15020729297106.jpg" alt=""></figure></p>
<figure class="highlight vbscript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">以如下命令重新启动kafka</div><div class="line"></div><div class="line">JMX_PORT=<span class="number">9999</span> bin/kafka-<span class="built_in">server</span>-start.sh config/<span class="built_in">server</span>.properties</div><div class="line">或者修改kafka-<span class="built_in">server</span>-start.sh 文件，追加JMX_PORT=<span class="string">"9999"</span></div><div class="line"></div><div class="line"> <span class="keyword">if</span> [ <span class="string">"x$KAFKA_HEAP_OPTS"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></div><div class="line">    export KAFKA_HEAP_OPTS=<span class="string">"-Xmx1G -Xms1G"</span></div><div class="line">    export JMX_PORT=<span class="string">"9999"</span></div><div class="line">fi</div><div class="line">然后重新启动kafka</div><div class="line">bin/kafka-<span class="built_in">server</span>-start.sh config/<span class="built_in">server</span>.properties</div><div class="line"></div><div class="line">但是Metrics中数据都是零</div><div class="line">查看 kafka manager 报错，无法连接jxm</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">解决方法 修改每个kafka broker的 kafka_2.11-0.10.1.0/bin/kafka-run-class.sh文件</div><div class="line">​</div><div class="line"><span class="comment"># JMX settings</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$KAFKA_JMX_OPTS</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  KAFKA_JMX_OPTS=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=75.126.5.162"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"></div><div class="line">-Djava.rmi.server.hostname 的值为当前kafka服务器ip</div><div class="line"></div><div class="line">这里说明下集群kafka都需要修改</div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="media/15020745879851.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725443386.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725288425.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725659340.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725864134.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014726361909.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014727525618.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014727932029.jpg" alt=""></figure></p>

	

	

</article>





	<span class="different-posts">📖 <a href="/page/2">more posts</a> 📖</span>



	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<h5>Hi there,</h5>
	<p style="text-align:justify;">my name is Yancy, they know me as Radiant🐑🐑. This blog is about Bi-data and my live.<br> I like 🌳🌳🌳</p>
</div>


	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2017 Yancy | Powered by <a href="http://blog.yancy.cc/">Yancy</a> | Theme <a href="https://github.com/yangcvo">Yancy_GitHub</a></span>
		</div>

	</div>


</footer>



</body>

</html>
