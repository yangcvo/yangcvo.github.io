<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Yancy&#39;s blog</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Yancy&#39;s blog" type="application/atom+xml">
    
</head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/" class="header__link">Home</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/archives" class="header__link">ARCHIVES</a>
			
				<a href="/About" class="header__link">ABOUT</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Yancy&#39;s blog</a></h1>
		<h2 class="header__subtitle">SIMPLICITY IS PREREQUISITE FOR  RELIABILITY</h2>
	</header>

	<main>
		



	<article>
	
		<h1><a href="/2017/08/11/日志分析平台/Elasticsearch/ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述/">ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-08-11</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/日志分析平台/">日志分析平台</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Elasticsearch/">Elasticsearch</a> <a class="article__tag-link" href="/tags/Kibana/">Kibana</a> <a class="article__tag-link" href="/tags/Log-analysis-platform/">Log analysis platform</a>
			</span>
		
	</div>

	

	
		<h1 id="ElasticStack-5-x-Kibana-5-5-x-Logstash版本部署概述"><a href="#ElasticStack-5-x-Kibana-5-5-x-Logstash版本部署概述" class="headerlink" title="ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述"></a>ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述</h1><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p><code>ELK=ElasticSearch + Logstash + Kibana</code></p>
<p>Elastic Stack是ELK日志系统的官方称呼，而ELK则是盛名在外的一款开源分布式日志系统，一般说来包括了Elasticsearch、Logstash和Kibana，涵盖了后端日志采集、日志搜索服务和前端数据展示等功能。<br>本文将会对Elastic Stack的安装部署流程进行一系列简单的介绍，并记录下了一些部署过程中遇到的坑及解决方法。</p>
<p>对于一个软件或互联网公司来说，对计算资源和应用进行监控和告警是非常基础的需求。对于大公司或成熟公司，一个高度定制化的监控系统应该已经存在了很长时间并且非常成熟了。而对于一个初创公司或小公司来说，如何利用现有开源工具快速搭建一套日志监控及分析平台是需要探索的事情。</p>
<h3 id="监控系统的用户："><a href="#监控系统的用户：" class="headerlink" title="监控系统的用户："></a>监控系统的用户：</h3><ul>
<li>运维，开发，产品</li>
</ul>
<p>监控系统应该可以解决如下的问题：</p>
<ul>
<li>监控server的各项基础指标，比如<code>memory,cpu,load,network</code>等</li>
<li>监控应用的状态。</li>
<li>搜集应用日志，并进行分析和统计。通过日志分析和统计可得到应用的访问统计，异常统计，业务统计。具有进行大规模日志数据的分析和处理能力。</li>
<li>可制定告警规则。各种监控数据进入系统后，可以根据条件触发告警，实时的将应用异常情况推送到运维、开发或业务人员的IM/SMS上。</li>
<li>可定制的看板。可以将各种实时统计或报表直观的显示出来。</li>
</ul>
<h3 id="可选方案："><a href="#可选方案：" class="headerlink" title="可选方案："></a>可选方案：</h3><p>日志宝，日志易，Logtail(阿里云) 这是我们后面换 <code>Graylog</code>这个是一批黑马 也分享出来。</p>
<p>优势：使用简单<br>劣势：需上传日志到外部，不灵活，不易扩展，需付费</p>
<p>flume-ng + kafka + spark streaming + hbase(es/mysql) + zepplin/自研web展示</p>
<p>优势：<code>灵活，易于扩展，数据分析和处理能力强</code><br>劣势：<code>开发难度高，周期长，维护成本高</code></p>
<h3 id="ELK优势和劣势："><a href="#ELK优势和劣势：" class="headerlink" title="ELK优势和劣势："></a>ELK优势和劣势：</h3><p>优势：<code>开源成熟解决方案，使用简单，扩展能力强</code><br>劣势：<code>日志分析和处理依靠logstash完成，处理能力较低，无法适应复杂的日志分析场景</code><br>结论：<code>初步选择ELK搭建起监控平台，其能够满足当前较为简单的监控和分析需求。未来如果不能适应，再考虑其他方案。</code></p>
<p>在本次实践中，我们所部署的ELK分布式日志系统，其架构大致如下：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/elasticstack_011.png" alt=""></figure><br>首先在各日志产生机上部署收集器<code>Filebeat</code>，然后Filebeat将监控到的log文件变化数据传至Kafka集群，<code>Logstash</code>负责将数据从<code>kafka</code>中拉取下来，并进行字段解析，向<code>Elasticsearch</code>输出结构化后的日志，<code>Kibana负责将Elasticsearch</code>中的数据进行可视化。</p>
<p>【重点参考】：<a href="http://kibana.logstash.es/content/" target="_blank" rel="external">ELK中文书</a></p>
<h3 id="一、Elasticsearch的部署"><a href="#一、Elasticsearch的部署" class="headerlink" title="一、Elasticsearch的部署"></a>一、Elasticsearch的部署</h3><p>首先在<code>https://www.elastic.co</code>中找到ES的安装包。下文中所用的安装包均为Linux 64的<code>tar.gz</code>压缩包，解压即可用。官网安装方法:<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html" target="_blank" rel="external">Installation example with tar</a></p>
<p><code>Elasticsearch</code>至少需要<code>Java 8</code>。在撰写本文时，建议您使用<code>Oracle JDK</code>版本<code>1.8.0_131</code>。Java安装因平台而异，所以我们在这里不再赘述。Oracle的推荐安装文档可以在Oracle的网站上找到。在安装Elasticsearch之前，请先检查您的Java版本，然后再运行（如果需要，请相应地进行安装/升级）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">java -version</div><div class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></div><div class="line"></div><div class="line">curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gz</div><div class="line">tar -xvf elasticsearch-5.5.2.tar.gz</div><div class="line"><span class="built_in">cd</span> elasticsearch-5.5.2/bin</div><div class="line">./elasticsearch <span class="_">-d</span> 后台启动</div><div class="line"></div><div class="line">这里我切换普通es用户启动，记得给予权限。</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@logstash ~]<span class="comment"># curl http://localhost:9200?pretty</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"name"</span> : <span class="string">"s-28M-e"</span>,</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elasticsearch"</span>,</div><div class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"0U3blQviRcSG_pq8KFQ5EA"</span>,</div><div class="line">  <span class="string">"version"</span> : &#123;</div><div class="line">    <span class="string">"number"</span> : <span class="string">"5.5.2"</span>,</div><div class="line">    <span class="string">"build_hash"</span> : <span class="string">"b2f0c09"</span>,</div><div class="line">    <span class="string">"build_date"</span> : <span class="string">"2017-08-14T12:33:14.154Z"</span>,</div><div class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</div><div class="line">    <span class="string">"lucene_version"</span> : <span class="string">"6.6.0"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="1-1-Elasticsearch的配置"><a href="#1-1-Elasticsearch的配置" class="headerlink" title="1.1 Elasticsearch的配置"></a>1.1 Elasticsearch的配置</h3><p>ES的配置文件在解压根目录下的config文件夹中，其中elasticsearch.yml是主配置文件。<br>以基本可用作为部署目标，在该文件中仅需要设置几个重要参数：</p>
<ol>
<li><code>cluster.name、node.name</code>这两者顾名思义，作为集群和节点的标识符。</li>
<li><code>Paths部分下的path.data和path.logs</code>，表示ES的数据存放位置，前者为数据存储位置，后者为ES的log存储位置。请尽量放到剩余空间足够的地方，此外在进行调优时有一种方法是将数据放置到SSD上。</li>
<li><code>bootstrap.memory_lock: true</code>，设为true以确保ES拥有足够的JVM内存。</li>
<li><code>network.host: localhost和http.port</code>，在此处设置ES对外服务的IP地址与端口<br>设置完以上几项参数后，即可在ES根目录下使用命令./bin/elasticsearch启动ES进程。也有相应的后台启动方式，具体不赘述。</li>
</ol>
<p><strong>主要配置文件</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@logstash elasticsearch-5.5.2]$ cat config/elasticsearch.yml | grep -Pv <span class="string">"^$|^#"</span></div><div class="line">cluster.name: jolly-cluster</div><div class="line">node.name: es-jollychic-node1</div><div class="line">path.data: /data/elasticsearch/es-data</div><div class="line">path.logs: /data/elasticsearch/es-logs</div><div class="line">bootstrap.memory_lock: <span class="literal">true</span></div><div class="line">network.host: 10.11.10.26</div><div class="line">http.port: 9200</div></pre></td></tr></table></figure>
<p>es 配置完以后启动会出现报错：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">[2017-08-19T22:06:01,722][INFO ][o.e.n.Node               ] [es-jollychic-node1] initializing ...</div><div class="line">[2017-08-19T22:06:01,789][INFO ][o.e.e.NodeEnvironment    ] [es-jollychic-node1] using [1] data paths, mounts [[/data (/dev/mapper/sdb--vg-sdb--lv)]], net usable_space [190.2gb], net total_space [196.7gb], spins? [possibly], types [ext4]</div><div class="line">[2017-08-19T22:06:01,789][INFO ][o.e.e.NodeEnvironment    ] [es-jollychic-node1] heap size [1.9gb], compressed ordinary object pointers [<span class="literal">true</span>]</div><div class="line">[2017-08-19T22:06:01,790][INFO ][o.e.n.Node               ] [es-jollychic-node1] node name [es-jollychic-node1], node ID [yjFGVTcIRg2YQectzevnOA]</div><div class="line">[2017-08-19T22:06:01,790][INFO ][o.e.n.Node               ] [es-jollychic-node1] version[5.5.2], pid[13530], build[b2f0c09/2017-08-14T12:33:14.154Z], OS[Linux/3.10.0-514.26.2.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_144/25.144-b01]</div><div class="line">[2017-08-19T22:06:01,790][INFO ][o.e.n.Node               ] [es-jollychic-node1] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=<span class="literal">true</span>, -Dfile.encoding=UTF-8, -Djna.nosys=<span class="literal">true</span>, -Djdk.io.permissionsUseCanonicalPath=<span class="literal">true</span>, -Dio.netty.noUnsafe=<span class="literal">true</span>, -Dio.netty.noKeySetOptimization=<span class="literal">true</span>, -Dio.netty.recycler.maxCapacityPerThread=0, -D<span class="built_in">log</span>4j.shutdownHookEnabled=<span class="literal">false</span>, -D<span class="built_in">log</span>4j2.disable.jmx=<span class="literal">true</span>, -D<span class="built_in">log</span>4j.skipJansi=<span class="literal">true</span>, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/usr/<span class="built_in">local</span>/elasticsearch-5.5.2]</div><div class="line">[2017-08-19T22:06:02,583][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [aggs-matrix-stats]</div><div class="line">[2017-08-19T22:06:02,583][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [ingest-common]</div><div class="line">[2017-08-19T22:06:02,586][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [lang-expression]</div><div class="line">[2017-08-19T22:06:02,586][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [lang-groovy]</div><div class="line">[2017-08-19T22:06:02,586][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [lang-mustache]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [lang-painless]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [parent-join]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [percolator]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [reindex]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [transport-netty3]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [transport-netty4]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] no plugins loaded</div><div class="line">[2017-08-19T22:06:04,212][INFO ][o.e.d.DiscoveryModule    ] [es-jollychic-node1] using discovery <span class="built_in">type</span> [zen]</div><div class="line">[2017-08-19T22:06:04,635][INFO ][o.e.n.Node               ] [es-jollychic-node1] initialized</div><div class="line">[2017-08-19T22:06:04,635][INFO ][o.e.n.Node               ] [es-jollychic-node1] starting ...</div><div class="line">[2017-08-19T22:06:04,812][INFO ][o.e.t.TransportService   ] [es-jollychic-node1] publish_address &#123;10.11.10.26:9300&#125;, bound_addresses &#123;10.11.10.26:9300&#125;</div><div class="line">[2017-08-19T22:06:04,821][INFO ][o.e.b.BootstrapChecks    ] [es-jollychic-node1] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks</div><div class="line">[2017-08-19T22:06:04,823][ERROR][o.e.b.Bootstrap          ] [es-jollychic-node1] node validation exception</div><div class="line">[1] bootstrap checks failed</div><div class="line">[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</div><div class="line">[2017-08-19T22:06:04,825][INFO ][o.e.n.Node               ] [es-jollychic-node1] stopping ...</div><div class="line">[2017-08-19T22:06:04,840][INFO ][o.e.n.Node               ] [es-jollychic-node1] stopped</div><div class="line">[2017-08-19T22:06:04,840][INFO ][o.e.n.Node               ] [es-jollychic-node1] closing ...</div><div class="line">[2017-08-19T22:06:04,851][INFO ][o.e.n.Node               ] [es-jollychic-node1] closed</div></pre></td></tr></table></figure>
<ul>
<li>如果修改完会报错，修改下Linux系统参数。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">max file descriptors [4096] <span class="keyword">for</span> elasticsearch process likely too low, increase to at least [65536]</div><div class="line">max number of threads [1024] <span class="keyword">for</span> user [lishang] likely too low, increase to at least [2048]</div><div class="line">解决：切换到root用户，编辑limits.conf 添加类似如下内容</div><div class="line">vi /etc/security/limits.conf </div><div class="line"></div><div class="line">添加如下内容:</div><div class="line"></div><div class="line">* soft nofile 65536</div><div class="line">* hard nofile 131072</div><div class="line">* soft nproc 2048</div><div class="line">* hard nproc 4096</div><div class="line"></div><div class="line">问题三：max number of threads [1024] <span class="keyword">for</span> user [lish] likely too low, increase to at least [2048]</div><div class="line"></div><div class="line">解决：切换到root用户，进入limits.d目录下修改配置文件。</div><div class="line">vi /etc/security/limits.d/90-nproc.conf </div><div class="line">修改如下内容：</div><div class="line"></div><div class="line">* soft nproc 1024</div><div class="line"><span class="comment">#修改为</span></div><div class="line">* soft nproc 2048</div><div class="line"></div><div class="line">问题四：max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]</div><div class="line"></div><div class="line">解决：切换到root用户修改配置sysctl.conf</div><div class="line"></div><div class="line">vi /etc/sysctl.conf </div><div class="line">添加下面配置：</div><div class="line">vm.max_map_count=655360</div><div class="line">并执行命令：</div><div class="line">sysctl -p</div><div class="line">然后，重新启动elasticsearch，即可启动成功。</div></pre></td></tr></table></figure>
<h3 id="集群健康检查-Cluster-Health"><a href="#集群健康检查-Cluster-Health" class="headerlink" title="集群健康检查 Cluster Health"></a>集群健康检查 Cluster Health</h3><p>要检查群集的运行状况，我们将使用<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.5/cat.html" target="_blank" rel="external">_catAPI</a>。您可以在<a href="https://www.elastic.co/guide/en/kibana/5.5/console-kibana.html" target="_blank" rel="external">Kibana</a>的控制台中运行以下命令，方法是 单击“查看控制台”或curl单击下面的“复制为CURL”链接并将其粘贴到终端中。</p>
<p><strong>创建索引：</strong><br>第一个命令使用PUT创建了一个叫做“customer”的索引。我们简单地将pretty附加到调用的尾部，使其以美观的形式打印出JSON响应 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -XPUT <span class="string">'http://10.11.10.26:9200/customer?pretty'</span></div></pre></td></tr></table></figure>
<p>我们也可以得到我们集群中节点的列表如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@logstash ~]<span class="comment">#  curl -XGET http://10.11.10.26:9200/_cat/nodes?v</span></div><div class="line">ip          heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name</div><div class="line">10.11.10.26           14          96  18    0.02    0.60     0.58 mdi       *      es-jollychic-node1</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@logstash ~]<span class="comment">#  curl -XGET http://10.11.10.26:9200/_cat/master?help</span></div><div class="line">id   |   | node id</div><div class="line">host | h | host name</div><div class="line">ip   |   | ip address</div><div class="line">node | n | node name</div></pre></td></tr></table></figure>
<p>查看所有连接的索引：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@logstash ~]$ curl <span class="_">-s</span>XGET <span class="string">"http://<span class="variable">$(hostname)</span>:9200/_cat/indices?v"</span></div><div class="line">health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-web-product-2017.08.25          LIKJyNXrRcmQSt8aXRoyLg   5   1        614            0    361.6kb        361.6kb</div><div class="line">yellow open   <span class="built_in">log</span>-payment-center-product-2017.08.26     w-ucGJUoTDOdhEkfrq9g-g   5   1    1712756            0    808.4mb        808.4mb</div><div class="line">yellow open   .kibana                                   p9MnvKHRS6K_jCeBtB7CXA   1   1         18            1     77.8kb         77.8kb</div><div class="line">yellow open   <span class="built_in">log</span>-jcm-product-2017.08.25                Cs0XHa-xRQOHxt1-HYz1Xw   5   1   46303455            0      8.1gb          8.1gb</div><div class="line">yellow open   <span class="built_in">log</span>-spm_mq-product-2017.08.25             FXYNV08zQselbnVKK7k01g   5   1     465127            0    148.1mb        148.1mb</div><div class="line">yellow open   <span class="built_in">log</span>-erpsearchservice-product-2017.08.26   Z4ECtHCJS2uma_UTxJqDLw   5   1       2217            0     15.3mb         15.3mb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-web-product-2017.08.24          mYAhjM5UQFyIZAN8nFeYQQ   5   1     951572            0    237.5mb        237.5mb</div><div class="line">yellow open   <span class="built_in">log</span>-spm_mq-product-2017.08.26             -PJCMMVqRwWXux8V5SRbCA   5   1    1457469            0    503.1mb        503.1mb</div><div class="line">yellow open   <span class="built_in">log</span>-jcm-product-2017.08.26                Jp7jQdciSMyrDmc82Y8YPQ   5   1   12753864            0      1.8gb          1.8gb</div><div class="line">yellow open   <span class="built_in">log</span>-gp-export-product-2017.08.26          tsFNVopYRXCM4vrCfU7IbQ   5   1     809526            0    169.8mb        169.8mb</div><div class="line">yellow open   <span class="built_in">log</span>-mq-product-2017.08.25                 dxXemS5RR_OpohepAUNcfg   5   1   32670880            0      5.6gb          5.6gb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-pda-product-2017.08.26          nE6s79S4RN2qIFkUt-FJsg   5   1       2323            0      1.6mb          1.6mb</div><div class="line">yellow open   <span class="built_in">log</span>-erpsearchservice-product-2017.08.25   Mj3-2ZrBRSuJaZQebPqcWA   5   1        369            0    390.6kb        390.6kb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-web-product-2017.08.26          X3GUYOZtTW6g1eGV_YW0HQ   5   1     275155            0     65.9mb         65.9mb</div><div class="line">yellow open   <span class="built_in">log</span>-mq-product-2017.08.26                 PwkQ9eOcQl2ie0f2fpnVRg   5   1   12519174            0      2.5gb          2.5gb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31_mq_consumer-product-2017.08.26  iG3bCUybSamJC9l58RlbNw   5   1      25703            0     13.1mb         13.1mb</div><div class="line">yellow open   <span class="built_in">log</span>-spm-product-2017.08.25                fr2fFfTKTOSl8Pm7qfU7eA   5   1        438            0    436.8kb        436.8kb</div><div class="line">yellow open   <span class="built_in">log</span>-dataexporttask-product-2017.08.26     01lRzmfYTDaFj9nXvTnmEg   5   1    1283861            0    145.2mb        145.2mb</div><div class="line">yellow open   <span class="built_in">log</span>-spm-task-scheduler-product-2017.08.26 7We8iXEHSYmk168PO6zUVA   5   1     463587            0    108.9mb        108.9mb</div><div class="line">yellow open   <span class="built_in">log</span>-wms-product-2017.08.25                XvyfywXmTYybccS1GLIkAw   5   1          1            0      8.1kb          8.1kb</div><div class="line">yellow open   <span class="built_in">log</span>-payment-center-product-2017.08.25     wUlvN1VhQ5Wb0pFgg1BkgA   5   1     691080            0    332.5mb        332.5mb</div><div class="line">yellow open   <span class="built_in">log</span>-dataexport-product-2017.08.26         SOh6vF8LQnOOWLsI9R_CcA   5   1     850019            0    499.5mb        499.5mb</div><div class="line">yellow open   <span class="built_in">log</span>-image-finder-product-2017.08.26       OGkYlpkzTPKReMsArJoyVA   5   1       9752            0      2.2mb          2.2mb</div><div class="line">yellow open   <span class="built_in">log</span>-gplatform_v2-product-2017.08.26       XEp5aM23SfaHfZ8qkgMjQw   5   1       8521            0      2.8mb          2.8mb</div><div class="line">yellow open   <span class="built_in">log</span>-payment-center-product-2017.08.24     4NDXMAQKRQqljZ1pcGp8yQ   5   1      70898            0       39mb           39mb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-pda-product-2017.08.25          caYLQMwlSZmBYl0YeZohpQ   5   1        237            0    214.8kb        214.8kb</div><div class="line">yellow open   <span class="built_in">log</span>-wms-product-2017.08.22                3NLXCSLyQJ2JHoCCeS41xQ   5   1          1            0      8.1kb          8.1kb</div></pre></td></tr></table></figure>
<p>删除指定索引：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -XDELETE <span class="string">"http://<span class="variable">$(hostname)</span>:9200/log-wms-product-2017.08.19"</span></div></pre></td></tr></table></figure>
<h3 id="1-2-Elasticsearch-5-x的Bootstrap-Checks"><a href="#1-2-Elasticsearch-5-x的Bootstrap-Checks" class="headerlink" title="1.2 Elasticsearch 5.x的Bootstrap Checks"></a>1.2 Elasticsearch 5.x的Bootstrap Checks</h3><p>Elasticsearch在升级到5.x版本后，启动时会强制执行<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html" target="_blank" rel="external">Bootstrap Checks</a>(官方文档)<br>其中经常性的问题是需要增大系统可使用的最大FileDescriptors数（参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html）" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html）</a><br>剩下的其他问题可以查询官方文档。</p>
<h3 id="1-3-Elasticsearch的X-pack插件"><a href="#1-3-Elasticsearch的X-pack插件" class="headerlink" title="1.3 Elasticsearch的X-pack插件"></a>1.3 Elasticsearch的X-pack插件</h3><p><code>X-pack</code>是官方提供的一系列集成插件，包括了<code>alert、monitor、secure</code>等功能，十分强大（但是并不免费）。<br>在ELK 5.0中安装大部分插件仅需要输入命令：<code>./bin/elasticsearch-plugin install &lt;plugin name&gt;</code>即可<br><code>X-pack</code>插件安装后会自动开启ELK的权限功能，需要注意的是如果启用了<code>X-pack</code>，则在向ES输入数据或发起API请求时，均需要附带相应的auth信息。<br>考虑到X-pack并非免费且价格昂贵，暂时不安装X-pack包。</p>
<h3 id="1-4-Elasticsearch的Head插件"><a href="#1-4-Elasticsearch的Head插件" class="headerlink" title="1.4 Elasticsearch的Head插件"></a>1.4 Elasticsearch的Head插件</h3><p><code>Head</code>插件作为<code>ELK 2.x</code>版本中较为通用的前端管理插件，在<code>ELK 5.x</code>版本中无法直接使用<code>./bin/elasticsearch-plugin install head</code>的方式安装，但是可以采取standalone的方式进行运行。</p>
<p>参考官方文档：<a href="https://github.com/mobz/elasticsearch-head#running-with-built-in-server" target="_blank" rel="external">elasticsearch-head</a></p>
<p>一篇较好的<a href="http://www.cnblogs.com/xing901022/p/6030296.html" target="_blank" rel="external">ES 5.x安装Head的博文：</a></p>
<p>【特别注意】：暂时没有找到x-pack和head相互兼容的方法，目前由于认证的问题，如果<code>启用了x-pack的secure</code>功能，会导致head插件无法连接ES集群。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> git://github.com/mobz/elasticsearch-head.git</div><div class="line"><span class="built_in">cd</span> elasticsearch-head</div><div class="line">npm install</div><div class="line">npm run start</div><div class="line">open http://localhost:9100/</div></pre></td></tr></table></figure>
<ul>
<li>修改head目录下的Gruntfile.js配置，head默认监听127.0.0.1 新增：hostname: ‘0.0.0.0’</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># vim Gruntfile.js</span></div><div class="line">                connect: &#123;</div><div class="line">                        server: &#123;</div><div class="line">                                options: &#123;</div><div class="line">                                        hostname: <span class="string">'0.0.0.0'</span>,</div><div class="line">                                        port: 9100,</div><div class="line">                                        base: <span class="string">'.'</span>,</div><div class="line">                                        keepalive: <span class="literal">true</span></div><div class="line">                                &#125;</div><div class="line">                        &#125;</div><div class="line">                &#125;</div><div class="line"></div><div class="line">        &#125;);</div></pre></td></tr></table></figure>
<ul>
<li>修改elasticsearch配置文件 elasticsearch.yml</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">http.cors.enabled: <span class="literal">true</span></div><div class="line">http.cors.allow-origin: <span class="string">"*"</span></div></pre></td></tr></table></figure>
<ul>
<li>重启elasticsearch，并启动node </li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ln <span class="_">-s</span> /usr/<span class="built_in">local</span>/elasticsearch-head/node_modules/grunt/bin/grunt /usr/bin/grunt</div><div class="line"></div><div class="line">后台启动：grunt server &amp;</div></pre></td></tr></table></figure>
<ul>
<li>访问效果：</li>
</ul>
<p><figure class="figure"><img src="media/15033045476000.jpg" alt=""></figure></p>
<h3 id="1-5-Elasticsearch-Kibana"><a href="#1-5-Elasticsearch-Kibana" class="headerlink" title="1.5 Elasticsearch-Kibana"></a>1.5 Elasticsearch-Kibana</h3><p>RPM方法安装参考官网文档：<a href="https://www.elastic.co/guide/en/kibana/current/rpm.html#rpm" target="_blank" rel="external">使用RPM 编辑安装Kibana</a></p>
<p>Running Kibana on Docker: <a href="https://www.elastic.co/guide/en/kibana/current/_pulling_the_image.html" target="_blank" rel="external">Docker 搭建方法</a></p>
<p>tar方式安装Kibana </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.2-linux-x86_64.tar.gz</div><div class="line">sha1sum kibana-5.5.2-linux-x86_64.tar.gz </div><div class="line">tar -xzf kibana-5.5.2-linux-x86_64.tar.gz</div><div class="line"><span class="built_in">cd</span> kibana/</div></pre></td></tr></table></figure>
<h3 id="通过配置文件编辑配置Kibana"><a href="#通过配置文件编辑配置Kibana" class="headerlink" title="通过配置文件编辑配置Kibana"></a>通过配置文件编辑配置Kibana</h3><p><code>kibana.yml</code>启动时Kibana服务器从文件读取属性。默认设置配置Kibana运行<code>localhost:5601</code>。要更改主机或端口号，或者连接到在其他机器上运行的<code>Elasticsearch</code>，您需要更新kibana.yml文件。您还可以启用SSL并设置各种其他选项。</p>
<p>kibana.yml 全部基本配置 ：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">server.port:</div><div class="line">默认值：5601 Kibana由后端服务器提供服务。此设置指定要使用的端口。</div><div class="line"></div><div class="line">server.host:</div><div class="line">默认值：“localhost”此设置指定后端服务器的主机。</div><div class="line"></div><div class="line">server.basePath:</div><div class="line">如果您在代理服务器后面运行，则允许您指定安装Kibana的路径。这仅影响由Kibana生成的URL，您的代理预期在将请求转发给Kibana之前删除basePath值。此设置无法以斜杠（/）结尾。</div><div class="line"></div><div class="line">server.maxPayloadBytes:</div><div class="line">默认值：1048576传入服务器请求的最大有效负载大小（以字节为单位）。</div><div class="line"></div><div class="line">server.name:</div><div class="line">默认值：“your-hostname”用于标识此Kibana实例的人性化显示名称。</div><div class="line"></div><div class="line">server.defaultRoute:</div><div class="line">默认值：“/ app / kibana”此设置指定打开Kibana时的默认路由。打开Kibana时，您可以使用此设置修改着陆页。</div><div class="line"></div><div class="line">elasticsearch.url:</div><div class="line">默认值：“http：// localhost：9200”用于所有查询的Elasticsearch实例的URL。</div><div class="line"></div><div class="line">elasticsearch.preserveHost:</div><div class="line">默认值：<span class="literal">true</span>当此设置的值为真时，Kibana使用server.host设置中指定的主机名。当此设置的值为<span class="literal">false</span>，Kibana使用连接到此Kibana实例的主机的主机名。</div><div class="line"></div><div class="line">kibana.index:</div><div class="line">默认：“.kibana” Kibana使用Elasticsearch中的索引来存储已保存的搜索，可视化和仪表板。如果索引不存在，Kibana将创建一个新的索引。</div><div class="line"></div><div class="line">kibana.defaultAppId:</div><div class="line">默认值：“discover”要加载的默认应用程序。</div><div class="line"></div><div class="line">tilemap.url:</div><div class="line">Kibana用于在tilemap可视化中显示地图图块的tile服务的URL。默认情况下，Kibana从外部元数据服务读取此URL，但用户仍然可以覆盖此参数以使用自己的Tile Map Service。例如：<span class="string">"https://tiles.elastic.co/v2/default/&#123;z&#125;/&#123;x&#125;/&#123;y&#125;.png?elastic_tile_service_tos=agree&amp;my_app_name=kibana"</span></div><div class="line"></div><div class="line">tilemap.options.minZoom:</div><div class="line">默认值：1最小缩放级别。</div><div class="line"></div><div class="line">tilemap.options.maxZoom:</div><div class="line">默认值：10最大缩放级别。</div><div class="line"></div><div class="line">tilemap.options.attribution:</div><div class="line">默认值：<span class="string">"© [Elastic Maps Service](https://www.elastic.co/elastic-maps-service)"</span>地图属性字符串。</div><div class="line"></div><div class="line">regionmap</div><div class="line">指定用于区域映射可视化的其他矢量图层。每个层对象都指向一个外部矢量文件，其中包含一个geojson FeatureCollection。该文件必须使用WGS84坐标参考系，并且只包括多边形。如果文件托管在与Kibana不同的域上，则服务器需要启用CORS，因此Kibana可以下载该文件。url字段也用作文件的唯一标识符。每个图层可以包含多个字段，以指示要公开的geojson要素的哪些属性。field.description是在Region Map可视化的字段菜单中显示的人类可读文本。也可以添加可选的归因值。以下示例显示有效的regionmap配置。</div><div class="line"></div><div class="line">regionmap：</div><div class="line">  层：</div><div class="line">     - 名称：“法国部”</div><div class="line">       url：“http://my.cors.enabled.server.org/france_departements.geojson”</div><div class="line">       归因：“INRAP”</div><div class="line">       字段：</div><div class="line">          - 名称：“部门”</div><div class="line">            说明：“全部名称”</div><div class="line">          - 名称：“INSEE”</div><div class="line">            说明：“INSEE数字标识符”</div><div class="line"></div><div class="line">elasticsearch.username: 和 elasticsearch.password:</div><div class="line">如果您的Elasticsearch受到基本身份验证的保护，这些设置将提供Kibana服务器在启动时对Kibana索引执行维护所用的用户名和密码。您的Kibana用户仍然需要使用通过Kibana服务器代理的Elasticsearch进行身份验证。</div><div class="line"></div><div class="line">server.ssl.enabled</div><div class="line">默认值：“<span class="literal">false</span>”启用从Kibana服务器到浏览器的传出请求的SSL。当设置为<span class="literal">true</span>，</div><div class="line">server.ssl.certificate并server.ssl.key要求</div><div class="line"></div><div class="line">server.ssl.certificate: 和 server.ssl.key:</div><div class="line">路由到PEM格式的SSL证书和SSL密钥文件。</div><div class="line"></div><div class="line">server.ssl.keyPassphrase</div><div class="line">将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。</div><div class="line"></div><div class="line">server.ssl.certificateAuthorities</div><div class="line">列出可信赖的PEM编码证书文件的路径。</div><div class="line"></div><div class="line">server.ssl.supportedProtocols</div><div class="line">默认值：TLSv1，TLSv1.1，TLSv1.2 支持的版本协议。有效协议：TLSv1，TLSv1.1，TLSv1.2</div><div class="line"></div><div class="line">server.ssl.cipherSuites</div><div class="line">默认值：ECDHE-RSA-AES128-GCM-SHA256，ECDHE-ECDSA-AES128-GCM-SHA256，ECDHE-RSA-AES256-GCM-SHA384，ECDHE-ECDSA-AES256-GCM-SHA384，DHE-RSA-AES128-GCM- SHA256，ECDHE-RSA-AES128-SHA256，DHE-RSA-AES128-SHA256，ECDHE-RSA-AES256-SHA384，DHE-RSA-AES256-SHA384，ECDHE-RSA-AES256-SHA256，DHE-RSA-AES256-SHA256， HIGH，！aNULL，！eNULL，！EXPORT，！DES，！RC4，！MD5，！PSK，！SRP，！CAMELLIA。格式和有效选项的详细信息可通过[OpenSSL密码列表格式文档]（ https://www.openssl.org/docs/man1.0.2/apps/ciphers.html<span class="comment">#CIPHER-LIST-FORMAT）获得。</span></div><div class="line"></div><div class="line"></div><div class="line">elasticsearch.ssl.certificate: 和 elasticsearch.ssl.key:</div><div class="line">提供PEM格式SSL证书和密钥文件路径的可选设置。这些文件验证您的Elasticsearch后端使用相同的密钥文件。</div><div class="line"></div><div class="line">elasticsearch.ssl.keyPassphrase</div><div class="line">将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。</div><div class="line"></div><div class="line">elasticsearch.ssl.certificateAuthorities:</div><div class="line">可选设置，使您能够指定Elasticsearch实例的证书颁发机构的PEM文件的路径列表。</div><div class="line"></div><div class="line">elasticsearch.ssl.verificationMode:</div><div class="line">默认值：full控制证书的验证。有效值是none，certificate和full。 full执行主机名验证，certificate不执行。</div><div class="line"></div><div class="line">elasticsearch.pingTimeout:</div><div class="line">默认值：elasticsearch.requestTimeout设置时间（以毫秒为单位）等待弹性搜索响应ping的值。</div><div class="line"></div><div class="line">elasticsearch.requestTimeout:</div><div class="line">默认值：30000等待后端或弹性搜索的响应的时间（毫秒）。该值必须为正整数。</div><div class="line"></div><div class="line">elasticsearch.requestHeadersWhitelist:</div><div class="line">默认值：[ <span class="string">'authorization'</span> ]要发送到Elasticsearch的Kibana客户端标题列表。要发送没有客户端标题，请将此值设置为[]（空列表）。</div><div class="line"></div><div class="line">elasticsearch.customHeaders:</div><div class="line">默认值：&#123;&#125;要发送到Elasticsearch的标题名称和值。无论elasticsearch.requestHeadersWhitelist配置如何，客户端头都不能覆盖任何自定义头文件。</div><div class="line"></div><div class="line">elasticsearch.shardTimeout:</div><div class="line">默认值：0 Elasticsearch等待分片响应的时间（以毫秒为单位）。设置为0以禁用。</div><div class="line"></div><div class="line">elasticsearch.startupTimeout:</div><div class="line">默认值：5000重试之前等待Kibana启动时的弹性搜索的时间（以毫秒为单位）。</div><div class="line"></div><div class="line">pid.file:</div><div class="line">指定Kibana创建进程ID文件的路径。</div><div class="line"></div><div class="line">logging.dest:</div><div class="line">默认值：stdout允许您指定Kibana存储日志输出的文件。</div><div class="line"></div><div class="line">logging.silent:</div><div class="line">默认值：<span class="literal">false</span>将此设置的值设置<span class="literal">true</span>为禁止所有日志输出。</div><div class="line"></div><div class="line">logging.quiet:</div><div class="line">默认值：<span class="literal">false</span>将此设置的值设置<span class="literal">true</span>为禁止除错误消息之外的所有日志输出。</div><div class="line"></div><div class="line">logging.verbose</div><div class="line">默认值：<span class="literal">false</span>将此设置的值设置为<span class="literal">true</span>记录所有事件，包括系统使用情况信息和所有请求。</div><div class="line"></div><div class="line">ops.interval</div><div class="line">默认值：5000设置采样系统和进程性能指标的间隔（以毫秒为单位）。最小值为100。</div><div class="line"></div><div class="line">status.allowAnonymous</div><div class="line">默认值：<span class="literal">false</span>如果启用了身份验证，<span class="literal">true</span>请将其设置为允许未经身份验证的用户访问Kibana服务器状态API和状态页面。</div><div class="line"></div><div class="line">cpu.cgroup.path.override</div><div class="line">覆盖cgroup cpu路径，以不一致的方式安装 /proc/self/cgroup</div><div class="line"></div><div class="line">cpuacct.cgroup.path.override</div><div class="line">在与不一致的方式安装时，覆盖cgroup cpuacct路径 /proc/self/cgroup</div><div class="line"></div><div class="line">console.enabled</div><div class="line">默认值：<span class="literal">true</span>设置为<span class="literal">false</span>以禁用控制台。切换此操作将导致服务器在下次启动时重新生成资源，这可能会在页面开始投放之前造成延迟。</div><div class="line"></div><div class="line">elasticsearch.tribe.url:</div><div class="line">用于所有查询的Elasticsearch部落实例的可选URL。</div><div class="line"></div><div class="line">elasticsearch.tribe.username: 和 elasticsearch.tribe.password:</div><div class="line">如果您的Elasticsearch受到基本身份验证的保护，这些设置将提供Kibana服务器在启动时对Kibana索引执行维护所用的用户名和密码。您的Kibana用户仍然需要使用通过Kibana服务器代理的Elasticsearch进行身份验证。</div><div class="line"></div><div class="line">elasticsearch.tribe.ssl.certificate: 和 elasticsearch.tribe.ssl.key:</div><div class="line">提供PEM格式SSL证书和密钥文件路径的可选设置。这些文件验证您的Elasticsearch后端使用相同的密钥文件。</div><div class="line"></div><div class="line">elasticsearch.tribe.ssl.keyPassphrase</div><div class="line">将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。</div><div class="line"></div><div class="line">elasticsearch.tribe.ssl.certificateAuthorities:</div><div class="line">可选设置，使您能够为您的部落Elasticsearch实例的证书颁发机构指定PEM文件的路径。</div><div class="line"></div><div class="line">elasticsearch.tribe.ssl.verificationMode:</div><div class="line">默认值：full控制证书的验证。有效值是none，certificate和full。full执行主机名验证，</div><div class="line">certificate不执行。</div><div class="line"></div><div class="line">elasticsearch.tribe.pingTimeout:</div><div class="line">默认值：elasticsearch.tribe.requestTimeout设置时间（以毫秒为单位）等待弹性搜索响应ping的值。</div><div class="line"></div><div class="line">elasticsearch.tribe.requestTimeout:</div><div class="line">默认值：30000等待后端或弹性搜索的响应的时间（毫秒）。该值必须为正整数。</div><div class="line"></div><div class="line">elasticsearch.tribe.requestHeadersWhitelist:</div><div class="line">默认值：[ <span class="string">'authorization'</span> ]要发送到Elasticsearch的Kibana客户端标题列表。要发送没有客户端标题，请将此值设置为[]（空列表）。</div><div class="line"></div><div class="line">elasticsearch.tribe.customHeaders:</div><div class="line">默认值：&#123;&#125;要发送到Elasticsearch的标题名称和值。无论</div><div class="line">elasticsearch.tribe.requestHeadersWhitelist配置如何，客户端头都不能覆盖任何自定义头文件。</div></pre></td></tr></table></figure>
<p>根据不同的需求配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@logstash config]<span class="comment"># cat kibana.yml | grep -Pv "^#|^$"</span></div><div class="line">server.port: 5601</div><div class="line">server.host: <span class="string">"10.11.10.26"</span></div><div class="line">server.name: <span class="string">"jollychic-log"</span></div><div class="line">elasticsearch.url: <span class="string">"http://10.11.10.26:9200"</span></div><div class="line">elasticsearch.preserveHost: <span class="literal">true</span></div><div class="line">kibana.index: <span class="string">".kibana"</span></div></pre></td></tr></table></figure>
<p>配置好配置，启动服务：<code>./bin/kibana &amp; 后台启动</code></p>
<h3 id="1-6访问Kibana"><a href="#1-6访问Kibana" class="headerlink" title="1.6访问Kibana"></a>1.6访问Kibana</h3><ul>
<li>检查Kibana状态</li>
</ul>
<p>您可以通过导航到达Kibana服务器的状态页面localhost:5601/status。状态页面显示有关服务器资源使用情况的信息，并列出已安装的插件。</p>
<p><code>http://10.11.10.26:5601/status</code></p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/elasticstack_022.png" alt=""></figure></p>
<ul>
<li>配置索引模式<br>指定与一个或多个弹性搜索索引的名称相匹配的索引模式。默认情况下，Kibana猜测您正在使用由Logstash提供给Elasticsearch的数据。如果是这样，您可以使用默认值logstash-<em>作为索引模式。星号（</em>）匹配索引名称中的零个或多个字符。如果您的弹性搜索索引遵循其他命名约定，请输入适当的模式。“模式”也可以简单地是单个索引的名称。<br>选择包含要用于执行基于时间的比较的时间戳的索引字段。Kibana读取索引映射以列出包含时间戳的所有字段。如果您的索引没有基于时间的数据，请禁用索引包含基于时间的事件选项。</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/elasticstack_033.png" alt=""></figure></p>
<ul>
<li>选择索引查看日志</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/elasticstack_044.png" alt=""></figure></p>
<p>官网中在生产环境中使用Kibana 说的很详细：<a href="https://www.elastic.co/guide/en/kibana/current/production.html" target="_blank" rel="external">在生产环境中使用Kibana</a></p>
<p>上面是我logstash已经搭建配置好索引才能获取到。下面讲如何部署logstash</p>
<h2 id="二、Logstash的部署"><a href="#二、Logstash的部署" class="headerlink" title="二、Logstash的部署"></a>二、Logstash的部署</h2><p>与Elasticsearch类似，在官网下载压缩包后，解压即可用。</p>
<p>在非高级场景下，Logstash本身不需要进行太多的配置（配置文件在logstash根目录下的<code>./config/logstash.yml</code>），高级场景请参考官方文档。<br>logstash的启动命令为:<code>./bin/logstash -f &lt;pipeline_conf_file&gt; --config.reload.automatic</code>，其中-f指定了pipeline配置文件的位置<code>，--config.reload.automatic</code>指定了pipeline配置文件可以进行热加载。<br>本次我们使用Logstash作为日志解析模块（Logstash其实也可以作为日志采集器），重点需要配置pipeline的三大部分：<code>input、filter和output。pipeline</code>文件需要自己创建。</p>
<h3 id="Installing-Logstash"><a href="#Installing-Logstash" class="headerlink" title="Installing Logstash"></a>Installing Logstash</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">java version <span class="string">"1.8.0_65"</span></div><div class="line">Java(TM) SE Runtime Environment (build 1.8.0_65-b17)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)</div><div class="line"></div><div class="line">rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch</div><div class="line"></div><div class="line">vim /etc/yum.repos.d/logstash.repo</div><div class="line">[logstash-5.x]</div><div class="line">name=Elastic repository <span class="keyword">for</span> 5.x packages</div><div class="line">baseurl=https://artifacts.elastic.co/packages/5.x/yum</div><div class="line">gpgcheck=1</div><div class="line">gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch</div><div class="line">enabled=1</div><div class="line">autorefresh=1</div><div class="line"><span class="built_in">type</span>=rpm-md</div><div class="line"></div><div class="line">sudo yum install logstash</div></pre></td></tr></table></figure>
<p>首先，我们通过运行最基本的<code>Logstash</code>管道来测试您的<code>Logstash</code>安装。</p>
<p><code>Logstash</code>管道有两个必需的元素，<code>input</code>并且<code>output</code>，和一个可选的元素，<code>filter</code>。输入插件消耗来自源的数据，过滤器插件会按照您指定的方式修改数据，并且输出插件将数据写入到目的地。</p>
<p><figure class="figure"><img src="https://www.elastic.co/guide/en/logstash/current/static/images/basic_logstash_pipeline.png" alt=""></figure></p>
<p>要测试您的Logstash安装，运行最基本的Logstash管道。例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> logstash-5.5.2</div><div class="line">bin / logstash <span class="_">-e</span><span class="string">'input &#123;stdin &#123;&#125;&#125; output &#123;stdout &#123;&#125;&#125;'</span></div></pre></td></tr></table></figure>
<p>该<code>-e</code>标志使您能够直接从命令行指定配置。在命令行中指定配置可以快速测试配置，而无需在迭代之间编辑文件。示例中的流水线从标准输入端输入，<code>stdin并stdout</code>以结构化格式将该输入移动到标准输出 。</p>
<p>启动<code>Logstash</code>后，等到看到<code>“Pipeline main started”</code>，然后hello world在命令提示符下输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hello world</div><div class="line">2017-07-21T01:22:14.405+0000 0.0.0.0 hello world</div></pre></td></tr></table></figure>
<p>Logstash将时间戳和IP地址信息添加到消息中。通过在运行Logstash的shell中发出CTRL-D命令退出Logstash 。</p>
<p>logstash的配置片段：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">#character at the beginning of a line indicates a comment. Use</span></div><div class="line"><span class="comment"># comments to describe your configuration.</span></div><div class="line">input &#123;</div><div class="line">    udp &#123;</div><div class="line">        port =&gt; 25826</div><div class="line">        buffer_size =&gt; 1452</div><div class="line">        workers =&gt; 3          <span class="comment"># Default is 2</span></div><div class="line">        queue_size =&gt; 30000   <span class="comment"># Default is 2000</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">output &#123;</div><div class="line">  elasticsearch &#123;</div><div class="line">    hosts =&gt; [ <span class="string">"10.11.10.26:9200"</span> ]</div><div class="line">    user =&gt; logstash</div><div class="line">    password =&gt; logstash</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>☺待整理续写~~</strong></p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/07/30/日志分析平台/Elasticsearch/研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？/">研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-07-30</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/日志分析平台/">日志分析平台</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Elasticsearch/">Elasticsearch</a> <a class="article__tag-link" href="/tags/Log-analysis-platform/">Log analysis platform</a>
			</span>
		
	</div>

	

	
		<p>ES 现在普遍大公司都已经在使用，有的用来做数据存储，也有做性能监控，也有做日志收集，跟大数据结合做日志分析。<br>今天在家无聊研究官网学习了下Elasticsearch5.0.0 功能新增哪些，个人觉得ES也是现在开源日志分析平台比较火的，从14年开始陆续使用频率不断提升。<br>之前也有了解过Graylog  现在比ES 做数据存储分析 不错的是Graylog 比ES性能好。</p>
<p>废话不多说了哈哈</p>
<h3 id="Elasticsearch-5-0-0发布"><a href="#Elasticsearch-5-0-0发布" class="headerlink" title="Elasticsearch 5.0.0发布"></a>Elasticsearch 5.0.0发布</h3><p>2016年10月26日发布 Elasticsearch 5.0.0 今天去下载吧！你知道你想。</p>
<p><a href="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="external">下载Elasticsearch 5.0.0</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.0/release-notes-5.0.0.html" target="_blank" rel="external">Elasticsearch 5.0.0发行说明</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.0/release-notes-5.0.0.html" target="_blank" rel="external">弹性搜索5.0突破变化</a></p>
<p>Elasticsearch 5.0.0对大家有所帮助 这是有史以来最快，最安全，最具弹性，最简单易用的Elasticsearch版本，它具有一系列增强功能和新功能。</p>
<ul>
<li>索引性能</li>
<li>取食节点</li>
<li>无痛脚本</li>
<li>新的数据结构</li>
<li>搜索和聚合</li>
<li>用户友好</li>
<li>弹性</li>
<li>Java REST客户端</li>
<li>迁移助手</li>
</ul>
<blockquote>
<p>下面从官网上面翻译得来的：</p>
</blockquote>
<h3 id="索引性能"><a href="#索引性能" class="headerlink" title="索引性能"></a>索引性能</h3><p>由于包括更好的数字数据结构（参见新数据结构）的一些更改，索引吞吐量在5.0.0中有了显着改善，锁中的争用减少，阻止对同一文档的并发更新，以及减少事务日志时的锁定需求。异步translog fsyncing将特别有利于旋转磁盘的用户，而依赖Elasticsearch自动生成文档ID时，仅追加用例（基于思考时间的事件）具有很大的吞吐量提高。对实时文档GET的支持的内部变化意味着索引缓冲区可用的内存更多，垃圾收集时间少得多。</p>
<p>根据您的用例，您可能会看到在提高吞吐量的25％至80％之间。</p>
<h3 id="Ingest节点"><a href="#Ingest节点" class="headerlink" title="Ingest节点"></a>Ingest节点</h3><p>　　向Elasticsearch添加数据更简单了。Logstash是一个强大的工具，而一些较小的用户只需要过滤器，不需要它所提供的众多路由选项。因此，Elastic将一些最流行的Logstash过滤器（如grok、split）直接在Elasticsearch中实现为处理器。多个处理器可以组合成一个管道，在索引时应用到文档上。</p>
<h3 id="Painless脚本"><a href="#Painless脚本" class="headerlink" title="Painless脚本"></a>Painless脚本</h3><p>　　Elasticsearch中很多地方用到了脚本，而出于安全考虑，脚本在默认情况下是禁用的，这令人相当失望。为此，Elastic开发了一种新的脚本语言Painless。该语言更快、更安全，而且默认是启用的。不仅如此，它的执行速度是Groovy的4倍，而且正在变得更快。Painless已经成为默认脚本语言，而Groovy、Javascript和Python都遭到弃用。要了解有关这门新语言的更多信息，请点击这里。</p>
<h3 id="新数据结构"><a href="#新数据结构" class="headerlink" title="新数据结构"></a>新数据结构</h3><p>　　Lucene 6带来了一个新的Points 数据结构K-D树，用于存储数值型和地理位置字段，彻底改变了数值型值的索引和搜索方式。基准测试表明，Points将查询速度提升了36%，将索引速度提升了71%，而占用的磁盘和内存空间分别减少了66%和85%（参见“在5.0中搜索数值”）。</p>
<h4 id="搜索和聚合"><a href="#搜索和聚合" class="headerlink" title="搜索和聚合"></a>搜索和聚合</h4><p>　　借助即时聚合，Kibana图表生成速度显著提升。Elastic用一年的时间对搜索API进行了重构，Elasticsearch现在可以更巧妙地执行范围查询，只针对已经发生变化的索引重新计算聚合，而不是针对每个查询从头开始重新计算。在搜索方面，默认的相关性计算已经由TF/IDF换成了更先进的BM25。补全建议程序经过了完全重写，将已删除的文档也考虑了进来。</p>
<h3 id="更友好"><a href="#更友好" class="headerlink" title="更友好"></a>更友好</h3><p>　　Elasticsearch 5.0更安全、更易用。他们采用了“尽早提示”的方法。如果出现了问题，则新版本会及早给出提示。例如，Elasticsearch 5.0会严格验证设置。如果它不能识别某项设置的值，就会给出提示和建议。不仅如此，集群和索引设置现在可以通过null进行解除。此外，还有其他的一些改进，例如，rollover和shrink API启用了一种新的模式来管理基于时间的索引，引入新的cluster-allocation-explain API，简化索引创建。</p>
<h3 id="弹性"><a href="#弹性" class="headerlink" title="弹性"></a>弹性</h3><p>　　Elasticsearch分布式模型的每一部分都被分解、重构和简化，提升了可靠性。集群状态更新现在会等待集群中的所有节点确认。如果一个“复制片（replica shard）”被“主片（primary）”标记为失败，则主片会等待“主节点（master）”的响应。索引现在使用数据路径中的UUID，而不是索引名，避免了命名冲突。另外，Elasticsearch现在进行启动检查，确保系统配置没有问题。配置比较麻烦，但如果只是试用，开发人员也可以选择localhost-only模式，避免繁琐的配置。另外，新版本还增加了断路器及其他一些软限制，限制请求使用的内存大小，保护集群免受恶意用户攻击。<br>　　此外，该版本还提供了一个底层的Java REST/HTTP客户端，可以用于监听、日志记录、请求轮询、故障节点重试等。它使用Java 7，将依赖降到了最低，比Transport客户端的依赖冲突少。而在基准测试中，它的性能并不输于Transport客户端。不过，这是一个底层客户端，目前还没有提供任何查询构建器或辅助器。它的输入参数和输出结果都是JSON。<br>　　需要注意的是，该版本引入了许多破坏性更改，好在他们提供了一个迁移辅助插件，可以帮助开发人员从Elasticsearch 2.3.x/2.4.x迁移到Elasticsearch 5.0。如果是从更早的Elasticsearch版本向最新的5.0版本迁移，则请查阅升级文档。</p>
<h3 id="Java-REST客户端"><a href="#Java-REST客户端" class="headerlink" title="Java REST客户端"></a>Java REST客户端</h3><p>经过多年的等待，我们终于发布了一个低级的Java HTTP / REST客户端。它提供了一个简单的HTTP客户端，具有最少的依赖关系，可以处理嗅探，记录，循环请求，并重试节点故障。它使用的REST层历史上比Java API更稳定，这意味着它可以跨越升级使用，甚至可能在主要版本之间进行升级。它与Java 7一起工作，并且具有最小的依赖性，导致比传输客户端更少的依赖冲突。它只是HTTP，因此可以像所有其他HTTP客户端一样进行防火墙/代理。在我们的基准测试中，Java REST客户端 与Transport客户端的功能类似。</p>
<p>请注意，这是一个低级客户端。在这个阶段，我们不提供任何可以在IDE中自动完成的查询构建器或帮助器。它是JSON-in，JSON-out，由你来构建JSON。开发不会停止在这里 - 我们将添加一个API，它将帮助您构建查询并解析响应。您可以按照问题＃19055中的更改进行操作。</p>
<h3 id="迁移助手"><a href="#迁移助手" class="headerlink" title="迁移助手"></a>迁移助手</h3><p>Elasticsearch Migration Helper是一个网站插件，可以帮助从<code>Elasticsearch 2.3.x / 2.4.x</code>迁移到Elasticsearch 5.0。它有三个工具：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">集群检查</div><div class="line">对集群，节点和索引执行一系列检查，并提醒您升级之前需要解决的任何已知问题。</div><div class="line"><span class="selector-tag">Reindex</span>助手</div><div class="line">在<span class="selector-tag">v2</span><span class="selector-class">.0</span><span class="selector-class">.0</span>之前创建的索引需要重新编号，才能在<span class="selector-tag">Elasticsearch</span> 5<span class="selector-class">.x</span>中使用。<span class="selector-tag">reindex</span>帮助器点击一个按钮升级旧索引。</div><div class="line">弃用日志</div><div class="line"><span class="selector-tag">Elasticsearch</span>附带了一个弃用记录器，每当使用不推荐使用的功能时，它将记录消息。此工具可启用或禁用群集上的弃用日志记录。</div></pre></td></tr></table></figure>
<p>官网推荐迁移文档：Instruction for <a href="(https://github.com/elastic/elasticsearch-migration/blob/2.x/README.asciidoc">install the Elasticsearch migration helper. </a>)</p>
<h3 id="Elasticsearch-5-0-0-安装需要哪些要求。"><a href="#Elasticsearch-5-0-0-安装需要哪些要求。" class="headerlink" title="Elasticsearch 5.0.0 安装需要哪些要求。"></a>Elasticsearch 5.0.0 安装需要哪些要求。</h3><p>Elasticsearch需要依赖Java JDK1.8</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/07/04/Bigdata-hadoop/Kafka/kafka性能优化–JVM参数配置优化/">Kafka性能优化–JVM参数配置优化</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-07-04</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h3 id="Kafka-Broker个数决定因素"><a href="#Kafka-Broker个数决定因素" class="headerlink" title="Kafka Broker个数决定因素"></a>Kafka Broker个数决定因素</h3><p>　　磁盘容量：首先考虑的是所需保存的消息所占用的总磁盘容量和每个broker所能提供的磁盘空间。如果Kafka集群需要保留 10 TB数据，单个broker能存储 2 TB，那么我们需要的最小Kafka集群大小5个broker。此外，如果启用副本参数，则对应的存储空间需至少增加一倍（取决于副本参数）。这意味着对应的Kafka集群至少需要 10 个broker。</p>
<p>　　请求量：另外一个要考虑的是Kafka集群处理请求的能力。这主要取决于对Kafka client请求的网络处理能力，特别是，有多个consumer或者网路流量不稳定。如果，高峰时刻，单个broker的网络流量达到80%，这时是撑不住两个consumer的，除非有两个broker。再者，如果启用了副本参数，则需要考虑副本这个额外的consumer。也可以扩展多个broker来减少磁盘的吞吐量和系统内存。</p>
<h3 id="Kafka集群稳定"><a href="#Kafka集群稳定" class="headerlink" title="Kafka集群稳定"></a>Kafka集群稳定</h3><p>GC调优<br>　　调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent，具体参数设置可以参考Google，这里不赘述。</p>
<p>　　Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent 分别设置为 20毫秒和 35。Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入。</p>
<h3 id="主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。"><a href="#主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。" class="headerlink" title="主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。"></a>主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。</h3><h3 id="1、JVM参数配置优化"><a href="#1、JVM参数配置优化" class="headerlink" title="1、JVM参数配置优化"></a>1、JVM参数配置优化</h3><p>如果使用的CMS GC算法，建议JVM Heap不要太大，在4GB以内就可以。JVM太大，导致Major GC或者Full GC产生的“stop the world”时间过长，导致broker和zk之间的session超时，比如重新选举controller节点和提升follow replica为leader replica。<br>JVM也不能过小，否则会导致频繁地触发gc操作，也影响Kafka的吞吐量。另外，需要避免CMS GC过程中的发生promotion failure和concurrent failure问题。CMSInitiatingOccupancyFraction=70可以预防concurrent failure问题，提前出发Major GC。<br>Kafka JVM参数可以直接修改启动脚本<code>bin/kafka-server-start.sh</code><br>中的变量值。下面是一些基本参数，也可以根据实际的gc状况和调试GC需要增加一些相关的参数。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx4G -Xms4G -Xmn2G -XX:PermSize=64m -XX:MaxPermSize=128m  -XX:SurvivorRatio=6  -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly"</span></div></pre></td></tr></table></figure>
<p>需要关注gc日志中的YGC时间以及CMS GC里面的CMS-initial-mark和CMS-remark两个阶段的时间，这些GC过程是“stop the world”方式完成的。</p>
<blockquote>
<p>jdk1.8 优化的话会提示MaxPermSize=128m,PermSize=64m 字面意思是MaxPermSize不需要我们配置了,jdk1.8 版本功能其实已不需要这个优化参数：</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka3 kafka_2.10-0.8.2.1]$ /data/tools/kafka_2.10-0.8.2.1/bin/kafka-server-start.sh /data/tools/kafka_2.10-0.8.2.1/config/server.properties &amp;</div><div class="line">[1] 10312</div><div class="line">[jollybi@kafka3 kafka_2.10-0.8.2.1]$ Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=64m; support was removed <span class="keyword">in</span> 8.0</div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed <span class="keyword">in</span> 8.0</div><div class="line"></div><div class="line">优化参数：</div><div class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx4G -Xms4G -Xmn2G  -XX:SurvivorRatio=6  -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly"</span></div><div class="line"><span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span>  <span class="comment">### Kafka Manager监控监听jmx端口,如果没有可不设置。</span></div></pre></td></tr></table></figure>
<h3 id="2、打开JMX端口"><a href="#2、打开JMX端口" class="headerlink" title="2、打开JMX端口"></a>2、打开JMX端口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">主要是为了通过JMX端口监控Kafka Broker信息。可以在bin/kafka-server-start.sh中打开JMX端口变量。</div><div class="line"><span class="built_in">export</span> JMX_PORT=9999</div></pre></td></tr></table></figure>
<h3 id="3、调整log4j的日志级别"><a href="#3、调整log4j的日志级别" class="headerlink" title="3、调整log4j的日志级别"></a>3、调整log4j的日志级别</h3><p>如果集群中topic和partition数量较大时，因为log4j的日志级别太低，导致进程持续很长的时间在打印日志。日志量巨大，导致很多额外的性能开销。特别是contoller日志级别为trace级别，这点比较坑。<br>Tips 通过JMX端口设置log4j日志级别，不用重启broker节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">设置日志级别：</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController <span class="built_in">set</span>LogLevel=kafka.controller,INFO</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController <span class="built_in">set</span>LogLevel=state.change.logger,INFO</div><div class="line"> </div><div class="line">检查日志级别：</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController getLogLevel=kafka.controller</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController getLogLevel=state.change.logger</div></pre></td></tr></table></figure>
<h3 id="4、性能优化技巧"><a href="#4、性能优化技巧" class="headerlink" title="4、性能优化技巧"></a>4、性能优化技巧</h3><p>4.1、配置合适的partitons数量。</p>
<p>这似乎是kafka新手必问得问题。首先，我们必须理解，partiton是kafka的并行单元。从producer和broker的视角看，向不同的partition写入是完全并行的；而对于consumer，并发数完全取决于partition的数量，即，如果consumer数量大于partition数量，则必有consumer闲置。所以，我们可以认为kafka的吞吐与partition时线性关系。partition的数量要根据吞吐来推断，假定p代表生产者写入单个partition的最大吞吐，c代表消费者从单个partition消费的最大吞吐，我们的目标吞吐是t，那么partition的数量应该是t/p和t/c中较大的那一个。实际情况中，p的影响因素有批处理的规模，压缩算法，确认机制和副本数等，然而，多次benchmark的结果表明，单个partition的最大写入吞吐在10MB/sec左右；c的影响因素是逻辑算法，需要在不同场景下实测得出。</p>
<p>这个结论似乎太书生气和不实用。我们通常建议partition的数量一定要大于等于消费者的数量来实现最大并发。官方曾测试过1万个partition的情况，所以不需要太担心partition过多的问题。下面的知识会有助于读者在生产环境做出最佳的选择：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">a、一个partition就是一个存储kafka-log的目录。</div><div class="line">b、一个partition只能寄宿在一个broker上。</div><div class="line">c、单个partition是可以实现消息的顺序写入的。</div><div class="line">d、单个partition只能被单个消费者进程消费，与该消费者所属于的消费组无关。这样做，有助于实现顺序消费。</div><div class="line">e、单个消费者进程可同时消费多个partition，即partition限制了消费端的并发能力。</div><div class="line">f、partition越多则file和memory消耗越大，要在服务器承受服务器设置。</div><div class="line">g、每个partition信息都存在所有的zk节点中。</div><div class="line">h、partition越多则失败选举耗时越长。</div><div class="line">k、offset是对每个partition而言的，partition越多，查询offset就越耗时。</div><div class="line">i、partition的数量是可以动态增加的（只能加不能减）。</div></pre></td></tr></table></figure>
<p>我们建议的做法是，如果是3个broker的集群，有5个消费者，那么建议partition的数量是15，也就是broker和consumer数量的最小公倍数。当然，也可以是一个大于消费者的broker数量的倍数，比如6或者9，还请读者自行根据实际环境裁定。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/30/Bigdata-hadoop/Kafka/Kafka日志存储解析与实践数据存储优化/">Kafka日志存储解析与实践数据存储优化</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-30</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h1 id="Kafka日志存储解析与实践数据存储优化"><a href="#Kafka日志存储解析与实践数据存储优化" class="headerlink" title="Kafka日志存储解析与实践数据存储优化"></a>Kafka日志存储解析与实践数据存储优化</h1><p>kafka是一款分布式消息发布和订阅的系统，具有高性能和高吞吐率。 </p>
<h4 id="Kafka的名词解释"><a href="#Kafka的名词解释" class="headerlink" title="Kafka的名词解释"></a>Kafka的名词解释</h4><ul>
<li>1，<code>Broker</code>： 一个单独的kafka机器节点就称为一个broker，多个broker组成的集群，称为kafka集群</li>
<li>2，<code>Topic</code>：类似数据库中的一个表，我们将数据存储在Topic里面，当然这只是逻辑上的，在物理上，一个Topic 可能被多个Broker分区存储，这对用户是透明的，用户只需关注消息的产生于消费即可.</li>
<li>3，<code>Partition</code>：类似分区表，每个Topic可根据设置将数据存储在多个整体有序的Partition中，每个顺序化partition会生成2个文件，一个是index文件一个是log文件，index文件存储索引和偏移量，log文件存储具体的数据.</li>
<li>4，<code>Producer</code>：生产者，向Topic里面发送消息的角色 </li>
<li>5，<code>Consumer</code>：消费者，从Topic里面读取消息的角色 </li>
<li>6，<code>Consumer Group</code>：每个Consumer属于一个特定的消费者组，可为Consumer指定group name，如果不指定默认属于group </li>
</ul>
<p><strong>集群安装略过~</strong></p>
<h4 id="日志存储"><a href="#日志存储" class="headerlink" title="日志存储"></a>日志存储</h4><p>Kafka的data是保存在文件系统中的。Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。</p>
<p>每个topic又可以分成几个不同的partition，每个topic有几个partition是在创建topic时指定的，每个partition存储一部分Message。</p>
<p><code>partition</code>是以文件的形式存储在文件系统中，比如，创建了一个名为<code>kakfa-node1</code>的topic，其有12个partition，那么在Kafka的数据目录中(由配置文件中的log.dirs指定的)中就有这样5个目录: </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]$ ll</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-0</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-1</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-10</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-11</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-2</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-3</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-4</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-5</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-6</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-7</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-8</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-9</div></pre></td></tr></table></figure>
<p>其命名规则为<code>&lt;topic_name&gt;-&lt;partition_id&gt;</code>，里面存储的分别就是这12个<code>partition</code>的数据。<br><code>zookeeper</code>会将分区平均分配创建到不同的<code>broker</code>上，例如</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 tools]$ ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic kakfa-node1</div><div class="line">Topic:kakfa-node1	PartitionCount:12	ReplicationFactor:3	Configs:</div><div class="line">	Topic: kakfa-node1	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3</div><div class="line">	Topic: kakfa-node1	Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</div><div class="line">	Topic: kakfa-node1	Partition: 2	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2</div><div class="line">	Topic: kakfa-node1	Partition: 3	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2</div><div class="line">	Topic: kakfa-node1	Partition: 4	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3</div><div class="line">	Topic: kakfa-node1	Partition: 5	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</div><div class="line">	Topic: kakfa-node1	Partition: 6	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3</div><div class="line">	Topic: kakfa-node1	Partition: 7	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</div><div class="line">	Topic: kakfa-node1	Partition: 8	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2</div><div class="line">	Topic: kakfa-node1	Partition: 9	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2</div><div class="line">	Topic: kakfa-node1	Partition: 10	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3</div><div class="line">	Topic: kakfa-node1	Partition: 11	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</div></pre></td></tr></table></figure>
<p>Isr表示分区创建在哪个broker上。<br>Partition中的每条Message由offset来表示它在这个partition中的偏移量，这个offset不是该Message在partition数据文件中的实际存储位置，而是逻辑上一个值，它唯一确定了partition中的一条Message。因此，可以认为offset是partition中Message的id。partition中的每条Message包含了以下三个属性：</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="title">offset</span></div><div class="line"><span class="type">MessageSize</span></div><div class="line"><span class="class"><span class="keyword">data</span></span></div></pre></td></tr></table></figure>
<p>其中offset为long型，MessageSize为int32，表示data有多大，data为message的具体内容。</p>
<h4 id="Kafka通过分段和索引的方式来提高查询效率"><a href="#Kafka通过分段和索引的方式来提高查询效率" class="headerlink" title="Kafka通过分段和索引的方式来提高查询效率"></a>Kafka通过分段和索引的方式来提高查询效率</h4><ul>
<li>1）分段</li>
</ul>
<p>Kafka解决查询效率的手段之一是将数据文件分段，比如有100条Message，它们的offset是从0到99。假设将数据文件分成5段，第一段为0-19，第二段为20-39，以此类推，每段放在一个单独的数据文件里面，数据文件以该段中最小的offset命名。这样在查找指定offset的Message的时候，用二分查找就可以定位到该Message在哪个段中。</p>
<figure class="highlight tap"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]$ ll /data/tools/kafka_2.10-0.8.2.1/kafka-logs/</div><div class="line">total 548</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-0</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-1</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-10</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-11</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-2</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-3</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-4</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-5</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-6</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-7</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-8</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-9</div></pre></td></tr></table></figure>
<h5 id="2）为数据文件建索引"><a href="#2）为数据文件建索引" class="headerlink" title="2）为数据文件建索引"></a>2）为数据文件建索引</h5><p>数据文件分段使得可以在一个较小的数据文件中查找对应offset的Message了，但是这依然需要顺序扫描才能找到对应<code>offset的Message</code>。为了进一步提高查找的效率，Kafka为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩展名为.index。<br>索引文件中包含若干个索引条目，每个条目表示数据文件中一条Message的索引。索引包含两个部分（均为4个字节的数字），分别为相对<code>offset和position</code>。</p>
<p>相对<code>offset</code>：因为数据文件分段以后，每个数据文件的起始offset不为0，相对offset表示这条Message相对于其所属数据文件中最小的offset的大小。举例，分段后的一个数据文件的offset是从20开始，那么offset为25的Message在index文件中的相对offset就是25-20 = 5。存储相对offset可以减小索引文件占用的空间。</p>
<p><code>position</code>，表示该条<code>Message</code>在数据文件中的绝对位置。只要打开文件并移动文件指针到这个position就可以读取对应的<code>Message</code>了。<br>index文件中并没有为数据文件中的每条Message建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/如何手动更新Kafka中某个Topic的偏移量/">Bigdata-如何手动更新Kafka中某个Topic的偏移量</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h1 id="如何手动更新Kafka中某个Topic的偏移量"><a href="#如何手动更新Kafka中某个Topic的偏移量" class="headerlink" title="如何手动更新Kafka中某个Topic的偏移量"></a>如何手动更新Kafka中某个Topic的偏移量</h1><p>　　我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如iteblog主题分区10的偏移量获取如下：<br>　　<br>　　在有些场景下，这个工具不满足我们的需求，我们需要的是能够手动设置分区的偏移量为任何有意义的值，而不仅仅是earliest或者latest。那咋办？</p>
<p>　　我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如<code>mongotail_lz4</code>主题分区10的偏移量获取如下：
　　</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 2]  get /consumers/ibm_event/offsets/mongotail_lz4/10</div><div class="line">293894</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x6000018c9</div><div class="line">mtime = Wed Jul 26 18:18:27 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 20</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 6</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure>
<p>所以，我们可以通过set命令来设置某个分区的偏移量，如下；</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECT get /consumers/ibm_event/offsets/mongotail_lz4/10 0 </div><div class="line">0</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x60000204c</div><div class="line">mtime = Wed Jul 26 18:37:21 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 21</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 1</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure>
<p>12个分区分别更新过去。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/Kafka-node模块实现调用js发送数据/">Bigdata-Kafka-node模块实现调用js发送数据</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<p>mongodb写到kafka 指定topic消费。为了保证数据稳定可靠性。<br>配合大数据在countly 使用开源<code>Kafka-node</code>是一个Node.js客户端 写js程序让countly三台集群分别数据到kafka 做新的topic主题备份。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install kafka-node</div></pre></td></tr></table></figure>
<p>进入kafka-node目录: vim kafka_test.js</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">var kafka = require(<span class="string">'kafka-node'</span>),</div><div class="line">HighLevelProducer = kafka.HighLevelProducer,</div><div class="line">    //Producer = kafka.Producer,</div><div class="line">    client = new kafka.Client(<span class="string">'169.44.62.139:2281,169.44.59.138:2281,169.44.62.137:2281'</span>),</div><div class="line">    //producer = new Producer(client);</div><div class="line">producer = new HighLevelProducer(client);</div><div class="line"></div><div class="line">console.log(<span class="string">'连接kafka中'</span>);</div><div class="line"></div><div class="line">var argv = &#123;</div><div class="line">    topic: <span class="string">"test1"</span></div><div class="line">&#125;;</div><div class="line">var topic = argv.topic || <span class="string">'test1'</span>;</div><div class="line">var p = argv.p || 0;</div><div class="line">var a = argv.a || 0;</div><div class="line">var producer = new HighLevelProducer(client, &#123;</div><div class="line">    requireAcks: 1,</div><div class="line">    partitionerType: 3</div><div class="line">&#125;);</div><div class="line"></div><div class="line">console.log(producer);</div><div class="line"></div><div class="line">producer.on(<span class="string">'ready'</span>, <span class="function"><span class="title">function</span></span>() &#123;</div><div class="line">    var args = &#123;</div><div class="line">        appid: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        createTime: <span class="string">'222-ddd'</span>,</div><div class="line">        toUserName: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        fromUserName: <span class="string">'222-wx238c28839a133d0e'</span></div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    producer.send([&#123;</div><div class="line">        topic: topic,</div><div class="line">        partition: p,</div><div class="line">        messages: [JSON.stringify(args)],</div><div class="line">        attributes: a</div><div class="line">    &#125;], <span class="keyword">function</span>(err, result) &#123;</div><div class="line">        console.log(err || result);</div><div class="line">        process.exit();</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    console.log(args);</div><div class="line">&#125;);</div></pre></td></tr></table></figure>
<h4 id="官网地址：https-www-npmjs-com-package-kafka-node-install-kafka"><a href="#官网地址：https-www-npmjs-com-package-kafka-node-install-kafka" class="headerlink" title="官网地址：https://www.npmjs.com/package/kafka-node#install-kafka"></a>官网地址：<a href="https://www.npmjs.com/package/kafka-node#install-kafka" target="_blank" rel="external">https://www.npmjs.com/package/kafka-node#install-kafka</a></h4>
	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/Kafka/Bigdata-Kafka三款监控工具比较/">Bigdata-Kafka三款监控工具比较</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<p>在之前的博客中，介绍了<code>Kafka Web Console</code>这个监控工具，在生产环境中使用，运行一段时间后，发现该工具会和Kafka生产者、消费者、ZooKeeper建立大量连接，从而导致网络阻塞。并且这个Bug也在其他使用者中出现过，看来使用开源工具要慎重！该Bug暂未得到修复，不得已，只能研究下其他同类的Kafka监控软件。</p>
<p>通过研究，发现主流的三种kafka监控程序分别为：</p>
<ul>
<li>1、Kafka Web Conslole</li>
<li>2、Kafka Manager</li>
<li>3、KafkaOffsetMonitor</li>
</ul>
<p>现在依次介绍以上三种工具：</p>
<h2 id="1、Kafka-Web-Conslole"><a href="#1、Kafka-Web-Conslole" class="headerlink" title="1、Kafka Web Conslole"></a>1、Kafka Web Conslole</h2><p>使用Kafka Web Console，可以监控：</p>
<ul>
<li>Brokers列表</li>
<li>Kafka 集群中 Topic列表，及对应的Partition、LogSiz e等信息</li>
<li>点击Topic，可以浏览对应的Consumer Groups、Offset、Lag等信息</li>
<li>生产和消费流量图、消息预览…</li>
</ul>
<p>程序运行后，会定时去读取kafka集群分区的日志长度，读取完毕后，连接没有正常释放，一段时间后产生大量的socket连接，导致网络堵塞。</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Kafka_web_console.png" alt=""></figure></p>
<h2 id="2、Kafka-Manager"><a href="#2、Kafka-Manager" class="headerlink" title="2、Kafka Manager"></a>2、Kafka Manager</h2><p>雅虎开源的Kafka集群管理工具:</p>
<ul>
<li>管理几个不同的集群</li>
<li>监控集群的状态(topics, brokers, 副本分布, 分区分布)</li>
<li>产生分区分配(Generate partition assignments)基于集群的当前状态</li>
<li>重新分配分区</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka_manager.png" alt=""></figure></p>
<h2 id="3、KafkaOffsetMonitor"><a href="#3、KafkaOffsetMonitor" class="headerlink" title="3、KafkaOffsetMonitor"></a>3、KafkaOffsetMonitor</h2><ul>
<li>KafkaOffsetMonitor可以实时监控：</li>
<li>Kafka集群状态</li>
<li>Topic、Consumer Group列表</li>
<li>图形化展示topic和consumer之间的关系</li>
<li>图形化展示consumer的Offset、Lag等信息</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafkaoffsetmonitor.png" alt=""></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过使用，个人总结以上三种监控程序的优缺点：</p>
<p><a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">Kafka Web Console</a>：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。</p>
<p><a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="external">Kafka Manager</a>：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。</p>
<p><a href="https://github.com/quantifind/KafkaOffsetMonitor" target="_blank" rel="external">KafkaOffsetMonitor</a>：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。</p>
<p>若只需要监控功能，推荐使用KafkaOffsetMonito，若偏重Kafka集群管理，推荐使用Kafka Manager。</p>
<p>因为都是开源程序，稳定性欠缺。故需先了解清楚目前已存在哪些Bug，多测试一下，避免出现类似于Kafka Web Console的问题。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/29/Bigdata-hadoop/countly/Bigdata-countly需要迁移/">Bigdata-countly需要迁移</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Big-data-Hadoop/">Big data Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Countly/">Countly</a>
			</span>
		
	</div>

	

	
		<h4 id="官方安装文档：http-resources-count-ly-docs-installing-countly-server"><a href="#官方安装文档：http-resources-count-ly-docs-installing-countly-server" class="headerlink" title="官方安装文档：http://resources.count.ly/docs/installing-countly-server"></a>官方安装文档：<a href="http://resources.count.ly/docs/installing-countly-server" target="_blank" rel="external">http://resources.count.ly/docs/installing-countly-server</a></h4><p>目前<code>countly</code>需要迁移，所需<code>countly</code>版本于官方提供的安装方案有冲突，所以如下安装：</p>
<p>安装官方<code>countly</code>让其设置所需环境变量及其启动脚本，手动指定安装nojs版本，拷贝原countly文件，具体如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1、<span class="built_in">cd</span> /data <span class="comment">#countly安装在data目录 我看了安装脚本，是当前在哪个目录，安装文件就在哪个目录</span></div><div class="line">wget -qO- http://c.ly/install | bash</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2、rpm -qa | grep -i nodejs | xargs -I&#123;&#125;  yum remove &#123;&#125; -y</div><div class="line">卸载掉官网安装的最新nodejs 然后新建如下yum源，用于安装旧版所需nodejs，也可以到nodejs官网下载所需nodejs</div><div class="line">cat /etc/yum.repos.d/nodesource-el.repo </div><div class="line">[nodesource]</div><div class="line">name=Node.js Packages <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span></div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/<span class="variable">$basearch</span></div><div class="line">failovermethod=priority</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">[nodesource-source]</div><div class="line">name=Node.js <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span> - Source</div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/SRPMS</div><div class="line">failovermethod=priority</div><div class="line">enabled=0</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">gpgcheck=1</div></pre></td></tr></table></figure>
<h3 id="安装老版本所需nodejs"><a href="#安装老版本所需nodejs" class="headerlink" title="安装老版本所需nodejs"></a>安装老版本所需nodejs</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum <span class="keyword">install</span> nodejs</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">3、拷贝源countly文件到/data/countly目录</div><div class="line">修改 /data/countly/api/config.js 和 /data/countly/frontend/express/config.js      </div><div class="line">3001端口和 6001端口监听地址换成 本地私有地址   <span class="comment">#源文件是监听的原来机器的内网地址，不修改的话，服务启动不了。</span></div></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="number">4</span><span class="string">、拷贝mongo数据目录到/data/mongo目录，修改mongo配置文件.</span></div><div class="line"></div><div class="line"><span class="string">cat</span> <span class="string">/etc/mongod.conf</span></div><div class="line"><span class="attr">systemLog:</span></div><div class="line"><span class="attr">       destination:</span> <span class="string">file</span></div><div class="line"><span class="attr">       logAppend:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       path:</span> <span class="string">/data/mongodb/mongod.log</span></div><div class="line"><span class="attr">storage:</span></div><div class="line"><span class="attr">       dbPath:</span> <span class="string">/data/mongo</span></div><div class="line"><span class="attr">       journal:</span></div><div class="line"><span class="attr">             enabled:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       engine:</span> <span class="string">mmapv1</span></div><div class="line"><span class="attr">processManagement:</span></div><div class="line"><span class="attr">       fork:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       pidFilePath:</span> <span class="string">/data/mongodb/mongod.pid</span></div><div class="line"><span class="attr">net:</span></div><div class="line"><span class="attr">       port:</span> <span class="number">27017</span></div><div class="line"><span class="attr">       bindIp:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></div><div class="line"><span class="attr">security:</span></div><div class="line"><span class="attr">       authorization:</span> <span class="string">enabled</span></div><div class="line"><span class="attr">operationProfiling:</span></div><div class="line"><span class="attr">       slowOpThresholdMs:</span> <span class="number">40960</span></div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Bigdatacountly01.jpeg" alt=""></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">5、修改硬盘block：</div><div class="line">     blockdev --setra 256 /dev/mapper/xvdc--vg-xvdc–lv    <span class="comment">##按照mongo提示操作</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">6、修改nginx配置文件  conf.d/default.conf</div><div class="line">      将127.0.0.1修改为本地私有地址</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">7、重启countly   mongodb  nginx</div><div class="line"></div><div class="line">countly restart</div><div class="line"></div><div class="line">/etc/init.d/mongod restart</div><div class="line"></div><div class="line">service nginx restart</div><div class="line">迁移完毕</div></pre></td></tr></table></figure>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/28/Bigdata-hadoop/Kafka/开源的Kafka集群管理器(Kafka Manager)/">Bigdata-开源的Kafka集群管理器(Kafka Manager)</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-28</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Big-data-Hadoop/">Big data Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h2 id="Kafka-Manager"><a href="#Kafka-Manager" class="headerlink" title="Kafka Manager"></a>Kafka Manager</h2><p>A tool for managing <a href="http://kafka.apache.org/" target="_blank" rel="external">Apache Kafka.</a></p>
<h4 id="It-supports-the-following"><a href="#It-supports-the-following" class="headerlink" title="It supports the following :"></a>It supports the following :</h4><ul>
<li>管理多个群集</li>
<li>容易检查集群状态（主题，消费者，偏移量，经纪人，副本分发，分区分配）</li>
<li>运行首选副本选举</li>
<li>使用选项生成分区分配，以选择要使用的代理</li>
<li>运行分区的重新分配（基于生成的分配）</li>
<li>创建可选主题配置的主题（0.8.1.1具有不同于0.8.2+的配置）</li>
<li>删除主题（仅支持0.8.2+，并记住在代理配 置中设置delete.topic.enable = true）</li>
<li>主题列表现在表示标记为删除的主题（仅支持0.8.2+）</li>
<li>批量生成多个主题的分区分配，并选择要使用的代理</li>
<li>批量运行多个主题的分区重新分配</li>
<li>将分区添加到现有主题</li>
<li>更新现有主题的配置</li>
<li>可选地，启用JMX轮询代理级和主题级度量。</li>
<li>可选地筛选出在zookeeper中没有ids / owner /＆offset /目录的消费者。</li>
</ul>
<p>参考开源地址：<a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="external">https://github.com/yahoo/kafka-manager</a></p>
<h4 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h4><p>Kafka 0.8.1.1 or 0.8.2.<em> or 0.9.0.</em> or 0.10.0.*<br>Java 8+</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo wget --header <span class="string">"Cookie: oraclelicense=accept-securebackup-cookie”   http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz</span></div><div class="line"></div><div class="line">sudo vim /etc/profile</div><div class="line">export JAVA_HOME=/home/jollybi/tools/jdk1.8.0_144</div><div class="line">export LASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib:<span class="variable">$JAVA_HOME</span>/bin</div><div class="line">export PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JAVA_HOME</span>/jre/bin:<span class="variable">$TOMCAT_HOME</span>/bin:<span class="variable">$PATH</span></div></pre></td></tr></table></figure>
<h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/yahoo/kafka-manager.git</div><div class="line">./sbt clean dist</div><div class="line"></div><div class="line">[info]   Compilation completed <span class="keyword">in</span> 13.366 s</div><div class="line">model contains 672 documentable templates</div><div class="line">[info] Main Scala API documentation successful.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-javadoc.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-sans-externalized.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info]</div><div class="line">[info] Your package is ready <span class="keyword">in</span> /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip</div><div class="line">[info]</div><div class="line">[success] Total time: 142 s, completed Jul 27, 2017 3:48:35 PM</div><div class="line">完成</div></pre></td></tr></table></figure>
<h4 id="Starting-the-service"><a href="#Starting-the-service" class="headerlink" title="Starting the service"></a>Starting the service</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">解压缩生成的zip文件后，将工作目录更改为可以运行的服务：</div><div class="line"></div><div class="line">unzip /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip</div><div class="line"></div><div class="line"></div><div class="line">修改zk地址和管理员账号和密码：</div><div class="line"></div><div class="line">vim kafka-manager-1.3.3.8/conf/application.conf</div><div class="line"></div><div class="line"><span class="comment">#kafka-manager.zkhosts="kafka-manager-zookeeper:2181"</span></div><div class="line"><span class="comment">#zk集群可以这么配置：</span></div><div class="line">kafka-manager.zkhosts=<span class="string">"kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#根据个人公司这里可以开启true 设置账号和密码</span></div><div class="line">basicAuthentication.enabled=<span class="literal">true</span></div><div class="line">basicAuthentication.username=<span class="string">"admin"</span></div><div class="line">basicAuthentication.password=<span class="string">"admin"</span></div><div class="line"></div><div class="line"></div><div class="line">默认情况下，它将选择端口9000.这是可以覆盖的，配置文件的位置也是如此。例如：</div><div class="line"></div><div class="line">$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080</div><div class="line"></div><div class="line">后台生效：</div><div class="line"></div><div class="line">$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;</div><div class="line"></div><div class="line">再次，如果java不在您的路径中，或者您需要针对不同版本的Java运行，请按如下所示添加-java-home选项：</div><div class="line"></div><div class="line">$ bin/kafka-manager -java-home /usr/<span class="built_in">local</span>/oracle-java-8</div></pre></td></tr></table></figure>
<h4 id="Packaging"><a href="#Packaging" class="headerlink" title="Packaging"></a>Packaging</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">If you<span class="string">'d like to create a Debian or RPM package instead, you can run one of:</span></div><div class="line"></div><div class="line">sbt debian:packageBin</div><div class="line"></div><div class="line">sbt rpm:packageBin</div></pre></td></tr></table></figure>
<h3 id="查看端口："><a href="#查看端口：" class="headerlink" title="查看端口："></a>查看端口：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">[jollybi@kafka1 conf]$ netstat -ntulp | grep 8080</div><div class="line">(Not all processes could be identified, non-owned process info</div><div class="line"> will not be shown, you would have to be root to see it all.)</div><div class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      70517/java</div></pre></td></tr></table></figure>
<h3 id="网站访问kafka-Manger"><a href="#网站访问kafka-Manger" class="headerlink" title="网站访问kafka Manger"></a>网站访问kafka Manger</h3><p>这里我设置了登录账号和密码： admin admin</p>
<p><figure class="figure"><img src="media/15014722977191.jpg" alt=""></figure></p>
<p>创建kafka名字;<br>选择kafka版本号;<br>JMX这个不需要;<br>下面选择默认点击确认即可.</p>
<p><figure class="figure"><img src="media/15014724051756.jpg" alt=""></figure></p>
<blockquote>
<p>(2)kafka 启用 JMX端口</p>
</blockquote>
<p><figure class="figure"><img src="media/15020729297106.jpg" alt=""></figure></p>
<figure class="highlight vbscript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">以如下命令重新启动kafka</div><div class="line"></div><div class="line">JMX_PORT=<span class="number">9999</span> bin/kafka-<span class="built_in">server</span>-start.sh config/<span class="built_in">server</span>.properties</div><div class="line">或者修改kafka-<span class="built_in">server</span>-start.sh 文件，追加JMX_PORT=<span class="string">"9999"</span></div><div class="line"></div><div class="line"> <span class="keyword">if</span> [ <span class="string">"x$KAFKA_HEAP_OPTS"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></div><div class="line">    export KAFKA_HEAP_OPTS=<span class="string">"-Xmx1G -Xms1G"</span></div><div class="line">    export JMX_PORT=<span class="string">"9999"</span></div><div class="line">fi</div><div class="line">然后重新启动kafka</div><div class="line">bin/kafka-<span class="built_in">server</span>-start.sh config/<span class="built_in">server</span>.properties</div><div class="line"></div><div class="line">但是Metrics中数据都是零</div><div class="line">查看 kafka manager 报错，无法连接jxm</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">解决方法 修改每个kafka broker的 kafka_2.11-0.10.1.0/bin/kafka-run-class.sh文件</div><div class="line">​</div><div class="line"><span class="comment"># JMX settings</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$KAFKA_JMX_OPTS</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  KAFKA_JMX_OPTS=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=75.126.5.162"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"></div><div class="line">-Djava.rmi.server.hostname 的值为当前kafka服务器ip</div><div class="line"></div><div class="line">这里说明下集群kafka都需要修改</div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="media/15020745879851.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725443386.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725288425.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725659340.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014725864134.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014726361909.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014727525618.jpg" alt=""></figure></p>
<p><figure class="figure"><img src="media/15014727932029.jpg" alt=""></figure></p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/06/28/Bigdata-hadoop/Kafka/Bigdata-开源的Kafka集群管理器(kafka-web-console)/">Bigdata-开源的Kafka集群管理器(kafka-web-console)</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-06-28</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>源码的地址在:<a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">kafka-web-console</a></p>
<p><code>Kafka Web Console</code>也是用Scala语言编写的<code>Java web</code>程序用于监控<code>Apache Kafka</code>。这个系统的功能和<code>KafkaOffsetMonitor</code>很类似，但是我们从源码角度来看，这款系统实现比<code>KafkaOffsetMonitor</code>要复杂很多，而且编译配置比<code>KafkaOffsetMonitor</code>较麻烦。</p>
<p>　要想运行这套系统我们需要的先行条件为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Play Framework 2.2.x</div><div class="line">Apache Kafka 0.8.x</div><div class="line">Zookeeper 3.3.3 or 3.3.4</div></pre></td></tr></table></figure>
<h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><p>同样，我们从<code>https://github.com/claudemamo/kafka-web-console</code>上面将源码下载下来，然后用<code>sbt</code>进行编译，在编译前我们需要做如下的修改：</p>
<p>Kafka Web控制台需要一个关系数据库。默认情况下，服务器连接到嵌入式H2数据库，不需要数据库安装或配置。请咨询Play！的文档以指定控制台的数据库。支持以下数据库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/claudemamo/kafka-web-console.git</div></pre></td></tr></table></figure>
<ul>
<li>H2（默认）</li>
<li>PostgreSql</li>
<li>Oracle</li>
<li>DB2</li>
<li>MySQL</li>
<li>Apache Derby</li>
<li>Microsoft SQL Server</li>
</ul>
<p>为了方便，我们可以使用Mysql数据库，只要做如下修改即可，找到 <code>conf/application.conf</code>文件，并修改如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">将这个</div><div class="line">db.default.driver=org.h2.Driver</div><div class="line">db.default.url=<span class="string">"jdbc:h2:file:play"</span></div><div class="line"><span class="comment"># db.default.user=sa</span></div><div class="line"><span class="comment"># db.default.password=""</span></div><div class="line"> </div><div class="line"> </div><div class="line">修改成</div><div class="line">db.default.driver=com.mysql.jdbc.Driver</div><div class="line">db.default.url=<span class="string">"jdbc:mysql://localhost:3306/kafkamonitor"</span></div><div class="line">db.default.user=iteblog</div><div class="line">db.default.pass=wyp</div></pre></td></tr></table></figure>
<p>我们还需要修改build.sbt，加入对Mysql的依赖:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="string">"mysql"</span> % <span class="string">"mysql-connector-java"</span> % <span class="string">"5.1.31"</span></div></pre></td></tr></table></figure>
<p>　2、执行<code>conf/evolutions/default/bak</code>目录下面的<code>1.sql、2.sql和3.sql</code>三个文件。需要注意的是，这三个sql文件不能直接运行，有语法错误，需要做一些修改。<br>上面的注意事项弄完之后，我们就可以编译下载过来的源码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> sbt package</span></div></pre></td></tr></table></figure>
<p>编译的过程比较慢，有些依赖包下载速度非常地慢，请耐心等待。<br>　在编译的过程中，可能会出现有些依赖包无法下载，如下错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">[warn] module not found: com.typesafe.play<span class="comment">#sbt-plugin;2.2.1</span></div><div class="line">[warn] ==== typesafe-ivy-releases: tried</div><div class="line">[warn] http://repo.typesafe.com/typesafe/ivy-releases/</div><div class="line">com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== sbt-plugin-releases: tried</div><div class="line">[warn] http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/</div><div class="line">com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== <span class="built_in">local</span>: tried</div><div class="line">[warn] /home/iteblog/.ivy2/<span class="built_in">local</span>/com.typesafe.play/</div><div class="line">sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== Typesafe repository: tried</div><div class="line">[warn] http://repo.typesafe.com/typesafe/releases/com/</div><div class="line">typesafe/play/sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom</div><div class="line">[warn] ==== public: tried</div><div class="line">[warn] http://repo1.maven.org/maven2/com/typesafe/play/</div><div class="line">sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom</div><div class="line">[warn] ::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">==== <span class="built_in">local</span>: tried</div><div class="line"> </div><div class="line">/home/iteblog/.ivy2/<span class="built_in">local</span>/org.scala-sbt/collections/0.13.0/jars/collections.jar</div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">:: FAILED DOWNLOADS ::</div><div class="line"> </div><div class="line">:: ^ see resolution messages <span class="keyword">for</span> details ^ ::</div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">:: org.scala-sbt<span class="comment">#collections;0.13.0!collections.jar</span></div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div></pre></td></tr></table></figure>
<p>我们可以手动地下载相关依赖，并放到类似<code>/home/iteblog/.ivy2/local/org.scala-sbt/collections/0.13.0/jars/</code>目录下面。然后再编译就可以了。</p>
<p>　　最后，我们可以通过下面命令启动<code>Kafka Web Console</code>监控系统：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> sbt run</span></div></pre></td></tr></table></figure>
<p>并可以在<a href="http://localhost:9000" target="_blank" rel="external">http://localhost:9000</a> 查看下面是一张效果图</p>
<p><figure class="figure"><img src="https://www.iteblog.com/pic/topics.png" alt=""></figure></p>

	

	

</article>





	<span class="different-posts">📖 <a href="/page/2">more posts</a> 📖</span>



	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<p>Hi there, <br />welcome to my Blog glad you found it. Have a look around, will you?</p>
</div>

	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2017 Yancy | Powered by <a href="https://hexo.io/">Hexo</a> | Theme <a href="https://github.com/HoverBaum/meilidu-hexo">MeiliDu</a></span>
		</div>

	</div>


</footer>



</body>

</html>
