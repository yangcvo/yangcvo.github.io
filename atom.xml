<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yancy&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/4bb94619692fa63fbfa18343b7c7965c</icon>
  <subtitle>SIMPLICITY IS PREREQUISITE FOR  RELIABILITY</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.yancy.cc/"/>
  <updated>2017-09-21T14:47:55.000Z</updated>
  <id>http://blog.yancy.cc/</id>
  
  <author>
    <name>Yancy</name>
    <email>yangcvo@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述</title>
    <link href="http://blog.yancy.cc/2017/08/11/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%10%E5%B9%B3%E5%8F%B0/Elasticsearch/ElasticStack%205.x+Kibana-5.5.x+Logstash%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%A6%82%E8%BF%B0/"/>
    <id>http://blog.yancy.cc/2017/08/11/日志分析平台/Elasticsearch/ElasticStack 5.x+Kibana-5.5.x+Logstash版本部署概述/</id>
    <published>2017-08-11T09:56:03.000Z</published>
    <updated>2017-09-21T14:47:55.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ElasticStack-5-x介绍"><a href="#ElasticStack-5-x介绍" class="headerlink" title="ElasticStack 5.x介绍"></a>ElasticStack 5.x介绍</h2><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p><code>ELK=ElasticSearch 官方博客:分布式以及 Elastic</code></p><p><figure class="figure"><img src="http://jolestar.com/images/elasticsearch/scaling-stories.svg" alt=""></figure></p><p>Elastic Stack是ELK日志系统的官方称呼，而ELK则是盛名在外的一款开源分布式日志系统，一般说来包括了Elasticsearch、Logstash和Kibana，涵盖了后端日志采集、日志搜索服务和前端数据展示等功能。<br>本文将会对Elastic Stack的安装部署流程进行一系列简单的介绍，并记录下了一些部署过程中遇到的坑及解决方法。</p><p>对于一个软件或互联网公司来说，对计算资源和应用进行监控和告警是非常基础的需求。对于大公司或成熟公司，一个高度定制化的监控系统应该已经存在了很长时间并且非常成熟了。而对于一个初创公司或小公司来说，如何利用现有开源工具快速搭建一套日志监控及分析平台是需要探索的事情。</p><a id="more"></a><h3 id="监控系统的用户："><a href="#监控系统的用户：" class="headerlink" title="监控系统的用户："></a>监控系统的用户：</h3><ul><li>运维，开发，产品</li></ul><p>监控系统应该可以解决如下的问题：</p><ul><li>监控server的各项基础指标，比如<code>memory,cpu,load,network</code>等</li><li>监控应用的状态。</li><li>搜集应用日志，并进行分析和统计。通过日志分析和统计可得到应用的访问统计，异常统计，业务统计。具有进行大规模日志数据的分析和处理能力。</li><li>可制定告警规则。各种监控数据进入系统后，可以根据条件触发告警，实时的将应用异常情况推送到运维、开发或业务人员的IM/SMS上。</li><li>可定制的看板。可以将各种实时统计或报表直观的显示出来。</li></ul><h3 id="可选方案："><a href="#可选方案：" class="headerlink" title="可选方案："></a>可选方案：</h3><p>日志宝，日志易，Logtail(阿里云) 这是我们后面换 <code>Graylog</code>这个是一批黑马 也分享出来。</p><p>优势：使用简单<br>劣势：需上传日志到外部，不灵活，不易扩展，需付费</p><p>flume-ng + kafka + spark streaming + hbase(es/mysql) + zepplin/自研web展示</p><p>优势：<code>灵活，易于扩展，数据分析和处理能力强</code><br>劣势：<code>开发难度高，周期长，维护成本高</code></p><h3 id="ELK优势和劣势："><a href="#ELK优势和劣势：" class="headerlink" title="ELK优势和劣势："></a>ELK优势和劣势：</h3><p>优势：<code>开源成熟解决方案，使用简单，扩展能力强</code><br>劣势：<code>日志分析和处理依靠logstash完成，处理能力较低，无法适应复杂的日志分析场景</code><br>结论：<code>初步选择ELK搭建起监控平台，其能够满足当前较为简单的监控和分析需求。未来如果不能适应，再考虑其他方案。</code></p><p>在本次实践中，我们所部署的ELK分布式日志系统，其架构大致如下：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/elasticstack_011.png" alt=""></figure><br>首先在各日志产生机上部署收集器<code>Filebeat</code>，然后Filebeat将监控到的log文件变化数据传至Kafka集群，<code>Logstash</code>负责将数据从<code>kafka</code>中拉取下来，并进行字段解析，向<code>Elasticsearch</code>输出结构化后的日志，<code>Kibana负责将Elasticsearch</code>中的数据进行可视化。</p><p>【重点参考】：<a href="http://kibana.logstash.es/content/" target="_blank" rel="external">ELK中文书</a></p><h3 id="一、Elasticsearch的部署"><a href="#一、Elasticsearch的部署" class="headerlink" title="一、Elasticsearch的部署"></a>一、Elasticsearch的部署</h3><p>首先在<code>https://www.elastic.co</code>中找到ES的安装包。下文中所用的安装包均为Linux 64的<code>tar.gz</code>压缩包，解压即可用。官网安装方法:<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html" target="_blank" rel="external">Installation example with tar</a></p><p><code>Elasticsearch</code>至少需要<code>Java 8</code>。在撰写本文时，建议您使用<code>Oracle JDK</code>版本<code>1.8.0_131</code>。Java安装因平台而异，所以我们在这里不再赘述。Oracle的推荐安装文档可以在Oracle的网站上找到。在安装Elasticsearch之前，请先检查您的Java版本，然后再运行（如果需要，请相应地进行安装/升级）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">java -version</div><div class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></div><div class="line"></div><div class="line">curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gz</div><div class="line">tar -xvf elasticsearch-5.5.2.tar.gz</div><div class="line"><span class="built_in">cd</span> elasticsearch-5.5.2/bin</div><div class="line">./elasticsearch <span class="_">-d</span> 后台启动</div><div class="line"></div><div class="line">这里我切换普通es用户启动，记得给予权限。</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@logstash ~]<span class="comment"># curl http://localhost:9200?pretty</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"name"</span> : <span class="string">"s-28M-e"</span>,</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elasticsearch"</span>,</div><div class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"0U3blQviRcSG_pq8KFQ5EA"</span>,</div><div class="line">  <span class="string">"version"</span> : &#123;</div><div class="line">    <span class="string">"number"</span> : <span class="string">"5.5.2"</span>,</div><div class="line">    <span class="string">"build_hash"</span> : <span class="string">"b2f0c09"</span>,</div><div class="line">    <span class="string">"build_date"</span> : <span class="string">"2017-08-14T12:33:14.154Z"</span>,</div><div class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</div><div class="line">    <span class="string">"lucene_version"</span> : <span class="string">"6.6.0"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="1-1-Elasticsearch的配置"><a href="#1-1-Elasticsearch的配置" class="headerlink" title="1.1 Elasticsearch的配置"></a>1.1 Elasticsearch的配置</h3><p>ES的配置文件在解压根目录下的config文件夹中，其中elasticsearch.yml是主配置文件。<br>以基本可用作为部署目标，在该文件中仅需要设置几个重要参数：</p><ol><li><code>cluster.name、node.name</code>这两者顾名思义，作为集群和节点的标识符。</li><li><code>Paths部分下的path.data和path.logs</code>，表示ES的数据存放位置，前者为数据存储位置，后者为ES的log存储位置。请尽量放到剩余空间足够的地方，此外在进行调优时有一种方法是将数据放置到SSD上。</li><li><code>bootstrap.memory_lock: true</code>，设为true以确保ES拥有足够的JVM内存。</li><li><code>network.host: localhost和http.port</code>，在此处设置ES对外服务的IP地址与端口<br>设置完以上几项参数后，即可在ES根目录下使用命令./bin/elasticsearch启动ES进程。也有相应的后台启动方式，具体不赘述。</li></ol><p><strong>主要配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@logstash elasticsearch-5.5.2]$ cat config/elasticsearch.yml | grep -Pv <span class="string">"^$|^#"</span></div><div class="line">cluster.name: jolly-cluster</div><div class="line">node.name: es-jollychic-node1</div><div class="line">path.data: /data/elasticsearch/es-data</div><div class="line">path.logs: /data/elasticsearch/es-logs</div><div class="line">bootstrap.memory_lock: <span class="literal">true</span></div><div class="line">network.host: 10.11.10.26</div><div class="line">http.port: 9200</div></pre></td></tr></table></figure><p>es 配置完以后启动会出现报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">[2017-08-19T22:06:01,722][INFO ][o.e.n.Node               ] [es-jollychic-node1] initializing ...</div><div class="line">[2017-08-19T22:06:01,789][INFO ][o.e.e.NodeEnvironment    ] [es-jollychic-node1] using [1] data paths, mounts [[/data (/dev/mapper/sdb--vg-sdb--lv)]], net usable_space [190.2gb], net total_space [196.7gb], spins? [possibly], types [ext4]</div><div class="line">[2017-08-19T22:06:01,789][INFO ][o.e.e.NodeEnvironment    ] [es-jollychic-node1] heap size [1.9gb], compressed ordinary object pointers [<span class="literal">true</span>]</div><div class="line">[2017-08-19T22:06:01,790][INFO ][o.e.n.Node               ] [es-jollychic-node1] node name [es-jollychic-node1], node ID [yjFGVTcIRg2YQectzevnOA]</div><div class="line">[2017-08-19T22:06:01,790][INFO ][o.e.n.Node               ] [es-jollychic-node1] version[5.5.2], pid[13530], build[b2f0c09/2017-08-14T12:33:14.154Z], OS[Linux/3.10.0-514.26.2.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_144/25.144-b01]</div><div class="line">[2017-08-19T22:06:01,790][INFO ][o.e.n.Node               ] [es-jollychic-node1] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=<span class="literal">true</span>, -Dfile.encoding=UTF-8, -Djna.nosys=<span class="literal">true</span>, -Djdk.io.permissionsUseCanonicalPath=<span class="literal">true</span>, -Dio.netty.noUnsafe=<span class="literal">true</span>, -Dio.netty.noKeySetOptimization=<span class="literal">true</span>, -Dio.netty.recycler.maxCapacityPerThread=0, -D<span class="built_in">log</span>4j.shutdownHookEnabled=<span class="literal">false</span>, -D<span class="built_in">log</span>4j2.disable.jmx=<span class="literal">true</span>, -D<span class="built_in">log</span>4j.skipJansi=<span class="literal">true</span>, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/usr/<span class="built_in">local</span>/elasticsearch-5.5.2]</div><div class="line">[2017-08-19T22:06:02,583][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [aggs-matrix-stats]</div><div class="line">[2017-08-19T22:06:02,583][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [ingest-common]</div><div class="line">[2017-08-19T22:06:02,586][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [lang-expression]</div><div class="line">[2017-08-19T22:06:02,586][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [lang-groovy]</div><div class="line">[2017-08-19T22:06:02,586][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [lang-mustache]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [lang-painless]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [parent-join]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [percolator]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [reindex]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [transport-netty3]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] loaded module [transport-netty4]</div><div class="line">[2017-08-19T22:06:02,587][INFO ][o.e.p.PluginsService     ] [es-jollychic-node1] no plugins loaded</div><div class="line">[2017-08-19T22:06:04,212][INFO ][o.e.d.DiscoveryModule    ] [es-jollychic-node1] using discovery <span class="built_in">type</span> [zen]</div><div class="line">[2017-08-19T22:06:04,635][INFO ][o.e.n.Node               ] [es-jollychic-node1] initialized</div><div class="line">[2017-08-19T22:06:04,635][INFO ][o.e.n.Node               ] [es-jollychic-node1] starting ...</div><div class="line">[2017-08-19T22:06:04,812][INFO ][o.e.t.TransportService   ] [es-jollychic-node1] publish_address &#123;10.11.10.26:9300&#125;, bound_addresses &#123;10.11.10.26:9300&#125;</div><div class="line">[2017-08-19T22:06:04,821][INFO ][o.e.b.BootstrapChecks    ] [es-jollychic-node1] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks</div><div class="line">[2017-08-19T22:06:04,823][ERROR][o.e.b.Bootstrap          ] [es-jollychic-node1] node validation exception</div><div class="line">[1] bootstrap checks failed</div><div class="line">[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</div><div class="line">[2017-08-19T22:06:04,825][INFO ][o.e.n.Node               ] [es-jollychic-node1] stopping ...</div><div class="line">[2017-08-19T22:06:04,840][INFO ][o.e.n.Node               ] [es-jollychic-node1] stopped</div><div class="line">[2017-08-19T22:06:04,840][INFO ][o.e.n.Node               ] [es-jollychic-node1] closing ...</div><div class="line">[2017-08-19T22:06:04,851][INFO ][o.e.n.Node               ] [es-jollychic-node1] closed</div></pre></td></tr></table></figure><ul><li>如果修改完会报错，修改下Linux系统参数。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">max file descriptors [4096] <span class="keyword">for</span> elasticsearch process likely too low, increase to at least [65536]</div><div class="line">max number of threads [1024] <span class="keyword">for</span> user [lishang] likely too low, increase to at least [2048]</div><div class="line">解决：切换到root用户，编辑limits.conf 添加类似如下内容</div><div class="line">vi /etc/security/limits.conf </div><div class="line"></div><div class="line">添加如下内容:</div><div class="line"></div><div class="line">* soft nofile 65536</div><div class="line">* hard nofile 131072</div><div class="line">* soft nproc 2048</div><div class="line">* hard nproc 4096</div><div class="line"></div><div class="line">问题三：max number of threads [1024] <span class="keyword">for</span> user [lish] likely too low, increase to at least [2048]</div><div class="line"></div><div class="line">解决：切换到root用户，进入limits.d目录下修改配置文件。</div><div class="line">vi /etc/security/limits.d/90-nproc.conf </div><div class="line">修改如下内容：</div><div class="line"></div><div class="line">* soft nproc 1024</div><div class="line"><span class="comment">#修改为</span></div><div class="line">* soft nproc 2048</div><div class="line"></div><div class="line">问题四：max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]</div><div class="line"></div><div class="line">解决：切换到root用户修改配置sysctl.conf</div><div class="line"></div><div class="line">vi /etc/sysctl.conf </div><div class="line">添加下面配置：</div><div class="line">vm.max_map_count=655360</div><div class="line">并执行命令：</div><div class="line">sysctl -p</div><div class="line">然后，重新启动elasticsearch，即可启动成功。</div></pre></td></tr></table></figure><h3 id="集群健康检查-Cluster-Health"><a href="#集群健康检查-Cluster-Health" class="headerlink" title="集群健康检查 Cluster Health"></a>集群健康检查 Cluster Health</h3><p>要检查群集的运行状况，我们将使用<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.5/cat.html" target="_blank" rel="external">_catAPI</a>。您可以在<a href="https://www.elastic.co/guide/en/kibana/5.5/console-kibana.html" target="_blank" rel="external">Kibana</a>的控制台中运行以下命令，方法是 单击“查看控制台”或curl单击下面的“复制为CURL”链接并将其粘贴到终端中。</p><p><strong>创建索引：</strong><br>第一个命令使用PUT创建了一个叫做“customer”的索引。我们简单地将pretty附加到调用的尾部，使其以美观的形式打印出JSON响应 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -XPUT <span class="string">'http://10.11.10.26:9200/customer?pretty'</span></div></pre></td></tr></table></figure><p>我们也可以得到我们集群中节点的列表如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@logstash ~]<span class="comment">#  curl -XGET http://10.11.10.26:9200/_cat/nodes?v</span></div><div class="line">ip          heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name</div><div class="line">10.11.10.26           14          96  18    0.02    0.60     0.58 mdi       *      es-jollychic-node1</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@logstash ~]<span class="comment">#  curl -XGET http://10.11.10.26:9200/_cat/master?help</span></div><div class="line">id   |   | node id</div><div class="line">host | h | host name</div><div class="line">ip   |   | ip address</div><div class="line">node | n | node name</div></pre></td></tr></table></figure><p>查看所有连接的索引：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@logstash ~]$ curl <span class="_">-s</span>XGET <span class="string">"http://<span class="variable">$(hostname)</span>:9200/_cat/indices?v"</span></div><div class="line">health status index                                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-web-product-2017.08.25          LIKJyNXrRcmQSt8aXRoyLg   5   1        614            0    361.6kb        361.6kb</div><div class="line">yellow open   <span class="built_in">log</span>-payment-center-product-2017.08.26     w-ucGJUoTDOdhEkfrq9g-g   5   1    1712756            0    808.4mb        808.4mb</div><div class="line">yellow open   .kibana                                   p9MnvKHRS6K_jCeBtB7CXA   1   1         18            1     77.8kb         77.8kb</div><div class="line">yellow open   <span class="built_in">log</span>-jcm-product-2017.08.25                Cs0XHa-xRQOHxt1-HYz1Xw   5   1   46303455            0      8.1gb          8.1gb</div><div class="line">yellow open   <span class="built_in">log</span>-spm_mq-product-2017.08.25             FXYNV08zQselbnVKK7k01g   5   1     465127            0    148.1mb        148.1mb</div><div class="line">yellow open   <span class="built_in">log</span>-erpsearchservice-product-2017.08.26   Z4ECtHCJS2uma_UTxJqDLw   5   1       2217            0     15.3mb         15.3mb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-web-product-2017.08.24          mYAhjM5UQFyIZAN8nFeYQQ   5   1     951572            0    237.5mb        237.5mb</div><div class="line">yellow open   <span class="built_in">log</span>-spm_mq-product-2017.08.26             -PJCMMVqRwWXux8V5SRbCA   5   1    1457469            0    503.1mb        503.1mb</div><div class="line">yellow open   <span class="built_in">log</span>-jcm-product-2017.08.26                Jp7jQdciSMyrDmc82Y8YPQ   5   1   12753864            0      1.8gb          1.8gb</div><div class="line">yellow open   <span class="built_in">log</span>-gp-export-product-2017.08.26          tsFNVopYRXCM4vrCfU7IbQ   5   1     809526            0    169.8mb        169.8mb</div><div class="line">yellow open   <span class="built_in">log</span>-mq-product-2017.08.25                 dxXemS5RR_OpohepAUNcfg   5   1   32670880            0      5.6gb          5.6gb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-pda-product-2017.08.26          nE6s79S4RN2qIFkUt-FJsg   5   1       2323            0      1.6mb          1.6mb</div><div class="line">yellow open   <span class="built_in">log</span>-erpsearchservice-product-2017.08.25   Mj3-2ZrBRSuJaZQebPqcWA   5   1        369            0    390.6kb        390.6kb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-web-product-2017.08.26          X3GUYOZtTW6g1eGV_YW0HQ   5   1     275155            0     65.9mb         65.9mb</div><div class="line">yellow open   <span class="built_in">log</span>-mq-product-2017.08.26                 PwkQ9eOcQl2ie0f2fpnVRg   5   1   12519174            0      2.5gb          2.5gb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31_mq_consumer-product-2017.08.26  iG3bCUybSamJC9l58RlbNw   5   1      25703            0     13.1mb         13.1mb</div><div class="line">yellow open   <span class="built_in">log</span>-spm-product-2017.08.25                fr2fFfTKTOSl8Pm7qfU7eA   5   1        438            0    436.8kb        436.8kb</div><div class="line">yellow open   <span class="built_in">log</span>-dataexporttask-product-2017.08.26     01lRzmfYTDaFj9nXvTnmEg   5   1    1283861            0    145.2mb        145.2mb</div><div class="line">yellow open   <span class="built_in">log</span>-spm-task-scheduler-product-2017.08.26 7We8iXEHSYmk168PO6zUVA   5   1     463587            0    108.9mb        108.9mb</div><div class="line">yellow open   <span class="built_in">log</span>-wms-product-2017.08.25                XvyfywXmTYybccS1GLIkAw   5   1          1            0      8.1kb          8.1kb</div><div class="line">yellow open   <span class="built_in">log</span>-payment-center-product-2017.08.25     wUlvN1VhQ5Wb0pFgg1BkgA   5   1     691080            0    332.5mb        332.5mb</div><div class="line">yellow open   <span class="built_in">log</span>-dataexport-product-2017.08.26         SOh6vF8LQnOOWLsI9R_CcA   5   1     850019            0    499.5mb        499.5mb</div><div class="line">yellow open   <span class="built_in">log</span>-image-finder-product-2017.08.26       OGkYlpkzTPKReMsArJoyVA   5   1       9752            0      2.2mb          2.2mb</div><div class="line">yellow open   <span class="built_in">log</span>-gplatform_v2-product-2017.08.26       XEp5aM23SfaHfZ8qkgMjQw   5   1       8521            0      2.8mb          2.8mb</div><div class="line">yellow open   <span class="built_in">log</span>-payment-center-product-2017.08.24     4NDXMAQKRQqljZ1pcGp8yQ   5   1      70898            0       39mb           39mb</div><div class="line">yellow open   <span class="built_in">log</span>-wms31-pda-product-2017.08.25          caYLQMwlSZmBYl0YeZohpQ   5   1        237            0    214.8kb        214.8kb</div><div class="line">yellow open   <span class="built_in">log</span>-wms-product-2017.08.22                3NLXCSLyQJ2JHoCCeS41xQ   5   1          1            0      8.1kb          8.1kb</div></pre></td></tr></table></figure><p>删除指定索引：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -XDELETE <span class="string">"http://<span class="variable">$(hostname)</span>:9200/log-wms-product-2017.08.19"</span></div></pre></td></tr></table></figure><h3 id="1-2-Elasticsearch-5-x的Bootstrap-Checks"><a href="#1-2-Elasticsearch-5-x的Bootstrap-Checks" class="headerlink" title="1.2 Elasticsearch 5.x的Bootstrap Checks"></a>1.2 Elasticsearch 5.x的Bootstrap Checks</h3><p>Elasticsearch在升级到5.x版本后，启动时会强制执行<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html" target="_blank" rel="external">Bootstrap Checks</a>(官方文档)<br>其中经常性的问题是需要增大系统可使用的最大FileDescriptors数（参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html）" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html）</a><br>剩下的其他问题可以查询官方文档。</p><h3 id="1-3-Elasticsearch的X-pack插件"><a href="#1-3-Elasticsearch的X-pack插件" class="headerlink" title="1.3 Elasticsearch的X-pack插件"></a>1.3 Elasticsearch的X-pack插件</h3><p><code>X-pack</code>是官方提供的一系列集成插件，包括了<code>alert、monitor、secure</code>等功能，十分强大（但是并不免费）。<br>在ELK 5.0中安装大部分插件仅需要输入命令：<code>./bin/elasticsearch-plugin install &lt;plugin name&gt;</code>即可<br><code>X-pack</code>插件安装后会自动开启ELK的权限功能，需要注意的是如果启用了<code>X-pack</code>，则在向ES输入数据或发起API请求时，均需要附带相应的auth信息。<br>考虑到X-pack并非免费且价格昂贵，暂时不安装X-pack包。</p><h3 id="1-4-Elasticsearch的Head插件"><a href="#1-4-Elasticsearch的Head插件" class="headerlink" title="1.4 Elasticsearch的Head插件"></a>1.4 Elasticsearch的Head插件</h3><p><code>Head</code>插件作为<code>ELK 2.x</code>版本中较为通用的前端管理插件，在<code>ELK 5.x</code>版本中无法直接使用<code>./bin/elasticsearch-plugin install head</code>的方式安装，但是可以采取standalone的方式进行运行。</p><p>参考官方文档：<a href="https://github.com/mobz/elasticsearch-head#running-with-built-in-server" target="_blank" rel="external">elasticsearch-head</a></p><p>一篇较好的<a href="http://www.cnblogs.com/xing901022/p/6030296.html" target="_blank" rel="external">ES 5.x安装Head的博文：</a></p><p>【特别注意】：暂时没有找到x-pack和head相互兼容的方法，目前由于认证的问题，如果<code>启用了x-pack的secure</code>功能，会导致head插件无法连接ES集群。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> git://github.com/mobz/elasticsearch-head.git</div><div class="line"><span class="built_in">cd</span> elasticsearch-head</div><div class="line">npm install</div><div class="line">npm run start</div><div class="line">open http://localhost:9100/</div></pre></td></tr></table></figure><ul><li>修改head目录下的Gruntfile.js配置，head默认监听127.0.0.1 新增：hostname: ‘0.0.0.0’</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># vim Gruntfile.js</span></div><div class="line">                connect: &#123;</div><div class="line">                        server: &#123;</div><div class="line">                                options: &#123;</div><div class="line">                                        hostname: <span class="string">'0.0.0.0'</span>,</div><div class="line">                                        port: 9100,</div><div class="line">                                        base: <span class="string">'.'</span>,</div><div class="line">                                        keepalive: <span class="literal">true</span></div><div class="line">                                &#125;</div><div class="line">                        &#125;</div><div class="line">                &#125;</div><div class="line"></div><div class="line">        &#125;);</div></pre></td></tr></table></figure><ul><li>修改elasticsearch配置文件 elasticsearch.yml</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">http.cors.enabled: <span class="literal">true</span></div><div class="line">http.cors.allow-origin: <span class="string">"*"</span></div></pre></td></tr></table></figure><ul><li>重启elasticsearch，并启动node </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ln <span class="_">-s</span> /usr/<span class="built_in">local</span>/elasticsearch-head/node_modules/grunt/bin/grunt /usr/bin/grunt</div><div class="line"></div><div class="line">后台启动：grunt server &amp;</div></pre></td></tr></table></figure><ul><li>访问效果：</li></ul><p><figure class="figure"><img src="media/15033045476000.jpg" alt=""></figure></p><h3 id="1-5-Elasticsearch-Kibana"><a href="#1-5-Elasticsearch-Kibana" class="headerlink" title="1.5 Elasticsearch-Kibana"></a>1.5 Elasticsearch-Kibana</h3><p>RPM方法安装参考官网文档：<a href="https://www.elastic.co/guide/en/kibana/current/rpm.html#rpm" target="_blank" rel="external">使用RPM 编辑安装Kibana</a></p><p>Running Kibana on Docker: <a href="https://www.elastic.co/guide/en/kibana/current/_pulling_the_image.html" target="_blank" rel="external">Docker 搭建方法</a></p><p>tar方式安装Kibana </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.2-linux-x86_64.tar.gz</div><div class="line">sha1sum kibana-5.5.2-linux-x86_64.tar.gz </div><div class="line">tar -xzf kibana-5.5.2-linux-x86_64.tar.gz</div><div class="line"><span class="built_in">cd</span> kibana/</div></pre></td></tr></table></figure><h3 id="通过配置文件编辑配置Kibana"><a href="#通过配置文件编辑配置Kibana" class="headerlink" title="通过配置文件编辑配置Kibana"></a>通过配置文件编辑配置Kibana</h3><p><code>kibana.yml</code>启动时Kibana服务器从文件读取属性。默认设置配置Kibana运行<code>localhost:5601</code>。要更改主机或端口号，或者连接到在其他机器上运行的<code>Elasticsearch</code>，您需要更新kibana.yml文件。您还可以启用SSL并设置各种其他选项。</p><p>kibana.yml 全部基本配置 ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">server.port:</div><div class="line">默认值：5601 Kibana由后端服务器提供服务。此设置指定要使用的端口。</div><div class="line"></div><div class="line">server.host:</div><div class="line">默认值：“localhost”此设置指定后端服务器的主机。</div><div class="line"></div><div class="line">server.basePath:</div><div class="line">如果您在代理服务器后面运行，则允许您指定安装Kibana的路径。这仅影响由Kibana生成的URL，您的代理预期在将请求转发给Kibana之前删除basePath值。此设置无法以斜杠（/）结尾。</div><div class="line"></div><div class="line">server.maxPayloadBytes:</div><div class="line">默认值：1048576传入服务器请求的最大有效负载大小（以字节为单位）。</div><div class="line"></div><div class="line">server.name:</div><div class="line">默认值：“your-hostname”用于标识此Kibana实例的人性化显示名称。</div><div class="line"></div><div class="line">server.defaultRoute:</div><div class="line">默认值：“/ app / kibana”此设置指定打开Kibana时的默认路由。打开Kibana时，您可以使用此设置修改着陆页。</div><div class="line"></div><div class="line">elasticsearch.url:</div><div class="line">默认值：“http：// localhost：9200”用于所有查询的Elasticsearch实例的URL。</div><div class="line"></div><div class="line">elasticsearch.preserveHost:</div><div class="line">默认值：<span class="literal">true</span>当此设置的值为真时，Kibana使用server.host设置中指定的主机名。当此设置的值为<span class="literal">false</span>，Kibana使用连接到此Kibana实例的主机的主机名。</div><div class="line"></div><div class="line">kibana.index:</div><div class="line">默认：“.kibana” Kibana使用Elasticsearch中的索引来存储已保存的搜索，可视化和仪表板。如果索引不存在，Kibana将创建一个新的索引。</div><div class="line"></div><div class="line">kibana.defaultAppId:</div><div class="line">默认值：“discover”要加载的默认应用程序。</div><div class="line"></div><div class="line">tilemap.url:</div><div class="line">Kibana用于在tilemap可视化中显示地图图块的tile服务的URL。默认情况下，Kibana从外部元数据服务读取此URL，但用户仍然可以覆盖此参数以使用自己的Tile Map Service。例如：<span class="string">"https://tiles.elastic.co/v2/default/&#123;z&#125;/&#123;x&#125;/&#123;y&#125;.png?elastic_tile_service_tos=agree&amp;my_app_name=kibana"</span></div><div class="line"></div><div class="line">tilemap.options.minZoom:</div><div class="line">默认值：1最小缩放级别。</div><div class="line"></div><div class="line">tilemap.options.maxZoom:</div><div class="line">默认值：10最大缩放级别。</div><div class="line"></div><div class="line">tilemap.options.attribution:</div><div class="line">默认值：<span class="string">"© [Elastic Maps Service](https://www.elastic.co/elastic-maps-service)"</span>地图属性字符串。</div><div class="line"></div><div class="line">regionmap</div><div class="line">指定用于区域映射可视化的其他矢量图层。每个层对象都指向一个外部矢量文件，其中包含一个geojson FeatureCollection。该文件必须使用WGS84坐标参考系，并且只包括多边形。如果文件托管在与Kibana不同的域上，则服务器需要启用CORS，因此Kibana可以下载该文件。url字段也用作文件的唯一标识符。每个图层可以包含多个字段，以指示要公开的geojson要素的哪些属性。field.description是在Region Map可视化的字段菜单中显示的人类可读文本。也可以添加可选的归因值。以下示例显示有效的regionmap配置。</div><div class="line"></div><div class="line">regionmap：</div><div class="line">  层：</div><div class="line">     - 名称：“法国部”</div><div class="line">       url：“http://my.cors.enabled.server.org/france_departements.geojson”</div><div class="line">       归因：“INRAP”</div><div class="line">       字段：</div><div class="line">          - 名称：“部门”</div><div class="line">            说明：“全部名称”</div><div class="line">          - 名称：“INSEE”</div><div class="line">            说明：“INSEE数字标识符”</div><div class="line"></div><div class="line">elasticsearch.username: 和 elasticsearch.password:</div><div class="line">如果您的Elasticsearch受到基本身份验证的保护，这些设置将提供Kibana服务器在启动时对Kibana索引执行维护所用的用户名和密码。您的Kibana用户仍然需要使用通过Kibana服务器代理的Elasticsearch进行身份验证。</div><div class="line"></div><div class="line">server.ssl.enabled</div><div class="line">默认值：“<span class="literal">false</span>”启用从Kibana服务器到浏览器的传出请求的SSL。当设置为<span class="literal">true</span>，</div><div class="line">server.ssl.certificate并server.ssl.key要求</div><div class="line"></div><div class="line">server.ssl.certificate: 和 server.ssl.key:</div><div class="line">路由到PEM格式的SSL证书和SSL密钥文件。</div><div class="line"></div><div class="line">server.ssl.keyPassphrase</div><div class="line">将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。</div><div class="line"></div><div class="line">server.ssl.certificateAuthorities</div><div class="line">列出可信赖的PEM编码证书文件的路径。</div><div class="line"></div><div class="line">server.ssl.supportedProtocols</div><div class="line">默认值：TLSv1，TLSv1.1，TLSv1.2 支持的版本协议。有效协议：TLSv1，TLSv1.1，TLSv1.2</div><div class="line"></div><div class="line">server.ssl.cipherSuites</div><div class="line">默认值：ECDHE-RSA-AES128-GCM-SHA256，ECDHE-ECDSA-AES128-GCM-SHA256，ECDHE-RSA-AES256-GCM-SHA384，ECDHE-ECDSA-AES256-GCM-SHA384，DHE-RSA-AES128-GCM- SHA256，ECDHE-RSA-AES128-SHA256，DHE-RSA-AES128-SHA256，ECDHE-RSA-AES256-SHA384，DHE-RSA-AES256-SHA384，ECDHE-RSA-AES256-SHA256，DHE-RSA-AES256-SHA256， HIGH，！aNULL，！eNULL，！EXPORT，！DES，！RC4，！MD5，！PSK，！SRP，！CAMELLIA。格式和有效选项的详细信息可通过[OpenSSL密码列表格式文档]（ https://www.openssl.org/docs/man1.0.2/apps/ciphers.html<span class="comment">#CIPHER-LIST-FORMAT）获得。</span></div><div class="line"></div><div class="line"></div><div class="line">elasticsearch.ssl.certificate: 和 elasticsearch.ssl.key:</div><div class="line">提供PEM格式SSL证书和密钥文件路径的可选设置。这些文件验证您的Elasticsearch后端使用相同的密钥文件。</div><div class="line"></div><div class="line">elasticsearch.ssl.keyPassphrase</div><div class="line">将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。</div><div class="line"></div><div class="line">elasticsearch.ssl.certificateAuthorities:</div><div class="line">可选设置，使您能够指定Elasticsearch实例的证书颁发机构的PEM文件的路径列表。</div><div class="line"></div><div class="line">elasticsearch.ssl.verificationMode:</div><div class="line">默认值：full控制证书的验证。有效值是none，certificate和full。 full执行主机名验证，certificate不执行。</div><div class="line"></div><div class="line">elasticsearch.pingTimeout:</div><div class="line">默认值：elasticsearch.requestTimeout设置时间（以毫秒为单位）等待弹性搜索响应ping的值。</div><div class="line"></div><div class="line">elasticsearch.requestTimeout:</div><div class="line">默认值：30000等待后端或弹性搜索的响应的时间（毫秒）。该值必须为正整数。</div><div class="line"></div><div class="line">elasticsearch.requestHeadersWhitelist:</div><div class="line">默认值：[ <span class="string">'authorization'</span> ]要发送到Elasticsearch的Kibana客户端标题列表。要发送没有客户端标题，请将此值设置为[]（空列表）。</div><div class="line"></div><div class="line">elasticsearch.customHeaders:</div><div class="line">默认值：&#123;&#125;要发送到Elasticsearch的标题名称和值。无论elasticsearch.requestHeadersWhitelist配置如何，客户端头都不能覆盖任何自定义头文件。</div><div class="line"></div><div class="line">elasticsearch.shardTimeout:</div><div class="line">默认值：0 Elasticsearch等待分片响应的时间（以毫秒为单位）。设置为0以禁用。</div><div class="line"></div><div class="line">elasticsearch.startupTimeout:</div><div class="line">默认值：5000重试之前等待Kibana启动时的弹性搜索的时间（以毫秒为单位）。</div><div class="line"></div><div class="line">pid.file:</div><div class="line">指定Kibana创建进程ID文件的路径。</div><div class="line"></div><div class="line">logging.dest:</div><div class="line">默认值：stdout允许您指定Kibana存储日志输出的文件。</div><div class="line"></div><div class="line">logging.silent:</div><div class="line">默认值：<span class="literal">false</span>将此设置的值设置<span class="literal">true</span>为禁止所有日志输出。</div><div class="line"></div><div class="line">logging.quiet:</div><div class="line">默认值：<span class="literal">false</span>将此设置的值设置<span class="literal">true</span>为禁止除错误消息之外的所有日志输出。</div><div class="line"></div><div class="line">logging.verbose</div><div class="line">默认值：<span class="literal">false</span>将此设置的值设置为<span class="literal">true</span>记录所有事件，包括系统使用情况信息和所有请求。</div><div class="line"></div><div class="line">ops.interval</div><div class="line">默认值：5000设置采样系统和进程性能指标的间隔（以毫秒为单位）。最小值为100。</div><div class="line"></div><div class="line">status.allowAnonymous</div><div class="line">默认值：<span class="literal">false</span>如果启用了身份验证，<span class="literal">true</span>请将其设置为允许未经身份验证的用户访问Kibana服务器状态API和状态页面。</div><div class="line"></div><div class="line">cpu.cgroup.path.override</div><div class="line">覆盖cgroup cpu路径，以不一致的方式安装 /proc/self/cgroup</div><div class="line"></div><div class="line">cpuacct.cgroup.path.override</div><div class="line">在与不一致的方式安装时，覆盖cgroup cpuacct路径 /proc/self/cgroup</div><div class="line"></div><div class="line">console.enabled</div><div class="line">默认值：<span class="literal">true</span>设置为<span class="literal">false</span>以禁用控制台。切换此操作将导致服务器在下次启动时重新生成资源，这可能会在页面开始投放之前造成延迟。</div><div class="line"></div><div class="line">elasticsearch.tribe.url:</div><div class="line">用于所有查询的Elasticsearch部落实例的可选URL。</div><div class="line"></div><div class="line">elasticsearch.tribe.username: 和 elasticsearch.tribe.password:</div><div class="line">如果您的Elasticsearch受到基本身份验证的保护，这些设置将提供Kibana服务器在启动时对Kibana索引执行维护所用的用户名和密码。您的Kibana用户仍然需要使用通过Kibana服务器代理的Elasticsearch进行身份验证。</div><div class="line"></div><div class="line">elasticsearch.tribe.ssl.certificate: 和 elasticsearch.tribe.ssl.key:</div><div class="line">提供PEM格式SSL证书和密钥文件路径的可选设置。这些文件验证您的Elasticsearch后端使用相同的密钥文件。</div><div class="line"></div><div class="line">elasticsearch.tribe.ssl.keyPassphrase</div><div class="line">将用于解密私钥的密码短语。该值是可选的，因为密钥可能未被加密。</div><div class="line"></div><div class="line">elasticsearch.tribe.ssl.certificateAuthorities:</div><div class="line">可选设置，使您能够为您的部落Elasticsearch实例的证书颁发机构指定PEM文件的路径。</div><div class="line"></div><div class="line">elasticsearch.tribe.ssl.verificationMode:</div><div class="line">默认值：full控制证书的验证。有效值是none，certificate和full。full执行主机名验证，</div><div class="line">certificate不执行。</div><div class="line"></div><div class="line">elasticsearch.tribe.pingTimeout:</div><div class="line">默认值：elasticsearch.tribe.requestTimeout设置时间（以毫秒为单位）等待弹性搜索响应ping的值。</div><div class="line"></div><div class="line">elasticsearch.tribe.requestTimeout:</div><div class="line">默认值：30000等待后端或弹性搜索的响应的时间（毫秒）。该值必须为正整数。</div><div class="line"></div><div class="line">elasticsearch.tribe.requestHeadersWhitelist:</div><div class="line">默认值：[ <span class="string">'authorization'</span> ]要发送到Elasticsearch的Kibana客户端标题列表。要发送没有客户端标题，请将此值设置为[]（空列表）。</div><div class="line"></div><div class="line">elasticsearch.tribe.customHeaders:</div><div class="line">默认值：&#123;&#125;要发送到Elasticsearch的标题名称和值。无论</div><div class="line">elasticsearch.tribe.requestHeadersWhitelist配置如何，客户端头都不能覆盖任何自定义头文件。</div></pre></td></tr></table></figure><p>根据不同的需求配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@logstash config]<span class="comment"># cat kibana.yml | grep -Pv "^#|^$"</span></div><div class="line">server.port: 5601</div><div class="line">server.host: <span class="string">"10.11.10.26"</span></div><div class="line">server.name: <span class="string">"jollychic-log"</span></div><div class="line">elasticsearch.url: <span class="string">"http://10.11.10.26:9200"</span></div><div class="line">elasticsearch.preserveHost: <span class="literal">true</span></div><div class="line">kibana.index: <span class="string">".kibana"</span></div></pre></td></tr></table></figure><p>配置好配置，启动服务：<code>./bin/kibana &amp; 后台启动</code></p><h3 id="1-6访问Kibana"><a href="#1-6访问Kibana" class="headerlink" title="1.6访问Kibana"></a>1.6访问Kibana</h3><ul><li>检查Kibana状态</li></ul><p>您可以通过导航到达Kibana服务器的状态页面localhost:5601/status。状态页面显示有关服务器资源使用情况的信息，并列出已安装的插件。</p><p><code>http://10.11.10.26:5601/status</code></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/elasticstack_022.png" alt=""></figure></p><ul><li>配置索引模式<br>指定与一个或多个弹性搜索索引的名称相匹配的索引模式。默认情况下，Kibana猜测您正在使用由Logstash提供给Elasticsearch的数据。如果是这样，您可以使用默认值logstash-<em>作为索引模式。星号（</em>）匹配索引名称中的零个或多个字符。如果您的弹性搜索索引遵循其他命名约定，请输入适当的模式。“模式”也可以简单地是单个索引的名称。<br>选择包含要用于执行基于时间的比较的时间戳的索引字段。Kibana读取索引映射以列出包含时间戳的所有字段。如果您的索引没有基于时间的数据，请禁用索引包含基于时间的事件选项。</li></ul><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/elasticstack_033.png" alt=""></figure></p><ul><li>选择索引查看日志</li></ul><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/elasticstack_044.png" alt=""></figure></p><p>官网中在生产环境中使用Kibana 说的很详细：<a href="https://www.elastic.co/guide/en/kibana/current/production.html" target="_blank" rel="external">在生产环境中使用Kibana</a></p><p>上面是我logstash已经搭建配置好索引才能获取到。下面讲如何部署logstash</p><h2 id="二、Logstash的部署"><a href="#二、Logstash的部署" class="headerlink" title="二、Logstash的部署"></a>二、Logstash的部署</h2><p>与Elasticsearch类似，在官网下载压缩包后，解压即可用。</p><p>在非高级场景下，Logstash本身不需要进行太多的配置（配置文件在logstash根目录下的<code>./config/logstash.yml</code>），高级场景请参考官方文档。<br>logstash的启动命令为:<code>./bin/logstash -f &lt;pipeline_conf_file&gt; --config.reload.automatic</code>，其中-f指定了pipeline配置文件的位置<code>，--config.reload.automatic</code>指定了pipeline配置文件可以进行热加载。<br>本次我们使用Logstash作为日志解析模块（Logstash其实也可以作为日志采集器），重点需要配置pipeline的三大部分：<code>input、filter和output。pipeline</code>文件需要自己创建。</p><h3 id="Installing-Logstash"><a href="#Installing-Logstash" class="headerlink" title="Installing Logstash"></a>Installing Logstash</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">java version <span class="string">"1.8.0_65"</span></div><div class="line">Java(TM) SE Runtime Environment (build 1.8.0_65-b17)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)</div><div class="line"></div><div class="line">rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch</div><div class="line"></div><div class="line">vim /etc/yum.repos.d/logstash.repo</div><div class="line">[logstash-5.x]</div><div class="line">name=Elastic repository <span class="keyword">for</span> 5.x packages</div><div class="line">baseurl=https://artifacts.elastic.co/packages/5.x/yum</div><div class="line">gpgcheck=1</div><div class="line">gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch</div><div class="line">enabled=1</div><div class="line">autorefresh=1</div><div class="line"><span class="built_in">type</span>=rpm-md</div><div class="line"></div><div class="line">sudo yum install logstash</div></pre></td></tr></table></figure><p>首先，我们通过运行最基本的<code>Logstash</code>管道来测试您的<code>Logstash</code>安装。</p><p><code>Logstash</code>管道有两个必需的元素，<code>input</code>并且<code>output</code>，和一个可选的元素，<code>filter</code>。输入插件消耗来自源的数据，过滤器插件会按照您指定的方式修改数据，并且输出插件将数据写入到目的地。</p><p><figure class="figure"><img src="https://www.elastic.co/guide/en/logstash/current/static/images/basic_logstash_pipeline.png" alt=""></figure></p><p>要测试您的Logstash安装，运行最基本的Logstash管道。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> logstash-5.5.2</div><div class="line">bin / logstash <span class="_">-e</span><span class="string">'input &#123;stdin &#123;&#125;&#125; output &#123;stdout &#123;&#125;&#125;'</span></div></pre></td></tr></table></figure><p>该<code>-e</code>标志使您能够直接从命令行指定配置。在命令行中指定配置可以快速测试配置，而无需在迭代之间编辑文件。示例中的流水线从标准输入端输入，<code>stdin并stdout</code>以结构化格式将该输入移动到标准输出 。</p><p>启动<code>Logstash</code>后，等到看到<code>“Pipeline main started”</code>，然后hello world在命令提示符下输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hello world</div><div class="line">2017-07-21T01:22:14.405+0000 0.0.0.0 hello world</div></pre></td></tr></table></figure><p>Logstash将时间戳和IP地址信息添加到消息中。通过在运行Logstash的shell中发出CTRL-D命令退出Logstash 。</p><p>logstash的配置片段：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">#character at the beginning of a line indicates a comment. Use</span></div><div class="line"><span class="comment"># comments to describe your configuration.</span></div><div class="line">input &#123;</div><div class="line">    udp &#123;</div><div class="line">        port =&gt; 25826</div><div class="line">        buffer_size =&gt; 1452</div><div class="line">        workers =&gt; 3          <span class="comment"># Default is 2</span></div><div class="line">        queue_size =&gt; 30000   <span class="comment"># Default is 2000</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">output &#123;</div><div class="line">  elasticsearch &#123;</div><div class="line">    hosts =&gt; [ <span class="string">"10.11.10.26:9200"</span> ]</div><div class="line">    user =&gt; logstash</div><div class="line">    password =&gt; logstash</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>☺待整理续写~~</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ElasticStack-5-x介绍&quot;&gt;&lt;a href=&quot;#ElasticStack-5-x介绍&quot; class=&quot;headerlink&quot; title=&quot;ElasticStack 5.x介绍&quot;&gt;&lt;/a&gt;ElasticStack 5.x介绍&lt;/h2&gt;&lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;&lt;code&gt;ELK=ElasticSearch 官方博客:分布式以及 Elastic&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://jolestar.com/images/elasticsearch/scaling-stories.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Elastic Stack是ELK日志系统的官方称呼，而ELK则是盛名在外的一款开源分布式日志系统，一般说来包括了Elasticsearch、Logstash和Kibana，涵盖了后端日志采集、日志搜索服务和前端数据展示等功能。&lt;br&gt;本文将会对Elastic Stack的安装部署流程进行一系列简单的介绍，并记录下了一些部署过程中遇到的坑及解决方法。&lt;/p&gt;
&lt;p&gt;对于一个软件或互联网公司来说，对计算资源和应用进行监控和告警是非常基础的需求。对于大公司或成熟公司，一个高度定制化的监控系统应该已经存在了很长时间并且非常成熟了。而对于一个初创公司或小公司来说，如何利用现有开源工具快速搭建一套日志监控及分析平台是需要探索的事情。&lt;/p&gt;
    
    </summary>
    
      <category term="Log Analysis Platform" scheme="http://blog.yancy.cc/categories/Log-Analysis-Platform/"/>
    
    
      <category term="Elasticsearch" scheme="http://blog.yancy.cc/tags/Elasticsearch/"/>
    
      <category term="Kibana" scheme="http://blog.yancy.cc/tags/Kibana/"/>
    
  </entry>
  
  <entry>
    <title>Kafka性能优化–JVM参数配置优化</title>
    <link href="http://blog.yancy.cc/2017/07/04/Bigdata-hadoop/Kafka/kafka%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E2%80%93JVM%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/"/>
    <id>http://blog.yancy.cc/2017/07/04/Bigdata-hadoop/Kafka/kafka性能优化–JVM参数配置优化/</id>
    <published>2017-07-04T03:22:00.000Z</published>
    <updated>2017-09-21T15:10:23.000Z</updated>
    
    <content type="html"><![CDATA[<p><figure class="figure"><img src="https://cdn2.hubspot.net/hubfs/540072/Kafka_Connect_graphic.png" alt=""></figure></p><h3 id="Kafka集群稳定"><a href="#Kafka集群稳定" class="headerlink" title="Kafka集群稳定"></a>Kafka集群稳定</h3><p>GC调优<br>　　调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent，具体参数设置可以参考Google，这里不赘述。</p><p>　　Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent 分别设置为 20毫秒和 35。Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入。</p><a id="more"></a><h3 id="Kafka-Broker个数决定因素"><a href="#Kafka-Broker个数决定因素" class="headerlink" title="Kafka Broker个数决定因素"></a>Kafka Broker个数决定因素</h3><p>　　磁盘容量：首先考虑的是所需保存的消息所占用的总磁盘容量和每个broker所能提供的磁盘空间。如果Kafka集群需要保留 10 TB数据，单个broker能存储 2 TB，那么我们需要的最小Kafka集群大小5个broker。此外，如果启用副本参数，则对应的存储空间需至少增加一倍（取决于副本参数）。这意味着对应的Kafka集群至少需要 10 个broker。</p><p>　　请求量：另外一个要考虑的是Kafka集群处理请求的能力。这主要取决于对Kafka client请求的网络处理能力，特别是，有多个consumer或者网路流量不稳定。如果，高峰时刻，单个broker的网络流量达到80%，这时是撑不住两个consumer的，除非有两个broker。再者，如果启用了副本参数，则需要考虑副本这个额外的consumer。也可以扩展多个broker来减少磁盘的吞吐量和系统内存。</p><h3 id="主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。"><a href="#主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。" class="headerlink" title="主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。"></a>主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。</h3><h3 id="1、JVM参数配置优化"><a href="#1、JVM参数配置优化" class="headerlink" title="1、JVM参数配置优化"></a>1、JVM参数配置优化</h3><p>如果使用的CMS GC算法，建议JVM Heap不要太大，在4GB以内就可以。JVM太大，导致Major GC或者Full GC产生的“stop the world”时间过长，导致broker和zk之间的session超时，比如重新选举controller节点和提升follow replica为leader replica。<br>JVM也不能过小，否则会导致频繁地触发gc操作，也影响Kafka的吞吐量。另外，需要避免CMS GC过程中的发生promotion failure和concurrent failure问题。CMSInitiatingOccupancyFraction=70可以预防concurrent failure问题，提前出发Major GC。<br>Kafka JVM参数可以直接修改启动脚本<code>bin/kafka-server-start.sh</code><br>中的变量值。下面是一些基本参数，也可以根据实际的gc状况和调试GC需要增加一些相关的参数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx4G -Xms4G -Xmn2G -XX:PermSize=64m -XX:MaxPermSize=128m  -XX:SurvivorRatio=6  -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly"</span></div></pre></td></tr></table></figure><p>需要关注gc日志中的YGC时间以及CMS GC里面的CMS-initial-mark和CMS-remark两个阶段的时间，这些GC过程是“stop the world”方式完成的。</p><blockquote><p>jdk1.8 优化的话会提示MaxPermSize=128m,PermSize=64m 字面意思是MaxPermSize不需要我们配置了,jdk1.8 版本功能其实已不需要这个优化参数：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka3 kafka_2.10-0.8.2.1]$ /data/tools/kafka_2.10-0.8.2.1/bin/kafka-server-start.sh /data/tools/kafka_2.10-0.8.2.1/config/server.properties &amp;</div><div class="line">[1] 10312</div><div class="line">[jollybi@kafka3 kafka_2.10-0.8.2.1]$ Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=64m; support was removed <span class="keyword">in</span> 8.0</div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed <span class="keyword">in</span> 8.0</div><div class="line"></div><div class="line">优化参数：</div><div class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx4G -Xms4G -Xmn2G  -XX:SurvivorRatio=6  -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly"</span></div><div class="line"><span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span>  <span class="comment">### Kafka Manager监控监听jmx端口,如果没有可不设置。</span></div></pre></td></tr></table></figure><h3 id="2、打开JMX端口"><a href="#2、打开JMX端口" class="headerlink" title="2、打开JMX端口"></a>2、打开JMX端口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">主要是为了通过JMX端口监控Kafka Broker信息。可以在bin/kafka-server-start.sh中打开JMX端口变量。</div><div class="line"><span class="built_in">export</span> JMX_PORT=9999</div></pre></td></tr></table></figure><h3 id="3、调整log4j的日志级别"><a href="#3、调整log4j的日志级别" class="headerlink" title="3、调整log4j的日志级别"></a>3、调整log4j的日志级别</h3><p>如果集群中topic和partition数量较大时，因为log4j的日志级别太低，导致进程持续很长的时间在打印日志。日志量巨大，导致很多额外的性能开销。特别是contoller日志级别为trace级别，这点比较坑。<br>Tips 通过JMX端口设置log4j日志级别，不用重启broker节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">设置日志级别：</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController <span class="built_in">set</span>LogLevel=kafka.controller,INFO</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController <span class="built_in">set</span>LogLevel=state.change.logger,INFO</div><div class="line"> </div><div class="line">检查日志级别：</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController getLogLevel=kafka.controller</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController getLogLevel=state.change.logger</div></pre></td></tr></table></figure><h3 id="4、性能优化技巧"><a href="#4、性能优化技巧" class="headerlink" title="4、性能优化技巧"></a>4、性能优化技巧</h3><p>4.1、配置合适的partitons数量。</p><p>这似乎是kafka新手必问得问题。首先，我们必须理解，partiton是kafka的并行单元。从producer和broker的视角看，向不同的partition写入是完全并行的；而对于consumer，并发数完全取决于partition的数量，即，如果consumer数量大于partition数量，则必有consumer闲置。所以，我们可以认为kafka的吞吐与partition时线性关系。partition的数量要根据吞吐来推断，假定p代表生产者写入单个partition的最大吞吐，c代表消费者从单个partition消费的最大吞吐，我们的目标吞吐是t，那么partition的数量应该是t/p和t/c中较大的那一个。实际情况中，p的影响因素有批处理的规模，压缩算法，确认机制和副本数等，然而，多次benchmark的结果表明，单个partition的最大写入吞吐在10MB/sec左右；c的影响因素是逻辑算法，需要在不同场景下实测得出。</p><p>这个结论似乎太书生气和不实用。我们通常建议partition的数量一定要大于等于消费者的数量来实现最大并发。官方曾测试过1万个partition的情况，所以不需要太担心partition过多的问题。下面的知识会有助于读者在生产环境做出最佳的选择：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">a、一个partition就是一个存储kafka-log的目录。</div><div class="line">b、一个partition只能寄宿在一个broker上。</div><div class="line">c、单个partition是可以实现消息的顺序写入的。</div><div class="line">d、单个partition只能被单个消费者进程消费，与该消费者所属于的消费组无关。这样做，有助于实现顺序消费。</div><div class="line">e、单个消费者进程可同时消费多个partition，即partition限制了消费端的并发能力。</div><div class="line">f、partition越多则file和memory消耗越大，要在服务器承受服务器设置。</div><div class="line">g、每个partition信息都存在所有的zk节点中。</div><div class="line">h、partition越多则失败选举耗时越长。</div><div class="line">k、offset是对每个partition而言的，partition越多，查询offset就越耗时。</div><div class="line">i、partition的数量是可以动态增加的（只能加不能减）。</div></pre></td></tr></table></figure><p>我们建议的做法是，如果是3个broker的集群，有5个消费者，那么建议partition的数量是15，也就是broker和consumer数量的最小公倍数。当然，也可以是一个大于消费者的broker数量的倍数，比如6或者9，还请读者自行根据实际环境裁定。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn2.hubspot.net/hubfs/540072/Kafka_Connect_graphic.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Kafka集群稳定&quot;&gt;&lt;a href=&quot;#Kafka集群稳定&quot; class=&quot;headerlink&quot; title=&quot;Kafka集群稳定&quot;&gt;&lt;/a&gt;Kafka集群稳定&lt;/h3&gt;&lt;p&gt;GC调优&lt;br&gt;　　调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent，具体参数设置可以参考Google，这里不赘述。&lt;/p&gt;
&lt;p&gt;　　Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent 分别设置为 20毫秒和 35。Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入。&lt;/p&gt;
    
    </summary>
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/categories/Big-data-Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka日志存储解析与实践数据存储优化</title>
    <link href="http://blog.yancy.cc/2017/06/30/Bigdata-hadoop/Kafka/Kafka%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8%E8%A7%A3%E6%9E%90%E4%B8%8E%E5%AE%9E%E8%B7%B5%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%BC%98%E5%8C%96/"/>
    <id>http://blog.yancy.cc/2017/06/30/Bigdata-hadoop/Kafka/Kafka日志存储解析与实践数据存储优化/</id>
    <published>2017-06-30T06:46:00.000Z</published>
    <updated>2017-09-21T15:10:17.000Z</updated>
    
    <content type="html"><![CDATA[<p><figure class="figure"><img src="https://cdn-images-1.medium.com/max/640/1*kV768OHeCILNIYVCNPUyuQ.jpeg" alt=""></figure></p><h4 id="Kafka的名词解释"><a href="#Kafka的名词解释" class="headerlink" title="Kafka的名词解释"></a>Kafka的名词解释</h4><p>kafka是一款分布式消息发布和订阅的系统，具有高性能和高吞吐率。 </p><ul><li>1，<code>Broker</code>： 一个单独的kafka机器节点就称为一个broker，多个broker组成的集群，称为kafka集群</li><li>2，<code>Topic</code>：类似数据库中的一个表，我们将数据存储在Topic里面，当然这只是逻辑上的，在物理上，一个Topic 可能被多个Broker分区存储，这对用户是透明的，用户只需关注消息的产生于消费即可.</li><li>3，<code>Partition</code>：类似分区表，每个Topic可根据设置将数据存储在多个整体有序的Partition中，每个顺序化partition会生成2个文件，一个是index文件一个是log文件，index文件存储索引和偏移量，log文件存储具体的数据.</li><li>4，<code>Producer</code>：生产者，向Topic里面发送消息的角色 </li><li>5，<code>Consumer</code>：消费者，从Topic里面读取消息的角色 </li><li>6，<code>Consumer Group</code>：每个Consumer属于一个特定的消费者组，可为Consumer指定group name，如果不指定默认属于group </li></ul><a id="more"></a><p><strong>集群安装略过~</strong></p><h4 id="日志存储"><a href="#日志存储" class="headerlink" title="日志存储"></a>日志存储</h4><p>Kafka的data是保存在文件系统中的。Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。</p><p>每个topic又可以分成几个不同的partition，每个topic有几个partition是在创建topic时指定的，每个partition存储一部分Message。</p><p><code>partition</code>是以文件的形式存储在文件系统中，比如，创建了一个名为<code>kakfa-node1</code>的topic，其有12个partition，那么在Kafka的数据目录中(由配置文件中的log.dirs指定的)中就有这样5个目录: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]$ ll</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-0</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-1</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-10</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-11</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-2</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-3</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-4</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-5</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-6</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-7</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-8</div><div class="line">drwxrwxr-x 2 jollybi jollybi  4096 Aug 23 15:56 kakfa-node1-9</div></pre></td></tr></table></figure><p>其命名规则为<code>&lt;topic_name&gt;-&lt;partition_id&gt;</code>，里面存储的分别就是这12个<code>partition</code>的数据。<br><code>zookeeper</code>会将分区平均分配创建到不同的<code>broker</code>上，例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 tools]$ ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic kakfa-node1</div><div class="line">Topic:kakfa-node1PartitionCount:12ReplicationFactor:3Configs:</div><div class="line">Topic: kakfa-node1Partition: 0Leader: 1Replicas: 1,2,3Isr: 1,2,3</div><div class="line">Topic: kakfa-node1Partition: 1Leader: 2Replicas: 2,3,1Isr: 2,3,1</div><div class="line">Topic: kakfa-node1Partition: 2Leader: 3Replicas: 3,1,2Isr: 3,1,2</div><div class="line">Topic: kakfa-node1Partition: 3Leader: 1Replicas: 1,3,2Isr: 1,3,2</div><div class="line">Topic: kakfa-node1Partition: 4Leader: 2Replicas: 2,1,3Isr: 2,1,3</div><div class="line">Topic: kakfa-node1Partition: 5Leader: 3Replicas: 3,2,1Isr: 3,2,1</div><div class="line">Topic: kakfa-node1Partition: 6Leader: 1Replicas: 1,2,3Isr: 1,2,3</div><div class="line">Topic: kakfa-node1Partition: 7Leader: 2Replicas: 2,3,1Isr: 2,3,1</div><div class="line">Topic: kakfa-node1Partition: 8Leader: 3Replicas: 3,1,2Isr: 3,1,2</div><div class="line">Topic: kakfa-node1Partition: 9Leader: 1Replicas: 1,3,2Isr: 1,3,2</div><div class="line">Topic: kakfa-node1Partition: 10Leader: 2Replicas: 2,1,3Isr: 2,1,3</div><div class="line">Topic: kakfa-node1Partition: 11Leader: 3Replicas: 3,2,1Isr: 3,2,1</div></pre></td></tr></table></figure><p>Isr表示分区创建在哪个broker上。<br>Partition中的每条Message由offset来表示它在这个partition中的偏移量，这个offset不是该Message在partition数据文件中的实际存储位置，而是逻辑上一个值，它唯一确定了partition中的一条Message。因此，可以认为offset是partition中Message的id。partition中的每条Message包含了以下三个属性：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="title">offset</span></div><div class="line"><span class="type">MessageSize</span></div><div class="line"><span class="class"><span class="keyword">data</span></span></div></pre></td></tr></table></figure><p>其中offset为long型，MessageSize为int32，表示data有多大，data为message的具体内容。</p><h4 id="Kafka通过分段和索引的方式来提高查询效率"><a href="#Kafka通过分段和索引的方式来提高查询效率" class="headerlink" title="Kafka通过分段和索引的方式来提高查询效率"></a>Kafka通过分段和索引的方式来提高查询效率</h4><ul><li>1）分段</li></ul><p>Kafka解决查询效率的手段之一是将数据文件分段，比如有100条Message，它们的offset是从0到99。假设将数据文件分成5段，第一段为0-19，第二段为20-39，以此类推，每段放在一个单独的数据文件里面，数据文件以该段中最小的offset命名。这样在查找指定offset的Message的时候，用二分查找就可以定位到该Message在哪个段中。</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]$ ll /data/tools/kafka_2.10-0.8.2.1/kafka-logs/</div><div class="line">total 548</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-0</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-1</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-10</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-11</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-2</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-3</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-4</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-5</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-6</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-7</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-8</div><div class="line">drwxrwxr-x<span class="number"> 2 </span>jollybi jollybi <span class="number"> 4096 </span>Aug<span class="number"> 25 </span>11:03 kafkanode-9</div></pre></td></tr></table></figure><h5 id="2）为数据文件建索引"><a href="#2）为数据文件建索引" class="headerlink" title="2）为数据文件建索引"></a>2）为数据文件建索引</h5><p>数据文件分段使得可以在一个较小的数据文件中查找对应offset的Message了，但是这依然需要顺序扫描才能找到对应<code>offset的Message</code>。为了进一步提高查找的效率，Kafka为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩展名为.index。<br>索引文件中包含若干个索引条目，每个条目表示数据文件中一条Message的索引。索引包含两个部分（均为4个字节的数字），分别为相对<code>offset和position</code>。</p><p>相对<code>offset</code>：因为数据文件分段以后，每个数据文件的起始offset不为0，相对offset表示这条Message相对于其所属数据文件中最小的offset的大小。举例，分段后的一个数据文件的offset是从20开始，那么offset为25的Message在index文件中的相对offset就是25-20 = 5。存储相对offset可以减小索引文件占用的空间。</p><p><code>position</code>，表示该条<code>Message</code>在数据文件中的绝对位置。只要打开文件并移动文件指针到这个position就可以读取对应的<code>Message</code>了。<br>index文件中并没有为数据文件中的每条Message建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/640/1*kV768OHeCILNIYVCNPUyuQ.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;Kafka的名词解释&quot;&gt;&lt;a href=&quot;#Kafka的名词解释&quot; class=&quot;headerlink&quot; title=&quot;Kafka的名词解释&quot;&gt;&lt;/a&gt;Kafka的名词解释&lt;/h4&gt;&lt;p&gt;kafka是一款分布式消息发布和订阅的系统，具有高性能和高吞吐率。 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1，&lt;code&gt;Broker&lt;/code&gt;： 一个单独的kafka机器节点就称为一个broker，多个broker组成的集群，称为kafka集群&lt;/li&gt;
&lt;li&gt;2，&lt;code&gt;Topic&lt;/code&gt;：类似数据库中的一个表，我们将数据存储在Topic里面，当然这只是逻辑上的，在物理上，一个Topic 可能被多个Broker分区存储，这对用户是透明的，用户只需关注消息的产生于消费即可.&lt;/li&gt;
&lt;li&gt;3，&lt;code&gt;Partition&lt;/code&gt;：类似分区表，每个Topic可根据设置将数据存储在多个整体有序的Partition中，每个顺序化partition会生成2个文件，一个是index文件一个是log文件，index文件存储索引和偏移量，log文件存储具体的数据.&lt;/li&gt;
&lt;li&gt;4，&lt;code&gt;Producer&lt;/code&gt;：生产者，向Topic里面发送消息的角色 &lt;/li&gt;
&lt;li&gt;5，&lt;code&gt;Consumer&lt;/code&gt;：消费者，从Topic里面读取消息的角色 &lt;/li&gt;
&lt;li&gt;6，&lt;code&gt;Consumer Group&lt;/code&gt;：每个Consumer属于一个特定的消费者组，可为Consumer指定group name，如果不指定默认属于group &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/categories/Big-data-Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-countly需要迁移</title>
    <link href="http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/countly/Bigdata-countly%E9%9C%80%E8%A6%81%E8%BF%81%E7%A7%BB/"/>
    <id>http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/countly/Bigdata-countly需要迁移/</id>
    <published>2017-06-29T06:46:00.000Z</published>
    <updated>2017-09-19T08:16:53.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="官方安装文档：http-resources-count-ly-docs-installing-countly-server"><a href="#官方安装文档：http-resources-count-ly-docs-installing-countly-server" class="headerlink" title="官方安装文档：http://resources.count.ly/docs/installing-countly-server"></a>官方安装文档：<a href="http://resources.count.ly/docs/installing-countly-server" target="_blank" rel="external">http://resources.count.ly/docs/installing-countly-server</a></h4><p>目前<code>countly</code>需要迁移，所需<code>countly</code>版本于官方提供的安装方案有冲突，所以如下安装：</p><p>安装官方<code>countly</code>让其设置所需环境变量及其启动脚本，手动指定安装nojs版本，拷贝原countly文件，具体如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1、<span class="built_in">cd</span> /data <span class="comment">#countly安装在data目录 我看了安装脚本，是当前在哪个目录，安装文件就在哪个目录</span></div><div class="line">wget -qO- http://c.ly/install | bash</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2、rpm -qa | grep -i nodejs | xargs -I&#123;&#125;  yum remove &#123;&#125; -y</div><div class="line">卸载掉官网安装的最新nodejs 然后新建如下yum源，用于安装旧版所需nodejs，也可以到nodejs官网下载所需nodejs</div><div class="line">cat /etc/yum.repos.d/nodesource-el.repo </div><div class="line">[nodesource]</div><div class="line">name=Node.js Packages <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span></div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/<span class="variable">$basearch</span></div><div class="line">failovermethod=priority</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">[nodesource-source]</div><div class="line">name=Node.js <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span> - Source</div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/SRPMS</div><div class="line">failovermethod=priority</div><div class="line">enabled=0</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">gpgcheck=1</div></pre></td></tr></table></figure><h3 id="安装老版本所需nodejs"><a href="#安装老版本所需nodejs" class="headerlink" title="安装老版本所需nodejs"></a>安装老版本所需nodejs</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum <span class="keyword">install</span> nodejs</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">3、拷贝源countly文件到/data/countly目录</div><div class="line">修改 /data/countly/api/config.js 和 /data/countly/frontend/express/config.js      </div><div class="line">3001端口和 6001端口监听地址换成 本地私有地址   <span class="comment">#源文件是监听的原来机器的内网地址，不修改的话，服务启动不了。</span></div></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="number">4</span><span class="string">、拷贝mongo数据目录到/data/mongo目录，修改mongo配置文件.</span></div><div class="line"></div><div class="line"><span class="string">cat</span> <span class="string">/etc/mongod.conf</span></div><div class="line"><span class="attr">systemLog:</span></div><div class="line"><span class="attr">       destination:</span> <span class="string">file</span></div><div class="line"><span class="attr">       logAppend:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       path:</span> <span class="string">/data/mongodb/mongod.log</span></div><div class="line"><span class="attr">storage:</span></div><div class="line"><span class="attr">       dbPath:</span> <span class="string">/data/mongo</span></div><div class="line"><span class="attr">       journal:</span></div><div class="line"><span class="attr">             enabled:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       engine:</span> <span class="string">mmapv1</span></div><div class="line"><span class="attr">processManagement:</span></div><div class="line"><span class="attr">       fork:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       pidFilePath:</span> <span class="string">/data/mongodb/mongod.pid</span></div><div class="line"><span class="attr">net:</span></div><div class="line"><span class="attr">       port:</span> <span class="number">27017</span></div><div class="line"><span class="attr">       bindIp:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></div><div class="line"><span class="attr">security:</span></div><div class="line"><span class="attr">       authorization:</span> <span class="string">enabled</span></div><div class="line"><span class="attr">operationProfiling:</span></div><div class="line"><span class="attr">       slowOpThresholdMs:</span> <span class="number">40960</span></div></pre></td></tr></table></figure><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Bigdatacountly01.jpeg" alt=""></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">5、修改硬盘block：</div><div class="line">     blockdev --setra 256 /dev/mapper/xvdc--vg-xvdc–lv    <span class="comment">##按照mongo提示操作</span></div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">6、修改nginx配置文件  conf.d/default.conf</div><div class="line">      将127.0.0.1修改为本地私有地址</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">7、重启countly   mongodb  nginx</div><div class="line"></div><div class="line">countly restart</div><div class="line"></div><div class="line">/etc/init.d/mongod restart</div><div class="line"></div><div class="line">service nginx restart</div><div class="line">迁移完毕</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;官方安装文档：http-resources-count-ly-docs-installing-countly-server&quot;&gt;&lt;a href=&quot;#官方安装文档：http-resources-count-ly-docs-installing-countly-serv
      
    
    </summary>
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/categories/Big-data-Hadoop/"/>
    
    
      <category term="Countly" scheme="http://blog.yancy.cc/tags/Countly/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-Kafka-node模块实现调用js发送数据</title>
    <link href="http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/Kafka/Kafka-node%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E8%B0%83%E7%94%A8js%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE/"/>
    <id>http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/Kafka/Kafka-node模块实现调用js发送数据/</id>
    <published>2017-06-29T06:46:00.000Z</published>
    <updated>2017-09-21T15:12:59.000Z</updated>
    
    <content type="html"><![CDATA[<p><figure class="figure"><img src="https://kafka.apache.org/0110/images/streams-interactive-queries-02.png" alt=""></figure></p><p>mongodb写到kafka 指定topic消费。为了保证数据稳定可靠性。<br>配合大数据在countly 使用开源<code>Kafka-node</code>是一个Node.js客户端 写js程序让countly三台集群分别数据到kafka 做新的topic主题备份。</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install kafka-node</div></pre></td></tr></table></figure><p>进入kafka-node目录: vim kafka_test.js</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">var kafka = require(<span class="string">'kafka-node'</span>),</div><div class="line">HighLevelProducer = kafka.HighLevelProducer,</div><div class="line">    //Producer = kafka.Producer,</div><div class="line">    client = new kafka.Client(<span class="string">'169.44.62.139:2281,169.44.59.138:2281,169.44.62.137:2281'</span>),</div><div class="line">    //producer = new Producer(client);</div><div class="line">producer = new HighLevelProducer(client);</div><div class="line"></div><div class="line">console.log(<span class="string">'连接kafka中'</span>);</div><div class="line"></div><div class="line">var argv = &#123;</div><div class="line">    topic: <span class="string">"test1"</span></div><div class="line">&#125;;</div><div class="line">var topic = argv.topic || <span class="string">'test1'</span>;</div><div class="line">var p = argv.p || 0;</div><div class="line">var a = argv.a || 0;</div><div class="line">var producer = new HighLevelProducer(client, &#123;</div><div class="line">    requireAcks: 1,</div><div class="line">    partitionerType: 3</div><div class="line">&#125;);</div><div class="line"></div><div class="line">console.log(producer);</div><div class="line"></div><div class="line">producer.on(<span class="string">'ready'</span>, <span class="function"><span class="title">function</span></span>() &#123;</div><div class="line">    var args = &#123;</div><div class="line">        appid: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        createTime: <span class="string">'222-ddd'</span>,</div><div class="line">        toUserName: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        fromUserName: <span class="string">'222-wx238c28839a133d0e'</span></div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    producer.send([&#123;</div><div class="line">        topic: topic,</div><div class="line">        partition: p,</div><div class="line">        messages: [JSON.stringify(args)],</div><div class="line">        attributes: a</div><div class="line">    &#125;], <span class="keyword">function</span>(err, result) &#123;</div><div class="line">        console.log(err || result);</div><div class="line">        process.exit();</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    console.log(args);</div><div class="line">&#125;);</div></pre></td></tr></table></figure><h4 id="官网地址：https-www-npmjs-com-package-kafka-node-install-kafka"><a href="#官网地址：https-www-npmjs-com-package-kafka-node-install-kafka" class="headerlink" title="官网地址：https://www.npmjs.com/package/kafka-node#install-kafka"></a>官网地址：<a href="https://www.npmjs.com/package/kafka-node#install-kafka" target="_blank" rel="external">https://www.npmjs.com/package/kafka-node#install-kafka</a></h4>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://kafka.apache.org/0110/images/streams-interactive-queries-02.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;mongodb写到kafka 指定topic消费。为了保证数据稳定可靠性。&lt;br&gt;配合大数据在countly 使用开源&lt;code&gt;Kafka-node&lt;/code&gt;是一个Node.js客户端 写js程序让countly三台集群分别数据到kafka 做新的topic主题备份。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/tags/Big-data-Hadoop/"/>
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-Kafka三款监控工具比较</title>
    <link href="http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/Kafka/Bigdata-Kafka%E4%B8%89%E6%AC%BE%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E6%AF%94%E8%BE%83/"/>
    <id>http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/Kafka/Bigdata-Kafka三款监控工具比较/</id>
    <published>2017-06-29T06:46:00.000Z</published>
    <updated>2017-08-23T10:44:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>在之前的博客中，介绍了<code>Kafka Web Console</code>这个监控工具，在生产环境中使用，运行一段时间后，发现该工具会和Kafka生产者、消费者、ZooKeeper建立大量连接，从而导致网络阻塞。并且这个Bug也在其他使用者中出现过，看来使用开源工具要慎重！该Bug暂未得到修复，不得已，只能研究下其他同类的Kafka监控软件。</p><p>通过研究，发现主流的三种kafka监控程序分别为：</p><ul><li>1、Kafka Web Conslole</li><li>2、Kafka Manager</li><li>3、KafkaOffsetMonitor</li></ul><p>现在依次介绍以上三种工具：</p><h2 id="1、Kafka-Web-Conslole"><a href="#1、Kafka-Web-Conslole" class="headerlink" title="1、Kafka Web Conslole"></a>1、Kafka Web Conslole</h2><p>使用Kafka Web Console，可以监控：</p><ul><li>Brokers列表</li><li>Kafka 集群中 Topic列表，及对应的Partition、LogSiz e等信息</li><li>点击Topic，可以浏览对应的Consumer Groups、Offset、Lag等信息</li><li>生产和消费流量图、消息预览…</li></ul><p>程序运行后，会定时去读取kafka集群分区的日志长度，读取完毕后，连接没有正常释放，一段时间后产生大量的socket连接，导致网络堵塞。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Kafka_web_console.png" alt=""></figure></p><h2 id="2、Kafka-Manager"><a href="#2、Kafka-Manager" class="headerlink" title="2、Kafka Manager"></a>2、Kafka Manager</h2><p>雅虎开源的Kafka集群管理工具:</p><ul><li>管理几个不同的集群</li><li>监控集群的状态(topics, brokers, 副本分布, 分区分布)</li><li>产生分区分配(Generate partition assignments)基于集群的当前状态</li><li>重新分配分区</li></ul><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka_manager.png" alt=""></figure></p><h2 id="3、KafkaOffsetMonitor"><a href="#3、KafkaOffsetMonitor" class="headerlink" title="3、KafkaOffsetMonitor"></a>3、KafkaOffsetMonitor</h2><ul><li>KafkaOffsetMonitor可以实时监控：</li><li>Kafka集群状态</li><li>Topic、Consumer Group列表</li><li>图形化展示topic和consumer之间的关系</li><li>图形化展示consumer的Offset、Lag等信息</li></ul><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafkaoffsetmonitor.png" alt=""></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过使用，个人总结以上三种监控程序的优缺点：</p><p><a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">Kafka Web Console</a>：监控功能较为全面，可以预览消息，监控Offset、Lag等信息，但存在bug，不建议在生产环境中使用。</p><p><a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="external">Kafka Manager</a>：偏向Kafka集群管理，若操作不当，容易导致集群出现故障。对Kafka实时生产和消费消息是通过JMX实现的。没有记录Offset、Lag等信息。</p><p><a href="https://github.com/quantifind/KafkaOffsetMonitor" target="_blank" rel="external">KafkaOffsetMonitor</a>：程序一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。</p><p>若只需要监控功能，推荐使用KafkaOffsetMonito，若偏重Kafka集群管理，推荐使用Kafka Manager。</p><p>因为都是开源程序，稳定性欠缺。故需先了解清楚目前已存在哪些Bug，多测试一下，避免出现类似于Kafka Web Console的问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在之前的博客中，介绍了&lt;code&gt;Kafka Web Console&lt;/code&gt;这个监控工具，在生产环境中使用，运行一段时间后，发现该工具会和Kafka生产者、消费者、ZooKeeper建立大量连接，从而导致网络阻塞。并且这个Bug也在其他使用者中出现过，看来使用开源工具
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/tags/Big-data-Hadoop/"/>
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-如何手动更新Kafka中某个Topic的偏移量</title>
    <link href="http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/Kafka/%E5%A6%82%E4%BD%95%E6%89%8B%E5%8A%A8%E6%9B%B4%E6%96%B0Kafka%E4%B8%AD%E6%9F%90%E4%B8%AATopic%E7%9A%84%E5%81%8F%E7%A7%BB%E9%87%8F/"/>
    <id>http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/Kafka/如何手动更新Kafka中某个Topic的偏移量/</id>
    <published>2017-06-29T06:46:00.000Z</published>
    <updated>2017-08-04T04:06:04.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何手动更新Kafka中某个Topic的偏移量"><a href="#如何手动更新Kafka中某个Topic的偏移量" class="headerlink" title="如何手动更新Kafka中某个Topic的偏移量"></a>如何手动更新Kafka中某个Topic的偏移量</h1><p>　　我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如iteblog主题分区10的偏移量获取如下：<br>　　<br>　　在有些场景下，这个工具不满足我们的需求，我们需要的是能够手动设置分区的偏移量为任何有意义的值，而不仅仅是earliest或者latest。那咋办？</p><p>　　我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如<code>mongotail_lz4</code>主题分区10的偏移量获取如下：　　</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 2]  get /consumers/ibm_event/offsets/mongotail_lz4/10</div><div class="line">293894</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x6000018c9</div><div class="line">mtime = Wed Jul 26 18:18:27 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 20</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 6</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure><p>所以，我们可以通过set命令来设置某个分区的偏移量，如下；</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECT get /consumers/ibm_event/offsets/mongotail_lz4/10 0 </div><div class="line">0</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x60000204c</div><div class="line">mtime = Wed Jul 26 18:37:21 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 21</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 1</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure><p>12个分区分别更新过去。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;如何手动更新Kafka中某个Topic的偏移量&quot;&gt;&lt;a href=&quot;#如何手动更新Kafka中某个Topic的偏移量&quot; class=&quot;headerlink&quot; title=&quot;如何手动更新Kafka中某个Topic的偏移量&quot;&gt;&lt;/a&gt;如何手动更新Kafka中某个Top
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/tags/Big-data-Hadoop/"/>
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-开源的Kafka集群管理器(Kafka Manager)</title>
    <link href="http://blog.yancy.cc/2017/06/28/Bigdata-hadoop/Kafka/%E5%BC%80%E6%BA%90%E7%9A%84Kafka%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%99%A8(Kafka%20Manager)/"/>
    <id>http://blog.yancy.cc/2017/06/28/Bigdata-hadoop/Kafka/开源的Kafka集群管理器(Kafka Manager)/</id>
    <published>2017-06-28T06:46:00.000Z</published>
    <updated>2017-08-07T03:00:38.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kafka-Manager"><a href="#Kafka-Manager" class="headerlink" title="Kafka Manager"></a>Kafka Manager</h2><p>A tool for managing <a href="http://kafka.apache.org/" target="_blank" rel="external">Apache Kafka.</a></p><h4 id="It-supports-the-following"><a href="#It-supports-the-following" class="headerlink" title="It supports the following :"></a>It supports the following :</h4><ul><li>管理多个群集</li><li>容易检查集群状态（主题，消费者，偏移量，经纪人，副本分发，分区分配）</li><li>运行首选副本选举</li><li>使用选项生成分区分配，以选择要使用的代理</li><li>运行分区的重新分配（基于生成的分配）</li><li>创建可选主题配置的主题（0.8.1.1具有不同于0.8.2+的配置）</li><li>删除主题（仅支持0.8.2+，并记住在代理配 置中设置delete.topic.enable = true）</li><li>主题列表现在表示标记为删除的主题（仅支持0.8.2+）</li><li>批量生成多个主题的分区分配，并选择要使用的代理</li><li>批量运行多个主题的分区重新分配</li><li>将分区添加到现有主题</li><li>更新现有主题的配置</li><li>可选地，启用JMX轮询代理级和主题级度量。</li><li>可选地筛选出在zookeeper中没有ids / owner /＆offset /目录的消费者。</li></ul><p>参考开源地址：<a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="external">https://github.com/yahoo/kafka-manager</a></p><h4 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h4><p>Kafka 0.8.1.1 or 0.8.2.<em> or 0.9.0.</em> or 0.10.0.*<br>Java 8+</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo wget --header <span class="string">"Cookie: oraclelicense=accept-securebackup-cookie”   http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz</span></div><div class="line"></div><div class="line">sudo vim /etc/profile</div><div class="line">export JAVA_HOME=/home/jollybi/tools/jdk1.8.0_144</div><div class="line">export LASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib:<span class="variable">$JAVA_HOME</span>/bin</div><div class="line">export PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JAVA_HOME</span>/jre/bin:<span class="variable">$TOMCAT_HOME</span>/bin:<span class="variable">$PATH</span></div></pre></td></tr></table></figure><h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/yahoo/kafka-manager.git</div><div class="line">./sbt clean dist</div><div class="line"></div><div class="line">[info]   Compilation completed <span class="keyword">in</span> 13.366 s</div><div class="line">model contains 672 documentable templates</div><div class="line">[info] Main Scala API documentation successful.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-javadoc.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info] Packaging /home/jollybi/kafka-manager/target/scala-2.11/kafka-manager_2.11-1.3.3.8-sans-externalized.jar ...</div><div class="line">[info] Done packaging.</div><div class="line">[info]</div><div class="line">[info] Your package is ready <span class="keyword">in</span> /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip</div><div class="line">[info]</div><div class="line">[success] Total time: 142 s, completed Jul 27, 2017 3:48:35 PM</div><div class="line">完成</div></pre></td></tr></table></figure><h4 id="Starting-the-service"><a href="#Starting-the-service" class="headerlink" title="Starting the service"></a>Starting the service</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">解压缩生成的zip文件后，将工作目录更改为可以运行的服务：</div><div class="line"></div><div class="line">unzip /home/jollybi/kafka-manager/target/universal/kafka-manager-1.3.3.8.zip</div><div class="line"></div><div class="line"></div><div class="line">修改zk地址和管理员账号和密码：</div><div class="line"></div><div class="line">vim kafka-manager-1.3.3.8/conf/application.conf</div><div class="line"></div><div class="line"><span class="comment">#kafka-manager.zkhosts="kafka-manager-zookeeper:2181"</span></div><div class="line"><span class="comment">#zk集群可以这么配置：</span></div><div class="line">kafka-manager.zkhosts=<span class="string">"kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#根据个人公司这里可以开启true 设置账号和密码</span></div><div class="line">basicAuthentication.enabled=<span class="literal">true</span></div><div class="line">basicAuthentication.username=<span class="string">"admin"</span></div><div class="line">basicAuthentication.password=<span class="string">"admin"</span></div><div class="line"></div><div class="line"></div><div class="line">默认情况下，它将选择端口9000.这是可以覆盖的，配置文件的位置也是如此。例如：</div><div class="line"></div><div class="line">$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080</div><div class="line"></div><div class="line">后台生效：</div><div class="line"></div><div class="line">$ ./bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;</div><div class="line"></div><div class="line">再次，如果java不在您的路径中，或者您需要针对不同版本的Java运行，请按如下所示添加-java-home选项：</div><div class="line"></div><div class="line">$ bin/kafka-manager -java-home /usr/<span class="built_in">local</span>/oracle-java-8</div></pre></td></tr></table></figure><h4 id="Packaging"><a href="#Packaging" class="headerlink" title="Packaging"></a>Packaging</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">If you<span class="string">'d like to create a Debian or RPM package instead, you can run one of:</span></div><div class="line"></div><div class="line">sbt debian:packageBin</div><div class="line"></div><div class="line">sbt rpm:packageBin</div></pre></td></tr></table></figure><h3 id="查看端口："><a href="#查看端口：" class="headerlink" title="查看端口："></a>查看端口：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">[jollybi@kafka1 conf]$ netstat -ntulp | grep 8080</div><div class="line">(Not all processes could be identified, non-owned process info</div><div class="line"> will not be shown, you would have to be root to see it all.)</div><div class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      70517/java</div></pre></td></tr></table></figure><h3 id="网站访问kafka-Manger"><a href="#网站访问kafka-Manger" class="headerlink" title="网站访问kafka Manger"></a>网站访问kafka Manger</h3><p>这里我设置了登录账号和密码： admin admin</p><p><figure class="figure"><img src="media/15014722977191.jpg" alt=""></figure></p><p>创建kafka名字;<br>选择kafka版本号;<br>JMX这个不需要;<br>下面选择默认点击确认即可.</p><p><figure class="figure"><img src="media/15014724051756.jpg" alt=""></figure></p><blockquote><p>(2)kafka 启用 JMX端口</p></blockquote><p><figure class="figure"><img src="media/15020729297106.jpg" alt=""></figure></p><figure class="highlight vbscript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">以如下命令重新启动kafka</div><div class="line"></div><div class="line">JMX_PORT=<span class="number">9999</span> bin/kafka-<span class="built_in">server</span>-start.sh config/<span class="built_in">server</span>.properties</div><div class="line">或者修改kafka-<span class="built_in">server</span>-start.sh 文件，追加JMX_PORT=<span class="string">"9999"</span></div><div class="line"></div><div class="line"> <span class="keyword">if</span> [ <span class="string">"x$KAFKA_HEAP_OPTS"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></div><div class="line">    export KAFKA_HEAP_OPTS=<span class="string">"-Xmx1G -Xms1G"</span></div><div class="line">    export JMX_PORT=<span class="string">"9999"</span></div><div class="line">fi</div><div class="line">然后重新启动kafka</div><div class="line">bin/kafka-<span class="built_in">server</span>-start.sh config/<span class="built_in">server</span>.properties</div><div class="line"></div><div class="line">但是Metrics中数据都是零</div><div class="line">查看 kafka manager 报错，无法连接jxm</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">解决方法 修改每个kafka broker的 kafka_2.11-0.10.1.0/bin/kafka-run-class.sh文件</div><div class="line">​</div><div class="line"><span class="comment"># JMX settings</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$KAFKA_JMX_OPTS</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  KAFKA_JMX_OPTS=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=75.126.5.162"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"></div><div class="line">-Djava.rmi.server.hostname 的值为当前kafka服务器ip</div><div class="line"></div><div class="line">这里说明下集群kafka都需要修改</div></pre></td></tr></table></figure><p><figure class="figure"><img src="media/15020745879851.jpg" alt=""></figure></p><p><figure class="figure"><img src="media/15014725443386.jpg" alt=""></figure></p><p><figure class="figure"><img src="media/15014725288425.jpg" alt=""></figure></p><p><figure class="figure"><img src="media/15014725659340.jpg" alt=""></figure></p><p><figure class="figure"><img src="media/15014725864134.jpg" alt=""></figure></p><p><figure class="figure"><img src="media/15014726361909.jpg" alt=""></figure></p><p><figure class="figure"><img src="media/15014727525618.jpg" alt=""></figure></p><p><figure class="figure"><img src="media/15014727932029.jpg" alt=""></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Kafka-Manager&quot;&gt;&lt;a href=&quot;#Kafka-Manager&quot; class=&quot;headerlink&quot; title=&quot;Kafka Manager&quot;&gt;&lt;/a&gt;Kafka Manager&lt;/h2&gt;&lt;p&gt;A tool for managing &lt;a hre
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/tags/Big-data-Hadoop/"/>
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-开源的Kafka集群管理器(kafka-web-console)</title>
    <link href="http://blog.yancy.cc/2017/06/28/Bigdata-hadoop/Kafka/Bigdata-%E5%BC%80%E6%BA%90%E7%9A%84Kafka%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%99%A8(kafka-web-console)/"/>
    <id>http://blog.yancy.cc/2017/06/28/Bigdata-hadoop/Kafka/Bigdata-开源的Kafka集群管理器(kafka-web-console)/</id>
    <published>2017-06-28T06:46:00.000Z</published>
    <updated>2017-09-19T08:34:15.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>源码的地址在:<a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">kafka-web-console</a></p><p><code>Kafka Web Console</code>也是用Scala语言编写的<code>Java web</code>程序用于监控<code>Apache Kafka</code>。这个系统的功能和<code>KafkaOffsetMonitor</code>很类似，但是我们从源码角度来看，这款系统实现比<code>KafkaOffsetMonitor</code>要复杂很多，而且编译配置比<code>KafkaOffsetMonitor</code>较麻烦。</p><p>　要想运行这套系统我们需要的先行条件为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Play Framework 2.2.x</div><div class="line">Apache Kafka 0.8.x</div><div class="line">Zookeeper 3.3.3 or 3.3.4</div></pre></td></tr></table></figure><h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><p>同样，我们从<code>https://github.com/claudemamo/kafka-web-console</code>上面将源码下载下来，然后用<code>sbt</code>进行编译，在编译前我们需要做如下的修改：</p><p>Kafka Web控制台需要一个关系数据库。默认情况下，服务器连接到嵌入式H2数据库，不需要数据库安装或配置。请咨询Play！的文档以指定控制台的数据库。支持以下数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/claudemamo/kafka-web-console.git</div></pre></td></tr></table></figure><ul><li>H2（默认）</li><li>PostgreSql</li><li>Oracle</li><li>DB2</li><li>MySQL</li><li>Apache Derby</li><li>Microsoft SQL Server</li></ul><p>为了方便，我们可以使用Mysql数据库，只要做如下修改即可，找到 <code>conf/application.conf</code>文件，并修改如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">将这个</div><div class="line">db.default.driver=org.h2.Driver</div><div class="line">db.default.url=<span class="string">"jdbc:h2:file:play"</span></div><div class="line"><span class="comment"># db.default.user=sa</span></div><div class="line"><span class="comment"># db.default.password=""</span></div><div class="line"> </div><div class="line"> </div><div class="line">修改成</div><div class="line">db.default.driver=com.mysql.jdbc.Driver</div><div class="line">db.default.url=<span class="string">"jdbc:mysql://localhost:3306/kafkamonitor"</span></div><div class="line">db.default.user=iteblog</div><div class="line">db.default.pass=wyp</div></pre></td></tr></table></figure><p>我们还需要修改build.sbt，加入对Mysql的依赖:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="string">"mysql"</span> % <span class="string">"mysql-connector-java"</span> % <span class="string">"5.1.31"</span></div></pre></td></tr></table></figure><p>　2、执行<code>conf/evolutions/default/bak</code>目录下面的<code>1.sql、2.sql和3.sql</code>三个文件。需要注意的是，这三个sql文件不能直接运行，有语法错误，需要做一些修改。<br>上面的注意事项弄完之后，我们就可以编译下载过来的源码：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> sbt package</span></div></pre></td></tr></table></figure><p>编译的过程比较慢，有些依赖包下载速度非常地慢，请耐心等待。<br>　在编译的过程中，可能会出现有些依赖包无法下载，如下错误：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">[warn] module not found: com.typesafe.play<span class="comment">#sbt-plugin;2.2.1</span></div><div class="line">[warn] ==== typesafe-ivy-releases: tried</div><div class="line">[warn] http://repo.typesafe.com/typesafe/ivy-releases/</div><div class="line">com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== sbt-plugin-releases: tried</div><div class="line">[warn] http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/</div><div class="line">com.typesafe.play/sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== <span class="built_in">local</span>: tried</div><div class="line">[warn] /home/iteblog/.ivy2/<span class="built_in">local</span>/com.typesafe.play/</div><div class="line">sbt-plugin/scala_2.9.2/sbt_0.12/2.2.1/ivys/ivy.xml</div><div class="line">[warn] ==== Typesafe repository: tried</div><div class="line">[warn] http://repo.typesafe.com/typesafe/releases/com/</div><div class="line">typesafe/play/sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom</div><div class="line">[warn] ==== public: tried</div><div class="line">[warn] http://repo1.maven.org/maven2/com/typesafe/play/</div><div class="line">sbt-plugin_2.9.2_0.12/2.2.1/sbt-plugin-2.2.1.pom</div><div class="line">[warn] ::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">==== <span class="built_in">local</span>: tried</div><div class="line"> </div><div class="line">/home/iteblog/.ivy2/<span class="built_in">local</span>/org.scala-sbt/collections/0.13.0/jars/collections.jar</div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">:: FAILED DOWNLOADS ::</div><div class="line"> </div><div class="line">:: ^ see resolution messages <span class="keyword">for</span> details ^ ::</div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div><div class="line"> </div><div class="line">:: org.scala-sbt<span class="comment">#collections;0.13.0!collections.jar</span></div><div class="line"> </div><div class="line">::::::::::::::::::::::::::::::::::::::::::::::</div></pre></td></tr></table></figure><p>我们可以手动地下载相关依赖，并放到类似<code>/home/iteblog/.ivy2/local/org.scala-sbt/collections/0.13.0/jars/</code>目录下面。然后再编译就可以了。</p><p>　　最后，我们可以通过下面命令启动<code>Kafka Web Console</code>监控系统：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> sbt run</span></div></pre></td></tr></table></figure><p>并可以在<a href="http://localhost:9000" target="_blank" rel="external">http://localhost:9000</a> 查看下面是一张效果图</p><p><figure class="figure"><img src="https://www.iteblog.com/pic/topics.png" alt=""></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;源码的地址在:&lt;a href=&quot;https://github.com/claudemamo/kafka-web-console&quot; targe
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Zabbix监控Kafka集群 Brokers服务</title>
    <link href="http://blog.yancy.cc/2017/06/01/Bigdata-hadoop/Zabbix%E7%9B%91%E6%8E%A7Kafka%E9%9B%86%E7%BE%A4%20Brokers%E6%9C%8D%E5%8A%A1/"/>
    <id>http://blog.yancy.cc/2017/06/01/Bigdata-hadoop/Zabbix监控Kafka集群 Brokers服务/</id>
    <published>2017-06-01T09:56:03.000Z</published>
    <updated>2017-09-19T08:16:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Zabbix监控Kafka集群-Brokers服务"><a href="#Zabbix监控Kafka集群-Brokers服务" class="headerlink" title="Zabbix监控Kafka集群 Brokers服务"></a>Zabbix监控Kafka集群 Brokers服务</h2><h4 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h4><p>公司让我处理Hadoop及相关服务监控、报警这里主要讲kafka集群服务。这里我也看了几篇kafka相关文章，好文贴出来：</p><ul><li><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-1" target="_blank" rel="external">infoq kafka 入门了解</a></li><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/index.html" target="_blank" rel="external">kafka 工作原理介绍</a></li></ul><p>在我们公司主要Kafka 的几个特性非常满足我们的需求：可扩展性、数据分区、低延迟、处理大量不同消费者的能力。</p><p>而这里我想帮忙BI团队实现Kafka全面监控。分两点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1. 监控Kafka Brokers服务</div><div class="line">2. 监控Kafka Lag堆积数</div></pre></td></tr></table></figure><p>对于Kafka的监控，已经有现成的开源软件了，在我们公司内部也使用了一段时间，有两种方案。我们公司用第三种方案。<br>一般都会选择两个开源的工具：KafkaOffsetMonitor和kafka-web-console，这两款我都有用过.</p><ul><li>Kafka三款监控工具比较</li></ul><p>目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">1、Kafka Web Conslole</div><div class="line">2、Kafka Manager</div><div class="line">3、KafkaOffsetMonitor</div></pre></td></tr></table></figure><p><a href="https://github.com/quantifind/KafkaOffsetMonitor" target="_blank" rel="external">KafkaOffsetMonitor</a>：最大的好处就是配置简单，只需要配个zookeeper的地址就能用了，坑爹的地方就是不能自动刷新，手动刷新时耗时较长，而且有时候都刷不出来，另外就是图像用了一段时间就完全显示不了了，不知道大家是不是这样。</p><p><a href="https://github.com/claudemamo/kafka-web-console" target="_blank" rel="external">kafka-web-console</a>：相比与前者，数据是落地的，因此刷新较快，而且支持在前端自定义zookeeper的地址，还能列出实时的topic里的具体内容。但是搭建比较复杂，而且github上的默认数据库是H2的，像我们一般用mysql的，还得自己转化。另外在用的过程中，我遇到一个问题，在连接kafka的leader失败的时候，会一直重试，其结果就是导致我kafka的那台机子连接数过高，都到2w了，不知道是不是它的一个bug。</p><p>具体介绍以及安装在我另外篇博文。这里不详细讲解开源软件，这里用zabbix监控Kafka Brokers服务。</p><h4 id="kafka-monitoring-Brokers服务"><a href="#kafka-monitoring-Brokers服务" class="headerlink" title="kafka-monitoring Brokers服务"></a>kafka-monitoring Brokers服务</h4><p>操作系统环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Centos 7.1 </div><div class="line">64G内存</div><div class="line">2T磁盘空间，指定kafka写入数据目录。</div></pre></td></tr></table></figure><p>说明下:</p><p>监控Brokers是利用Zabbix JMX监控获取数据。</p><p>☝️第一步：不用解释前提你在zabbix-server端已经安装过abbix-java-gataway 如果没有安装可以看下面，安装过可以略过第一步。</p><p>1.0 安装配置zabbix-java-gataway</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">yum install -y zabbix-java-gataway</div><div class="line">vi /etc/zabbix/zabbix_java_gateway.conf</div><div class="line">START_POLLERS=10 </div><div class="line">Uncoment并设置为StartJavaPollers = 5 更改JavaGateway的 IP = IP_address_java_gateway</div><div class="line">``` </div><div class="line">1.1 重新启动zabbix-server</div><div class="line"></div><div class="line">```bash</div><div class="line">/etc/init.d/zabbix-java-gataway restart</div><div class="line">chkconfig --level 345 zabbix-java-gataway on</div><div class="line">/etc/init.d/zabbix-java-gataway start</div></pre></td></tr></table></figure><p>☝️第二步：Kafka配置</p><p>这里我们服务器给大数据那边安装在指定用户目录下面：/home/jollybi/tools/kafka_2.10-0.8.2.1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">su - jollybi 进入相应用户</div><div class="line">vim /home/jollybi/tools/kafka_2.10-0.8.2.1/kafka-run-class.sh</div><div class="line"></div><div class="line">从</div><div class="line"></div><div class="line"><span class="comment"># JMX settings</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$KAFKA_JMX_OPTS</span>"</span> ]; <span class="keyword">then</span></div><div class="line">KAFKA_JMX_OPTS=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -   Dcom.sun.management.jmxremote.ssl=false "</span></div><div class="line"><span class="keyword">fi</span></div><div class="line">至</div><div class="line"></div><div class="line"><span class="comment"># JMX settings</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$KAFKA_JMX_OPTS</span>"</span> ]; <span class="keyword">then</span></div><div class="line">KAFKA_JMX_OPTS=<span class="string">"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=12345 -    Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false "</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><p>重启服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/home/jollybi/tools/kafka_2.10-0.8.2.1/bin/kafka-server-stop.sh</div><div class="line">/home/jollybi/tools/kafka_2.10-0.8.2.1/bin/kafka-server-start.sh</div></pre></td></tr></table></figure><p>☝️第三步：导入模板登录到您的zabbix网页.</p><p>导入模板登录到您的zabbix网页</p><p>单击配置 - &gt;模板 - &gt;导入</p><p>下载模板 <a href="https://github.com/yangcvo/Zabbix-Monitoring-Kafka" target="_blank" rel="external">zbx_kafka_templates.xml</a> 并上传到zabbix然后将此模板添加到Kafka并在zabbix上配置JMX接口</p><p>输入Kafka IP地址和JMX端口如果看到jmx图标，您配置了JMX监控好！</p><ul><li>查看数据：</li></ul><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Brokers_01.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Brokers_02.png" alt=""></figure></p><ul><li>查看效果图：</li></ul><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Brokers_03.png" alt=""></figure></p><p>参考官网监控指标的含义;</p><p><a href="http://docs.confluent.io/current/kafka/monitoring.html" target="_blank" rel="external">Servilo Metrics broker Metrics</a><br><a href="http://docs.confluent.io/current/kafka/monitoring.html" target="_blank" rel="external">Producer Metrics Global Request Metrics</a><br><a href="http://docs.confluent.io/current/kafka/monitoring.html" target="_blank" rel="external">Global Connection Metrics</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Zabbix监控Kafka集群-Brokers服务&quot;&gt;&lt;a href=&quot;#Zabbix监控Kafka集群-Brokers服务&quot; class=&quot;headerlink&quot; title=&quot;Zabbix监控Kafka集群 Brokers服务&quot;&gt;&lt;/a&gt;Zabbix监控Kaf
      
    
    </summary>
    
      <category term="Kafka" scheme="http://blog.yancy.cc/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
      <category term="Zabbix" scheme="http://blog.yancy.cc/tags/Zabbix/"/>
    
  </entry>
  
  <entry>
    <title>Zabbix-Monitoring Kafka集群 Consumer | kafka的监控和告警</title>
    <link href="http://blog.yancy.cc/2017/05/29/Bigdata-hadoop/Zabbix-Monitoring%20Kafka%20Consumer%20%7C%20kafka%E7%9A%84%E7%9B%91%E6%8E%A7%E5%92%8C%E5%91%8A%E8%AD%A6/"/>
    <id>http://blog.yancy.cc/2017/05/29/Bigdata-hadoop/Zabbix-Monitoring Kafka Consumer | kafka的监控和告警/</id>
    <published>2017-05-29T09:56:03.000Z</published>
    <updated>2017-09-21T15:09:14.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Zabbix-Monitoring-Kafka集群-Consumer-kafka的监控和告警"><a href="#Zabbix-Monitoring-Kafka集群-Consumer-kafka的监控和告警" class="headerlink" title="Zabbix-Monitoring Kafka集群 Consumer | kafka的监控和告警"></a>Zabbix-Monitoring Kafka集群 Consumer | kafka的监控和告警</h3><p>前面一篇讲了我们监控kafka集群Brokers服务状态监控。生产环境监控，可以在Zabbix中对Kafka进行监控，一种是监控JMX端口，另外一种是直接写脚本，使用bin/kafka-run-class.sh里提供的相关方法类。</p><p>根据我们的业务场景，最为主要的的是监控消费者Lag的情况。所有我直接写脚本了。<br>我们对某一个Topic的30个分区，每个分区，当前Consumer的Lag情况。<br>当然还可以生成汇总图，在此不做多展示。在Zabbix中配置对应的Triggers，当Lag超过阀值，实现报警。</p><p>这里我用命令在kafka当中一台服务器获取了其中一个Topic group的Lag。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">[jollybi@countly2 kafka_2.10]$ </div><div class="line">bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper 75.126.39.4:2281,75.126.5.2:2281,75.126.5.1:2281 --group group_zybi --topic mongotail_lz4  </div><div class="line">消费组           话题id                        分区ID 当前已消费的条数   总条数         未消费的            </div><div class="line">Group           Topic                          Pid Offset          <span class="built_in">log</span>Size         Lag             Owner</div><div class="line">group_zybi      mongotail_lz4                  0   1092163900      2253744158      1161580258      none</div><div class="line">group_zybi      mongotail_lz4                  1   1092281117      2253795899      1161514782      none</div><div class="line">group_zybi      mongotail_lz4                  2   1092834883      2253771036      1160936153      none</div><div class="line">group_zybi      mongotail_lz4                  3   1092822121      2253772916      1160950795      none</div><div class="line">group_zybi      mongotail_lz4                  4   1092040532      2253775432      1161734900      none</div><div class="line">group_zybi      mongotail_lz4                  5   1095163824      2253744329      1158580505      none</div><div class="line">group_zybi      mongotail_lz4                  6   1097623779      2253781500      1156157721      none</div><div class="line">group_zybi      mongotail_lz4                  7   1097459357      2253782032      1156322675      none</div><div class="line">group_zybi      mongotail_lz4                  8   1099080546      2253741287      1154660741      none</div><div class="line">group_zybi      mongotail_lz4                  9   1099028190      2253785053      1154756863      none</div><div class="line">group_zybi      mongotail_lz4                  10  1099535928      2253795680      1154259752      none</div><div class="line">group_zybi      mongotail_lz4                  11  1099701993      2253742880      1154040887      none</div></pre></td></tr></table></figure><p>其实命令大体的逻辑就是通过Consumer获取到当前的offset，就能有lag信息了，如何写成脚本获取我们想要的消费和未消费与总消息条数呢。</p><p>脚本贴出来了，很简单理解，把自己获取到topic的消费条数过滤。我公司有两个主题topic分别对不同组获取lag.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">[root@countly2 zabbix]<span class="comment"># vim monitor_kafka.sh</span></div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> JAVA_HOME=/home/jollybi/tools/java-7-sun</div><div class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$CLASSPATH</span>:.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JAVA_HOME</span>/jre/bin:<span class="variable">$PATH</span></div><div class="line">topic=mongotail_lz4_imp</div><div class="line">group=group_imp</div><div class="line"></div><div class="line"><span class="comment">#topic2</span></div><div class="line"></div><div class="line">topic2=mongotail_lz4</div><div class="line">group2=group_event</div><div class="line">group3=group_event_spm</div><div class="line"></div><div class="line"><span class="comment">#启动目录</span></div><div class="line">kafkaserver=/home/jollybi/tools/kafka_2.10-0.8.2.1/bin/kafka-run-class.sh</div><div class="line">zk=75.126.39.4:2281,75.126.5.2:2281,75.126.5.8:2281</div><div class="line"></div><div class="line"><span class="keyword">function</span> imp_lag &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group</span>  --topic <span class="variable">$topic</span>  | sed -n 2p | awk '&#123;print <span class="variable">$6</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> imp_logsize &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group</span>  --topic <span class="variable">$topic</span>  | sed -n 2p | awk '&#123;print <span class="variable">$5</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> imp_offset &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group</span>  --topic <span class="variable">$topic</span>  | sed -n 2p | awk '&#123;print <span class="variable">$4</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> event_spm_lag &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group2</span>  --topic <span class="variable">$topic2</span>  | sed -n 2p | awk '&#123;print <span class="variable">$6</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> event_spm_logsize &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group2</span>  --topic <span class="variable">$topic2</span>  | sed -n 2p | awk '&#123;print <span class="variable">$5</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> event_spm_offset &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group2</span>  --topic <span class="variable">$topic2</span>  | sed -n 2p | awk '&#123;print <span class="variable">$4</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> event_lag &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group3</span>  --topic <span class="variable">$topic2</span>  | sed -n 2p | awk '&#123;print <span class="variable">$6</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> event_logsize &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group3</span>  --topic <span class="variable">$topic2</span>  | sed -n 2p | awk '&#123;print <span class="variable">$5</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> event_offset &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`<span class="variable">$kafkaserver</span> kafka.tools.ConsumerOffsetChecker --zookeeper <span class="variable">$zk</span> --group <span class="variable">$group3</span>  --topic <span class="variable">$topic2</span>  | sed -n 2p | awk '&#123;print <span class="variable">$4</span>&#125;'`"</span></div><div class="line">&#125;</div><div class="line"><span class="comment"># Run the requested function</span></div><div class="line"><span class="variable">$1</span></div></pre></td></tr></table></figure><p>⏰ <strong>这里脚本我更新过了，这个脚本实现是当个分片的消费情况。 不是总的消费和未消费的取到的值。</strong><br>🎋 需要原脚本的留言喔，下面步骤都是一样的效果。</p><h4 id="脚本放到kafka服务器-etc-zabbix-下面，并配合zabbix监控。"><a href="#脚本放到kafka服务器-etc-zabbix-下面，并配合zabbix监控。" class="headerlink" title="脚本放到kafka服务器/etc/zabbix/下面，并配合zabbix监控。"></a>脚本放到kafka服务器/etc/zabbix/下面，并配合zabbix监控。</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@countly2 zabbix]<span class="comment"># ll</span></div><div class="line">total 8</div><div class="line">-rwxrwxr-x 1 jollybi jollybi 1932 Jun  7 10:40 monitor_kafka.sh</div><div class="line">-rw-r--r-- 1 root    root     218 Jun  7 14:19 zabbix_agentd.conf</div><div class="line">drwxr-xr-x 2 root    root      45 Sep 20  2016 zabbix_agentd.d</div></pre></td></tr></table></figure><h4 id="然后zabbix-agentd-conf扩展配置"><a href="#然后zabbix-agentd-conf扩展配置" class="headerlink" title="然后zabbix_agentd.conf扩展配置"></a>然后zabbix_agentd.conf扩展配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">UserParameter=kafka.[*],/etc/zabbix/monitor_kafka.sh <span class="variable">$1</span></div></pre></td></tr></table></figure><blockquote><p>也可以这么写：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">UserParameter=kafka.imp_lag,/etc/zabbix/monitor_kafka.sh imp_lag</div><div class="line">UserParameter=kafka.imp_offset,/etc/zabbix/monitor_kafka.sh imp_offset</div><div class="line">UserParameter=kafka.imp_logsize,/etc/zabbix/monitor_kafka.sh imp_logsize</div><div class="line">UserParameter=kafka.event_spm_lag,/etc/zabbix/monitor_kafka.sh event_spm_lag</div><div class="line">UserParameter=kafka.event_spm_logsize,/etc/zabbix/monitor_kafka.sh event_spm_logsize</div><div class="line">UserParameter=kafka.event_spm_offset,/etc/zabbix/monitor_kafka.sh event_spm_offset </div><div class="line">UserParameter=kafka.event_lag,/etc/zabbix/monitor_kafka.sh event_lag</div><div class="line">UserParameter=kafka.event_offset,/etc/zabbix/monitor_kafka.sh event_offset</div><div class="line">UserParameter=kafka.event_logsize,/etc/zabbix/monitor_kafka.sh event_logsize</div></pre></td></tr></table></figure><p>这里写键值就不需要加[]</p><p>然后配置完成重启zabbix-agent服务。</p><h4 id="zabbix设置Key"><a href="#zabbix设置Key" class="headerlink" title="zabbix设置Key"></a>zabbix设置Key</h4><p>zabbix-组态-模板-创建模板。</p><p>创建：KafkaConsumers模板<br>然后点击KafkaConsumers模板，创建相应的项目:kafka.event_lag 未消费的条数 </p><p>设置zabbix代理。默认的代理方式。</p><p>zabbix,key 键值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kafka.[event_lag]</div></pre></td></tr></table></figure><p>数据类型默认浮点就可以了。 后面应用集选择下KafkaConsumers 即可。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.01.png" alt=""></figure><br>陆陆续续配置完所有项目。</p><h3 id="然后配置图形。"><a href="#然后配置图形。" class="headerlink" title="然后配置图形。"></a>然后配置图形。</h3><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.02.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.04.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.05.png" alt=""></figure></p><h3 id="查看最新数据图："><a href="#查看最新数据图：" class="headerlink" title="查看最新数据图："></a>查看最新数据图：</h3><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.06.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.07.png" alt=""></figure></p><p>出现问题第一时间发送报警消息。<br>报警的Trigger触发规则也是对lag的值做报警，具体阀值设置为多少，还是看大家各自业务需求了。</p><h3 id="这里我对未消费告警定时60分钟触发一次。"><a href="#这里我对未消费告警定时60分钟触发一次。" class="headerlink" title="这里我对未消费告警定时60分钟触发一次。"></a>这里我对未消费告警定时60分钟触发一次。</h3><p>⚠️注释：设置group_event_lag最近消息条数的60分钟时间之内一直超过200万消费则报警</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.08.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.09.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-Consumer.10.png" alt=""></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Zabbix-Monitoring-Kafka集群-Consumer-kafka的监控和告警&quot;&gt;&lt;a href=&quot;#Zabbix-Monitoring-Kafka集群-Consumer-kafka的监控和告警&quot; class=&quot;headerlink&quot; title=&quot;
      
    
    </summary>
    
      <category term="Kafka" scheme="http://blog.yancy.cc/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
      <category term="Zabbix" scheme="http://blog.yancy.cc/tags/Zabbix/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-Kafka集群快速搭建与命令讲解</title>
    <link href="http://blog.yancy.cc/2017/05/28/Bigdata-hadoop/Kafka/Bigdata-Kafka%E9%9B%86%E7%BE%A4%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%E5%91%BD%E4%BB%A4%E8%AE%B2%E8%A7%A3/"/>
    <id>http://blog.yancy.cc/2017/05/28/Bigdata-hadoop/Kafka/Bigdata-Kafka集群快速搭建与增删改查命令讲解/</id>
    <published>2017-05-28T06:46:00.000Z</published>
    <updated>2017-09-19T08:35:25.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bigdata-Kafka集群快速搭建与命令讲解"><a href="#Bigdata-Kafka集群快速搭建与命令讲解" class="headerlink" title="Bigdata-Kafka集群快速搭建与命令讲解"></a>Bigdata-Kafka集群快速搭建与命令讲解</h1><h3 id="kafka集群和zookeeper集群规范"><a href="#kafka集群和zookeeper集群规范" class="headerlink" title="kafka集群和zookeeper集群规范"></a>kafka集群和zookeeper集群规范</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">kafka集群和zookeeper集群规范</div><div class="line">kafka版本：kafka_2.10-0.8.2.1</div><div class="line">zookeeper版本：zookeeper-3.4.5</div><div class="line"></div><div class="line">启动用户：jollybi</div><div class="line">kafka安装目录：/data/tools/</div><div class="line">kafka新建消息目录：/data/tools/kafka_2.10-0.8.2.1/kafka-logs</div><div class="line"></div><div class="line">启动用户：jollybi</div><div class="line">zookeeper安装目录：/data/tools/</div><div class="line">zookeeper新建日志目录：/data/tools/zookeeper-3.4.5/tmp/logs</div><div class="line"></div><div class="line">统一配置hosts</div><div class="line"></div><div class="line"></div><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure><p>本文使用了3台机器部署Kafka集群，IP和主机名对应关系如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure><h3 id="Step1-配置-etc-hosts-（3台一致）"><a href="#Step1-配置-etc-hosts-（3台一致）" class="headerlink" title="Step1: 配置/etc/hosts （3台一致）"></a>Step1: 配置/etc/hosts （3台一致）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">127.0.0.1 localhost.localdomain localhost</div><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure><h3 id="Step2-KafKa集群搭建"><a href="#Step2-KafKa集群搭建" class="headerlink" title="Step2: KafKa集群搭建"></a>Step2: KafKa集群搭建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">下载地址：http://mirrors.hust.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.8.2.1.tgz</div><div class="line">[jollybi@kafka1 ]<span class="comment"># tar -xvf kafka_2.10-0.8.2.1.tgz -C /data/tools/</span></div><div class="line"><span class="built_in">cd</span> /data/tools/kafka_2.10-0.8.2.1/config</div></pre></td></tr></table></figure><h3 id="设置data目录，最好不要用默认的-tmp-kafka-logs"><a href="#设置data目录，最好不要用默认的-tmp-kafka-logs" class="headerlink" title="设置data目录，最好不要用默认的/tmp/kafka-logs"></a>设置data目录，最好不要用默认的/tmp/kafka-logs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mkdir -p /data/tools/kafka_2.10-0.8.2.1/kafka-logs/</div></pre></td></tr></table></figure><p><em>修改kafka配置文件</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@kafka_01 config]<span class="comment"># vim server.properties</span></div><div class="line"></div><div class="line"></div><div class="line"> 设置brokerid（从0开始，3个节点分别设为1,2,3，不能重复）在这里id=0跟zookeeper id设置一样就行。 集群机器：按顺序写1</div><div class="line">broker.id=1  </div><div class="line"></div><div class="line">auto.leader.rebalance.enable=<span class="literal">true</span> </div><div class="line"></div><div class="line"><span class="comment">#修改本地IP地址：</span></div><div class="line">listeners=PLAINTEXT://10.46.72.172:9092</div><div class="line">port=9292 //设置访问端口</div><div class="line">host.name=10.155.90.153   kafka本机IP</div><div class="line">log.dirs=/data/tools/kafka_2.10-0.8.2.1/kafka-logs</div><div class="line"></div><div class="line"><span class="comment">#设置注册地址（重要，默认会把本机的hostanme注册到zk中，客户端连接时需要解析该hostanme，所以这里直接注册本机的IP地址，避免hostname解析失败，报错java.nio.channels.UnresolvedAddressException或java.io.IOException: Can not resolve address）</span></div><div class="line"><span class="comment">#设置zookeeper地址</span></div><div class="line"><span class="comment">#zookeeper.connect=10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181</span></div><div class="line">zookeeper.connect=kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281</div><div class="line"></div><div class="line">//这里我设置hosts代替IP</div></pre></td></tr></table></figure><p><em>配置zookeeper地址</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">vim zookeeper.properties </div><div class="line">dataDir=/data/tools/zookeeper-3.4.5/tmp</div><div class="line"><span class="comment"># the port at which the clients will connect</span></div><div class="line">clientPort=2281</div><div class="line"><span class="comment"># disable the per-ip limit on the number of connections since this is a non-production config</span></div><div class="line">maxClientCnxns=0</div><div class="line">~</div></pre></td></tr></table></figure><p><em>配置kafka访问地址</em></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">vim</span> producer.properties </div><div class="line">metadata.broker.list=<span class="number">169.44.62.139:9292</span>,<span class="number">169.44.59.138:9292</span>,<span class="number">169.44.62.137:9292</span>   </div><div class="line"><span class="comment"># name of the partitioner class for partitioning events; default partition spreads data randomly</span></div><div class="line"><span class="comment">#partitioner.class=</span></div><div class="line"></div><div class="line"><span class="comment"># specifies whether the messages are sent asynchronously (async) or synchronously (sync)</span></div><div class="line">producer.type=sync</div><div class="line"></div><div class="line"><span class="comment"># specify the compression codec for all data generated: none, gzip, snappy, lz4.</span></div><div class="line"><span class="comment"># the old config values work as well: 0, 1, 2, 3 for none, gzip, snappy, lz4, respectively</span></div><div class="line">compression.codec=<span class="literal">none</span></div><div class="line"></div><div class="line"><span class="comment"># message encoder</span></div><div class="line">serializer.class=kafka.serializer.DefaultEncoder</div></pre></td></tr></table></figure><h3 id="Step3-集群机器快速拷贝配置"><a href="#Step3-集群机器快速拷贝配置" class="headerlink" title="Step3: 集群机器快速拷贝配置:"></a>Step3: 集群机器快速拷贝配置:</h3><p>远程复制分发安装文件<br>最好是把文件打包scp过去<br>接下来将上面的安装文件拷贝到集群中的其他机器上对应的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scp -P58958 -r kafka_2.10-0.8.2.1 jollybi@169.44.62.137:/home/jollybi/tools/. </div><div class="line">scp -P58958 -r kafka_2.10-0.8.2.1 jollybi@169.44.62.138:/home/jollybi/tools/.</div></pre></td></tr></table></figure><p>统一修改分别其他两台kafka <code>server.properties</code></p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">broker.id=<span class="number">2</span>  </div><div class="line">host.name=<span class="number">169.44</span><span class="number">.62</span><span class="number">.137</span>   kafka2本机IP</div><div class="line"></div><div class="line">broker.id=<span class="number">3</span> </div><div class="line">host.name=<span class="number">169.44</span><span class="number">.62</span><span class="number">.138</span>   kafka3本机IP</div></pre></td></tr></table></figure><h3 id="Step4-启动-kafka集群"><a href="#Step4-启动-kafka集群" class="headerlink" title="Step4:启动 kafka集群"></a>Step4:启动 kafka集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">./kafka-server-start.sh -daemon ../config/server.properties</div><div class="line"></div><div class="line">[jollybi@kafka1 config]$ jps</div><div class="line">3443 QuorumPeerMain</div><div class="line">16280 Jps</div><div class="line">3628 Kafka</div><div class="line">Step5: Kafka常用命令(普及)</div></pre></td></tr></table></figure><h4 id="Step5-测试集群"><a href="#Step5-测试集群" class="headerlink" title="Step5:测试集群"></a>Step5:测试集群</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### 默认创建kafka Topic:  1个副本备份 1个分区消费</span></div><div class="line"></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 1 --partitions 1 --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 1 --partitions 1 --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 大数据这边需要12个分区这里我删除Topic重新创建Topic：</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 删除kafka Topic:</span></div><div class="line"></div><div class="line"></div><div class="line">第一步：删除kafka topic</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --delete --topic  mongotail_lz4 </div><div class="line"></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --delete --topic  mongotail_lz4_imp</div><div class="line"></div><div class="line"><span class="comment">### 这是由于删除topic没删干净会报错：</span></div><div class="line"></div><div class="line">org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 删除线程执行删除操作的真正逻辑是：</span></div><div class="line">1. 它首先会给当前所有broker发送更新元数据信息的请求，告诉这些broker说这个topic要删除了，你们可以把它的信息从缓存中删掉了</div><div class="line">2. 开始删除这个topic的所有分区</div><div class="line">2.1 给所有broker发请求，告诉它们这些分区要被删除。broker收到后就不再接受任何在这些分区上的客户端请求了</div><div class="line">2.2 把每个分区下的所有副本都置于OfflineReplica状态，这样ISR就不断缩小，当leader副本最后也被置于OfflineReplica状态时leader信息将被更新为-1</div><div class="line">2.3 将所有副本置于ReplicaDeletionStarted状态</div><div class="line">2.4 副本状态机捕获状态变更，然后发起StopReplicaRequest给broker，broker接到请求后停止所有fetcher线程、移除缓存，然后删除底层<span class="built_in">log</span>文件</div><div class="line">2.5 关闭所有空闲的Fetcher线程</div><div class="line">    </div><div class="line"><span class="comment">### 第二步：删除zookeeper相关的路径：</span></div><div class="line">[jollybi@kafka1 zookeeper-3.4.5]$ ./bin/zkCli.sh -server 127.0.0.1:2281 </div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /brokers/topics/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 0] rmr /consumers/group_ml_general/offsets/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /consumers/group_ml_general/owners/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 3] rmr  /config/topics/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 4] rmr /admin/delete_topics/mongotail_lz4</div><div class="line"></div><div class="line"><span class="comment">### 删除topic上面消费组 </span></div><div class="line"></div><div class="line">删除 ZooKeeper 下面的 /consumers/[group_id] 路径就可以了。ZooKeeper 原生API只支持删除空的路径，所以建议你使用 curator framework 进行这个删除操作，ZkPaths.deleteChildren 会递归式地删除整个路径（包括子路径）。</div><div class="line"></div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 0] rmr /consumers/kafka-node1-imp-group</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /consumers/kafka-node1-group</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 2] rmr /consumers/console-consumer-59053</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 3] rmr /consumers/</div><div class="line"></div><div class="line"></div><div class="line">1、设置配置文件允许删除</div><div class="line">delete.topic.enable=<span class="literal">true</span> 配置添加到 config/server.properties</div><div class="line">2、执行删除命令</div><div class="line">./bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic <span class="built_in">test</span>-topic</div><div class="line"></div><div class="line">在zookeeper中确认：</div><div class="line"></div><div class="line"> 删除zookeeper下/brokers/topics/<span class="built_in">test</span>-topic节点</div><div class="line"> 删除zookeeper下/config/topics/<span class="built_in">test</span>-topic节点</div><div class="line"> 删除zookeeper下/admin/delete_topics/<span class="built_in">test</span>-topic节点</div><div class="line"> </div><div class="line">consumer 的话 删除groupid</div><div class="line"> </div><div class="line"></div><div class="line"><span class="comment">### 重启kafka集群</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 大数据kafka建topic，不能建一个分区，那样吞吐量上不去，不够用，我们以前建了12个分区的</span></div><div class="line"><span class="comment">### 创建新 kafka Topic: 3个副本，12个分区</span></div><div class="line"></div><div class="line"> ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 3 --partitions 12  --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 3 --partitions 12  --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 查看创建的Topic:</span></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 查看集群情况：</span></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### kafka生产客户端生产数据命令：</span></div><div class="line"></div><div class="line">./bin/kafka-console-producer.sh --broker-list 75.126.39.124:9292,75.126.5.162:9292,75.126.5.178:9292 --topic mongotail_lz4_imp                      </div><div class="line">......</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line">^C</div><div class="line"><span class="comment">### kafka消费客户端数据命令 现在我们来看看消息：</span></div><div class="line"></div><div class="line"> ./bin/kafka-console-consumer.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --fm-beginning --topic mongotail_lz4_imp</div><div class="line">......</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line"></div><div class="line">^C</div></pre></td></tr></table></figure><h3 id="Kafka常用命令"><a href="#Kafka常用命令" class="headerlink" title="Kafka常用命令"></a>Kafka常用命令</h3><p>以下是kafka常用命令行总结：  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">1、 list topic 显示所有topic</div><div class="line">./bin/kafka-topics.sh --list --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281</div><div class="line"></div><div class="line">2、 查看topic的详细信息  </div><div class="line"> ./bin/kafka-topics.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  -describe -topic mongotail_lz4_imp</div><div class="line"></div><div class="line">3、为topic增加partition分区 参数：--alter --partitions</div><div class="line">./bin/kafka-topics.sh  --alter --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --partitions 20 --topic mongotail_lz4_imp</div><div class="line"></div><div class="line">4、kafka生产者客户端命令  </div><div class="line">./bin/kafka-console-producer.sh --broker-list 75.126.39.124:9292,75.126.5.162:9292,75.126.5.178:9292 --topic mongotail_lz4_imp                        </div><div class="line"></div><div class="line">5、kafka消费者客户端命令  </div><div class="line">./bin/kafka-console-consumer.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --fm-beginning --topic </div><div class="line"></div><div class="line">6、kafka服务启动  </div><div class="line">./kafka-server-start.sh -daemon ../config/server.properties   </div><div class="line"></div><div class="line">7、下线broker  </div><div class="line">./kafka-run-class.sh kafka.admin.ShutdownBroker --zookeeper 127.0.0.1:2181 --broker <span class="comment">#brokerId# --num.retries 3 --retry.interval.ms 60  </span></div><div class="line">shutdown broker  </div><div class="line"></div><div class="line">8、删除topic  </div><div class="line">./kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic <span class="built_in">test</span>KJ1 --zookeeper 127.0.0.1:2181  </div><div class="line">./kafka-topics.sh --zookeeper localhost:2181 --delete --topic <span class="built_in">test</span>KJ1  </div><div class="line"></div><div class="line">9、查看consumer组内消费的offset  </div><div class="line">./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group <span class="built_in">test</span> --topic <span class="built_in">test</span>KJ1</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bigdata-Kafka集群快速搭建与命令讲解&quot;&gt;&lt;a href=&quot;#Bigdata-Kafka集群快速搭建与命令讲解&quot; class=&quot;headerlink&quot; title=&quot;Bigdata-Kafka集群快速搭建与命令讲解&quot;&gt;&lt;/a&gt;Bigdata-Kafka集
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/tags/Big-data-Hadoop/"/>
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-ZooKeeper的配置详解优化</title>
    <link href="http://blog.yancy.cc/2017/05/27/Bigdata-hadoop/zookeeper/Bigdata-ZooKeeper%E7%9A%84%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%E4%BC%98%E5%8C%96/"/>
    <id>http://blog.yancy.cc/2017/05/27/Bigdata-hadoop/zookeeper/Bigdata-ZooKeeper的配置详解优化/</id>
    <published>2017-05-27T06:46:00.000Z</published>
    <updated>2017-09-19T08:37:19.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><code>ZooKeeper</code>的功能特性通过 <code>ZooKeeper</code> 配置文件来进行控制管理（ <code>zoo.cfg</code> 配置文件）。 ZooKeeper 这样的设计其实是有它自身的原因的。通过前面对 <code>ZooKeeper</code> 的配置可以看出，对 <code>ZooKeeper</code>集群进行配置的时候，它的配置文档是完全相同的（对于集群伪分布模式来说，只有很少的部分是不同的）。这样的配置方使得在部署 <code>ZooKeeper</code> 服务的时候非常地方便。另外，如果服务器使用不同的配置文件，必须要确保不同配置文件中的服务器列表相匹配。</p><p>在设置 <code>ZooKeeper</code> 配置文档的时候，某些参数是可选的，但是某些参数是必须的。这些必须的参数就构成了<code>ZooKeeper</code> 配置文档的最低配置要求。</p><h5 id="最近发现在用zookeeper出现经常出现连接超时，出现连接中断，数据丢失等原因。后面看了官网配置自己整理优化几点："><a href="#最近发现在用zookeeper出现经常出现连接超时，出现连接中断，数据丢失等原因。后面看了官网配置自己整理优化几点：" class="headerlink" title="最近发现在用zookeeper出现经常出现连接超时，出现连接中断，数据丢失等原因。后面看了官网配置自己整理优化几点："></a>最近发现在用zookeeper出现经常出现连接超时，出现连接中断，数据丢失等原因。后面看了官网配置自己整理优化几点：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">2.1错误日志：</div><div class="line"></div><div class="line">2016-04-11 15:00:58,981 [myid:] - WARN  [SyncThread:0:FileTxnLog@334] - fsync-ing the write ahead <span class="built_in">log</span> <span class="keyword">in</span> SyncThread:0 took 13973ms <span class="built_in">which</span> will adversely effect operation latency. See the ZooKeeper troubleshooting guide</div><div class="line"></div><div class="line"></div><div class="line">&gt;2.2，错误原因分析</div><div class="line"></div><div class="line">“FOLLOWER”在跟“LEADER”同步时，fsync操作时间过长，导致超时。</div><div class="line"></div><div class="line"></div><div class="line">第一步：分析服务器问题：</div><div class="line"></div><div class="line">我查看了服务器io和负载都不高。内存空间实际使用率不高。可是编辑文件出现了卡顿.</div><div class="line"></div><div class="line">可以发现：</div><div class="line"></div><div class="line">服务器并没有占用很多内存的进程；</div><div class="line">服务器也没有存在很多的进程；</div><div class="line">cat /var/<span class="built_in">log</span>/message查看系统日志并没有发现什么异常；</div><div class="line">另外ping服务器只有0.1ms多的延迟，因此不是网络问题。</div><div class="line"></div><div class="line">后面发现硬盘有故障重新更换了一块硬盘。或者更换服务器。</div><div class="line"></div><div class="line"></div><div class="line">&gt;2.3，错误解决</div><div class="line"></div><div class="line">增加“tickTime”或者“initLimit和syncLimit”的值，或者两者都增大。</div><div class="line"></div><div class="line"></div><div class="line">&gt;2.4，其他</div><div class="line"></div><div class="line">这个错误在上线“使用ZooKeeper获取地址方案”之前也存在，只不过过没有这么高频率，而上线了“ZooKeeper获取地址方案”之后，ZooKeeper Server之间的同步数据量增大，ZooKeeper Server的负载加重，因而最终导致高频率出现上述错误。</div></pre></td></tr></table></figure><h2 id="下面是在最低配置要求中必须配置的参数："><a href="#下面是在最低配置要求中必须配置的参数：" class="headerlink" title="下面是在最低配置要求中必须配置的参数："></a>下面是在最低配置要求中必须配置的参数：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### 最小配置</span></div><div class="line"></div><div class="line">最小配置意味着所有的配置文件中必须要包含这些配置选项。</div><div class="line"></div><div class="line"><span class="comment">#### clientPort</span></div><div class="line"></div><div class="line">服务器监听客户端连接的端口, 亦即客户端尝试连接到服务器上的指定端口。</div><div class="line"></div><div class="line"><span class="comment">##### dataDir</span></div><div class="line"></div><div class="line">ZooKeeper 存储内存数据库快照文件的路径, 并且如果没有指定其它路径的话, 数据库更新的事务日志也将存储到该路径下。</div><div class="line"></div><div class="line">注意: 事务日志会影响 ZooKeeper 服务器的整体性能, 所以建议将事务日志放置到由 dataLogDir 参数指定的路径下。</div><div class="line"></div><div class="line"><span class="comment">##### tickTime</span></div><div class="line"></div><div class="line">单个 tick 的时间长度, 它是 ZooKeeper 中使用的基本时间单元, 以毫秒为单位。它用来调节心跳和超时时间。例如, 最小会话超时时间是 2 个 tick。</div></pre></td></tr></table></figure><p>生产环境例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">1.tickTime：CS通信心跳数</div><div class="line"></div><div class="line">Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。</div><div class="line"></div><div class="line">tickTime=2000  </div><div class="line"></div><div class="line">2.initLimit：LF初始通信时限</div><div class="line">集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。</div><div class="line"></div><div class="line">initLimit=5  </div><div class="line"></div><div class="line">3.syncLimit：LF同步通信时限</div><div class="line">集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。</div><div class="line"></div><div class="line">syncLimit=2  </div><div class="line"> </div><div class="line">4.dataDir：数据文件目录</div><div class="line">Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。</div><div class="line"></div><div class="line">dataDir=/home/michael/opt/zookeeper/data  </div><div class="line"></div><div class="line">5.dataLogDir：日志文件目录</div><div class="line">Zookeeper保存日志文件的目录。</div><div class="line"></div><div class="line">dataLogDir=/home/michael/opt/zookeeper/<span class="built_in">log</span>  </div><div class="line"></div><div class="line">6.clientPort：客户端连接端口</div><div class="line">客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。</div><div class="line"></div><div class="line">clientPort=2333  </div><div class="line"></div><div class="line">7.服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口）</div><div class="line">这个配置项的书写格式比较特殊，规则如下：</div><div class="line"></div><div class="line">server.N=YYY:A:B  </div><div class="line"></div><div class="line">其中N表示服务器编号，YYY表示服务器的IP地址，A为LF通信端口，表示该服务器与集群中的leader交换的信息的端口。B为选举端口，表示选举新leader时服务器间相互通信的端口（当leader挂掉时，其余服务器会相互通信，选择出新的leader）。一般来说，集群中每个服务器的A端口都是一样，每个服务器的B端口也是一样。但是当所采用的为伪集群时，IP地址都一样，只能时A端口和B端口不一样。</div><div class="line">下面是一个非伪集群的例子：</div><div class="line"></div><div class="line">server.0=233.34.9.144:2008:6008  </div><div class="line">server.1=233.34.9.145:2008:6008  </div><div class="line">server.2=233.34.9.146:2008:6008  </div><div class="line">server.3=233.34.9.147:2008:6008  </div><div class="line"></div><div class="line">下面是一个伪集群的例子：</div><div class="line"></div><div class="line">server.0=127.0.0.1:2008:6008  </div><div class="line">server.1=127.0.0.1:2007:6007  </div><div class="line">server.2=127.0.0.1:2006:6006  </div><div class="line">server.3=127.0.0.1:2005:6005</div></pre></td></tr></table></figure><h2 id="高级配置"><a href="#高级配置" class="headerlink" title="高级配置"></a>高级配置</h2><p>本节的配置选项是可选的。你可以使用它们进一步的优化 ZooKeeper 服务器的行为。有些可以使用 Java 系统属性来设置, 一般的格式是 “zookeeper.keyword”。如果有具体的系统属性, 会在配置选项下面标注出来。</p><h5 id="dataLogDir"><a href="#dataLogDir" class="headerlink" title="dataLogDir"></a>dataLogDir</h5><p>没有对应的 Java 系统属性。</p><p>该参数用于配置 ZooKeeper 服务器存储事务日志文件的路径, ZooKeeper 默认将事务日志文件和数据快照存储在同一个目录下, 应尽量将它们分开存储。</p><p>注意: 将事务日志文件存储到一个专门的日志设备上对于服务器的吞吐量和稳定的延迟有很大的影响。事务日志对磁盘性能要求比较高, 为了保证数据一致性, ZooKeeper 在响应客户端事务请求之前, 需要将请求的事务日志写到磁盘上, 所以事务日志的写入性能直接影响 ZooKeeper 服务器处理请求的吞吐。所以建议给事务日志的输出配置一个单独的磁盘或者挂载点。</p><h5 id="globalOutstandingLimit"><a href="#globalOutstandingLimit" class="headerlink" title="globalOutstandingLimit"></a>globalOutstandingLimit</h5><p>对应的 Java 系统属性: zookeeper.globalOutstandingLimit。</p><p>客户端提交请求的速度可能比 ZooKeeper 处理的速度快得多, 特别是当客户端的数量非常多的时候。为了防止 ZooKeeper 因为排队的请求而耗尽内存, ZooKeeper 将会对客户端进行限流, 即限制系统中未处理的请求数量不超过 globalOutstandingLimit 设置的值。默认的限制是 1,000。</p><h5 id="preAllocSize"><a href="#preAllocSize" class="headerlink" title="preAllocSize"></a>preAllocSize</h5><p>对应的 Java 系统属性: zookeeper.preAllocSize。</p><p>用于配置 ZooKeeper 事务日志文件预分配的磁盘空间大小。默认的块大小是 64M。改变块大小的其中一个原因是当数据快照文件生成比较频繁时可以适当减少块大小。比如 1000 次事务会新产生一个快照(参数为snapCount), 新产生快照后会用新的事务日志文件, 假设一个事务信息大小100b, 那么事务日志预分配的磁盘空间大小为100kb会比较好。</p><h5 id="snapCount"><a href="#snapCount" class="headerlink" title="snapCount"></a>snapCount</h5><p>对应的 Java 系统属性: zookeeper.snapCount。</p><p>ZooKeeper 将事务记录到事务日志中。当 snapCount 个事务被写到一个日志文件后, 启动一个快照并创建一个新的事务日志文件。snapCount 的默认值是 100,000。</p><h5 id="traceFile"><a href="#traceFile" class="headerlink" title="traceFile"></a>traceFile</h5><p>对应的 Java 系统属性: requestTraceFile。</p><p>如果定义了该选项, 那么请求将会记录到一个名为 traceFile.year.month.day 的跟踪文件中。使用该选项可以提供很有用的调试信息, 但是会影响性能。</p><p>注意: requestTraceFile 这个系统属性没有 zookeeper 前缀, 并且配置的变量名称和系统属性不一样。</p><h5 id="maxClientCnxns"><a href="#maxClientCnxns" class="headerlink" title="maxClientCnxns"></a>maxClientCnxns</h5><p>没有对应的 Java 系统属性</p><p>在 socket 级别限制单个客户端到 ZooKeeper 集群中单台服务器的并发连接数量, 可以通过 IP 地址来区分不同的客户端。它用来阻止某种类型的 DoS 攻击, 包括文件描述符资源耗尽。默认值是 60。将值设置为 0 将完全移除并发连接的限制。</p><h5 id="clientPortAddress"><a href="#clientPortAddress" class="headerlink" title="clientPortAddress"></a>clientPortAddress</h5><p>服务器监听客户端连接的地址 (ipv4, ipv6 或 主机名) , 亦即客户端尝试连接到服务器上的地址。该参数是可选的, 默认我们以这样一种方式绑定, 即对于服务器上任意 address/interface/nic, 任何连接到 clientPort 的请求将会被接受。</p><h5 id="minSessionTimeout"><a href="#minSessionTimeout" class="headerlink" title="minSessionTimeout"></a>minSessionTimeout</h5><p>没有对应的 Java 系统属性</p><p>服务器允许客户端会话的最小超时时间, 以毫秒为单位。默认值是 2 倍的 tickTime。</p><h5 id="maxSessionTimeout"><a href="#maxSessionTimeout" class="headerlink" title="maxSessionTimeout"></a>maxSessionTimeout</h5><p>没有对应的 Java 系统属性</p><p>服务器允许客户端会话的最大超时时间, 以毫秒为单位。默认值是 20 倍的 tickTime。</p><h5 id="fsync-warningthresholdms"><a href="#fsync-warningthresholdms" class="headerlink" title="fsync.warningthresholdms"></a>fsync.warningthresholdms</h5><p>对应的 Java 系统属性: fsync.warningthresholdms。</p><p>用于配置 ZooKeeper 进行事务日志 (WAL) fsync 操作消耗时间的报警阈值, 一旦超过这个阈值将会打印输出报警日志。该参数的默认值是 1000, 以毫秒为单位。参数值只能作为系统属性来设置。</p><h5 id="autopurge-snapRetainCount"><a href="#autopurge-snapRetainCount" class="headerlink" title="autopurge.snapRetainCount"></a>autopurge.snapRetainCount</h5><p>没有对应的 Java 系统属性。</p><p>当启用自动清理功能后, ZooKeeper 将只保留 autopurge.snapRetainCount 个最近的数据快照(dataDir)和对应的事务日志文件(dataLogDir), 其余的将会删除掉。默认值是 3。最小值也是 3。</p><h5 id="autopurge-purgeInterval"><a href="#autopurge-purgeInterval" class="headerlink" title="autopurge.purgeInterval"></a>autopurge.purgeInterval</h5><p>没有对应的 Java 系统属性。</p><p>用于配置触发清理任务的时间间隔, 以小时为单位。要启用自动清理, 可以将其值设置为一个正整数 (大于 1) 。默认值是 0。</p><h5 id="syncEnabled"><a href="#syncEnabled" class="headerlink" title="syncEnabled"></a>syncEnabled</h5><p>对应的 Java 系统属性: zookeeper.observer.syncEnabled。</p><p>和参与者一样, 观察者现在默认将事务日志以及数据快照写到磁盘上, 这将减少观察者在服务器重启时的恢复时间。将其值设置为 “false” 可以禁用该特性。默认值是 “true”。</p><p>集群配置选项</p><p>本节中的选项主要用于ZooKeeper集群。</p><h5 id="electionAlg"><a href="#electionAlg" class="headerlink" title="electionAlg"></a>electionAlg</h5><p>没有对应的 Java 系统属性。</p><p>用于选择使用的 leader 选举算法。”0” 对应于原始的基于 UDP 的版本, “1” 对应于快速 leader 选举基于UDP的无身份验证的版本, “2” 对应于快速 leader 选举有基于UDP的身份验证的版本, 而 “3” 对应于快速 leader 选举基于TCP的版本。目前默认值是算法 3。</p><p>注意: leader 选举 0, 1, 2 这三种实现已经废弃, 在接下来的版本中将会移除它们, 这样就只剩下 FastLeaderElection 算法。</p><h5 id="initLimit"><a href="#initLimit" class="headerlink" title="initLimit"></a>initLimit</h5><p>没有对应的 Java 系统属性。</p><p>默认值是 10, 即 tickTime 属性值的 10 倍。它用于配置允许 followers 连接并同步到 leader 的最大时间。如果 ZooKeeper 管理的数据量很大的话可以增加这个值。</p><h5 id="leaderServes"><a href="#leaderServes" class="headerlink" title="leaderServes"></a>leaderServes</h5><p>对应的 Java 系统属性: zookeeper.leaderServes。</p><p>用于配置 Leader 是否接受客户端连接, 默认值是 “yes”, 即 leader 将会接受客户端连接。在 ZooKeeper 中, leader 服务器主要协调事务更新请求。对于事务更新请求吞吐很高而读取请求吞吐很低的情况可以配置 leader 不接受客户端连接, 这样就可以专注于协调工作。</p><p>注意: 当 ZooKeeper 集群中服务器的数量超过 3 个时, 建议开启 leader 选举。</p><h5 id="server-x-hostname-nnnnn-nnnnn"><a href="#server-x-hostname-nnnnn-nnnnn" class="headerlink" title="server.x=[hostname]:nnnnn:nnnnn"></a>server.x=[hostname]:nnnnn:nnnnn</h5><p>没有对应的 Java 系统属性。</p><p>组成 ZooKeeper 集群的服务器。当服务器启动时, 可以通过查找数据目录中的 myid 文件来决定它是哪一台服务器。myid 文件包含服务器编号, 并且它要匹配 “server.x” 中的 x。</p><p>客户端用来组成 ZooKeeper 集群的服务器列表必须和每个 ZooKeeper 服务器中配置的 ZooKeeper 服务器列表相匹配。</p><p>有两个端口号 nnnnn, 第一个是 followers 用来连接到 leader, 第二个是用于 leader 选举。如果想在单台机器上测试多个服务, 则可以为每个服务配置不同的端口。</p><h5 id="syncLimit"><a href="#syncLimit" class="headerlink" title="syncLimit"></a>syncLimit</h5><p>没有对应的 Java 系统属性。</p><p>默认值是 5, 即 tickTime 属性值的 5 倍。它用于配置leader 和 followers 间进行心跳检测的最大延迟时间。如果在设置的时间内 followers 无法与 leader 进行通信, 那么 followers 将会被丢弃。</p><h5 id="group-x-nnnnn-nnnnn"><a href="#group-x-nnnnn-nnnnn" class="headerlink" title="group.x=nnnnn[:nnnnn]"></a>group.x=nnnnn[:nnnnn]</h5><p>没有对应的 Java 系统属性。</p><p>Enables a hierarchical quorum construction.”x” 是一个组的标识, 等号右边的数字对应于服务器的标识. 赋值操作右边是冒号分隔的服务器标识。注意: 组必须是不相交的, 并且所有组联合后必须是 ZooKeeper 集群。</p><h5 id="weight-x-nnnnn"><a href="#weight-x-nnnnn" class="headerlink" title="weight.x=nnnnn"></a>weight.x=nnnnn</h5><p>没有对应的 Java 系统属性。</p><p>和 “group” 一起使用, 当形成集群时它给每个服务器赋权重值。这个值对应于投票时服务器的权重。ZooKeeper 中只有少数部分需要投票, 比如 leader 选举以及原子的广播协议。服务器权重的默认值是 1。如果配置文件中定义了组, 但是没有权重, 那么所有服务器的权重将会赋值为 1。</p><h5 id="cnxTimeout"><a href="#cnxTimeout" class="headerlink" title="cnxTimeout"></a>cnxTimeout</h5><p>对应的 Java 系统属性: zookeeper.cnxTimeout。</p><p>用于配置 leader选举过程中，打开一次连接（选举的 server 互相通信建立连接）的超时时间。默认值是 5s。</p><h2 id="身份认证和授权选项"><a href="#身份认证和授权选项" class="headerlink" title="身份认证和授权选项"></a>身份认证和授权选项</h2><p>本节的选项允许通过身份认证和授权来控制服务执行。</p><p>zookeeper.DigestAuthenticationProvider.superDigest</p><p>对应的 Java 系统属性: zookeeper.DigestAuthenticationProvider.superDigest。</p><p>该功能默认是禁用的。</p><p>能够使 ZooKeeper 集群管理员可以作为一个 “super” 用户来访问 znode 层级结构。特别是对于一个已经认证为超级管理员的用户不需要 ACL 检查。</p><p>org.apache.zookeeper.server.auth.DigestAuthenticationProvider 可以用来生成 superDigest, 调用它带有 “super:<password>“ 参数的方法。当启动集群中的每台服务器时, 将生成的 “super:<data>“ 作为系统属性提供。</data></password></p><p>当 ZooKeeper 客户端向 ZooKeeper 服务器进行身份认证时, 会传递一个 “digest” 和 “super:<password>“ 的认证数据. 注意摘要式身份验证将认证数据以普通文本的形式传递给服务器, 在网络中需要谨慎使用该认证方法, 要么只在本机上或通过一个加密的连接。</password></p><h5 id="实验性选项-特性"><a href="#实验性选项-特性" class="headerlink" title="实验性选项/特性"></a>实验性选项/特性</h5><p>本节列举了一些目前还处于实验阶段的新特性。</p><h5 id="服务器只读模式"><a href="#服务器只读模式" class="headerlink" title="服务器只读模式"></a>服务器只读模式</h5><p>对应的 Java 系统属性: readonlymode.enabled。</p><p>将其设置为 true 将会启用服务器只读模式支持, 默认是禁用的。ROM 允许请求了 ROM 支持的客户端会话连接到服务器, 即使当服务器可能已经从集群中分隔出去。在该模式中, ROM 客户端仍然可以从 ZK 服务中读取值, 但是不能进行写操作以及看见其它客户端所做的一些变更。更多详细信息可以参见 ZOOKEEPER-784 获取更多详细信息。</p><h5 id="不安全的选项"><a href="#不安全的选项" class="headerlink" title="不安全的选项"></a>不安全的选项</h5><p>下面的选项会很有用, 但是使用的时候需要特别小心。</p><h5 id="forceSync"><a href="#forceSync" class="headerlink" title="forceSync"></a>forceSync</h5><p>对应的 Java 系统属性: zookeeper.forceSync。</p><p>用于配置是否需要在事务日志提交的时候调用 FileChannel.force 来保证数据完全同步到磁盘。默认值是 “yes”。如果该选项设置为 “no”, ZooKeeper 将不会强制同步事务更新日志到磁盘。</p><h5 id="jute-maxbuffer"><a href="#jute-maxbuffer" class="headerlink" title="jute.maxbuffer:"></a>jute.maxbuffer:</h5><p>对应的 Java 系统属性: jute.maxbuffer。没有 zookeeper 前缀。</p><p>用于指定一个 znode 中可以存储数据量的最大值, 默认值是 0xfffff, 或 1M 内。如果这个选项改变了, 那么该系统属性必须在所有的服务端和客户端进行设置, 否则会出现问题。ZooKeeper旨在存储大小为千字节数量的数据。</p><h5 id="skipACL"><a href="#skipACL" class="headerlink" title="skipACL"></a>skipACL</h5><p>对应的 Java 系统属性: zookeeper.skipACL。</p><p>用于配置 ZooKeeper 服务器跳过 ACL 权限检查。这将一定程度的提高服务器吞吐量, 但是也向所有客户端完全开放数据访问。</p><h5 id="quorumListenOnAllIPs"><a href="#quorumListenOnAllIPs" class="headerlink" title="quorumListenOnAllIPs"></a>quorumListenOnAllIPs</h5><p>当设置为 true 时, ZooKeeper 服务器将会在所有可用的 IP 地址上监听来自其对等点的连接请求, 而不仅是配置文件的服务器列表中配置的地址。它会影响处理 ZAB 协议和 Fast Leader Election 协议的连接。默认值是 false。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;&lt;code&gt;ZooKeeper&lt;/code&gt;的功能特性通过 &lt;code&gt;ZooKeeper&lt;/code&gt; 配置文件来进行控制管理（ &lt;cod
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="ZooKeeper" scheme="http://blog.yancy.cc/tags/ZooKeeper/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-ZooKeeper集群快速搭建与优化</title>
    <link href="http://blog.yancy.cc/2017/05/26/Bigdata-hadoop/zookeeper/Bigdata-ZooKeeper%E9%9B%86%E7%BE%A4%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BC%98%E5%8C%96/"/>
    <id>http://blog.yancy.cc/2017/05/26/Bigdata-hadoop/zookeeper/Bigdata-ZooKeeper集群快速搭建与优化/</id>
    <published>2017-05-26T06:46:00.000Z</published>
    <updated>2017-09-19T08:36:29.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ZooKeeper集群快速搭建与优化"><a href="#ZooKeeper集群快速搭建与优化" class="headerlink" title="ZooKeeper集群快速搭建与优化"></a>ZooKeeper集群快速搭建与优化</h3><p>之前搞过了hadoop和spark，hue，现在在弄下zookeeper集群，文档就整理下。<br>本文是<code>ZooKeeper</code>的快速搭建,旨在帮助大家以最快的速度完成一个<code>ZK</code>集群的搭建,以便开展其它工作。</p><h2 id="集群："><a href="#集群：" class="headerlink" title="集群："></a>集群：</h2><p>本文使用了3台机器部署ZooKeeper集群，IP和主机名对应关系如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">IP            主机名</div><div class="line">10.46.72.172  namenode</div><div class="line">10.47.88.103  datenode1</div><div class="line">10.47.88.206  datenode2</div></pre></td></tr></table></figure><h3 id="安装说明"><a href="#安装说明" class="headerlink" title="安装说明"></a>安装说明</h3><p><code>Zookeeper</code>机器间不需要设置免密码登录，其它hadoop也可以不设置，只要不使用hadoop-daemons.sh来启动、停止进程，注意不是hadoop-daemon.sh，而是带“s”的那个，带“s”的会读取hadoop的salves文件，远程ssh启动DataNode和备NameNode等。</p><h3 id="配置-etc-hosts"><a href="#配置-etc-hosts" class="headerlink" title="配置/etc/hosts"></a>配置/etc/hosts</h3><p>将3台机器的IP和主机名映射关系，在3台机器上都配置一下，亦即在3台机器的/etc/hosts文件中，均增加以下内容（可以先配置好一台，然后通过scp等命令复制到其它机器上，注意主机名不能包含任何下划线）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">127.0.0.1 localhost  namenode</div><div class="line">::1        localhost localhost.localdomain localhost6 localhost6.localdomain6</div><div class="line">10.47.100.90 salt</div><div class="line">10.46.72.172  namenode</div><div class="line">10.47.88.103 datenode1</div><div class="line">10.47.88.206 datenode2</div><div class="line">10.47.102.137 datenode3</div></pre></td></tr></table></figure><h3 id="Step1"><a href="#Step1" class="headerlink" title="Step1:"></a>Step1:</h3><p>配置 JAVA 环境。检验方法:执行 <code>java –version</code> 和 <code>javac –version</code> 命令。</p><p>下载并解压 <code>zookeeper</code>。<a href="http://mirror.bjtu.edu.cn/apache/zookeeper/zookeeper-3.4.3/" target="_blank" rel="external">链接一</a> ，<a href="http://www-eu.apache.org/dist/zookeeper/" target="_blank" rel="external">链接二</a> (更多版本:<a href="http://dwz.cn/37HGI" target="_blank" rel="external">http://dwz.cn/37HGI</a>)</p><h3 id="Step2"><a href="#Step2" class="headerlink" title="Step2:"></a>Step2:</h3><p>2.zookeeper的环境变量的配置：<br>为了今后操作方便，我们需要对Zookeeper的环境变量进行配置，方法如下：<br>在/etc/profile文件中加入如下的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#set zookeeper environment</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/srv/hadoop/zookeeper-3.3.6</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin:<span class="variable">$ZOOKEEPER_HOME</span>/conf</div></pre></td></tr></table></figure><h3 id="Step3"><a href="#Step3" class="headerlink" title="Step3:"></a>Step3:</h3><p>下载以后解压到我自己新建的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[hadoop@namenode ~]$ tar -zxvf zookeeper-3.3.6.tar.gz -C /srv/hadoop/</div><div class="line">[hadoop@namenode ]$ <span class="built_in">cd</span> /srv/hadoop/zookeeper-3.3.6/conf/</div></pre></td></tr></table></figure><p>将<code>zoo_sample.cfg</code>拷贝一份命名为<code>zoo.cfg</code>,这里我拷贝一份命名为：<code>zoo.cfg</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@namenode conf]$ cp -r zoo_sample.cfg zoo.cfg</div></pre></td></tr></table></figure><p>这里先创建/data和/logs 这两个目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /srv/hadoop/zookeeper-3.3.6/zookeeper/data</div><div class="line">mkdir -p /srv/hadoop/zookeeper-3.3.6/zookeeper/logs</div></pre></td></tr></table></figure><p>注意上图的配置中master，slave1分别为主机名。</p><p>在上面的配置文件中<code>&quot;server.id=host:port:port&quot;</code>中的第一个port是从机器<code>（follower）</code>连接到主机器<code>（leader）</code>的端口号，第二个port是进行leadership选举的端口号。</p><p>修改配置：</p><p><code>vi zoo.cfg</code>，修改有的默认存在，添加红色的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">tickTime=2000</div><div class="line"></div><div class="line">clientPort=2181</div><div class="line"></div><div class="line">initLimit=10</div><div class="line"></div><div class="line">syncLimit=5</div><div class="line">maxClientCnxns=0 这个是设置连接数0没有做限制</div><div class="line">dataDir=/haozhuo/hadoop/zookeeper-3.3.6/zookeeper/data</div><div class="line">dataLogDir=/haozhuo/hadoop/zookeeper-3.3.6/zookeeper/logs</div><div class="line"></div><div class="line">server.0=namenode:2888:3888</div><div class="line">server.1=datanode1:2888:3888</div><div class="line">server.2=datanode2:2888:3888</div></pre></td></tr></table></figure><h3 id="创建maid"><a href="#创建maid" class="headerlink" title="创建maid:"></a>创建maid:</h3><p>这里所有节点都需要创建。<br>接下来在<code>dataDir</code>所指定的目录下创建一个文件名为<code>myid</code>的文件，文件中的内容只有一行，为本主机对应的id值，也就是上图中server.id中的id。例如：在服务器1中的myid的内容应该写入1。<br>创建myid：在<code>zoo.cfg</code>配置文件中的dataDir的目录下面创建<code>myid</code>，每个节点myid要求不一样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /srv/hadoop/zookeeper-3.3.6/zookeeper/data</div><div class="line">touch myid</div></pre></td></tr></table></figure><h3 id="Step4"><a href="#Step4" class="headerlink" title="Step4:"></a>Step4:</h3><p>远程复制分发安装文件<br>最好是把文件打包scp过去<br>接下来将上面的安装文件拷贝到集群中的其他机器上对应的目录下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">haduser@master:~/zookeeper$ scp -r zookeeper-3.4.5/ slave1:/srv/hadoop/</div><div class="line"></div><div class="line">haduser@master:~/zookeeper$ scp -r zookeeper-3.4.5/ slave2:/srv/hadoop/</div></pre></td></tr></table></figure><p>拷贝完成后修改对应的机器上的myid。例如修改slave1中的myid如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">haduser@slave1:~/zookeeper/zookeeper-3.4.5$ <span class="built_in">echo</span> <span class="string">"2"</span> &gt; data/myid</div><div class="line">haduser@slave1:~/zookeeper/zookeeper-3.4.5$ cat data/myid</div><div class="line">[hadoop@namenode hadoop]$ <span class="built_in">echo</span> 0 &gt; /haozhuo/hadoop/zookeeper-3.3.6/zookeeper/data/myid</div><div class="line">[hadoop@datanode1 hadoop]$ <span class="built_in">echo</span> 1 &gt; /haozhuo/hadoop/zookeeper-3.3.6/zookeeper/data/myid</div><div class="line">[hadoop@datanode2 hadoop]$ <span class="built_in">echo</span> 2 &gt; /haozhuo/hadoop/zookeeper-3.3.6/zookeeper/data/myid</div></pre></td></tr></table></figure><h3 id="Step5"><a href="#Step5" class="headerlink" title="Step5:"></a>Step5:</h3><p>启动 ZooKeeper集群<br>在ZooKeeper集群的每个结点上，执行启动ZooKeeper服务的脚本，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@namenode:~ /srv/hadoop/zookeeper-3.3.6/bin/zkServer.sh start</div><div class="line">hadoop@datanode1:~ /srv/hadoop/zookeeper-3.3.6/bin/zkServer.sh start</div><div class="line">haduser@datanode2:~ /srv/hadoop/zookeeper-3.3.6/bin/zkServer.sh start</div></pre></td></tr></table></figure><p>执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/haozhuo/hadoop/zookeeper-3.3.6/bin/zkServer.sh start</div></pre></td></tr></table></figure><h3 id="Step6"><a href="#Step6" class="headerlink" title="Step6:"></a>Step6:</h3><p>检测是否成功启动:执行 <code>jps</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">24933 QuorumPeerMain</div></pre></td></tr></table></figure><p>其中，QuorumPeerMain是zookeeper进程，启动正常。</p><p><code>./zkServer.sh status</code> 查看当前运行状态。</p><p>namenode1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[hadoop@namenode zookeeper-3.3.6]<span class="comment"># ./zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div><div class="line"></div><div class="line">[hadoop@datanode1 zookeeper-3.3.6]<span class="comment"># ./bin/zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfg</div><div class="line">Mode: leader</div><div class="line"></div><div class="line">[hadoop@datanode2 zookeeper-3.3.6]<span class="comment"># ./bin/zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfg</div><div class="line">Mode: leader</div></pre></td></tr></table></figure><h3 id="链接测试"><a href="#链接测试" class="headerlink" title="链接测试"></a>链接测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bin/zkCli.sh -server 127.0.0.1:2181</div><div class="line">bin/zkCli.sh -server 127.0.0.1:2181</div><div class="line">bin/zkCli.sh -server 127.0.0.1:2181</div></pre></td></tr></table></figure><h3 id="Step7"><a href="#Step7" class="headerlink" title="Step7:"></a>Step7:</h3><p>如果单台可以其他几台不行，看配置，如果没有问题。启动查看状态出现异常。</p><p>异常解决:<code>Error contacting service. It is probably not running.</code></p><p>而其他一个节点却是现实正常;</p><p>先<code>stop</code> 掉原<code>zk</code></p><p><code>./bin/zkServer.sh stop</code></p><p>然后以start-foreground方式启动，会看到启动日志</p><p><code>./bin/zkServer.sh start</code></p><p>当出现问题的时候，记得查看日志zookeeper.out，在你配置的dataDir（在conf/zoo.cfg中查看）目录下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">2015-12-29 11:09:38,034 [myid:1] - WARN  [WorkerSender[myid=1]:QuorumCnxManager@400] - Cannot open channel to 3 at election address Node2/10.0.0.102:38888</div><div class="line">java.net.ConnectException: 拒绝连接</div><div class="line">        at java.net.PlainSocketImpl.socketConnect(Native Method)</div><div class="line">        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)</div><div class="line">        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)</div><div class="line">        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)</div><div class="line">        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)</div><div class="line">        at java.net.Socket.connect(Socket.java:579)</div><div class="line">        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:381)</div><div class="line">        at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:354)</div><div class="line">        at org.apache.zookeeper.server.quorum.FastLeaderElection<span class="variable">$Messenger</span><span class="variable">$WorkerSender</span>.process(FastLeaderElection.java:452)</div><div class="line">        at org.apache.zookeeper.server.quorum.FastLeaderElection<span class="variable">$Messenger</span><span class="variable">$WorkerSender</span>.run(FastLeaderElection.java:433)</div><div class="line">        at java.lang.Thread.run(Thread.java:745)</div></pre></td></tr></table></figure><p>可以看到是连接到Node2的3888端口不通（我配置文件设置的节点端口，server.3=Node2:2888:3888），这样就找到问题了，所以当遇到问题的时候记得查看日志文件，这才是最有帮助的，而不是修改什么nc参数。</p><p>这里主要看下是否加入hosts</p><p>查看Node2节点发现，38888端口绑带到127.0.0.1上了，这让Master节点怎么连接呀，只需修改/etc/hosts文件即可，同理，修改Node1，然后重启zookeeper，发现问题解决。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">127.0.0.1 localhost  namenode</div><div class="line">::1        localhost localhost.localdomain localhost6 localhost6.localdomain6</div><div class="line">10.47.100.90 salt</div><div class="line">10.46.72.172  namenode</div><div class="line">10.47.88.103 datenode1</div><div class="line">10.47.88.206 datenode2</div><div class="line">10.47.102.137 datenode3</div></pre></td></tr></table></figure><p>这里我<code>127.0.0.1 localhost namenode</code> 就可以了。</p><h3 id="Step8-如何扩容zookeeper？"><a href="#Step8-如何扩容zookeeper？" class="headerlink" title="Step8: 如何扩容zookeeper？"></a>Step8: 如何扩容zookeeper？</h3><p>只需要将已有的zookeeper打包复制到新的机器上，然后修改myid文件并设置好，然后启动zookeeper即可。</p><hr><h3 id="设置开机自动启动"><a href="#设置开机自动启动" class="headerlink" title="设置开机自动启动"></a>设置开机自动启动</h3><p>1.写个启动脚本放到/etc/rc.d/init.d/zookeeper</p><p>这里touch zookeeper &amp;&amp; chmod +x zookeeper &amp;&amp; vim zookeeper</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">!/bin/bash</div><div class="line"><span class="comment">#chkconfig:2345 20 90</span></div><div class="line"><span class="comment">#description:zookeeper</span></div><div class="line"><span class="comment">#processname:zookeeper</span></div><div class="line"><span class="built_in">export</span> JAVA_HOME=/srv/jdk1.8.0_66</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></div><div class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></div><div class="line">    start) su root /srv/zookeeper-3.3.6/bin/zkServer.sh start;;</div><div class="line">    stop) su root /srv/zookeeper-3.3.6/bin/zkServer.sh stop;;</div><div class="line">    status) su root /srv/zookeeper-3.3.6/bin/zkServer.sh status;;</div><div class="line">    restart) su root /srv/zookeeper-3.3.6/bin/zkServer.shrestart;;</div><div class="line">    *)  <span class="built_in">echo</span> <span class="string">"requirestart|stop|status|restart"</span>;;</div><div class="line"><span class="keyword">esac</span></div></pre></td></tr></table></figure><p>2.设置开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">chkconfig zookeeper on</div></pre></td></tr></table></figure><p>3.验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">chkconfig --add zookeeper</div><div class="line">chkconfig --list zookeeper</div></pre></td></tr></table></figure><p>这个时候我们就可以用service zookeeper start/stop来启动停止zookeeper服务了.<br>使用<code>chkconfig--add zookeeper</code>命令把<code>zookeeper</code>添加到开机启动里面<br>添加完成之后接这个使用<code>chkconfig--list</code>来看看我们添加的<code>zookeeper</code>是否在里面<br>如果上面的操作都正常的话；重启服务器测试就行。</p><div class="tip"><br><br>注意：zookeeper重启出现几种报错：<br><br>1. 启动服务报错找不到指定好的pid文件。<br>2. 关闭服务报错没有在/tmp/路径下面没有/srv/zookeeper-3.3.6/zookeeper/data/zookeeper_server.pid<br></div><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@zookeeper zookeeper-3.3.6]<span class="comment"># ./bin/zkServer.sh start</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfg</div><div class="line">Starting zookeeper ... ./bin/zkServer.sh: line 93: [: /tmp/zookeeper: binary operator expected</div><div class="line">./bin/zkServer.sh: line 103: /tmp/zookeeper</div><div class="line">/srv/zookeeper-3.3.6/zookeeper/data/zookeeper_server.pid: 没有那个文件或目录</div><div class="line">FAILED TO WRITE PID</div><div class="line">[root@zookeeper zookeeper-3.3.6]<span class="comment"># ./bin/zkServer.sh stop</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /srv/zookeeper-3.3.6/bin/../conf/zoo.cfg</div><div class="line">Stopping zookeeper ... no zookeeper to stop (could not find file /tmp/zookeeper</div><div class="line">/srv/zookeeper-3.3.6/zookeeper/data/zookeeper_server.pid)</div></pre></td></tr></table></figure><p>解决方法：网上很少有人讲这么详细，这里我就说下，就是你在修改zoo.cfg配置文件里面：</p><p>dataDir指定的路径是自定义的话等于的时候不要空格写。</p><p>如果重新另外写dataDir ,不要注释掉之前的，最好直接删除，重新指定这样就不会报错了，如果注释掉默认的<code>#dataDir = /tmp</code>这里需要空格。</p><h3 id="报错-占用端口："><a href="#报错-占用端口：" class="headerlink" title="报错 占用端口："></a>报错 占用端口：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/app/zookeeper/bin/../conf/zoo3.cfg</div><div class="line">Starting zookeeper ... STARTED</div></pre></td></tr></table></figure><p>查看状态：<br>用jps命令查看进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># jps</span></div><div class="line">24617 QuorumPeerMain （这个就是zookeeper进程）</div><div class="line"></div><div class="line">/opt/app/zookeeper/bin/zkServer.sh status zoo1.cfg</div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/app/zookeeper/bin/../conf/zoo1.cfg</div><div class="line">Error contacting service. It is probably not running.</div></pre></td></tr></table></figure><p>Ihaozhuo_b3314<br>说明有错误，查看日志文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cd &lt;zookeeper_home&gt;</span></div><div class="line"><span class="comment"># less zookeeper.out</span></div><div class="line">java.net.BindException: 地址已在使用</div><div class="line">杀掉当前进程：</div><div class="line"><span class="comment"># kill -9 24617</span></div></pre></td></tr></table></figure><h3 id="报错-启动服务正常。"><a href="#报错-启动服务正常。" class="headerlink" title="报错: 启动服务正常。"></a>报错: 启动服务正常。</h3><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@api1 zookeeper<span class="number">-3.3</span><span class="number">.6</span>]<span class="comment"># ./bin/zkServer.sh start</span></div><div class="line">JMX enabled <span class="keyword">by</span> <span class="keyword">default</span></div><div class="line">Using config: <span class="regexp">/srv/zookeeper-3.3.6/bin/</span>../conf/zoo.cfg</div><div class="line">Starting zookeeper ... STARTED</div></pre></td></tr></table></figure><p>可是查询进程不存在，可是看pid是有的，然后关闭服务就说没有这个进程。</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">JMX enabled by <span class="keyword">default</span></div><div class="line">Using <span class="string">config:</span> <span class="regexp">/srv/</span>zookeeper<span class="number">-3.3</span><span class="number">.6</span><span class="regexp">/bin/</span>..<span class="regexp">/conf/</span>zoo.cfg</div><div class="line">Stopping zookeeper ... ./zkServer.<span class="string">sh:</span> line <span class="number">133</span>: <span class="string">kill:</span> (<span class="number">31415</span>) - 没有那个进程</div></pre></td></tr></table></figure><h3 id="走kafka查看是否所有节点都启动："><a href="#走kafka查看是否所有节点都启动：" class="headerlink" title="走kafka查看是否所有节点都启动："></a>走kafka查看是否所有节点都启动：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@kafka_03 bin]<span class="comment"># sh zkCli.sh</span></div><div class="line">Connecting to localhost:2181</div><div class="line">2017-01-04 19:20:24,849 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT</div><div class="line">2017-01-04 19:20:24,853 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=kafka_03</div><div class="line">2017-01-04 19:20:24,853 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_66</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/srv/jdk1.8.0_66/jre</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/srv/zookeeper-3.4.6/bin/../build/classes:/srv/zookeeper-3.4.6/bin/../build/lib/*.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/srv/zookeeper-3.4.6/bin/../lib/<span class="built_in">log</span>4j-1.2.16.jar:/srv/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/srv/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/srv/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/srv/zookeeper-3.4.6/bin/../conf:</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line"></div><div class="line">[zk: localhost:2181(CONNECTED) 0] ls /</div><div class="line">[controller_epoch, brokers, zookeeper, kafka, dubbo, admin, isr_change_notification, consumers, config, sthp]</div><div class="line">[zk: localhost:2181(CONNECTED) 5] ls /kafka/brokers/ids</div><div class="line">[0, 1, 2]</div><div class="line"></div><div class="line">显示0 1 2 三台集群机器。</div></pre></td></tr></table></figure><p>kafka 三台集群这里可以看到获取到ids。</p><h3 id="Step9-相关文档"><a href="#Step9-相关文档" class="headerlink" title="Step9: 相关文档"></a>Step9: 相关文档</h3><p>《HBase-0.98.0分布式安装指南》</p><p>《Hive 0.12.0安装指南》</p><p>《ZooKeeper-3.4.6分布式安装指南》</p><p>《Hadoop 2.3.0源码反向工程》</p><p>《在Linux上编译Hadoop-2.4.0》</p><p>《Accumulo-1.5.1安装指南》</p><p>《Drill 1.0.0安装指南》</p><p>《Shark 0.9.1安装指南》</p><p>更多，敬请关注技术博客：<a href="http://blog.yancy.cc">http://blog.yancy.cc</a></p><h3 id="Step10-结束语"><a href="#Step10-结束语" class="headerlink" title="Step10:  结束语"></a>Step10:  结束语</h3><p>至此，ZooKeeper分布式安装大告成功！更多细节，请浏览<a href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html" target="_blank" rel="external">官方文档</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ZooKeeper集群快速搭建与优化&quot;&gt;&lt;a href=&quot;#ZooKeeper集群快速搭建与优化&quot; class=&quot;headerlink&quot; title=&quot;ZooKeeper集群快速搭建与优化&quot;&gt;&lt;/a&gt;ZooKeeper集群快速搭建与优化&lt;/h3&gt;&lt;p&gt;之前搞过了h
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/tags/Big-data-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-hadoop新增节点集群启动请求异常：Last contact：200</title>
    <link href="http://blog.yancy.cc/2017/05/08/Bigdata-hadoop/%20hadoop%E6%96%B0%E5%A2%9E%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E8%AF%B7%E6%B1%82%E5%BC%82%E5%B8%B8%EF%BC%9ALast%20contact%EF%BC%9A200/"/>
    <id>http://blog.yancy.cc/2017/05/08/Bigdata-hadoop/ hadoop新增节点集群启动请求异常：Last contact：200/</id>
    <published>2017-05-08T09:56:03.000Z</published>
    <updated>2017-09-19T08:16:31.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言闲谈："><a href="#前言闲谈：" class="headerlink" title="前言闲谈："></a>前言闲谈：</h3><p>之前做CDN云计算公司来到美年大健康现在在一家医疗大数据公司负责运维部门大大小小的活，既然是医疗大数据当然离不开大数据存储的维护，现在也同时维护大数据运维相关工作 <code>hadoop,spark,sqoop,hue,hive,Hbase,zookeeper</code>等等 测试开发生产使用起来,从集群环境维护 提升数据稳定性 高可用维护。</p><p>前面说了一堆自己闲聊，真正解决这次问题是hadoop新增节点需要注意哪几点：</p><p><code>新增节点如何新增我会在另外一篇详细说的这里我讲一些需要注意掉的问题。</code></p><h3 id="需要修改几个配置："><a href="#需要修改几个配置：" class="headerlink" title="需要修改几个配置："></a>需要修改几个配置：</h3><p>（1）hadoop data 数据目录 VERSION 里面的搭建集群时，直接克隆会出现这个问题。解决方法同上两种，最好修改${/hadoop/tmp/dir}/dfs/data/current/VERSION中的storageID，使其不同。第一种会导致hdfs数据丢失。</p><p>解决方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">（1） datanode启动是会加载DataStorage，如果不存在则会format</div><div class="line">（2）初次启动DataStorage的storageID是空的，所以会生成一个storageID</div></pre></td></tr></table></figure><p>参考我解决的：<code>这里我拷贝过来 直接删除。等集群namenode启动 会自动生成。</code></p><p>这个解决以后 新增的机器必须关闭防火墙。因为这个原因会导致我 hadoop新增节点集群启动请求异常：Last contact：200</p><p>（2） 集群重启时防火墙自动开启导致：</p><p>这里贴张图片给大家看看：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/hadoop1.png" alt=""></figure></p><p>问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">2017-07-04 18:43:30,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /192.168.18.218:9000. Already tried 8 time(s).</div><div class="line">2017-07-04 18:43:31,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /192.168.18.218:9000. Already tried 9 time(s).</div><div class="line">2017-07-04 18:43:31,479 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Call to /192.168.18.218:9000 failed on <span class="built_in">local</span> exception: java.net.NoRouteToHostException: 没有到主机的路由</div><div class="line">        at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:743)</div><div class="line">解决方法：在root权限下关闭防火墙：service iptables stop</div></pre></td></tr></table></figure><p>最好配置成机器重启默认防火墙关闭：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@junlings ~]<span class="comment"># chkconfig iptables off   #开机不启动防火墙服务</span></div><div class="line">[root@junlings ~]<span class="comment"># chkconfig iptables on   #开机启动防火墙服务</span></div></pre></td></tr></table></figure><p>解决以后服务重新跑一遍已经搞定。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/hadoop2.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/hadoop3.png" alt=""></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言闲谈：&quot;&gt;&lt;a href=&quot;#前言闲谈：&quot; class=&quot;headerlink&quot; title=&quot;前言闲谈：&quot;&gt;&lt;/a&gt;前言闲谈：&lt;/h3&gt;&lt;p&gt;之前做CDN云计算公司来到美年大健康现在在一家医疗大数据公司负责运维部门大大小小的活，既然是医疗大数据当然离不开大数
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/tags/Big-data-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>HORTONW0RKS数据平台实现搭建Ambari配置和部署HDP集群监控Hadoop集群</title>
    <link href="http://blog.yancy.cc/2017/05/08/Bigdata-hadoop/HORTONW0RKS%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E5%AE%9E%E7%8E%B0%E6%90%AD%E5%BB%BAAmbari%E9%85%8D%E7%BD%AE%E5%92%8C%E9%83%A8%E7%BD%B2HDP%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7Hadoop%E9%9B%86%E7%BE%A4%20/"/>
    <id>http://blog.yancy.cc/2017/05/08/Bigdata-hadoop/HORTONW0RKS数据平台实现搭建Ambari配置和部署HDP集群监控Hadoop集群 /</id>
    <published>2017-05-08T09:56:03.000Z</published>
    <updated>2017-09-19T08:16:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HORTONW0RKS数据平台搭建Ambari管理监控Hadoop集群"><a href="#HORTONW0RKS数据平台搭建Ambari管理监控Hadoop集群" class="headerlink" title="HORTONW0RKS数据平台搭建Ambari管理监控Hadoop集群"></a>HORTONW0RKS数据平台搭建Ambari管理监控Hadoop集群</h2><p><strong> 题外话 </strong></p><p>我现在在一家上市公司旗下控股子公司负责运维部门，负责IT网络安全办公：主要做的应用运维和网络运维，兼大数据运维。<br>最近跟新来的架构师聊了下Hadoop监控方面：HORTONW0RKS数据平台搭建Ambari监控Hadoop集群.</p><p><strong>HORTONW0RKS数据平台（HDP ®）</strong><br>HDP是业内唯一真正安全的企业级开源的<code>Apache的Hadoop™ ®</code>分配基于集中式架构。HDP解决了静态数据的完整需求，为实时客户应用提供支持，并提供可加速决策和创新的可靠分析。</p><p>使用Hortonworks Sandbox试用最新的HDP功能，或者为生产环境设置HDP，安装和配置群集。<br>查看官网文档：<a href="https://hortonworks.com/downloads/#data-platform" target="_blank" rel="external">HORTONWORKS CONNECTED DATA PLATFORMS DOWNLOADS</a></p><h3 id="1-将Ambari服务存储库文件下载到安装主机上的目录。"><a href="#1-将Ambari服务存储库文件下载到安装主机上的目录。" class="headerlink" title="1. 将Ambari服务存储库文件下载到安装主机上的目录。"></a>1. 将Ambari服务存储库文件下载到安装主机上的目录。</h3><ul><li>Centos6.5 </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget -nv http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.2.0/ambari.repo -O /etc/yum.repos.d/ambari.repo</div></pre></td></tr></table></figure><p>⚠️警告：不要修改<code>ambari.repo</code>文件名。在代理注册期间，此文件应在Ambari服务器主机上可用。</p><ol><li>通过检查repo列表确认存储库已配置。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum repolist</div></pre></td></tr></table></figure><p>您应该在列表中看到类似于以下Ambari存储库的值。</p><p>版本值因安装而异。</p><ol><li>安装Ambari服务。这也安装了默认的PostgreSQL Ambari数据库。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install ambari-server</div></pre></td></tr></table></figure><p>输入<code>y</code>提示，以确认交易和依赖性检查时。</p><p>成功安装将显示类似于以下内容的输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">安装：postgresql-libs-8.4.20-3.el6_6.x86_64 1/4</div><div class="line">安装：postgresql-8.4.20-3.el6_6.x86_64 2/4</div><div class="line">安装：postgresql-server-8.4.20-3.el6_6.x86_64 3/4</div><div class="line">安装：ambari-server-2.4.2.0-1470.x86_64 4/4</div><div class="line">验证：ambari-server-2.4.2.0-1470.x86_64 1/4</div><div class="line">验证：postgresql-8.4.20-3.el6_6.x86_64 2/4</div><div class="line">验证：postgresql-server-8.4.20-3.el6_6.x86_64 3/4</div><div class="line">验证：postgresql-libs-8.4.20-3.el6_6.x86_64 4/4</div><div class="line"></div><div class="line">安装：</div><div class="line">  ambari-server.x86_64 0：2.4.2.0-1470  安装这里的时候会有点慢，因为是访问国外网站下载资源。</div><div class="line"></div><div class="line">已安装依赖关系：</div><div class="line"> postgresql.x86_64 0：8.4.20-3.el6_6           </div><div class="line"> postgresql-libs.x86_64 0：8.4.20-3.el6_6        </div><div class="line"> postgresql-server.x86_64 0：8.4.20-3.el6_6</div></pre></td></tr></table></figure><p>❗️❗️【注意】</p><p>接受有关信任<code>Hortonworks GPG</code>密钥的警告。该键将自动下载并用于验证Hortonworks的软​​件包。您将看到以下消息：</p><p>Importing GPG key 0x07513CAD: Userid: “Jenkins (HDP Builds) <a href="&#109;&#x61;&#105;&#108;&#116;&#x6f;&#x3a;&#106;&#x65;&#110;&#107;&#x69;&#x6e;&#x40;&#x68;&#111;&#x72;&#x74;&#x6f;&#x6e;&#x77;&#111;&#x72;&#107;&#x73;&#x2e;&#99;&#111;&#x6d;">&#106;&#x65;&#110;&#107;&#x69;&#x6e;&#x40;&#x68;&#111;&#x72;&#x74;&#x6f;&#x6e;&#x77;&#111;&#x72;&#107;&#x73;&#x2e;&#99;&#111;&#x6d;</a>“ From :<code>http://s3.amazonaws.com/dev.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</code></p><p>❗️❗️【注意】</p><p>在具有有限Internet访问或没有Internet访问的群集上部署HDP时，应使用其他方法提供对位的访问。<br>有关设置本地存储库的详细信息，请参阅使用本地存储库。</p><p><code>Ambari</code>服务器默认使用嵌入式<code>PostgreSQL</code>数据库。当您安装<code>Ambari</code>服务器时，PostgreSQL软件包和依赖关系必须可用于安装。这些包通常作为操作系统存储库的一部分提供。请确认您具有适用于<code>postgresql-server</code>软件包的相应存储库。</p><h3 id="2-设置Ambari服务器"><a href="#2-设置Ambari服务器" class="headerlink" title="2.设置Ambari服务器"></a>2.设置Ambari服务器</h3><p>在启动<code>Ambari</code>服务器之前，必须设置Ambari服务器。安装程序将Ambari配置为与Ambari数据库通信，安装JDK并允许您自定义Ambari Server守护程序将作为运行的用户帐户。该 <code>ambari-server setup</code>命令管理设置过程。在Ambari服务器主机上运行以下命令以开始设置过程。您还可以将“ 设置选项”附加到命令。</p><ul><li>启动服务</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">ambari-server setup</div><div class="line"></div><div class="line"><span class="comment">###这里由于下载jdk1.8太慢速度过慢。我提前把jdk下载下来放到了/srv/jdk1.8.0_66 目录</span></div><div class="line"></div><div class="line">[root@ambari-server_01 ~]<span class="comment"># ambari-server setup</span></div><div class="line">Using python  /usr/bin/python</div><div class="line">Setup ambari-server</div><div class="line">Checking SELinux...</div><div class="line">SELinux status is <span class="string">'enabled'</span></div><div class="line">SELinux mode is <span class="string">'permissive'</span></div><div class="line">WARNING: SELinux is <span class="built_in">set</span> to <span class="string">'permissive'</span> mode and temporarily disabled.</div><div class="line">OK to <span class="built_in">continue</span> [y/n] (y)? y</div><div class="line">Customize user account <span class="keyword">for</span> ambari-server daemon [y/n] (n)? y</div><div class="line">Enter user account <span class="keyword">for</span> ambari-server daemon (root):</div><div class="line">Adjusting ambari-server permissions and ownership...</div><div class="line">Checking firewall status...</div><div class="line">Checking JDK...</div><div class="line">[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8</div><div class="line">[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7</div><div class="line">[3] Custom JDK</div><div class="line">==============================================================================</div><div class="line">Enter choice (1): 3</div><div class="line">WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.</div><div class="line">WARNING: JCE Policy files are required <span class="keyword">for</span> configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.</div><div class="line">Path to JAVA_HOME: /srv/jdk1.8.0_66</div><div class="line">Validating JDK on Ambari Server...done.</div><div class="line">Completing setup...</div><div class="line">Configuring database...</div><div class="line">Enter advanced database configuration [y/n] (n)? y</div><div class="line">Configuring database...</div><div class="line">==============================================================================</div><div class="line">Choose one of the following options:</div><div class="line">[1] - PostgreSQL (Embedded)</div><div class="line">[2] - Oracle</div><div class="line">[3] - MySQL / MariaDB</div><div class="line">[4] - PostgreSQL</div><div class="line">[5] - Microsoft SQL Server (Tech Preview)</div><div class="line">[6] - SQL Anywhere</div><div class="line">[7] - BDB</div><div class="line">==============================================================================</div><div class="line">Enter choice (1):3</div><div class="line">Database name (ambari):</div><div class="line">Postgres schema (ambari):</div><div class="line">Username (ambari):</div><div class="line">Enter Database Password (bigdata):</div><div class="line">Default properties detected. Using built-in database.</div><div class="line">Configuring ambari database...</div><div class="line">Checking PostgreSQL...</div><div class="line">Running initdb: This may take up to a minute.</div><div class="line">正在初始化数据库：[确定]</div><div class="line"></div><div class="line">About to start PostgreSQL</div><div class="line">Configuring <span class="built_in">local</span> database...</div><div class="line">Connecting to <span class="built_in">local</span> database...done.</div><div class="line">Configuring PostgreSQL...</div><div class="line">Restarting PostgreSQL</div><div class="line">Extracting system views...</div><div class="line">ambari-admin-2.4.2.0.136.jar</div><div class="line">............</div><div class="line">Adjusting ambari-server permissions and ownership...</div><div class="line">Ambari Server <span class="string">'setup'</span> completed successfully.</div><div class="line">You have mail <span class="keyword">in</span> /var/spool/mail/root</div></pre></td></tr></table></figure><h5 id="2-1-响应安装提示："><a href="#2-1-响应安装提示：" class="headerlink" title="2.1 响应安装提示："></a>2.1 响应安装提示：</h5><p>如果您没有暂时禁用<code>SELinux</code>，您可能会收到警告。接受默认值<code>（y）</code>，然后继续。</p><p>默认情况下，Ambari服务器运行在<code>root</code>。在<code>Customize user account for ambari-server daemon</code>提示符处接受默认<code>（n）</code>，以继续root。</p><p>如果要创建其他用户以运行<code>Ambari</code>服务器或分配以前创建的用户，请<code>y</code>在 <code>Customize user account for ambari-server daemon</code>提示符处选择，然后提供用户名。<br>有关以非root用户身份运行Ambari服务器的更多信息，请参阅Hortonworks数据平台Apache Ambari参考&gt; <a href="http://docs.hortonworks.com/HDPDocuments/Ambari-2.4.2.0/bk_ambari-security/content/configuring_ambari_for_non-root.html" target="_blank" rel="external">为非根用户配置Ambari</a></p><p>如果您没有暂时停用<code>iptables</code>，可能会收到警告。输入<code>y</code>以继续。</p><p>JDK<br>选择要下载的<code>JDK</code>版本。输入<code>1</code>以下载<code>Oracle JDK 1.8</code>。或者，您可以选择输入<code>自定义JDK</code>。如果选择“<code>自定义JDK</code>”，则必须在所有主机上手动安装JDK并指定<code>Java Home</code>路径。</p><p>❗️❗️【注意】</p><p>JDK支持完全取决于您选择的<code>HDP Stack</code>版本。请参阅<a href="http://docs.hortonworks.com/HDPDocuments/Ambari-2.4.2.0/bk_ambari-reference/content/ch_changing_the_jdk_version_on_an_existing_cluster.html" target="_blank" rel="external">Hortonworks数据平台Apache Ambari</a>参考以查看要安装的HDP Stack版本支持的JDK版本。默认情况下，Ambari服务器设置下载并安装Oracle JDK 1.8和随附的Java密码术扩展（JCE）策略文件。如果计划使用其他版本的JDK，请参阅 设置选项以获取更多信息。</p><p>出现提示时接受<code>Oracle JDK</code>许可证。您必须接受此许可证才能从Oracle下载必需的JDK。JDK在部署阶段安装。</p><blockquote><p>数据库选择：</p></blockquote><p>选择<code>n</code>为，<code>Enter advanced database configuration</code>以便为<code>Ambari</code>使用默认的嵌入式<code>PostgreSQL</code>数据库。默认的PostgreSQL数据库名是ambari。默认用户名和密码为ambari/bigdata。否则，要使用现有的PostgreSQL，MySQL / MariaDB或Oracle数据库与Ambari，请选择y。</p><p>如果使用现有的PostgreSQL，MySQL / MariaDB或Oracle数据库实例，请使用以下提示之一：</p><p>❗️❗️[重要]<br>在运行安装程序和输入高级数据库配置之前，必须使用<code>“使用非默认数据库</code>- Ambari”中详述的步骤准备非默认数据库实例。</p><p>❗️❗️[重要]</p><p>不支持使用Microsoft SQL Server或SQL Anywhere数据库选项。</p><p>要使用现有的Oracle实例，并为该数据库选择自己的数据库名称，用户名和密码，请输入<code>2</code>。<br>选择要使用的数据库，并提供在提示中请求的任何信息，包括主机名，端口，服务名或SID，用户名和密码。<br>要使用现有的MySQL / MariaDB数据库，并为该数据库选择自己的数据库名称，用户名和密码，请输入3。<br>选择要使用的数据库，并提供在提示中请求的任何信息，包括主机名，端口，数据库名称，用户名和密码。<br>要使用现有的PostgreSQL数据库，并为该数据库选择自己的数据库名称，用户名和密码，请输入4。<br>选择要使用的数据库，并提供在提示中请求的任何信息，包括主机名，端口，数据库名称，用户名和密码。<br>继续配置远程数据库连接属性[y / n]选择<code>y</code>。</p><blockquote><p>这里数据库用户名和密码都是默认安装：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Database name (ambari)</div><div class="line">Postgres schema (ambari)</div><div class="line">Username (ambari)</div><div class="line">Enter Database Password (bigdata)</div></pre></td></tr></table></figure><blockquote><p>这里我做了一层Nginx代理：将Ambari服务器配置为使用此代理服务器</p></blockquote><h3 id="3-启动Ambari服务器"><a href="#3-启动Ambari服务器" class="headerlink" title="3.启动Ambari服务器"></a>3.启动Ambari服务器</h3><p>在Ambari服务器主机上运行以下命令：</p><pre><code>ambari-server start</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@ambari-server ~]<span class="comment"># ambari-server start</span></div><div class="line">Using python  /usr/bin/python</div><div class="line">Starting ambari-server</div><div class="line">Ambari Server running with administrator privileges.</div><div class="line">Organizing resource files at /var/lib/ambari-server/resources...</div><div class="line">Ambari database consistency check started...</div><div class="line">No errors were found.</div><div class="line">Ambari database consistency check finished</div><div class="line">Server PID at: /var/run/ambari-server/ambari-server.pid</div><div class="line">Server out at: /var/<span class="built_in">log</span>/ambari-server/ambari-server.out</div><div class="line">Server <span class="built_in">log</span> at: /var/<span class="built_in">log</span>/ambari-server/ambari-server.log</div><div class="line">Waiting <span class="keyword">for</span> server start....................</div><div class="line">Ambari Server <span class="string">'start'</span> completed successfully.</div><div class="line">You have mail <span class="keyword">in</span> /var/spool/mail/root</div></pre></td></tr></table></figure><p>要检查Ambari服务器进程：</p><pre><code>ambari-server status</code></pre><p>停止Ambari服务器：</p><pre><code>ambari-server stop</code></pre><p>在Ambari服务器启动时，<code>Ambari</code>运行数据库一致性检查，查找问题。如果发现任何问题，Ambari服务器启动将中止，并且一条消息将打印到控制台“数据库配置一致性检查失败”。更多详细信息将写入以下日志文​​件：</p><pre><code>/var/log/ambari-server/ambari-server-check-database.log</code></pre><p>您可以通过使用以下选项跳过此检查来强制Ambari服务器启动：</p><pre><code>ambari-server start --skip-database-check</code></pre><p>如果存在数据库问题，请选择跳过此检查，在更正数据库一致性问题之前，不要对集群拓扑进行任何更改或执行集群升级。最好查看官网操作。</p><h2 id="第3章安装，配置和部署HDP集群"><a href="#第3章安装，配置和部署HDP集群" class="headerlink" title="第3章安装，配置和部署HDP集群"></a>第3章安装，配置和部署HDP集群</h2><h3 id="1-登录到Apache-Ambari"><a href="#1-登录到Apache-Ambari" class="headerlink" title="1.登录到Apache Ambari"></a>1.登录到Apache Ambari</h3><ul><li><p>将浏览器指向 <code>http://&lt;your.ambari.server&gt; :8080</code>，其中<code>&lt;your.ambari.server&gt;</code>是您的ambari服务器主机的名称。例如，默认Ambari服务器主机位于<code>http://c6401.ambari.apache.org:8080</code>。</p></li><li><p>使用默认用户名/密码登录Ambari服务器：<code>admin / admin</code>。您可以稍后更改这些凭据。</p></li></ul><h3 id="2-启动Ambari安装向导"><a href="#2-启动Ambari安装向导" class="headerlink" title="2.启动Ambari安装向导"></a>2.启动Ambari安装向导</h3><p>从<code>Ambari Welcome</code>页面，选择启动安装向导。</p><p>提供集群，管理谁可以访问群集，以及自定义视图为Ambari用户。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Ambariwelcome.png" alt=""></figure></p><p><figure class="figure"><img src="http://docs.hortonworks.com/HDPDocuments/Ambari-2.4.2.0/bk_ambari-installation/content/figures/2/figures/170AmbariWelcome.png" alt=""></figure></p><h3 id="3-命名您的群集"><a href="#3-命名您的群集" class="headerlink" title="3.命名您的群集"></a>3.命名您的群集</h3><p>在Name your cluster，键入要创建的集群的名称。名称中不要使用空格或特殊字符。</p><p>选择Next。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ambari_02.png" alt=""></figure></p><h3 id="4-选择版本"><a href="#4-选择版本" class="headerlink" title="4.选择版本"></a>4.选择版本</h3><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ambari_03.png" alt=""></figure></p><p>这里我选择<code>redhat6</code> 其他的都<code>remove</code>掉。</p><p>在此步骤中，您将选择群集的软件版本和交付方式。使用公共存储库需要Internet连接。使用本地存储库需要您在网络中可用的存储库中配置软件。</p><ul><li>选择堆栈</li></ul><p>可用的HDP版本显示在TAB中。当您选择TAB时，Ambari会尝试发现该HDP堆栈的特定版本可用。该列表显示在DROPDOWN中。对于该特定版本，将显示可用的服务，其中的版本显示在TABLE中。</p><ul><li>选择版本</li></ul><p>如果Ambari可以访问Internet，则特定版本将作为选项列在DROPDOWN中。如果您有未列出的版本的版本定义文件，您可以单击添加版本…并上载VDF文件。此外，如果您无法访问Internet或不确定要安装哪个特定版本，则 默认版本定义也包含在列表中。</p><ul><li>选择存储库</li></ul><p>Ambari允许您选择从公共存储库（如果您有Internet访问权限）或本地存储库安装软件。无论您的选择如何，您都可以编辑存储库的基本URL。将显示可用的操作系统，您可以从列表中添加/删除操作系统以适合您的环境</p><p>❗️❗️注意<br>UI显示基于操作系统系列（OS系列）的存储库基本URL。请确保基于正在运行的操作系统设置正确的操作系统系列。下表将OS系列映射到操作系统。</p><ul><li><p><strong>高级选项</strong></p></li><li><p><strong>有高级存储库选项可用。</strong></p></li></ul><p>跳过存储库基本URL验证（高级）： 当您单击下一步时，Ambari将尝试连接到存储库基本URL，并验证您已输入验证存储库。如果没有，将显示一个错误，您必须在继续之前纠正。</p><p>使用<code>RedHat Satellite/Spacewalk：</code>仅当计划使用本地存储库时，才会启用此选项。当您为软件存储库选择此选项时，您负责配置<code>Satellite/Spacewalk</code>中的存储库通道，并确认所选群集版本的存储库在群集中的主机上可用。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ambari_04.png" alt=""></figure></p><h3 id="5-安装选项"><a href="#5-安装选项" class="headerlink" title="5.安装选项"></a>5.安装选项</h3><p>为了构建集群，安装向导将提示您有关如何设置它的一般信息。您需要提供每个主机的FQDN。该向导还需要访问在设置无密码SSH中创建的私钥文件  。使用主机名和密钥文件信息，向导可以定位，访问和与群集中的所有主机安全交互。</p><p>使用Target Hosts文本框输入主机名列表，每行一个。您可以使用括号内的范围来表示较大的主机集。例如，对于host01.domain通过host10.domain使用 host[01-10].domain</p><p>⚠️ <strong>安装服务器集群机器一定要系统版本要一致不然安装会提示版本不兼容。</strong></p><p>❗️❗️    注意<br>如果要在EC2上部署，请使用内部专用<code>DNS主机名</code>。</p><p>在ambari服务器配置hosts</p><pre><code>vim /etc/hosts</code></pre><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">192<span class="selector-class">.168</span><span class="selector-class">.1</span><span class="selector-class">.151</span>    <span class="selector-tag">datanode151</span></div><div class="line">192<span class="selector-class">.168</span><span class="selector-class">.1</span><span class="selector-class">.173</span>    <span class="selector-tag">datanode_173</span> <span class="selector-tag">datanode-173</span><span class="selector-class">.hadoop</span></div></pre></td></tr></table></figure><p>如果要让<code>Ambari</code>使用SSH在所有主机上自动安装Ambari代理，请选择<code>Provide your SSH Private Key</code>并使用部分中的 <code>Choose File</code>按钮<code>Host Registration Information</code>查找与先前在所有主机上安装的公钥相匹配的私钥文件，或者剪切并粘贴键手动插入文本框。</p><p>填写您选择的SSH密钥的用户名。如果不想使用root用户，则必须为可以在不输入密码的情况下执行sudo的帐户提供用户名。如果您的环境中的主机上的SSH配置为22以外的端口，您也可以更改它。</p><p>如果您不希望Ambari自动安装Ambari代理，请选择<code>Perform manual registration</code>。有关更多信息，请参阅手动安装Ambari代理。</p><p>选择<code>Register and Confirm</code>继续。</p><p>这里提示：The following hostnames are not valid FQDNs: datanode_173.hadoop</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ambari_06.png" alt=""></figure></p><p>这里跳转到安装页面：发现报错提示datanode-173.hadoop主机访问Ambari机器不能访问。<br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ambari_07.png" alt=""></figure></p><h3 id="6-确认主机"><a href="#6-确认主机" class="headerlink" title="6.确认主机"></a>6.确认主机</h3><p><code>Confirm Hosts</code> 提示您确认<code>Ambari</code>已为您的集群找到正确的主机，并检查这些主机以确保它们具有继续安装所需的正确目录，软件包和进程。</p><p>如果选择了错误的主机，您可以通过选择相应的复选框并单击灰色<code>Remove Selected</code>按钮来删除它们。要删除单个主机，请单击Remove“操作”列中的小白色按钮。</p><p>在屏幕底部，您可能会注意到一个黄色框，表示在检查过程中遇到了一些警告。例如，您的主机可能已有<code>wget</code>或的副本 <code>curl</code>。选择<code>Click here to see the warnings</code> 查看检查内容和导致警告的原因的列表。警告页面还提供对python脚本的访问，可以帮助您清除可能遇到的任何问题，让您运行<code>Rerun Checks</code>。</p><p>在datanode_173服务器配置hosts</p><pre><code>vim /etc/hosts</code></pre><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">192<span class="selector-class">.168</span><span class="selector-class">.1</span><span class="selector-class">.151</span>    <span class="selector-tag">datanode151</span></div><div class="line">192<span class="selector-class">.168</span><span class="selector-class">.1</span><span class="selector-class">.173</span>    <span class="selector-tag">datanode_173</span> <span class="selector-tag">datanode-173</span><span class="selector-class">.hadoop</span></div></pre></td></tr></table></figure><p>每台节点里配置FQDN，如下以主节点为例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">vi /etc/sysconfig/network</div><div class="line">NETWORKING=yes</div><div class="line">HOSTNAME=SY-001.hadoop</div></pre></td></tr></table></figure><p>配置上就可以了。 把datanode151也是ambari机器配置在hosts就可以了。</p><p><strong>最好设置root的无密码登录，因为我们配置的集群都是内网的，没什么安全性问题，使用root操作可以省去一些麻烦，非root用户可能在安装Hadoop组件时不能成功</strong></p><p>下面是我安装这里提示没有找到文件目录查看安装报错操作：</p><pre><code>mkdir /var/lib/ambari-agent/data</code></pre><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Ambari_08.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ambari_09.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ambari_10.png" alt=""></figure></p><h3 id="7-选择服务"><a href="#7-选择服务" class="headerlink" title="7.选择服务"></a>7.选择服务</h3><p>将看到选择要安装到群集中的服务。HDP堆栈包括许多服务。您可以选择立即安装任何其他可用服务，或稍后添加服务。默认情况下，安装向导将选择所有可用的服务进行安装。</p><p>选择<code>none</code>清除所有选择，或选择 <code>all</code>选择所有列出的服务。</p><p>选择或清除单个复选框以定义一组要立即安装的服务。</p><p>选择要立即安装的服务后，选择Next。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ambari_11.png" alt=""></figure></p><h3 id="8-分配主站"><a href="#8-分配主站" class="headerlink" title="8.分配主站"></a>8.分配主站</h3><p>Ambari安装向导会将所选服务的主组件分配给集群中的相应主机，并在Assign Masters中显示分配。左列显示服务和当前主机。右列显示主机的当前主组件分配，指示每个主机上安装的CPU内核数量和RAM数量。</p><p>要更改服务的主机分配，请从该服务的下拉菜单中选择主机名。</p><p>要删除ZooKeeper实例，请单击要删除的主机地址旁边的绿色减号图标。</p><p>当您对作业感到满意时，选择Next。</p><p><figure class="figure"><img src="" alt=""></figure></p><h3 id="9-分配从属和客户端"><a href="#9-分配从属和客户端" class="headerlink" title="9.分配从属和客户端"></a>9.分配从属和客户端</h3><p>Ambari安装向导将从属组件（DataNodes，NodeManager和RegionServers）分配给集群中的相应主机。它还会尝试选择主机以安装适当的客户端集。</p><p>使用<code>all</code>或<code>none</code>可分别选择列中的所有主机或不选择任何主机。</p><p>如果主机旁边有星号，则该主机也运行一个或多个主组件。将鼠标悬停在星号上，以查看该主机上的哪些主组件。</p><p>通过使用特定主机旁边的复选框来微调您的选择。</p><p>当你对你的作业感到满意时，选择<code>Next</code>。</p><h3 id="10-自定义服务"><a href="#10-自定义服务" class="headerlink" title="10.自定义服务"></a>10.自定义服务</h3><p>自定义服务步骤为您提供一组选项卡，您可以查看和修改HDP集群设置。向导会尝试为每个选项设置合理的默认值。你是强烈建议，以检查这些设置为您的要求可能略有不同。</p><p>浏览每个服务标签，然后将光标悬停在每个属性上，您可以看到属性做什么的简要说明。显示的服务选项卡数取决于您决定在群集中安装的服务。任何需要输入的选项卡都会显示一个红色徽章，其中包含需要注意的属性数。选择显示红色徽章编号的每个服务选项卡，然后输入相应的信息。</p><ul><li>目录</li></ul><p>HDP将存储信息的目录的选择是至关重要的。Ambari将尝试根据您环境中可用的安装点选择合理的默认值，但强烈建议您查看Ambari推荐的默认目录设置。特别是，确认目录，例如<code>/tmp和 /var</code>被不被用于下<code>HDFS的NameNode</code>目录和数据管理部目录HDFS标签。</p><ul><li>密码</li></ul><p>您必须为Hive和Oozie服务以及Knox的主密钥提供数据库密码。使用Hive作为示例，选择Hive选项卡并展开高级部分。在以红色标记的数据库密码字段中，提供密码，然后重新键入以确认。</p><p>安装各个服务，并且完成安装后会启动相关服务，安装过程比较长，如中中出现错误，根据具体提供或日志进行操作。 </p><p>这里我就不贴出来了，因为测试环境机器我做测试用机器配置不够所有后面结果经验写出来了。</p><p>安装的还是提示失败：<code>ImportError: No module named rpm</code></p><p>参考这篇文章重新安装解决了这个问题：<a href="http://stackoverflow.com/questions/17490921/no-module-named-rpm-when-i-call-yum-on-shell" target="_blank" rel="external">ImportError: No module named rpm</a></p><h3 id="11-安装，启动和测试"><a href="#11-安装，启动和测试" class="headerlink" title="11.安装，启动和测试"></a>11.安装，启动和测试</h3><p>安装的进度将显示在屏幕上。Ambari安装，启动，并对每个组件运行一个简单的测试。过程的总体状态显示在屏幕顶部的进度栏中，主机的主机状态显示在主要部分。在此过程中不要刷新浏览器。刷新浏览器可能会中断进度指示器。</p><p>要查看每个主机已完成的任务的具体信息，请单击Message相应主机列中的链接。在 Tasks弹出窗口中，单击单个任务以查看相关的日志文件。您可以使用Show下拉列表选择过滤条件。要查看更大版本的日志内容，请单击Open图标或将内容复制到剪贴板，使用Copy图标。</p><p><strong>安装完成效果图:</strong></p><p><figure class="figure"><img src="http://hortonassets.s3.amazonaws.com/ambari1_7/ambari002.png" alt=""></figure></p><blockquote><p>让我们从左侧栏或下拉菜单中选择Yarn进入YARN信息中心。</p></blockquote><p><figure class="figure"><img src="http://hortonassets.s3.amazonaws.com/ambari1_7/ambari004.png" alt=""></figure></p><blockquote><p>我们将开始更新线程容量调度策略的配置。</p></blockquote><p><figure class="figure"><img src="http://hortonassets.s3.amazonaws.com/ambari1_7/ambari005.png" alt=""></figure></p><p>向下滚动到<code>Scheduler</code>页面的部分。默认容量调度策略只有一个队列。</p><p><figure class="figure"><img src="http://hortonassets.s3.amazonaws.com/ambari1_7/ambari006.png" alt=""></figure></p><p>让我们看看调度策略。向上滚动到页面顶部，然后点击快速链接。然后从下拉列表中选择<code>ResourceManager UI</code>。</p><p><figure class="figure"><img src="http://hortonassets.s3.amazonaws.com/ambari1_7/ambari007.png" alt=""></figure></p><blockquote><p>正如你可以看到，我们只有默认策略。</p></blockquote><p><figure class="figure"><img src="http://hortonassets.s3.amazonaws.com/ambari1_7/ambari008.png" alt=""></figure></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>安装Ambari官网地址<a href="https://docs.hortonworks.com/HDPDocuments/Ambari-2.4.2.0/bk_ambari-installation/content/download_the_ambari_repo_lnx6.html" target="_blank" rel="external">install Ambari</a><br><a href="https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.4.2" target="_blank" rel="external">编译安装Ambari 2.4.2安装指南</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;HORTONW0RKS数据平台搭建Ambari管理监控Hadoop集群&quot;&gt;&lt;a href=&quot;#HORTONW0RKS数据平台搭建Ambari管理监控Hadoop集群&quot; class=&quot;headerlink&quot; title=&quot;HORTONW0RKS数据平台搭建Ambar
      
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Big data Hadoop" scheme="http://blog.yancy.cc/tags/Big-data-Hadoop/"/>
    
      <category term="Ambari" scheme="http://blog.yancy.cc/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？</title>
    <link href="http://blog.yancy.cc/2017/03/30/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%10%E5%B9%B3%E5%8F%B0/Elasticsearch/%E7%A0%94%E7%A9%B6%E5%AD%A6%E4%B9%A0%20Elasticsearch%205.0.0%20%E5%8A%9F%E8%83%BD%E6%8F%90%E5%8D%87%E5%93%AA%E4%BA%9B,ES%E5%A6%82%E4%BD%95%E5%BC%B9%E6%80%A7%E8%BF%81%E7%A7%BB%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%AE%89%E8%A3%85%E9%9C%80%E8%A6%81%E4%BB%80%E4%B9%88%E7%8E%AF%E5%A2%83%EF%BC%9F/"/>
    <id>http://blog.yancy.cc/2017/03/30/日志分析平台/Elasticsearch/研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？/</id>
    <published>2017-03-30T09:56:03.000Z</published>
    <updated>2017-09-21T14:48:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>ES 现在普遍大公司都已经在使用，有的用来做数据存储，也有做性能监控，也有做日志收集，跟大数据结合做日志分析。<br>今天在家无聊研究官网学习了下Elasticsearch5.0.0 功能新增哪些，个人觉得ES也是现在开源日志分析平台比较火的，从14年开始陆续使用频率不断提升。<br>之前也有了解过Graylog  现在比ES 做数据存储分析 不错的是Graylog 比ES性能好。</p><p>废话不多说了哈哈</p><h3 id="Elasticsearch-5-0-0发布"><a href="#Elasticsearch-5-0-0发布" class="headerlink" title="Elasticsearch 5.0.0发布"></a>Elasticsearch 5.0.0发布</h3><p>2016年10月26日发布 Elasticsearch 5.0.0 今天去下载吧！你知道你想。</p><p><a href="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="external">下载Elasticsearch 5.0.0</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.0/release-notes-5.0.0.html" target="_blank" rel="external">Elasticsearch 5.0.0发行说明</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.0/release-notes-5.0.0.html" target="_blank" rel="external">弹性搜索5.0突破变化</a></p><p>Elasticsearch 5.0.0对大家有所帮助 这是有史以来最快，最安全，最具弹性，最简单易用的Elasticsearch版本，它具有一系列增强功能和新功能。</p><ul><li>索引性能</li><li>取食节点</li><li>无痛脚本</li><li>新的数据结构</li><li>搜索和聚合</li><li>用户友好</li><li>弹性</li><li>Java REST客户端</li><li>迁移助手</li></ul><blockquote><p>下面从官网上面翻译得来的：</p></blockquote><h3 id="索引性能"><a href="#索引性能" class="headerlink" title="索引性能"></a>索引性能</h3><p>由于包括更好的数字数据结构（参见新数据结构）的一些更改，索引吞吐量在5.0.0中有了显着改善，锁中的争用减少，阻止对同一文档的并发更新，以及减少事务日志时的锁定需求。异步translog fsyncing将特别有利于旋转磁盘的用户，而依赖Elasticsearch自动生成文档ID时，仅追加用例（基于思考时间的事件）具有很大的吞吐量提高。对实时文档GET的支持的内部变化意味着索引缓冲区可用的内存更多，垃圾收集时间少得多。</p><p>根据您的用例，您可能会看到在提高吞吐量的25％至80％之间。</p><h3 id="Ingest节点"><a href="#Ingest节点" class="headerlink" title="Ingest节点"></a>Ingest节点</h3><p>　　向Elasticsearch添加数据更简单了。Logstash是一个强大的工具，而一些较小的用户只需要过滤器，不需要它所提供的众多路由选项。因此，Elastic将一些最流行的Logstash过滤器（如grok、split）直接在Elasticsearch中实现为处理器。多个处理器可以组合成一个管道，在索引时应用到文档上。</p><h3 id="Painless脚本"><a href="#Painless脚本" class="headerlink" title="Painless脚本"></a>Painless脚本</h3><p>　　Elasticsearch中很多地方用到了脚本，而出于安全考虑，脚本在默认情况下是禁用的，这令人相当失望。为此，Elastic开发了一种新的脚本语言Painless。该语言更快、更安全，而且默认是启用的。不仅如此，它的执行速度是Groovy的4倍，而且正在变得更快。Painless已经成为默认脚本语言，而Groovy、Javascript和Python都遭到弃用。要了解有关这门新语言的更多信息，请点击这里。</p><h3 id="新数据结构"><a href="#新数据结构" class="headerlink" title="新数据结构"></a>新数据结构</h3><p>　　Lucene 6带来了一个新的Points 数据结构K-D树，用于存储数值型和地理位置字段，彻底改变了数值型值的索引和搜索方式。基准测试表明，Points将查询速度提升了36%，将索引速度提升了71%，而占用的磁盘和内存空间分别减少了66%和85%（参见“在5.0中搜索数值”）。</p><h4 id="搜索和聚合"><a href="#搜索和聚合" class="headerlink" title="搜索和聚合"></a>搜索和聚合</h4><p>　　借助即时聚合，Kibana图表生成速度显著提升。Elastic用一年的时间对搜索API进行了重构，Elasticsearch现在可以更巧妙地执行范围查询，只针对已经发生变化的索引重新计算聚合，而不是针对每个查询从头开始重新计算。在搜索方面，默认的相关性计算已经由TF/IDF换成了更先进的BM25。补全建议程序经过了完全重写，将已删除的文档也考虑了进来。</p><h3 id="更友好"><a href="#更友好" class="headerlink" title="更友好"></a>更友好</h3><p>　　Elasticsearch 5.0更安全、更易用。他们采用了“尽早提示”的方法。如果出现了问题，则新版本会及早给出提示。例如，Elasticsearch 5.0会严格验证设置。如果它不能识别某项设置的值，就会给出提示和建议。不仅如此，集群和索引设置现在可以通过null进行解除。此外，还有其他的一些改进，例如，rollover和shrink API启用了一种新的模式来管理基于时间的索引，引入新的cluster-allocation-explain API，简化索引创建。</p><h3 id="弹性"><a href="#弹性" class="headerlink" title="弹性"></a>弹性</h3><p>　　Elasticsearch分布式模型的每一部分都被分解、重构和简化，提升了可靠性。集群状态更新现在会等待集群中的所有节点确认。如果一个“复制片（replica shard）”被“主片（primary）”标记为失败，则主片会等待“主节点（master）”的响应。索引现在使用数据路径中的UUID，而不是索引名，避免了命名冲突。另外，Elasticsearch现在进行启动检查，确保系统配置没有问题。配置比较麻烦，但如果只是试用，开发人员也可以选择localhost-only模式，避免繁琐的配置。另外，新版本还增加了断路器及其他一些软限制，限制请求使用的内存大小，保护集群免受恶意用户攻击。<br>　　此外，该版本还提供了一个底层的Java REST/HTTP客户端，可以用于监听、日志记录、请求轮询、故障节点重试等。它使用Java 7，将依赖降到了最低，比Transport客户端的依赖冲突少。而在基准测试中，它的性能并不输于Transport客户端。不过，这是一个底层客户端，目前还没有提供任何查询构建器或辅助器。它的输入参数和输出结果都是JSON。<br>　　需要注意的是，该版本引入了许多破坏性更改，好在他们提供了一个迁移辅助插件，可以帮助开发人员从Elasticsearch 2.3.x/2.4.x迁移到Elasticsearch 5.0。如果是从更早的Elasticsearch版本向最新的5.0版本迁移，则请查阅升级文档。</p><h3 id="Java-REST客户端"><a href="#Java-REST客户端" class="headerlink" title="Java REST客户端"></a>Java REST客户端</h3><p>经过多年的等待，我们终于发布了一个低级的Java HTTP / REST客户端。它提供了一个简单的HTTP客户端，具有最少的依赖关系，可以处理嗅探，记录，循环请求，并重试节点故障。它使用的REST层历史上比Java API更稳定，这意味着它可以跨越升级使用，甚至可能在主要版本之间进行升级。它与Java 7一起工作，并且具有最小的依赖性，导致比传输客户端更少的依赖冲突。它只是HTTP，因此可以像所有其他HTTP客户端一样进行防火墙/代理。在我们的基准测试中，Java REST客户端 与Transport客户端的功能类似。</p><p>请注意，这是一个低级客户端。在这个阶段，我们不提供任何可以在IDE中自动完成的查询构建器或帮助器。它是JSON-in，JSON-out，由你来构建JSON。开发不会停止在这里 - 我们将添加一个API，它将帮助您构建查询并解析响应。您可以按照问题＃19055中的更改进行操作。</p><h3 id="迁移助手"><a href="#迁移助手" class="headerlink" title="迁移助手"></a>迁移助手</h3><p>Elasticsearch Migration Helper是一个网站插件，可以帮助从<code>Elasticsearch 2.3.x / 2.4.x</code>迁移到Elasticsearch 5.0。它有三个工具：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">集群检查</div><div class="line">对集群，节点和索引执行一系列检查，并提醒您升级之前需要解决的任何已知问题。</div><div class="line"><span class="selector-tag">Reindex</span>助手</div><div class="line">在<span class="selector-tag">v2</span><span class="selector-class">.0</span><span class="selector-class">.0</span>之前创建的索引需要重新编号，才能在<span class="selector-tag">Elasticsearch</span> 5<span class="selector-class">.x</span>中使用。<span class="selector-tag">reindex</span>帮助器点击一个按钮升级旧索引。</div><div class="line">弃用日志</div><div class="line"><span class="selector-tag">Elasticsearch</span>附带了一个弃用记录器，每当使用不推荐使用的功能时，它将记录消息。此工具可启用或禁用群集上的弃用日志记录。</div></pre></td></tr></table></figure><p>官网推荐迁移文档：Instruction for <a href="(https://github.com/elastic/elasticsearch-migration/blob/2.x/README.asciidoc">install the Elasticsearch migration helper. </a>)</p><h3 id="Elasticsearch-5-0-0-安装需要哪些要求。"><a href="#Elasticsearch-5-0-0-安装需要哪些要求。" class="headerlink" title="Elasticsearch 5.0.0 安装需要哪些要求。"></a>Elasticsearch 5.0.0 安装需要哪些要求。</h3><p>Elasticsearch需要依赖Java JDK1.8</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ES 现在普遍大公司都已经在使用，有的用来做数据存储，也有做性能监控，也有做日志收集，跟大数据结合做日志分析。&lt;br&gt;今天在家无聊研究官网学习了下Elasticsearch5.0.0 功能新增哪些，个人觉得ES也是现在开源日志分析平台比较火的，从14年开始陆续使用频率不断提
      
    
    </summary>
    
      <category term="Log Analysis Platform" scheme="http://blog.yancy.cc/categories/Log-Analysis-Platform/"/>
    
    
      <category term="Elasticsearch" scheme="http://blog.yancy.cc/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台</title>
    <link href="http://blog.yancy.cc/2016/12/29/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%10%E5%B9%B3%E5%8F%B0/Elasticsearch/%E6%90%AD%E5%BB%BA(ElasticSearch-2.x%20Logstash-2.x%20Kibana-4.5.x%20zookeeper3.4.6)%20Kafka%E4%B8%BA%E6%B6%88%E6%81%AF%E4%B8%AD%E5%BF%83%E7%9A%84ELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0/"/>
    <id>http://blog.yancy.cc/2016/12/29/日志分析平台/Elasticsearch/搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台/</id>
    <published>2016-12-29T09:56:03.000Z</published>
    <updated>2017-09-21T14:48:05.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="搭建-ElasticSearch-2-x-Logstash-2-x-Kibana-4-5-x-zookeeper3-4-6-Kafka为消息中心的ELK日志平台"><a href="#搭建-ElasticSearch-2-x-Logstash-2-x-Kibana-4-5-x-zookeeper3-4-6-Kafka为消息中心的ELK日志平台" class="headerlink" title="搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台"></a>搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台</h3><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>ELK是业界标准的日志采集,存储索引,展示分析系统解决方案</p><p>logstash提供了灵活多样的插件支持不同的input/output</p><p>主流使用redis/kafka作为日志/消息的中间环节</p><p>如果已有kafka的环境了,使用kafka比使用redis更佳</p><p>以下是一个最简化的配置做个笔记,elastic官网提供了非常丰富的文档</p><p>不要用搜索引擎去搜索,没多少结果的,请直接看官网文档</p><h3 id="版本及连接"><a href="#版本及连接" class="headerlink" title="版本及连接"></a>版本及连接</h3><p>elasticseearch版本: 2.4.3</p><h3 id="系统要求"><a href="#系统要求" class="headerlink" title="系统要求"></a>系统要求</h3><p>如果仅作测试用, 不需要两天机器, 可以将两个节点部署在同一台机器上, 对磁盘/cpu要求不高, 内存大于2g基本足够了</p><p>如果是正式环境, 需要根据日志量进行评估, 例如, 每天日志量占硬盘约约10G, 且保留30天日志, 则磁盘会占用约300g, es设定的阈值是磁盘空间占满85%则日志开始告警. 所以, 需要至少 300/0.85=354g.</p><p>准备4台机器, 在同一个局域网内(可ping通), 分别在每台机器上部署相应es节点, 搭建一套日志集群.</p><p>4台机器, 最少的资源了, 但是没法做到高可用, 所以, 还需要再加一台机器, 防止脑裂, 具体见最后(两台主力机器+一台稳定的机器就行)</p><p>集群节点: 最少4台机器<br>内存: 8G及以上<br>cpu: 4核及以上<br>硬盘: 800G及以上, 建议1T, 集群容量约10亿级(取决于对应日志大小)<br>操作系统: centos</p><h3 id="准备工作-应用-网络-环境"><a href="#准备工作-应用-网络-环境" class="headerlink" title="准备工作: 应用/网络 环境"></a>准备工作: 应用/网络 环境</h3><p>SLB： 阿里云做负载均衡&amp; 或者自己搭建nginx</p><blockquote><p>ELK服务端集群：</p></blockquote><p>系统centos 6.7  JDK1.8  版本：Elasticsearch-2.4.0</p><p>es_01 10.47.88.206<br>es_02 10.47.88.188</p><blockquote><p>Kibana服务端集群：</p></blockquote><p>系统centos 6.7  JDK1.8  版本：kibana-4.5.1</p><p>es_01 10.47.88.206<br>es_02 10.47.88.188</p><blockquote><p>KafKa集群</p></blockquote><p>系统centos 6.7  JDK1.8  版本：kafka_2.10-0.9</p><p>kafka_01 10.46.72.172<br>kafka_02 10.47.88.103<br>kafka_03 10.47.102.137</p><blockquote><p>zookeeper集群</p></blockquote><p>系统centos 6.7  JDK1.8  版本：zookeeper-3.4.6</p><p>kafka_01 10.46.72.172<br>kafka_02 10.47.88.103<br>kafka_03 10.47.102.137</p><blockquote><p>logstash-2.4</p></blockquote><p>客户端：系统centos 6.7  JDK1.8  版本： logstash-2.4</p><p>tomcat-account_01: 10.27.232.85</p><p>都要jdk1.8支持。</p><h3 id="整体说明"><a href="#整体说明" class="headerlink" title="整体说明"></a>整体说明</h3><h4 id="数据流向-gt-日志-消息整体流向"><a href="#数据流向-gt-日志-消息整体流向" class="headerlink" title="数据流向=&gt;日志/消息整体流向"></a>数据流向=&gt;日志/消息整体流向</h4><p>logstash =&gt; kafka =&gt; logstash =&gt; elasticsearch =&gt; kibana</p><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="1-确认JDK版本及安装"><a href="#1-确认JDK版本及安装" class="headerlink" title="1. 确认JDK版本及安装"></a>1. 确认JDK版本及安装</h4><p>es依赖java的版本最小为1.7</p><p>如果系统中未安装JDK<br>则命令返回<code>bash: java: command not found,</code> 需要安装<code>JDK</code></p><p>如果系统中安装了JDK, 需确认版本是否大于java 1.7, 否则需要升级</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">java -version</div><div class="line">java version <span class="string">"1.7.0_51"</span> Java(TM) SE Runtime Environment (build 1.7.0_51-b13) Java HotSpot(TM) Server VM (build 24.51-b03, mixed mode)</div><div class="line"></div><div class="line">安装及升级java(注意根据系统不同运行对应安装命令)</div><div class="line"></div><div class="line"><span class="comment"># Redhat/Centos/Fedora</span></div><div class="line">sudo yum install java-1.7.0-openjdk</div><div class="line"></div><div class="line">或者到官网, 下载最新的jdk的rpm包, 然后安装</div><div class="line"></div><div class="line">wget http://download.oracle.com/otn-pub/java/jdk/8u91-b14/jdk-8u91-linux-x64.rpm</div><div class="line">rpm -Uvh jdk-8u91-linux-x64.rpm</div></pre></td></tr></table></figure><p>再次确认安装成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -version</div></pre></td></tr></table></figure><h4 id="基本配置设置FQDN："><a href="#基本配置设置FQDN：" class="headerlink" title="基本配置设置FQDN："></a>基本配置设置FQDN：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#修改hostname</span></div><div class="line">cat /etc/hostname</div><div class="line">es_01</div><div class="line"></div><div class="line"><span class="comment">#修改hosts</span></div><div class="line">cat /etc/hosts</div><div class="line">127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4</div><div class="line">::1 localhost localhost.localdomain localhost6 localhost6.localdomain6</div><div class="line">10.47.88.206 es.ihaozhuo.com es_01</div><div class="line"></div><div class="line"><span class="comment">#刷新环境</span></div><div class="line">hostname -F /etc/hostname</div><div class="line"></div><div class="line"><span class="comment">#复查结果</span></div><div class="line">hostname <span class="_">-f</span></div><div class="line">es.ihaozhuo.com</div><div class="line"></div><div class="line">hostname</div><div class="line">es_01</div></pre></td></tr></table></figure><h3 id="防火墙配置"><a href="#防火墙配置" class="headerlink" title="防火墙配置"></a>防火墙配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#service iptables stop</span></div><div class="line"><span class="comment">#setenforce 0</span></div><div class="line"></div><div class="line">不过这里我防火墙是开启的，后期添加出去端口即可。</div><div class="line">或者可以不关闭防火墙，但是要在iptables中打开相关的端口：</div><div class="line"></div><div class="line"><span class="comment"># vim /etc/sysconfig/iptables</span></div><div class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT</div><div class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 9200 -j ACCEPT</div><div class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 9292 -j ACCEPT</div><div class="line"><span class="comment"># service iptables restart</span></div></pre></td></tr></table></figure><h4 id="RPM快速安装"><a href="#RPM快速安装" class="headerlink" title="RPM快速安装"></a>RPM快速安装</h4><p>elk所有安装都可以使用rpm二进制包的方式,增加<code>elastic官网</code>的仓库repo就可以用yum安装了</p><p>elasticsearch看这里 —– <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html" target="_blank" rel="external">elasticsearch-rpm官方文档</a></p><p>logstash看这里 —-<a href="https://www.elastic.co/guide/en/logstash/current/installing-logstash.html" target="_blank" rel="external">logstash-rpm官网文档</a></p><p>kibana看这里 —<a href="https://www.elastic.co/guide/en/kibana/current/setup.html" target="_blank" rel="external">kibana-rpm官网文档</a></p><h3 id="es-01服务端源码安装"><a href="#es-01服务端源码安装" class="headerlink" title="es_01服务端源码安装"></a>es_01服务端源码安装</h3><p>这里我是源码安装的<br>下载ElasticSearch ElasticSearch默认的对外服务的HTTP端口是9200，节点间交互的TCP端口是9300。</p><p>下载地址：<a href="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="external">Elasticsearch</a></p><p>2.4版本：<br><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-4-3" target="_blank" rel="external">Elasticsearch2.4.3</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">解压源码包：</div><div class="line">[root@es_01 ~]<span class="comment"># tar -zxvf elasticsearch-2.4.3.tar.gz -C /usr/local/</span></div><div class="line">然后给目录做个软链接：</div><div class="line">[root@es_01 <span class="built_in">local</span>]<span class="comment"># ln -s  /usr/local/elasticsearch-2.4.3/ /usr/local/elasticsearch</span></div><div class="line"></div><div class="line">这里需要修改配置文件：</div><div class="line">配置前先创建几个目录文件</div><div class="line">新建目录, 假设/data/目录挂载的硬盘最大(500G以上)</div><div class="line">[root@es_01 srv]]<span class="comment"># mkdir /srv/data/es-data -p</span></div><div class="line">[root@es_01 srv]<span class="comment"># mkdir /srv/data/es-work </span></div><div class="line"></div><div class="line">[root@es_01 <span class="built_in">local</span>]<span class="comment"># mkdir /usr/local/elasticsearch/logs</span></div><div class="line">[root@es_01 <span class="built_in">local</span>]<span class="comment"># mkdir /usr/local/elasticsearch/config/plugins</span></div><div class="line"></div><div class="line">新建用户</div><div class="line">修改源码目录属性属组：</div><div class="line">[root@es_01 elasticsearch]<span class="comment"># useradd  -s /sbin/nologin elasticsearch</span></div><div class="line">[root@es_01 elasticsearch]<span class="comment"># chown -R elasticsearch:elasticsearch /usr/local/elasticsearch</span></div><div class="line">[root@es_01 elasticsearch]<span class="comment"># chown -R elasticsearch:elasticsearch /srv/data/</span></div><div class="line"></div><div class="line"></div><div class="line">切换用户</div><div class="line">切换到elasticsearch用户, 并进入elasticsearch目录</div><div class="line"></div><div class="line">su elasticsearch</div><div class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/elasticsearch/</div></pre></td></tr></table></figure><h3 id="配置Elasticsearch："><a href="#配置Elasticsearch：" class="headerlink" title="配置Elasticsearch："></a>配置Elasticsearch：</h3><p>以用户es的身份进行操作</p><p>文件路径: <code>config/elasticsearch.yml</code><br>修改该文件中配置项: (注意, 原始文件中都是被#号注释掉了, 需要去掉对应注释并修改配置值)</p><ul><li>集群名: cluster.name, 注意: 两台机器配置一致</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cluster.name: elk_cluster</div></pre></td></tr></table></figure><ul><li>节点名: node.name, 注意: 两台机器配置不同, 一台为01, 另一台为02</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># 第一台机器</span></div><div class="line"> </div><div class="line">node.name: inner_es_node_01</div><div class="line"></div><div class="line"><span class="comment"># 第二台机器</span></div><div class="line">node.name: inner_es_node_02</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">[root@es_01 config]<span class="comment"># vim elasticsearch.yml</span></div><div class="line"></div><div class="line"><span class="comment"># Use a descriptive name for your cluster:</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#</span></div><div class="line">cluster.name: elk_cluster</div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># ------------------------------------ Node ------------------------------------</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Use a descriptive name for the node:</span></div><div class="line"><span class="comment">#</span></div><div class="line">node.name: es_01</div><div class="line"></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Add custom attributes to the node:</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># node.rack: r1</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># ----------------------------------- Paths ------------------------------------</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Path to directory where to store the data (separate multiple locations by comma):</span></div><div class="line"><span class="comment">#</span></div><div class="line">path.data: /srv/data/es-data</div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Path to log files:</span></div><div class="line"><span class="comment">#</span></div><div class="line">path.logs: /usr/<span class="built_in">local</span>/elasticsearch/logs</div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># ----------------------------------- Memory -----------------------------------</span></div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Lock the memory on startup:</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># bootstrap.memory_lock: true</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Make sure that the `ES_HEAP_SIZE` environment variable is set to about half the memory</span></div><div class="line"><span class="comment"># available on the system and that the owner of the process is allowed to use this limit.</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Elasticsearch performs poorly when the system is swapping the memory.</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># ---------------------------------- Network -----------------------------------</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Set the bind address to a specific IP (IPv4 or IPv6):</span></div><div class="line"><span class="comment">#</span></div><div class="line">network.host: 10.47.88.206</div></pre></td></tr></table></figure><p>切换到elasticsearch用户启动服务。</p><p>源码安装启动需要执行 ：<code>/usr/local/elasticsearch/bin/elasticsearch &amp;</code><br>才能启动；</p><h3 id="测试访问服务正常："><a href="#测试访问服务正常：" class="headerlink" title="测试访问服务正常："></a>测试访问服务正常：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@es_01 elasticsearch]$  curl http://10.47.88.206:9200</div><div class="line">&#123;</div><div class="line">  <span class="string">"name"</span> : <span class="string">"es_01"</span>,</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elk_cluster"</span>,</div><div class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"mspLZT5nTL-d124suNbBBQ"</span>,</div><div class="line">  <span class="string">"version"</span> : &#123;</div><div class="line">    <span class="string">"number"</span> : <span class="string">"2.4.3"</span>,</div><div class="line">    <span class="string">"build_hash"</span> : <span class="string">"d38a34e7b75af4e17ead16f156feffa432b22be3"</span>,</div><div class="line">    <span class="string">"build_timestamp"</span> : <span class="string">"2016-12-07T16:28:56Z"</span>,</div><div class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</div><div class="line">    <span class="string">"lucene_version"</span> : <span class="string">"5.5.2"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>下面是写开机启动脚本，不写的直接切换es用户到目录启动 -d后台启动。<br>这里需要/etc/init.d/创建启动脚本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@ELK ~]<span class="comment"># git clone https://github.com/elastic/elasticsearch-servicewrapper.git</span></div><div class="line">Initialized empty Git repository <span class="keyword">in</span> /root/elasticsearch-servicewrapper/.git/</div><div class="line">remote: Counting objects: 184, done.</div><div class="line">remote: Total 184 (delta 0), reused 0 (delta 0), pack-reused 184</div><div class="line">Receiving objects: 100% (184/184), 4.55 MiB | 511 KiB/s, done.</div><div class="line">Resolving deltas: 100% (53/53), done.</div><div class="line">[root@ELK elasticsearch-servicewrapper]<span class="comment"># mv service/ /usr/local/elasticsearch/bin/</span></div><div class="line">[root@ELK elasticsearch-servicewrapper]<span class="comment"># cd /usr/local/elasticsearch</span></div><div class="line">[root@ELK elasticsearch]<span class="comment"># /usr/local/elasticsearch/bin/service/elasticsearch install    这里是安装es</span></div><div class="line">Detected RHEL or Fedora:</div><div class="line">Installing the Elasticsearch daemon..</div><div class="line">[root@ELK elasticsearch]<span class="comment"># vim /etc/init.d/elasticsearch   查看安装es启动配置文件</span></div><div class="line">[root@ELK elasticsearch]<span class="comment"># service elastic search start  启动es </span></div><div class="line">Starting Elasticsearch...</div><div class="line">Waiting <span class="keyword">for</span> Elasticsearch......</div><div class="line">running: PID:31360   服务已启动了。</div><div class="line"></div><div class="line">启动相关服务</div><div class="line">service elasticsearch start</div><div class="line">service elasticsearch status</div><div class="line"></div><div class="line">配置 elasticsearch 服务随系统自动启动</div><div class="line"><span class="comment"># chkconfig --add elasticsearch</span></div><div class="line"></div><div class="line">测试ElasticSearch服务是否正常，预期返回200的状态码</div><div class="line"><span class="comment"># curl -X GET http://localhost:9200</span></div></pre></td></tr></table></figure><h3 id="es-02服务端节点："><a href="#es-02服务端节点：" class="headerlink" title="es_02服务端节点："></a>es_02服务端节点：</h3><p>第一步基础配置都是一样的，跟es_01节点一样。  其他只需要到es_01拷贝过来,然后创建下es用户，修改下配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">/usr/<span class="built_in">local</span>/elasticsearch  目录拷贝到es_02机器。</div><div class="line">这里需要修改配置文件：</div><div class="line">配置前先创建几个目录文件</div><div class="line">[root@es_01 srv]]<span class="comment"># mkdir /srv/data/es-data -p</span></div><div class="line">[root@es_01 srv]<span class="comment"># mkdir /srv/data/es-work </span></div><div class="line">修改源码目录属性属组：</div><div class="line">[root@es_01 elasticsearch]<span class="comment"># useradd  -s /sbin/nologin elasticsearch</span></div><div class="line">[root@es_01 elasticsearch]<span class="comment"># chown -R elasticsearch:elasticsearch /usr/local/elasticsearch/*</span></div><div class="line">[root@es_01 elasticsearch]<span class="comment"># chown -R elasticsearch:elasticsearch /srv/data/</span></div><div class="line"></div><div class="line">修改配置文件</div><div class="line">vim elasticsearch.yml</div><div class="line">node.name: es_02</div><div class="line">network.host: 10.47.88.188</div><div class="line"></div><div class="line">其他不需要修改</div></pre></td></tr></table></figure><h3 id="集群节点es-02测试："><a href="#集群节点es-02测试：" class="headerlink" title="集群节点es_02测试："></a>集群节点es_02测试：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@es_02 home]<span class="comment"># curl http://10.47.88.188:9200</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"name"</span> : <span class="string">"es_02"</span>,</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elk_cluster"</span>,</div><div class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"-4Rqn4IzS1GfnsodqZD8Tg"</span>,</div><div class="line">  <span class="string">"version"</span> : &#123;</div><div class="line">    <span class="string">"number"</span> : <span class="string">"2.4.3"</span>,</div><div class="line">    <span class="string">"build_hash"</span> : <span class="string">"d38a34e7b75af4e17ead16f156feffa432b22be3"</span>,</div><div class="line">    <span class="string">"build_timestamp"</span> : <span class="string">"2016-12-07T16:28:56Z"</span>,</div><div class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</div><div class="line">    <span class="string">"lucene_version"</span> : <span class="string">"5.5.2"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>elk集群已安装配置完毕，我这里配置了nginx做下反向代理，走80端口出去。然后在nginx设置下内部公司访问不对外开放。</p><h3 id="安装-head、marvel、bigdesk插件"><a href="#安装-head、marvel、bigdesk插件" class="headerlink" title="安装 head、marvel、bigdesk插件:"></a>安装 head、marvel、bigdesk插件:</h3><p>es1.5插件安装是<code>./plugin -install xxx</code>,而es2.4插件安装没有减号<code>./plugin install xxx</code></p><p>1.5版本方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">* head插件</div><div class="line"></div><div class="line">插件安装方法1：</div><div class="line">/usr/<span class="built_in">local</span>/elasticsearch/bin/plugin -install mobz/elasticsearch-head</div><div class="line">重启es 即可。</div><div class="line">打开http://localhost:9200/_plugin/head/</div><div class="line"></div><div class="line">插件安装方法2：</div><div class="line">1.https://github.com/mobz/elasticsearch-head下载zip 解压</div><div class="line">2.建立/usr/<span class="built_in">local</span>/elasticsearch/plugins/head/文件</div><div class="line">3.将解压后的elasticsearch-head-master文件夹下的文件copy到/usr/<span class="built_in">local</span>/elasticsearch/plugins/head/</div><div class="line">重启es 即可。</div><div class="line"></div><div class="line">打开http://localhost:9200/_plugin/head/</div></pre></td></tr></table></figure><p>2.4版本以上安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">* head插件</div><div class="line"></div><div class="line">插件安装方法1：</div><div class="line">/usr/<span class="built_in">local</span>/elasticsearch/bin/plugin install mobz/elasticsearch-head</div><div class="line">重启es 即可。</div><div class="line">打开http://localhost:9200/_plugin/head/</div><div class="line"></div><div class="line">插件安装方法2：</div><div class="line">1.https://github.com/mobz/elasticsearch-head下载zip 解压</div><div class="line">2.建立elasticsearch-1.0.0\plugins\head\_site文件</div><div class="line">3.将解压后的elasticsearch-head-master文件夹下的文件copy到_site</div><div class="line">重启es 即可。</div><div class="line"></div><div class="line">打开http://localhost:9200/_plugin/head/</div></pre></td></tr></table></figure><p>为了保障搜索服务的稳定性，增加了一台机器，将Elasticsearch部署成了集群模式， 部署到生产环境时发现，新的节点并不能被发现，后台发现阿里云并不支持多播，最后只能改为单播的方式配置了，好在之后一切顺利。</p><p>下面附上测试环境配置示例：添加下下面监听集群IP和端口。</p><ul><li>es_01</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@es_01 config]<span class="comment"># vim elasticsearch.yml</span></div><div class="line"></div><div class="line">discovery.zen.ping.multicast.enabled: <span class="literal">false</span></div><div class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"10.47.88.206:9300"</span>,<span class="string">"10.47.88.188:9300"</span>]</div></pre></td></tr></table></figure><ul><li>es_02</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@es_02 config]<span class="comment"># vim elasticsearch.yml</span></div><div class="line"></div><div class="line">discovery.zen.ping.multicast.enabled: <span class="literal">false</span></div><div class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"10.47.88.206:9300"</span>,<span class="string">"10.47.88.188:9300"</span>]</div></pre></td></tr></table></figure><p>然后重启服务，查看集群节点。</p><h3 id="es-02安装Kibana"><a href="#es-02安装Kibana" class="headerlink" title="es_02安装Kibana:"></a>es_02安装Kibana:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div></pre></td><td class="code"><pre><div class="line">到https://www.elastic.co/downloads/kibana  找合适的版本。</div><div class="line"></div><div class="line">wget https://download.elastic.co/kibana/kibana/kibana-4.5.1-linux-x64.tar.gz</div><div class="line"></div><div class="line"><span class="comment">#解压</span></div><div class="line"></div><div class="line">＃tar zxvf kibana-4.1.2-linux-x64.tar.gz -C /usr/<span class="built_in">local</span> </div><div class="line">＃<span class="built_in">cd</span>  /usr/<span class="built_in">local</span>/ &amp;&amp; mv kibana-4.1.2-linux-x64 kibana</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#创建kibana启动脚本服务</span></div><div class="line">vi /etc/rc.d/init.d/kibana</div><div class="line"></div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment">### BEGIN INIT INFO</span></div><div class="line"><span class="comment"># Provides:          kibana</span></div><div class="line"><span class="comment"># Default-Start:     2 3 4 5</span></div><div class="line"><span class="comment"># Default-Stop:      0 1 6</span></div><div class="line"><span class="comment"># Short-Description: Runs kibana daemon</span></div><div class="line"><span class="comment"># Description: Runs the kibana daemon as a non-root user</span></div><div class="line"><span class="comment">### END INIT INFO</span></div><div class="line"></div><div class="line"><span class="comment"># Process name</span></div><div class="line">NAME=kibana</div><div class="line">DESC=<span class="string">"Kibana4"</span></div><div class="line">PROG=<span class="string">"/etc/init.d/kibana"</span></div><div class="line"></div><div class="line"><span class="comment"># Configure location of Kibana bin</span></div><div class="line">KIBANA_BIN=/usr/<span class="built_in">local</span>/kibana/bin</div><div class="line"></div><div class="line"><span class="comment"># PID Info</span></div><div class="line">PID_FOLDER=/var/run/kibana/</div><div class="line">PID_FILE=/var/run/kibana/<span class="variable">$NAME</span>.pid</div><div class="line">LOCK_FILE=/var/lock/subsys/<span class="variable">$NAME</span></div><div class="line">PATH=/bin:/usr/bin:/sbin:/usr/sbin:<span class="variable">$KIBANA_BIN</span></div><div class="line">DAEMON=<span class="variable">$KIBANA_BIN</span>/<span class="variable">$NAME</span></div><div class="line"></div><div class="line"><span class="comment"># Configure User to run daemon process</span></div><div class="line">DAEMON_USER=root</div><div class="line"><span class="comment"># Configure logging location</span></div><div class="line">KIBANA_LOG=/var/<span class="built_in">log</span>/kibana.log</div><div class="line"></div><div class="line"><span class="comment"># Begin Script</span></div><div class="line">RETVAL=0</div><div class="line"></div><div class="line"><span class="keyword">if</span> [ `id -u` <span class="_">-ne</span> 0 ]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"You need root privileges to run this script"</span></div><div class="line">        <span class="built_in">exit</span> 1</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment"># Function library</span></div><div class="line">. /etc/init.d/<span class="built_in">functions</span></div><div class="line"> </div><div class="line"><span class="function"><span class="title">start</span></span>() &#123;</div><div class="line">        <span class="built_in">echo</span> -n <span class="string">"Starting <span class="variable">$DESC</span> : "</span></div><div class="line"></div><div class="line">pid=`pidofproc -p <span class="variable">$PID_FILE</span> kibana`</div><div class="line">        <span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$pid</span>"</span> ] ; <span class="keyword">then</span></div><div class="line">                <span class="built_in">echo</span> <span class="string">"Already running."</span></div><div class="line">                <span class="built_in">exit</span> 0</div><div class="line">        <span class="keyword">else</span></div><div class="line">        <span class="comment"># Start Daemon</span></div><div class="line"><span class="keyword">if</span> [ ! <span class="_">-d</span> <span class="string">"<span class="variable">$PID_FOLDER</span>"</span> ] ; <span class="keyword">then</span></div><div class="line">                        mkdir <span class="variable">$PID_FOLDER</span></div><div class="line">                <span class="keyword">fi</span></div><div class="line">daemon --user=<span class="variable">$DAEMON_USER</span> --pidfile=<span class="variable">$PID_FILE</span> <span class="variable">$DAEMON</span> 1&gt;<span class="string">"<span class="variable">$KIBANA_LOG</span>"</span> 2&gt;&amp;1 &amp;</div><div class="line">                sleep 2</div><div class="line">                pidofproc node &gt; <span class="variable">$PID_FILE</span></div><div class="line">                RETVAL=$?</div><div class="line">                [[ $? <span class="_">-eq</span> 0 ]] &amp;&amp; success || failure</div><div class="line"><span class="built_in">echo</span></div><div class="line">                [ <span class="variable">$RETVAL</span> = 0 ] &amp;&amp; touch <span class="variable">$LOCK_FILE</span></div><div class="line">                <span class="built_in">return</span> <span class="variable">$RETVAL</span></div><div class="line">        <span class="keyword">fi</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="title">reload</span></span>()</div><div class="line">&#123;</div><div class="line">    <span class="built_in">echo</span> <span class="string">"Reload command is not implemented for this service."</span></div><div class="line">    <span class="built_in">return</span> <span class="variable">$RETVAL</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="title">stop</span></span>() &#123;</div><div class="line">        <span class="built_in">echo</span> -n <span class="string">"Stopping <span class="variable">$DESC</span> : "</span></div><div class="line">        killproc -p <span class="variable">$PID_FILE</span> <span class="variable">$DAEMON</span></div><div class="line">        RETVAL=$?</div><div class="line"><span class="built_in">echo</span></div><div class="line">        [ <span class="variable">$RETVAL</span> = 0 ] &amp;&amp; rm <span class="_">-f</span> <span class="variable">$PID_FILE</span> <span class="variable">$LOCK_FILE</span></div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="keyword">case</span> <span class="string">"<span class="variable">$1</span>"</span> <span class="keyword">in</span></div><div class="line">  start)</div><div class="line">        start</div><div class="line">;;</div><div class="line">  stop)</div><div class="line">        stop</div><div class="line">        ;;</div><div class="line">  status)</div><div class="line">        status -p <span class="variable">$PID_FILE</span> <span class="variable">$DAEMON</span></div><div class="line">        RETVAL=$?</div><div class="line">        ;;</div><div class="line">  restart)</div><div class="line">        stop</div><div class="line">        start</div><div class="line">        ;;</div><div class="line">  reload)</div><div class="line">reload</div><div class="line">;;</div><div class="line">  *)</div><div class="line"><span class="comment"># Invalid Arguments, print the following message.</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"Usage: <span class="variable">$0</span> &#123;start|stop|status|restart&#125;"</span> &gt;&amp;2</div><div class="line"><span class="built_in">exit</span> 2</div><div class="line">        ;;</div><div class="line"><span class="keyword">esac</span></div><div class="line"></div><div class="line"></div><div class="line">修改启动权限</div><div class="line">chmod +x /etc/rc.d/init.d/kibana</div></pre></td></tr></table></figure><p>配置Kibana：</p><h3 id="编辑kibana-yaml-修改端口，设置host-可以设置本地服务器IP"><a href="#编辑kibana-yaml-修改端口，设置host-可以设置本地服务器IP" class="headerlink" title="编辑kibana.yaml  修改端口，设置host 可以设置本地服务器IP"></a>编辑kibana.yaml  修改端口，设置host 可以设置本地服务器IP</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">vim /usr/<span class="built_in">local</span>/kibana/config/kibana.yml</div><div class="line"></div><div class="line"><span class="comment"># Kibana is served by a back end server. This controls which port to use.</span></div><div class="line">server.port: 5601</div><div class="line"></div><div class="line"><span class="comment"># The host to bind the server to.</span></div><div class="line">server.host: <span class="string">"10.47.88.188"</span></div><div class="line"></div><div class="line"><span class="comment"># If you are running kibana behind a proxy, and want to mount it at a path,</span></div><div class="line"><span class="comment"># specify that path here. The basePath can't end in a slash.</span></div><div class="line"><span class="comment"># server.basePath: ""</span></div><div class="line"></div><div class="line"><span class="comment"># The maximum payload size in bytes on incoming server requests.</span></div><div class="line"><span class="comment"># server.maxPayloadBytes: 1048576</span></div><div class="line"></div><div class="line"><span class="comment"># The Elasticsearch instance to use for all your queries.</span></div><div class="line">elasticsearch.url: <span class="string">"http://10.47.88.188:9200"</span></div><div class="line"></div><div class="line"><span class="comment"># preserve_elasticsearch_host true will send the hostname specified in `elasticsearch`. If you set it to false,</span></div><div class="line"><span class="comment"># then the host you use to connect to *this* Kibana instance will be sent.</span></div><div class="line"></div><div class="line">elasticsearch.preserveHost: <span class="literal">true</span></div><div class="line"></div><div class="line"><span class="comment"># Kibana uses an index in Elasticsearch to store saved searches, visualizations</span></div><div class="line"><span class="comment"># and dashboards. It will create a new index if it doesn't already exist.</span></div><div class="line"><span class="comment"># kibana.index: ".kibana"</span></div><div class="line"></div><div class="line"><span class="comment"># The default application to load.</span></div><div class="line">kibana.defaultAppId: <span class="string">"discover"</span></div><div class="line"></div><div class="line"><span class="comment"># If your Elasticsearch is protected with basic auth, these are the user credentials</span></div><div class="line"><span class="comment"># used by the Kibana server to perform maintenance on the kibana_index at startup. Your Kibana</span></div><div class="line"><span class="comment"># users will still need to authenticate with Elasticsearch (which is proxied through</span></div><div class="line"><span class="comment"># the Kibana server)</span></div><div class="line"></div><div class="line"><span class="comment"># elasticsearch.ssl.key: /path/to/your/client.key</span></div><div class="line"></div><div class="line"><span class="comment"># If you need to provide a CA certificate for your Elasticsearch instance, put</span></div><div class="line"><span class="comment"># the path of the pem file here.</span></div><div class="line"><span class="comment"># elasticsearch.ssl.ca: /path/to/your/CA.pem</span></div><div class="line"></div><div class="line"><span class="comment"># Set to false to have a complete disregard for the validity of the SSL</span></div><div class="line"><span class="comment"># certificate.</span></div><div class="line"><span class="comment"># elasticsearch.ssl.verify: true</span></div><div class="line"></div><div class="line"><span class="comment"># Time in milliseconds to wait for elasticsearch to respond to pings, defaults to</span></div><div class="line"><span class="comment"># request_timeout setting</span></div><div class="line"><span class="comment"># elasticsearch.pingTimeout: 1500</span></div><div class="line"></div><div class="line"><span class="comment"># Time in milliseconds to wait for responses from the back end or elasticsearch.</span></div><div class="line"><span class="comment"># This must be &gt; 0</span></div><div class="line"></div><div class="line">elasticsearch.requestTimeout: 30000</div><div class="line"></div><div class="line"><span class="comment"># Time in milliseconds for Elasticsearch to wait for responses from shards.</span></div><div class="line"><span class="comment"># Set to 0 to disable.</span></div><div class="line"><span class="comment"># elasticsearch.shardTimeout: 0</span></div></pre></td></tr></table></figure><h3 id="启动kibana服务"><a href="#启动kibana服务" class="headerlink" title="启动kibana服务"></a>启动kibana服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">service kibana start</div><div class="line">service kibana status</div></pre></td></tr></table></figure><h3 id="查看端口"><a href="#查看端口" class="headerlink" title="查看端口"></a>查看端口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">netstat -nltp</div><div class="line">[root@es_02 config]<span class="comment"># netstat -nltp</span></div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name</div><div class="line">tcp        0      0 127.0.0.1:32000             0.0.0.0:*                   LISTEN      2517/java</div><div class="line">tcp        0      0 10.47.88.188:5601           0.0.0.0:*                   LISTEN      6474/node</div><div class="line">tcp        0      0 10.47.88.188:10050          0.0.0.0:*                   LISTEN      305/zabbix_agentd</div><div class="line">tcp        0      0 10.47.88.188:9200           0.0.0.0:*                   LISTEN      5198/java</div><div class="line">tcp        0      0 10.47.88.188:9300           0.0.0.0:*                   LISTEN      5198/java</div><div class="line">tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      25265/sshd</div></pre></td></tr></table></figure><p>到我Github上面下载kabana启动脚本。</p><p>es_01机器从es_02机器拷贝过去修改下配置就可以。 </p><h3 id="kibana安装插件参考："><a href="#kibana安装插件参考：" class="headerlink" title="kibana安装插件参考："></a>kibana安装插件参考：</h3><p><a href="https://www.elastic.co/guide/en/marvel/current/installing-marvel.html#installing-marvel" target="_blank" rel="external">Installing Marvel</a></p><p>这里kibana我做了nginx反向代理，集群代理。</p><h3 id="nginx配置kibana反向代理："><a href="#nginx配置kibana反向代理：" class="headerlink" title="nginx配置kibana反向代理："></a>nginx配置kibana反向代理：</h3><p>这里我只允许我公司IP访问：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">  upstream kibana.ihaozhuo.com &#123;</div><div class="line">        server 10.47.88.206:5601 weight=1;</div><div class="line">        server 10.47.88.188:5601 weight=1;</div><div class="line">&#125;</div><div class="line">  server &#123;</div><div class="line">        listen       80;</div><div class="line">        server_name  kibana.ihaozhuo.com;</div><div class="line">        location / &#123;</div><div class="line">             index        index.html index.php index.jsp index.htm;</div><div class="line">             allow 202.107.202.82/32;</div><div class="line">             deny all;</div><div class="line">             proxy_pass           http://kibana.ihaozhuo.com;</div><div class="line">             proxy_ignore_client_abort on;</div><div class="line">             proxy_redirect               off;</div><div class="line">             proxy_set_header     Host    <span class="variable">$host</span>;</div><div class="line">             proxy_set_header     X-Real-IP       <span class="variable">$remote_addr</span>;</div><div class="line">             proxy_set_header     X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</div><div class="line">             &#125;</div><div class="line">      &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="KafKa集群搭建"><a href="#KafKa集群搭建" class="headerlink" title="KafKa集群搭建"></a>KafKa集群搭建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">下载地址：http://mirrors.hust.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.9.0.0.tgz</div><div class="line">[root@kafka_01 srv]<span class="comment"># tar -xvf kafka_2.10-0.9.0.0.tgz</span></div><div class="line">[root@kafka_01 srv]<span class="comment"># mv kafka_2.10-0.9.0.0 kafka</span></div></pre></td></tr></table></figure><p><em>修改kafka配置文件</em> </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@kafka_01 config]<span class="comment"># vim /srv/kafka/config/server.properties</span></div><div class="line"></div><div class="line"><span class="comment">#设置brokerid（从0开始，3个节点分别设为0,1,2，不能重复）在这里id=0跟zookeeper id设置一样就行。 集群机器：按顺序写1</span></div><div class="line">broker.id=0  </div><div class="line"></div><div class="line"><span class="comment">#设置data目录，最好不要用默认的/tmp/kafka-logs</span></div><div class="line">mkdir -p /srv/kafka/data/kafka-logs</div><div class="line"> </div><div class="line"></div><div class="line"><span class="comment">#修改本地IP地址：</span></div><div class="line">listeners=PLAINTEXT://10.46.72.172:9092</div><div class="line"> </div><div class="line">log.dirs=/srv/kafka/data/kafka-logs</div><div class="line"><span class="comment">#设置注册地址（重要，默认会把本机的hostanme注册到zk中，客户端连接时需要解析该hostanme，所以这里直接注册本机的IP地址，避免hostname解析失败，报错java.nio.channels.UnresolvedAddressException或java.io.IOException: Can not resolve address）</span></div><div class="line"><span class="comment">#设置zookeeper地址</span></div><div class="line">zookeeper.connect=10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181</div></pre></td></tr></table></figure><p><em>配置zookeeper地址</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">vim zookeeper.properties </div><div class="line">dataDir=/home/jollybi/tools/zookeeper-3.4.5/tmp</div><div class="line"><span class="comment"># the port at which the clients will connect</span></div><div class="line">clientPort=2281</div><div class="line"><span class="comment"># disable the per-ip limit on the number of connections since this is a non-production config</span></div><div class="line">maxClientCnxns=0</div><div class="line">~</div></pre></td></tr></table></figure><p><em>配置kafka访问地址</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">vim producer.properties </div><div class="line">metadata.broker.list=169.44.62.139:9292,169.44.59.138:9292,169.44.62.137:9292   </div><div class="line"><span class="comment"># name of the partitioner class for partitioning events; default partition spreads data randomly</span></div><div class="line"><span class="comment">#partitioner.class=</span></div><div class="line"></div><div class="line"><span class="comment"># specifies whether the messages are sent asynchronously (async) or synchronously (sync)</span></div><div class="line">producer.type=sync</div><div class="line"></div><div class="line"><span class="comment"># specify the compression codec for all data generated: none, gzip, snappy, lz4.</span></div><div class="line"><span class="comment"># the old config values work as well: 0, 1, 2, 3 for none, gzip, snappy, lz4, respectively</span></div><div class="line">compression.codec=none</div><div class="line"></div><div class="line"><span class="comment"># message encoder</span></div><div class="line">serializer.class=kafka.serializer.DefaultEncoder</div></pre></td></tr></table></figure><h3 id="Kafka常用命令-普及"><a href="#Kafka常用命令-普及" class="headerlink" title="Kafka常用命令(普及)"></a>Kafka常用命令(普及)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">Kafka常用命令</div><div class="line">以下是kafka常用命令行总结：  </div><div class="line">1.查看topic的详细信息  </div><div class="line">./kafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic <span class="built_in">test</span>KJ1  </div><div class="line">2、为topic增加副本  </div><div class="line">./kafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute  </div><div class="line">3、创建topic </div><div class="line">./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic <span class="built_in">test</span>KJ1  </div><div class="line">4、为topic增加partition  </div><div class="line">./bin/kafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic <span class="built_in">test</span>KJ1  </div><div class="line">5、kafka生产者客户端命令  </div><div class="line">./kafka-console-producer.sh --broker-list localhost:9092 --topic <span class="built_in">test</span>KJ1  </div><div class="line">6、kafka消费者客户端命令  </div><div class="line">./kafka-console-consumer.sh -zookeeper localhost:2181 --from-beginning --topic <span class="built_in">test</span>KJ1  </div><div class="line">7、kafka服务启动  </div><div class="line">./kafka-server-start.sh -daemon ../config/server.properties   </div><div class="line">8、下线broker  </div><div class="line">./kafka-run-class.sh kafka.admin.ShutdownBroker --zookeeper 127.0.0.1:2181 --broker <span class="comment">#brokerId# --num.retries 3 --retry.interval.ms 60  </span></div><div class="line">shutdown broker  </div><div class="line">9、删除topic  </div><div class="line">./kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic <span class="built_in">test</span>KJ1 --zookeeper 127.0.0.1:2181  </div><div class="line">./kafka-topics.sh --zookeeper localhost:2181 --delete --topic <span class="built_in">test</span>KJ1  </div><div class="line">10、查看consumer组内消费的offset  </div><div class="line">./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group <span class="built_in">test</span> --topic <span class="built_in">test</span>KJ1</div></pre></td></tr></table></figure><h3 id="Kafka群集新建一个Topic"><a href="#Kafka群集新建一个Topic" class="headerlink" title="Kafka群集新建一个Topic"></a>Kafka群集新建一个Topic</h3><p>叫做logstash  Topic</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#查看tocpic列表（--zookeeper指定任意一个zk节点即可，用于获取集群信息）</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-topics.sh --zookeeper zk1.yazuoyw.com:2181 --describe</div><div class="line"> </div><div class="line"><span class="comment">#创建topic（--replication-factor表示复制到多少个节点，--partitions表示分区数，一般都设置为2或与节点数相等，不能大于总节点数）</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-topics.sh --zookeeper zk1.yazuoyw.com:2181 --create --topic topic1 --replication-factor 2 --partitions 2</div><div class="line"> </div><div class="line"><span class="comment">#发送消息（--topic 指定topic）</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-console-producer.sh --broker-list kafka1.yazuoyw.com:9092,kafka2.yazuoyw.com:9092,kafka3.yazuoyw.com:9092 --topic topic1</div><div class="line">message1</div><div class="line">message2</div><div class="line"> </div><div class="line"><span class="comment">#消费消息</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-console-consumer.sh --zookeeper zk1.yazuoyw.com:2181 --topic topic1</div><div class="line"></div><div class="line"><span class="comment">#replica检查</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-replica-verification.sh --broker-list kafka1.yazuoyw.com:9092,kafka2.yazuoyw.com:9092,kafka3.yazuoyw.com:9092</div></pre></td></tr></table></figure><p>每条发布到Kafka集群的消息都有一个类别，这个类别被称为topic。（物理上不同topic的消息分开存储，逻辑上一个topic的消息虽然保存于一个或多个broker上但用户只需指定消息的topic即可生产或消费数据而不必关心数据存于何处）</p><p><code>ElasticSearch</code>机器<code>logstash</code>把数据从<code>kafka</code>存到<code>elasticsearch</code>的配置</p><p>其中选取kafka群集任意一个有zk的ip做连接使用</p><p><code>topic_id</code>就是kafka中设置的<code>topic logstash</code></p><p>在es上面安装logstash配置<br>/usr/local/logstash/config/kafka_to_es.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"> input &#123;</div><div class="line">    kafka &#123;</div><div class="line">                zk_connect =&gt; <span class="string">"10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181/kafka"</span></div><div class="line">                group_id =&gt; <span class="string">"logstash"</span></div><div class="line">                topic_id =&gt; <span class="string">"logstash"</span></div><div class="line">                reset_beginning =&gt; <span class="literal">false</span> <span class="comment"># boolean (optional)， default: false</span></div><div class="line">                consumer_threads =&gt; 2  <span class="comment"># number (optional)， default: 1</span></div><div class="line">                decorate_events =&gt; <span class="literal">false</span> <span class="comment"># boolean (optional)， default: false</span></div><div class="line">        &#125;</div><div class="line">  &#125;</div><div class="line">  output &#123;</div><div class="line">    elasticsearch &#123;</div><div class="line">      hosts =&gt; [<span class="string">"10.47.88.206:9200"</span>,<span class="string">"10.47.88.188:9200"</span>]</div><div class="line">      index =&gt; <span class="string">"%&#123;host&#125;-%&#123;+YYYY.MM.dd&#125;"</span></div><div class="line">&#125;</div><div class="line">   <span class="comment"># stdout &#123; codec =&gt; rubydebug &#125;</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure><p>新建了个测试的，测试下发送是否成功：/usr/local/logstash/config/stdin_to_es.conf </p><figure class="highlight puppet"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">input</span> &#123;</div><div class="line"> stdin &#123;&#125;</div><div class="line">&#125;</div><div class="line">  <span class="keyword">output</span> &#123;</div><div class="line">    elasticsearch &#123;</div><div class="line">      <span class="attr">hosts</span> =&gt; <span class="string">"10.47.88.206"</span>&#125;</div><div class="line">    <span class="keyword">stdout</span> &#123;</div><div class="line">      <span class="attr">codec</span> =&gt; rubydebug &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure><h3 id="Step-2-启动服务"><a href="#Step-2-启动服务" class="headerlink" title="Step 2: 启动服务"></a>Step 2: 启动服务</h3><figure class="highlight dts"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Kafka用到了Zookeeper，所有首先启动Zookper，下面简单的启用一个单实例的Zookkeeper服务。可以在命令的结尾加个&amp;符号，这样就可以启动后离开控制台。</div><div class="line"></div><div class="line"><span class="meta">#现在启动Kafka:</span></div><div class="line"></div><div class="line"><span class="meta-keyword">/srv/</span>kafka<span class="meta-keyword">/bin/</span>kafka-server-start.sh -daemon config/server.properties</div><div class="line"></div><div class="line"><span class="meta">#添加开机启动</span></div><div class="line">echo ‘</div><div class="line"><span class="meta"># start kafka</span></div><div class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/kafka/</span>bin/kafka-server-start.sh -daemon <span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/kafka/</span>config/server.properties</div><div class="line">‘ &gt;&gt; <span class="meta-keyword">/etc/</span>rc.local</div><div class="line"> </div><div class="line"><span class="meta">#关闭</span></div><div class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/kafka/</span>bin/kafka-server-stop.sh</div></pre></td></tr></table></figure><h3 id="kafka配置防火墙："><a href="#kafka配置防火墙：" class="headerlink" title="kafka配置防火墙："></a>kafka配置防火墙：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 4888 -j ACCEPT</div></pre></td></tr></table></figure><h3 id="zookeeper集群"><a href="#zookeeper集群" class="headerlink" title="zookeeper集群"></a>zookeeper集群</h3><p>查看我之前写的这篇文档 <a href="http://blog.yangcvo.me/2016/05/28/%E5%A4%A7%E6%95%B0%E6%8D%AEhadoop/zookeeper/ZooKeeper%E9%9B%86%E7%BE%A4%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BC%98%E5%8C%96/" target="_blank" rel="external">ZooKeeper的集群快速搭建与优化</a> </p><h3 id="走kafka查看是否所有节点都启动："><a href="#走kafka查看是否所有节点都启动：" class="headerlink" title="走kafka查看是否所有节点都启动："></a>走kafka查看是否所有节点都启动：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@kafka_03 bin]<span class="comment"># sh zkCli.sh</span></div><div class="line">Connecting to localhost:2181</div><div class="line">2017-01-04 19:20:24,849 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT</div><div class="line">2017-01-04 19:20:24,853 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=kafka_03</div><div class="line">2017-01-04 19:20:24,853 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_66</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/srv/jdk1.8.0_66/jre</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/srv/zookeeper-3.4.6/bin/../build/classes:/srv/zookeeper-3.4.6/bin/../build/lib/*.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/srv/zookeeper-3.4.6/bin/../lib/<span class="built_in">log</span>4j-1.2.16.jar:/srv/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/srv/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/srv/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/srv/zookeeper-3.4.6/bin/../conf:</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line"></div><div class="line">[zk: localhost:2181(CONNECTED) 0] ls /</div><div class="line">[controller_epoch, brokers, zookeeper, kafka, dubbo, admin, isr_change_notification, consumers, config, sthp]</div><div class="line">[zk: localhost:2181(CONNECTED) 5] ls /kafka/brokers/ids</div><div class="line">[0, 1, 2]</div></pre></td></tr></table></figure><p>kafka 三台集群这里可以看到获取到ids。</p><h4 id="安全问题"><a href="#安全问题" class="headerlink" title="安全问题"></a>安全问题</h4><p>特别要注意elk所有软件的端口监听,切勿暴露监听到公网上去,另外即便是内网你也得注意配置内网的访问限制。</p><h3 id="logstash-客户端安装："><a href="#logstash-客户端安装：" class="headerlink" title="logstash 客户端安装："></a>logstash 客户端安装：</h3><h5 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line">我这里源码包安装</div><div class="line"><span class="comment"># wget https://download.elasticsearch.org/logstash/logstash/logstash-2.4.0.tar.gz</span></div><div class="line"><span class="comment">#curl -O https://download.elastic.co/logstash/logstash/logstash-2.4.0.tar.gz</span></div><div class="line"><span class="comment">#tar -zxvf logstash-2.4.0.tar.gz</span></div><div class="line"><span class="comment">#mv logstash-2.4.0 /usr/local/</span></div><div class="line"><span class="comment">#ln -s /usr/local/logstash-2.4.0/ /usr/local/logstash</span></div><div class="line"></div><div class="line">下载启动脚本</div><div class="line">生产都是运行在后台的，我这里源码安装没有init脚本启动。 去Github下载  https://github.com/benet1006/ELK_config.git</div><div class="line"><span class="comment">#cp logstash.init /etc/init.d/logstash</span></div><div class="line"><span class="comment">#chmod +x /etc/init.d/logstash</span></div><div class="line">这个脚本我做过修改。</div><div class="line"></div><div class="line"><span class="comment">#启动logstash服务</span></div><div class="line">service logstash start</div><div class="line">service logstash status</div><div class="line"></div><div class="line"><span class="comment">#查看5000端口</span></div><div class="line">netstat -nltp</div><div class="line"></div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name</div><div class="line">tcp 0 0 0.0.0.0:9200 0.0.0.0:* LISTEN 1765/java</div><div class="line">tcp 0 0 0.0.0.0:9300 0.0.0.0:* LISTEN 1765/java</div><div class="line">tcp 0 0 0.0.0.0:9301 0.0.0.0:* LISTEN 2309/java</div><div class="line">tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1509/sshd</div><div class="line">tcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 1876/node</div><div class="line">tcp 0 0 0.0.0.0:5000 0.0.0.0:* LISTEN 2309/java</div><div class="line">tcp 0 0 :::22 :::* LISTEN 1509/sshd</div><div class="line"></div><div class="line"></div><div class="line">修改启动脚本</div><div class="line">vim /etc/init.d/logstash </div><div class="line">指定的目录自己源码安装的路径。</div><div class="line"></div><div class="line">name=logstash</div><div class="line">pidfile=<span class="string">"/var/run/<span class="variable">$name</span>.pid"</span></div><div class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/jre/lib/rt.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</div><div class="line">LS_USER=logstash</div><div class="line">LS_GROUP=logstash</div><div class="line">LS_HOME=/usr/<span class="built_in">local</span>/logstash 安装路径</div><div class="line">LS_HEAP_SIZE=<span class="string">"1000m"</span></div><div class="line">LS_JAVA_OPTS=<span class="string">"-Djava.io.tmpdir=<span class="variable">$&#123;LS_HOME&#125;</span>"</span></div><div class="line">LS_LOG_DIR=/usr/<span class="built_in">local</span>/logstash</div><div class="line">LS_LOG_FILE=<span class="string">"<span class="variable">$&#123;LS_LOG_DIR&#125;</span>/<span class="variable">$name</span>.log"</span></div><div class="line">LS_CONF_FILE=/etc/logstash.conf     收集日志的规则conf</div><div class="line">LS_OPEN_FILES=16384</div><div class="line">LS_NICE=19</div><div class="line">LS_OPTS=<span class="string">""</span></div><div class="line"></div><div class="line">https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html</div><div class="line">这个是<span class="built_in">log</span> stash的官方文档的配置说明。</div><div class="line">这个配置说明上面我先修改下我之前的配置文件。</div></pre></td></tr></table></figure><h4 id="logstash-agent配置："><a href="#logstash-agent配置：" class="headerlink" title="logstash agent配置："></a>logstash agent配置：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">配置<span class="built_in">log</span> stash－实现系统日志收集input</div><div class="line">file_to_kafka.conf 日志文件读出写入到kafka</div><div class="line">input &#123;</div><div class="line">file &#123;</div><div class="line">path =&gt; <span class="string">"/srv/tomcat/logs/account/logFile.*.log"</span></div><div class="line"><span class="built_in">type</span> =&gt; <span class="string">"tomcat"</span></div><div class="line">discover_interval =&gt; 15 <span class="comment">#logstash</span></div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line"><span class="comment">#stdout &#123; codec =&gt; rubydebug &#125;</span></div><div class="line">kafka&#123;</div><div class="line">bootstrap_servers =&gt; <span class="string">"10.46.72.172:9092,10.47.88.103:9092,10.47.102.137:9092"</span></div><div class="line"><span class="comment">#group_id =&gt; "logstash"</span></div><div class="line">topic_id =&gt; <span class="string">"logstash"</span></div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">2.2 logstash indexer 配置</div><div class="line">kafka_to_es.conf</div><div class="line">input &#123;</div><div class="line">kafka &#123;</div><div class="line">zk_connect =&gt; <span class="string">"10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181kafka"</span></div><div class="line">group_id =&gt; <span class="string">"logstash"</span></div><div class="line">topic_id =&gt; <span class="string">"logstash"</span></div><div class="line">reset_beginning =&gt; <span class="literal">false</span> <span class="comment"># boolean (optional)， default: false</span></div><div class="line">consumer_threads =&gt; 2 <span class="comment"># number (optional)， default: 1</span></div><div class="line">decorate_events =&gt; <span class="literal">false</span> <span class="comment"># boolean (optional)， default: false</span></div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line">elasticsearch &#123;</div><div class="line">hosts =&gt; [<span class="string">"10.47.88.206:9200"</span>,<span class="string">"10.47.88.188:9200"</span>]</div><div class="line">index =&gt; <span class="string">"%&#123;host&#125;-%&#123;+YYYY.MM.dd&#125;"</span></div><div class="line">&#125;</div><div class="line"><span class="comment"># stdout &#123; codec =&gt; rubydebug &#125;</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="es安装插件head查看下效果："><a href="#es安装插件head查看下效果：" class="headerlink" title="es安装插件head查看下效果："></a>es安装插件head查看下效果：</h4><p>然后打开网站：<a href="http://elk.ihaozhuo.com/_plugin/head/" target="_blank" rel="external">http://elk.ihaozhuo.com/_plugin/head/</a></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/es_head.png" alt=""></figure></p><p>####kibana网站效果：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/es_kibana.png" alt=""></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;搭建-ElasticSearch-2-x-Logstash-2-x-Kibana-4-5-x-zookeeper3-4-6-Kafka为消息中心的ELK日志平台&quot;&gt;&lt;a href=&quot;#搭建-ElasticSearch-2-x-Logstash-2-x-Kibana
      
    
    </summary>
    
      <category term="Log Analysis Platform" scheme="http://blog.yancy.cc/categories/Log-Analysis-Platform/"/>
    
    
      <category term="Elasticsearch" scheme="http://blog.yancy.cc/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>graylog 升级2.1 集中日志解决方案</title>
    <link href="http://blog.yancy.cc/2016/11/14/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%10%E5%B9%B3%E5%8F%B0/Graylog/graylog%20%E5%8D%87%E7%BA%A72.1%20%E9%9B%86%E4%B8%AD%E6%97%A5%E5%BF%97%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>http://blog.yancy.cc/2016/11/14/日志分析平台/Graylog/graylog 升级2.1 集中日志解决方案/</id>
    <published>2016-11-14T09:56:03.000Z</published>
    <updated>2017-09-21T14:48:13.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="graylog-升级2-1-集中日志解决方案"><a href="#graylog-升级2-1-集中日志解决方案" class="headerlink" title="graylog 升级2.1 集中日志解决方案"></a>graylog 升级2.1 集中日志解决方案</h1><p>参考官网：<a href="http://docs.graylog.org/en/2.1/pages/upgrade/graylog-2.1.html" target="_blank" rel="external">升级到Graylog的2.1.x</a></p><h3 id="一：为什么需要集中日志解决方案？"><a href="#一：为什么需要集中日志解决方案？" class="headerlink" title="一：为什么需要集中日志解决方案？"></a>一：为什么需要集中日志解决方案？</h3><p>在公司服务机子部署越来越多的情况下，让我们来想想会遇到的问题：</p><ul><li>开发人员不能登录线上服务器查看详细日志，经过运维周转费时费力</li><li>日志数据分散在多个系统，难以查找</li><li>日志数据量大，查询速度慢</li><li>一个调用会涉及多个系统，难以在这些系统的日志中快速定位数据</li><li>数据不够实时</li><li>很难对数据进行挖掘，分析，业务告警，审计</li><li>这些问题的存在让开发以及运维人员很是头痛，严重影响效率！</li></ul><h3 id="二：什么是graylog技术栈？"><a href="#二：什么是graylog技术栈？" class="headerlink" title="二：什么是graylog技术栈？"></a>二：什么是graylog技术栈？</h3><p>为了解决上述问题，我们需要一个日志的集中管理方案，graylog技术栈：</p><ul><li>java （jdk1.8.0_66/）环境</li><li>Collector-sidecar（收集日志）或者syslog</li><li>Mongodb（存储日志源文件）</li><li>Elasticsearch（提供搜索日志）</li><li>Graylog2.1.1（搜索和视图展示日志，告警和权限）</li></ul><p>有了这些，我们就能把日志先收集起来，进行我们想要的分析之后，web的形式展示出来，提供查询！</p><h3 id="三：graylog的安装部署"><a href="#三：graylog的安装部署" class="headerlink" title="三：graylog的安装部署"></a>三：graylog的安装部署</h3><p>安装环境：linux centOS系统安装，已安装JDK1.8版本，安装启动顺序</p><p>1.安装部署mongodb<br>2.安装部署elasticsearch<br>3.安装部署graylog<br>4.安装部署Graylog Collector Sidecar</p><hr><p> <strong>1：安装部署mongodb</strong> <strong>Java也安装</strong>  参考我博文有介绍。 </p><p> <strong>2：安装部署elasticsearch</strong></p><p><strong>(1)下载jar包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz</div><div class="line"></div><div class="line">如果报错</div><div class="line">执行</div><div class="line"></div><div class="line">wget --no-check-certificate</div><div class="line"></div><div class="line">https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz</div></pre></td></tr></table></figure><p><strong>(2)解压jar包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tar -zxvf elasticsearch-2.3.5.tar.gz -C /opt/</div></pre></td></tr></table></figure><p><strong>(3)修改elasticsearch.yml配置文件</strong></p><p> 这里需要修改配置文件：配置前先创建几个目录文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /home/data/es-data -p</div><div class="line">mkdir /home/data/es-work -p</div></pre></td></tr></table></figure><p>elasticsearch-2.3.5安装目录conf下执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">vim elasticsearch-2.3.5/config/elasticsearch.yml</div><div class="line"> </div><div class="line">cluster.name: graylog              <span class="comment">#集群名称建议命名graylog，便于识别区分</span></div><div class="line">node.name: elasticsearch-node-1     <span class="comment"># elasticsearch集群节点名称</span></div><div class="line">network.host: 192.168.1.234    <span class="comment"># 绑定节点IP</span></div><div class="line">http.port: 9200         <span class="comment"># 外部访问端口，默认，也可以安全考虑修改</span></div><div class="line">path.logs: /home/data/logs</div><div class="line">path.data: /home/data/es-data</div><div class="line">discovery.zen.ping.multicast.enabled: <span class="literal">false</span>   <span class="comment">#多播发现方式关闭，因为graylog采用单播方式发现elasticsearch集群方式</span></div><div class="line">discovery.zen.ping.unicast.hosts                   <span class="comment">#多个节点用逗号隔开</span></div><div class="line">discovery.zen.minimum_master_nodes: 3    <span class="comment"># elasticsearch集群节点，最少选举数，这个数一定要设置为整个集群节点个数的一半加1，即N/2+1，必须为奇数</span></div></pre></td></tr></table></figure><p><strong>(4)启动elasticsearch服务</strong></p><p>新建一个elasticsearch用户，出于安全考虑，elasticsearch服务不能使用root用户启动</p><p>创建elasticsearch用户组及elasticsearch用户，执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">groupadd elasticsearch</div><div class="line">useradd elasticsearch -g elasticsearch -p elasticsearch</div></pre></td></tr></table></figure><p>其中-g使用户属于某个组，-p为新用户使用加密密码）<br>更改elasticsearch-2.3.5文件夹及内部文件的所属用户及组为elasticsearch:elasticsearch</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">chown -R elasticsearch:elasticsearch  /home/data/</div><div class="line">chown -R elasticsearch:elasticsearch  elasticsearch-2.3.5</div></pre></td></tr></table></figure><p>切换用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">su elasticsearch</div></pre></td></tr></table></figure><p>在elasticsearch-2.3.5/bin目录下执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@graylog bin]$ ps -ef | grep elasticsearch</div><div class="line">root     18265 16004  0 14:27 pts/1    00:00:00 su elasticsearch</div><div class="line">502      18405     1 62 14:27 pts/1    00:00:13 /srv/jdk1.8.0_66/bin/java -Xms256m -Xmx1g -Djava.awt.headless=<span class="literal">true</span> -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=<span class="literal">true</span> -Des.path.home=/opt/elasticsearch-2.3.5 -cp /opt/elasticsearch-2.3.5/lib/elasticsearch-2.3.5.jar:/opt/elasticsearch-2.3.5/lib/* org.elasticsearch.bootstrap.Elasticsearch start <span class="_">-d</span></div><div class="line">502      18530 18266  0 14:27 pts/1    00:00:00 grep --color=auto ela</div></pre></td></tr></table></figure><p><strong>(5)检查elasticsearch服务状态</strong></p><p>执行如下命令测试Elasticsearch是否正常运行：</p><p>$ curl -XGET ‘<a href="http://localhost:9200/_cluster/health?pretty=true" target="_blank" rel="external">http://localhost:9200/_cluster/health?pretty=true</a>‘</p><p>输出的信息如下表示Elasticsearch安装成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@graylog bin]$ curl -XGET <span class="string">'http://192.168.1.234:9200/_cluster/health?pretty=true'</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"graylog"</span>,</div><div class="line">  <span class="string">"status"</span> : <span class="string">"yellow"</span>,</div><div class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</div><div class="line">  <span class="string">"number_of_nodes"</span> : 1,</div><div class="line">  <span class="string">"number_of_data_nodes"</span> : 1,</div><div class="line">  <span class="string">"active_primary_shards"</span> : 30,</div><div class="line">  <span class="string">"active_shards"</span> : 30,</div><div class="line">  <span class="string">"relocating_shards"</span> : 0,</div><div class="line">  <span class="string">"initializing_shards"</span> : 0,</div><div class="line">  <span class="string">"unassigned_shards"</span> : 30,</div><div class="line">  <span class="string">"delayed_unassigned_shards"</span> : 0,</div><div class="line">  <span class="string">"number_of_pending_tasks"</span> : 0,</div><div class="line">  <span class="string">"number_of_in_flight_fetch"</span> : 0,</div><div class="line">  <span class="string">"task_max_waiting_in_queue_millis"</span> : 0,</div><div class="line">  <span class="string">"active_shards_percent_as_number"</span> : 50.0</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="3：安装部署graylog-2-1版本"><a href="#3：安装部署graylog-2-1版本" class="headerlink" title="3：安装部署graylog-2.1版本"></a>3：安装部署graylog-2.1版本</h3><p><strong>(1)下载安装包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget https://packages.graylog2.org/releases/graylog/graylog-2.1.1.tgz</div></pre></td></tr></table></figure><p><strong>(2)解压安装包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tar -zxvf graylog-2.1.1.tgz -C /opt/.</div></pre></td></tr></table></figure><p><strong>(3)修改配置文件</strong></p><p>修改安装目录下 graylog.conf.example 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim graylog.conf.example</div></pre></td></tr></table></figure><p>web_listen_uri 值是graylog启动成功后，web服务访问地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">web_listen_uri = http://192.168.1.234:9000/</div></pre></td></tr></table></figure><p>rest_listen_uri 的值，是graylog启动成功后，api访问地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rest_listen_uri = http://192.168.1.234:9000/api/</div></pre></td></tr></table></figure><p>root_timezone = UTC  #设置时区，否则默认使用的是UTC时间也就是世界时间 这里是必须改下的，因为后期收集日志显示时间是有变化的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">root_timezone = Asia/Shanghai</div></pre></td></tr></table></figure><p>其中 password_secret 的值用命令生成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">yum install -y pwgen</div><div class="line">pwgen -N 1 <span class="_">-s</span> 96       H1R5v17kWtyDxj2PzxLMxu41D6HDt9JzhfZcj6QlCURVddgkLAdnUmpkdIscmmu4ELKsTrHwKvPmxFKSYyTn0YlqebbpQqyr</div><div class="line"></div><div class="line">    password_secret = H1R5v17kWtyDxj2PzxLMxu41D6HDt9JzhfZcj6QlCURVddgkLAdnUmpkdIscmmu4ELKsTrHwKvPmxFKSYyTn0YlqebbpQqyr</div></pre></td></tr></table></figure><p>其中root_password_sha2 的值使用命令生成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="built_in">echo</span> -n Ihaozhuo_b313 | sha256sum  (这里对密码123456哈希加密)</div><div class="line"></div><div class="line"><span class="built_in">fc</span>88c28d48b0cb97f3fb5286cc35c520409ef037acd30ec687f0c0bd3d5a5115 </div><div class="line">   </div><div class="line">root_password_sha2 = <span class="built_in">fc</span>88c28d48b0cb97f3fb5286cc35c520409ef037acd30ec687f0c0bd3d5a5115</div></pre></td></tr></table></figure><p>elasticsearch_cluster_name 值必须是elasticsearch配置文件中的cluster_name</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">elasticsearch_cluster_name = graylog</div></pre></td></tr></table></figure><p>elasticsearch_discovery_zen_ping_unicast_hosts 填写elasticsearch地址，如果是多个，用逗号隔开</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">elasticsearch_discovery_zen_ping_unicast_hosts = 192.168.1.234:9300</div></pre></td></tr></table></figure><p>elasticsearch_discovery_zen_ping_multicast_enabled = false 多播模式关闭</p><p>由于我们只有一个Elasticsearch shard，需要把elasticsearch_shards参数设置为1：<br>elasticsearch集群分片数量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">elasticsearch_shards = 1</div></pre></td></tr></table></figure><p>elasticsearch绑定的节点IP</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">299 elasticsearch_network_host = 192.168.1.234</div><div class="line">300 elasticsearch_network_bind_host = 192.168.1.234</div><div class="line">301 elasticsearch_network_publish_host = 192.168.1.234</div></pre></td></tr></table></figure><p>mongodb安装服务的ip地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mongodb_uri = mongodb://192.168.1.234/graylog</div></pre></td></tr></table></figure><p>这里mongodb是安装在我同一台服务器上面的。如果要把mongodb单独服务器跑连接方式配置文件里面也有例子说明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mongodb_uri =mongodb://graylog:123456@160.17.2.251:27017/graylog2             <span class="comment">#连接到mongodb的服务器地址为160.17.2.251:27017，账号为graylog，密码为123456 数据库为graylog2</span></div></pre></td></tr></table></figure><p> 设置告警邮件发送者信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># Email transport</span></div><div class="line">transport_email_enabled = <span class="literal">false</span></div><div class="line">transport_email_hostname = smtp.exmail.qq.com</div><div class="line">transport_email_port = 465</div><div class="line">transport_email_use_auth = <span class="literal">true</span></div><div class="line">transport_email_use_tls = <span class="literal">true</span></div><div class="line">transport_email_use_ssl = <span class="literal">true</span></div><div class="line">transport_email_auth_username = chengyangyang@qq.cn</div><div class="line">transport_email_auth_password = beneTqq</div><div class="line">transport_email_subject_prefix = [graylog]</div><div class="line">transport_email_from_email = chengyangyang@qq.cn</div></pre></td></tr></table></figure><p><strong>(4)复制配置文件</strong></p><p> 因为graylog安装bin目录下，默认启动配置文件</p><p>配置文件路径：<code>/etc/graylog/server/server.conf</code></p><p>所以需要将<code>graylog.conf.example</code> 复制到<code>/etc/graylog/server/</code>目录下，并且改名 <code>server.conf</code></p><p>执行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /etc/graylog/server/ </div><div class="line">cp graylog.conf.example /etc/graylog/server/server.conf</div></pre></td></tr></table></figure><p><strong>(5)启动graylog</strong></p><p>在graylog安装bin目录下执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./graylogctl start</div></pre></td></tr></table></figure><p>查看日志，在graylog安装目录下执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tail -200f /<span class="built_in">log</span>/graylog-server.log</div></pre></td></tr></table></figure><p>如果报错：</p><p>原因：</p><p>在mongodb版本2.6之后，是需要日志journaling设置的，而默认情况下是关闭的</p><p>解决办法：</p><p>在mongodb启动命令加上 –journal</p><p>最后启动命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./mongod --dbpath=/usr/<span class="built_in">local</span>/mongodb/data/ --fork --logpath=/usr/<span class="built_in">local</span>/mongodb/logs --storageEngine=mmapv1 --journal</div></pre></td></tr></table></figure><p>重启mongodb后，重启graylog服务即可！</p><p><strong>(6)启动graylog(报错二)</strong></p><p>查看日志，在graylog安装目录下执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">tail -200f /<span class="built_in">log</span>/graylog-server.log</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123;com.mongodb.MongoSocketOpenException: Exception opening socket&#125;, caused by &#123;java.net.ConnectException: 拒绝连接&#125;&#125;]&#125;. Waiting <span class="keyword">for</span> 30000 ms before timing out</div><div class="line">2016-11-22 15:10:29,355 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Elastic Beats Input 1.1.1 [org.graylog.plugins.beats.BeatsInputPlugin]</div><div class="line">2016-11-22 15:10:29,357 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Collector 1.1.1 [org.graylog.plugins.collector.CollectorPlugin]</div><div class="line">2016-11-22 15:10:29,357 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Enterprise Integration Plugin 1.1.1 [org.graylog.plugins.enterprise_integration.EnterpriseIntegrationPlugin]</div><div class="line">2016-11-22 15:10:29,358 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: MapWidgetPlugin 1.1.1 [org.graylog.plugins.map.MapWidgetPlugin]</div><div class="line">2016-11-22 15:10:29,359 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Pipeline Processor Plugin 1.1.1 [org.graylog.plugins.pipelineprocessor.ProcessorPlugin]</div><div class="line">2016-11-22 15:10:29,359 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Anonymous Usage Statistics 2.1.1 [org.graylog.plugins.usagestatistics.UsageStatsPlugin]</div><div class="line">2016-11-22 15:10:29,487 INFO : org.graylog2.bootstrap.CmdLineTool - Running with JVM arguments: -Djava.library.path=./../lib/sigar -Xms1g -Xmx1g -XX:NewRatio=1 -XX:+ResizeTLAB -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseParNewGC -XX:-OmitStackTraceInFastThrow</div></pre></td></tr></table></figure><p>提示mongodb 拒绝连接。 这个时候需要看下端口地址是否是IP地址还是127.0.0.1 如果不是需要修改下在重启服务就可以了。</p><p><strong>查看启动服务端口：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@graylog graylog-2.1.1]<span class="comment"># netstat -ntulp</span></div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name</div><div class="line">tcp        0      0 127.0.0.1:27017             0.0.0.0:*                   LISTEN      22308/mongod</div><div class="line">tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      1974/sshd</div><div class="line">tcp        0      0 127.0.0.1:25                0.0.0.0:*                   LISTEN      1287/master</div><div class="line">tcp        0      0 ::ffff:192.168.1.234:9350   :::*                        LISTEN      23642/java</div><div class="line">tcp        0      0 ::ffff:192.168.1.234:9000   :::*                        LISTEN      23642/java</div><div class="line">tcp        0      0 ::ffff:192.168.1.234:9200   :::*                        LISTEN      18405/java</div><div class="line">tcp        0      0 ::ffff:192.168.1.234:9300   :::*                        LISTEN      18405/java</div><div class="line">tcp        0      0 :::22                       :::*                        LISTEN      1974/sshd</div><div class="line">tcp        0      0 ::1:25                      :::*                        LISTEN      1287/master</div></pre></td></tr></table></figure><p><strong>(6)访问graylog</strong></p><p>graylog启动成功后，浏览器访问：<strong>graylog安装IP:9000</strong></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog10.png" alt=""></figure></p><h3 id="四：总结"><a href="#四：总结" class="headerlink" title="四：总结"></a>四：总结</h3><p>到此，基础的集中日志管理graylog安装完毕！，后续将继续介绍：</p><p>安装部署Graylog Collector Sidecar 收集应用日志</p><p>采用syslog收集日志方式</p><p>graylog一些使用，包括日志截取，告警等。</p><p>之前也实践过ELK技术栈，后来选型graylog，是基于graylog带有的权限管理，和告警功能比较完善！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;graylog-升级2-1-集中日志解决方案&quot;&gt;&lt;a href=&quot;#graylog-升级2-1-集中日志解决方案&quot; class=&quot;headerlink&quot; title=&quot;graylog 升级2.1 集中日志解决方案&quot;&gt;&lt;/a&gt;graylog 升级2.1 集中日志解决
      
    
    </summary>
    
      <category term="Log Analysis Platform" scheme="http://blog.yancy.cc/categories/Log-Analysis-Platform/"/>
    
    
      <category term="Log analysis platform" scheme="http://blog.yancy.cc/tags/Log-analysis-platform/"/>
    
  </entry>
  
  <entry>
    <title>Graylog——日志分析平台完美代替Elasticsearch</title>
    <link href="http://blog.yancy.cc/2016/11/13/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%10%E5%B9%B3%E5%8F%B0/Graylog/Graylog%E2%80%94%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%E5%AE%8C%E7%BE%8E%E4%BB%A3%E6%9B%BFElasticsearch/"/>
    <id>http://blog.yancy.cc/2016/11/13/日志分析平台/Graylog/Graylog—日志分析平台完美代替Elasticsearch/</id>
    <published>2016-11-13T09:56:03.000Z</published>
    <updated>2017-09-21T14:48:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>摘要: 提起日志聚合工具，有开源界的ELK，商业界的Splunk，但我要介绍开源的后起之秀Graylog，可以说是龙头老大Splunk的开源版。</p><h1 id="Graylog——日志分析平台完美代替Elasticsearch"><a href="#Graylog——日志分析平台完美代替Elasticsearch" class="headerlink" title="Graylog——日志分析平台完美代替Elasticsearch"></a>Graylog——日志分析平台完美代替Elasticsearch</h1><p>先看看 推荐！<a href="https://my.oschina.net/HeAlvin/blog/378262" target="_blank" rel="external">国外程序员整理的系统管理员资源大全</a> 中，国外程序员整理的日志聚合工具的列表：</p><p><strong>日志管理工具：收集，解析，可视化</strong></p><ul><li>Elasticsearch - 一个基于Lucene的文档存储，主要用于日志索引、存储和分析。</li><li>Fluentd - 日志收集和发出</li><li>Flume -分布式日志收集和聚合系统</li><li>Graylog2 -具有报警选项的可插入日志和事件分析服务器</li><li>Heka -流处理系统，可用于日志聚合</li><li>Kibana - 可视化日志和时间戳数据</li><li>Logstash -管理事件和日志的工具</li><li>Octopussy -日志管理解决方案（可视化/报警/报告）</li></ul><p>Graylog与ELK方案的对比</p><ul><li>ELK： Logstash -&gt; Elasticsearch -&gt; Kibana （使用了一些插件head ，marvel）</li><li>Graylog： Graylog Collector -&gt; Graylog Server(封装Elasticsearch) -&gt; Graylog Web</li></ul><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Graylog.png" alt=""></figure></p><p>做为运维，公司内部使用elk处理日志发现很多问题。</p><p>顺便截图了几张：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog-elk.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog-elk1.png" alt=""></figure></p><p>之前试过Logstash + Elasticsearch + Kibana的方案，发现有几个缺点：</p><ol><li>不能处理多行日志，比如Mysql慢查询，Tomcat/Jetty应用的Java异常打印</li><li>不能保留原始日志，只能把原始日志分字段保存，这样搜索日志结果是一堆Json格式文本，无法阅读。</li><li>不复合正则表达式匹配的日志行，被全部丢弃。</li><li>kibana结合使用经常会出现卡死。资源消耗非常大。</li></ol><p>本着解决以上3个缺点的原则，再次寻找替代方案。 首先找到了商业日志工具Splunk，号称日志界的Google，意思是全文搜索日志的能力，不光能解决以上3个缺点，还提供搜索单词高亮显示，不同错误级别日志标色等吸引人的特性，但是免费版有500M限制，付费版据说要3万美刀，只能放弃，继续寻找。 最后找到了Graylog，第一眼看到Graylog，只是系统日志syslog的采集工具，一点也没吸引到我。但后来深入了解后，才发现Graylog简直就是开源版的Splunk。 我自己总结的Graylog吸引人的地方：</p><ol><li>一体化方案，安装方便，不像ELK有3个独立系统间的集成问题。</li><li>采集原始日志，并可以事后再添加字段，比如http_status_code，response_time等等。</li><li>自己开发采集日志的脚本，并用curl/nc发送到Graylog Server，发送格式是自定义的GELF，Flunted和Logstash都有相应的输出GELF消息的插件。自己开发带来很大的自由度。实际上只需要用inotifywait监控日志的modify事件，并把日志的新增行用curl/netcat发送到Graylog Server就可。</li><li>搜索结果高亮显示，就像google一样。</li><li>搜索语法简单，比如： source:mongo AND reponse_time_ms:&gt;5000，避免直接输入elasticsearch搜索json语法</li><li>搜索条件可以导出为elasticsearch的搜索json文本，方便直接开发调用elasticsearch rest api的搜索脚本。</li></ol><p>Graylog图解</p><p>Graylog开源版官网： <a href="https://www.graylog.org/" target="_blank" rel="external">https://www.graylog.org/</a></p><p>来几张官网的截图：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog2.png" alt=""></figure></p><p><figure class="figure"><img src="https://www.graylog.org/assets/itops_full-87fe2811ab676116142c145affa95b902635b1d6da99718057e8ae9d45c15007.png" alt=""></figure></p><p><figure class="figure"><img src="https://www.graylog.org/assets/overview_dashboard-3817012f923d4a00e9bde1c0547796319cf0396149464f434cfc2ae83a9da826.png" alt=""></figure></p><p>Graylog是强大的日志管理、分析工具。它基于 Elasticsearch, Java和MongoDB。</p><p>Graylog可以收集监控多种不同应用的日志。但是为了示范说明，我只收集syslog。并且，我将会把用到的组件全部安装到一个单独的服务器上。对于大型、生产系统你可以把组件分开安装在不同的服务器上，这样可以提高效率。</p><h3 id="Graylog的组件"><a href="#Graylog的组件" class="headerlink" title="Graylog的组件"></a>Graylog的组件</h3><p>Graylog有4个基本组件：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Graylog <span class="built_in">Server</span>：这个服务负责接收和处理日志/消息，并且和其他组件沟通。</div><div class="line">Elasticsearch：存储所有的日志，它的性能依赖内存和硬盘IO。</div><div class="line">MongoDB：存储元数据，负载不高。</div><div class="line">graylog-Web接口：用户接口。</div></pre></td></tr></table></figure><p>下面是Graylog组件之间的关系图：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog3.png" alt=""></figure></p><p>下面来自我公司内部分享的PPT拍图：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Graylog4-ppt.png" alt=""></figure></p><p>生产环境 我参考网上一些博客画的图：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog5.png" alt=""></figure></p><h3 id="系统要求："><a href="#系统要求：" class="headerlink" title="系统要求："></a>系统要求：</h3><ul><li>CentOS 6.7</li><li>内存至少2GB</li><li>有root权限</li><li>服务器ip是192.168.1.234，已安装 1.8.0_77-b03</li></ul><p>这里我只是举例单一模式跑服务。</p><h3 id="安装MongoDB"><a href="#安装MongoDB" class="headerlink" title="安装MongoDB"></a>安装MongoDB</h3><p>MongoDB的安装非常简单，执行如下命令导入MongoDB GPG密钥到rpm：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">vim</span> <span class="string">/etc/yum.repos.d/mongodb-org-3.0.repo</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">[mongodb-org-3.0]</span></div><div class="line"><span class="string">name=MongoDB</span> <span class="string">Repository</span></div><div class="line"><span class="string">baseurl=http://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.0/x86_64/</span></div><div class="line"><span class="string">gpgcheck=0</span></div><div class="line"><span class="string">enabled=1</span></div><div class="line"><span class="meta">---</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">yum</span> <span class="string">install</span> <span class="bullet">-y</span> <span class="string">mongodb-org</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">vi</span> <span class="string">/etc/yum.conf</span></div><div class="line"><span class="string">最后一行添加：</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">exclude=mongodb-org,mongodb-org-server,mongodb-org-shell,mongodb-org-mongos,mongodb-org-tools</span></div><div class="line"><span class="meta">---</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">service</span> <span class="string">mongod</span> <span class="string">start</span></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">chkconfig</span> <span class="string">mongod</span> <span class="string">on</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">vi</span> <span class="string">/etc/security/limits.conf</span></div><div class="line"><span class="string">最后一行添加：</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">*</span>                <span class="string">soft</span>    <span class="string">nproc</span>           <span class="number">65536</span></div><div class="line"><span class="string">*</span>                <span class="string">hard</span>    <span class="string">nproc</span>           <span class="number">65536</span></div><div class="line"><span class="string">mongod</span>           <span class="string">soft</span>    <span class="string">nproc</span>           <span class="number">65536</span></div><div class="line"></div><div class="line"><span class="string">*</span>                <span class="string">soft</span>    <span class="string">nofile</span>          <span class="number">131072</span></div><div class="line"><span class="string">*</span>                <span class="string">hard</span>    <span class="string">nofile</span>          <span class="number">131072</span></div><div class="line"><span class="meta">---</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">vi</span> <span class="string">/etc/init.d/mongod</span></div><div class="line"><span class="string">ulimit</span> <span class="bullet">-f</span> <span class="string">unlimited</span> <span class="string">行前插入：</span></div><div class="line"><span class="meta">---</span></div><div class="line">  <span class="string">if</span> <span class="string">test</span> <span class="bullet">-f</span> <span class="string">/sys/kernel/mm/transparent_hugepage/enabled;</span> <span class="string">then</span></div><div class="line">    <span class="string">echo</span> <span class="string">never</span> <span class="string">&gt; /sys/kernel/mm/transparent_hugepage/enabled</span></div><div class="line">  fi</div><div class="line">  if test -f /sys/kernel/mm/transparent_hugepage/defrag; then</div><div class="line">    echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</div><div class="line">  fi</div><div class="line">---</div><div class="line">[root@graylog ~]# /etc/init.d/mongod restart</div></pre></td></tr></table></figure><h3 id="安装Elasticsearch"><a href="#安装Elasticsearch" class="headerlink" title="安装Elasticsearch"></a>安装Elasticsearch</h3><p>Graylog目前为止只能使用Elasticsearch 2.0以前的版本，所以，在这一步中，我将安装Elasticsearch 1.7.x。</p><p>添加Elasticsearch GPG密钥：</p><pre><code>$ sudo rpm --import http://packages.elastic.co/GPG-KEY-elasticsearch</code></pre><p>创建Elasticsearch源：</p><pre><code>$ sudo vim /etc/yum.repos.d/elasticsearch.repo</code></pre><p>写入如下内容：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="section">[elasticsearch-1.7]</span></div><div class="line"><span class="attr">name</span>=Elasticsearch repository for <span class="number">1.7</span>.x packages</div><div class="line"><span class="attr">baseurl</span>=http://packages.elastic.co/elasticsearch/<span class="number">1.7</span>/centos</div><div class="line"><span class="attr">gpgcheck</span>=<span class="number">1</span></div><div class="line"><span class="attr">gpgkey</span>=http://packages.elastic.co/GPG-KEY-elasticsearch</div><div class="line"><span class="attr">enabled</span>=<span class="number">1</span></div></pre></td></tr></table></figure><p>安装Elasticsearch：</p><pre><code>$ sudo yum -y install elasticsearch</code></pre><p>配置前先创建几个目录文件</p><pre><code>mkdir /data/es-data -pmkdir /data/es-work -p</code></pre><p>Elasticsearch安装完成之后，编辑配置文件：</p><pre><code>sudo vim /etc/elasticsearch/elasticsearch.yml node.data: true  # 数据存放true</code></pre><p>找到cluster.name一行，取消这一行的注释，并把值改为graylog-development：</p><pre><code>cluster.name: graylog-development path.data: /data/es-data     es数据存放目录  这里需要自己新建目录 path.work: /data/es-work   </code></pre><p>你也许想要限制外部访问Elasticsearch（端口9200），这样可以提高系统的安全性。找到network.host一行，取消注释，并把值改为localhost：</p><pre><code>network.host: 192.168.1.234</code></pre><p>保存退出文件。</p><p>重启Elasticsearch：</p><pre><code>$ service elasticsearch start</code></pre><p>设置开机启动：</p><pre><code>$ chkconfig --add elasticsearch</code></pre><p>执行如下命令测试Elasticsearch是否正常运行：</p><pre><code>$ curl -XGET &apos;http://localhost:9200/_cluster/health?pretty=true&apos;</code></pre><p>输出的信息如下表示Elasticsearch安装成功：</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">drwxr-xr-x. <span class="number">2</span> root   root  <span class="number">4096</span> <span class="number">9</span>月  <span class="number">23</span> <span class="number">2011</span> src</div><div class="line">[root<span class="symbol">@ELK</span> <span class="keyword">local</span>]<span class="meta">#  curl -XGET <span class="string">'http://192.168.1.234:9200/_cluster/health?pretty=true'</span></span></div><div class="line">&#123;</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"graylog-development"</span>,</div><div class="line">  <span class="string">"status"</span> : <span class="string">"yellow"</span>,</div><div class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</div><div class="line">  <span class="string">"number_of_nodes"</span> : <span class="number">1</span>,</div><div class="line">  <span class="string">"number_of_data_nodes"</span> : <span class="number">1</span>,</div><div class="line">  <span class="string">"active_primary_shards"</span> : <span class="number">37</span>,</div><div class="line">  <span class="string">"active_shards"</span> : <span class="number">37</span>,</div><div class="line">  <span class="string">"relocating_shards"</span> : <span class="number">0</span>,</div><div class="line">  <span class="string">"initializing_shards"</span> : <span class="number">0</span>,</div><div class="line">  <span class="string">"unassigned_shards"</span> : <span class="number">37</span>,</div><div class="line">  <span class="string">"delayed_unassigned_shards"</span> : <span class="number">0</span>,</div><div class="line">  <span class="string">"number_of_pending_tasks"</span> : <span class="number">0</span>,</div><div class="line">  <span class="string">"number_of_in_flight_fetch"</span> : <span class="number">0</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>安装Graylag</p><p>Graylog的最新版是1.1.4，下载链接如下：<br><a href="https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-server-1.1.4-1.noarch.rpm" target="_blank" rel="external">https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-server-1.1.4-1.noarch.rpm</a><br><a href="https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-web-1.1.4-1.noarch.rpm" target="_blank" rel="external">https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-web-1.1.4-1.noarch.rpm</a></p><p>现在Graylog的所有依赖软件安装完成，这一步我们来安装graylog-server。</p><p>首先，下载Graylog RPM软件包：</p><pre><code>$ cd $ sudo rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-1.3-repository-el7_latest.rpm</code></pre><p>安装graylog-server：</p><pre><code>yum -y install graylog-server</code></pre><p>安装pwgen，我们使用它生成随机密码：</p><pre><code>yum -y install epel-releaseyum -y install pwgen</code></pre><p>现在我们来设置Graylog管理员的密钥。配置文件位于/etc/graylog/server/server.conf目录，需要修改password_secret参数：</p><pre><code>$ SECRET=$(pwgen -s 96 1)$ sudo -E sed -i -e &apos;s/password_secret =.*/password_secret = &apos;$SECRET&apos;/&apos; /etc/graylog/server/server.conf</code></pre><p>执行完上面命令之后，password_secret参数的样子：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog6.png" alt=""></figure></p><p>这一步，设置管理员密码。由于密码使用sha哈希算法，我们需要把明文密码转换为hash，然后赋值给root_password_sha2参数。例如，我要设置的管理员密码是 Ihaozhuo_b313，它对应的hash为：</p><pre><code>$ echo -n Ihaozhuo_b313 | sha256sum | awk &apos;{print $1}&apos;</code></pre><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog7.png" alt=""></figure></p><p>编辑/etc/graylog/server/server.conf，设置root_password_sha2参数：</p><pre><code>$ sudo vim /etc/graylog/server/server.conf</code></pre><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog8.png" alt=""></figure></p><p>现在Graylog管理员密码为Ihaozhuo_b313。</p><p>配置rest_transport_uri参数，设置Graylog web接口和服务器的沟通方式。由于我们把所有组件都安装到了单独的一个服务器上，需要把值设置为<strong>127.0.0.1</strong> 或 <strong>localhost</strong>。找到rest_transport_uri一行，取消注释，并把值设置为：</p><pre><code>rest_transport_uri = http://192.168.1.234:12900/</code></pre><p>由于我们只有一个Elasticsearch shard，需要把elasticsearch_shards参数设置为1：</p><pre><code>elasticsearch_shards = 1</code></pre><p>更改elasticsearch_cluster_name参数，应该和前面Elasticsearch的<strong>cluster.name</strong>参数相对应：</p><pre><code>elasticsearch_cluster_name = graylog-development</code></pre><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog9.png" alt=""></figure></p><p>取消下面两行的注释，检测Elasticsearch：</p><pre><code>172 elasticsearch_discovery_zen_ping_multicast_enabled = false173 elasticsearch_discovery_zen_ping_unicast_hosts = 192.168.1.234:9300</code></pre><p>启动graylog-server：</p><pre><code>/etc/init.d/graylog-web restart</code></pre><h3 id="安装Graylog-Web"><a href="#安装Graylog-Web" class="headerlink" title="安装Graylog Web"></a>安装Graylog Web</h3><p>安装Graylog Web：</p><pre><code>$ sudo yum -y install graylog-web</code></pre><p>安装完成之后配置Graylog Web的密钥，配置文件位于/etc/graylog/web/web.conf，更改application.secret参数：</p><pre><code>$ SECRET=$(pwgen -s 96 1)$ sudo -E sed -i -e &apos;s/application\.secret=&quot;&quot;/application\.secret=&quot;&apos;$SECRET&apos;&quot;/&apos; /etc/graylog/web/web.conf</code></pre><p>配置graylog2-server.uris参数，它的值应该和Graylog的rest_listen_uri参数相对应：</p><pre><code>$ sudo vim /etc/graylog/web/web.confgraylog2-server.uris=&quot;http://127.0.0.1:12900/&quot;</code></pre><p>重启graylog-web：</p><pre><code>/etc/init.d/graylog-web restart</code></pre><h3 id="配置Graylog服务器接收其他服务器的syslog日志"><a href="#配置Graylog服务器接收其他服务器的syslog日志" class="headerlink" title="配置Graylog服务器接收其他服务器的syslog日志"></a>配置Graylog服务器接收其他服务器的syslog日志</h3><h4 id="登录Graylog-Web"><a href="#登录Graylog-Web" class="headerlink" title="登录Graylog Web"></a>登录Graylog Web</h4><p>使用浏览器访问Graylog服务器的域名或IP：<strong><a href="http://graylog_public_IP_domain:9000/" target="_blank" rel="external">http://graylog_public_IP_domain:9000/</a></strong>。</p><p>你应该能看到一个登录界面，使用admin做为用户名和前面设置的密码登录。</p><p>登录之后：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog10.png" alt=""></figure></p><p>上面的红数字1是通知（you have a node without any running inputs），下面设置通过UDP接收syslog。</p><p>创建syslog UDP输入</p><p>添加要接收的其他服务器syslog日志：System-&gt;Inputs-&gt;Syslog UDP-&gt;Launch new input。</p><p>在弹出的窗口上输入如下信息：</p><ul><li>Title: syslog</li><li>Port: 8514</li><li>Bind address: 这里写Graylog-server服务器主机IP</li></ul><p>点击Launch</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog11.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog12.png" alt=""></figure></p><p>如果你需要收集多个服务器的日志，重复上面步骤。</p><p>现在，我们的Graylog服务器已经做好了接收其他服务器发来日志的准备。下面我们还需要配置其他服务器，让这些服务器给Graylog服务器发送日志。</p><h4 id="配置其他服务器给Graylog服务器发送syslog"><a href="#配置其他服务器给Graylog服务器发送syslog" class="headerlink" title="配置其他服务器给Graylog服务器发送syslog"></a>配置其他服务器给Graylog服务器发送syslog</h4><p>参考官网：<a href="https://marketplace.graylog.org/addons/a47beb3b-0bd9-4792-a56a-33b27b567856" target="_blank" rel="external">从Linux系统日志发送到Graylog</a></p><p>SSH登录“其他服务器”，创建rsyslog配置文件90-graylog.conf：</p><pre><code>sudo vim /etc/rsyslog.d/90-graylog.conf</code></pre><p>添加如下代码，把 graylog_server_IP 替换为Graylog服务器ip地址：</p><pre><code>$template GRAYLOGRFC5424,&quot;&lt;%pri%&gt;%protocol-version% %timestamp:::date-rfc3339%    %HOSTNAME% %app-name% %procid% %msg%\n&quot;*.* @graylog_server_IP:8514;GRAYLOGRFC5424</code></pre><p>重启rsyslog服务使生效：</p><pre><code>/etc/init.d/rsyslog restart</code></pre><p>配置完成之后，回到Graylog Web，点击Sources，查看是否有新添加rsyslog。</p><h4 id="Graylog使用http协议发送："><a href="#Graylog使用http协议发送：" class="headerlink" title="Graylog使用http协议发送："></a>Graylog使用http协议发送：</h4><p>添加要接收的其他服务器syslog日志：System-&gt;Inputs-&gt;GELF HTTP-&gt;Launch new input。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog14.png" alt=""></figure></p><p>然后在服务器上面发送下面命令。</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog-development ~]</span># curl -XPOST http://<span class="number">192.168.1.234:12201</span>/gelf -p0 -d '&#123;<span class="string">"short_message"</span>:<span class="string">"Hello there"</span>, <span class="string">"host"</span>:<span class="string">"example.org"</span>, <span class="string">"facility"</span>:<span class="string">"test"</span>, <span class="string">"_foo"</span>:<span class="string">"bar"</span>&#125;'</div><div class="line"><span class="string">[root@graylog-development ~]</span># curl -XPOST http://<span class="number">192.168.1.234:12201</span>/gelf -p0 -d '&#123;<span class="string">"short_message"</span>:<span class="string">"测试"</span>, <span class="string">"host"</span>:<span class="string">"example.org"</span>, <span class="string">"facility"</span>:<span class="string">"test"</span>, <span class="string">"_foo"</span>:<span class="string">"bar"</span>&#125;'</div></pre></td></tr></table></figure><p>这里定义192.168.1.234 是graylog-server服务器地址 12201 端口是之前创建好的。</p><h3 id="搜素Graylog"><a href="#搜素Graylog" class="headerlink" title="搜素Graylog"></a>搜素Graylog</h3><p>假如你要搜索hello：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog13.png" alt=""></figure></p><p>上面安装配置了基本的Graylog服务器。</p><h3 id="时区和高亮设置"><a href="#时区和高亮设置" class="headerlink" title="时区和高亮设置"></a>时区和高亮设置</h3><p>admin帐号的时区：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">vi</span> <span class="string">/etc/graylog/server/server.conf</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="number">30</span> <span class="string">root_timezone</span> <span class="string">=</span> <span class="string">Asia/Shanghai</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">/etc/init.d/graylog-server</span> <span class="string">restart</span></div></pre></td></tr></table></figure><p>其他帐号的默认时区：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">vi</span> <span class="string">/etc/graylog/web/web.conf</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="number">18</span> <span class="string">timezone="Asia/Shanghai"</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">/etc/init.d/graylog-web</span> <span class="string">restart</span></div></pre></td></tr></table></figure><p>允许查询结果高亮：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">vi</span> <span class="string">/etc/graylog/server/server.conf</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="number">147</span> <span class="string">allow_highlighting</span> <span class="string">=</span> <span class="literal">true</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">/etc/init.d/graylog-server</span> <span class="string">restart</span></div></pre></td></tr></table></figure><h3 id="移动数据目录"><a href="#移动数据目录" class="headerlink" title="移动数据目录"></a>移动数据目录</h3><figure class="highlight livescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">移动elasticsearch的数据目录</div><div class="line">[root@graylog ~]<span class="comment"># sudo /etc/init.d/elasticsearch stop</span></div><div class="line">[root@graylog ~]<span class="comment"># sudo cp -rp /var/lib/elasticsearch/ /data/</span></div><div class="line">[root@graylog ~]<span class="comment"># sudo vi /etc/sysconfig/elasticsearch</span></div><div class="line">+<span class="number">16</span> DATA_DIR=/data/elasticsearch</div><div class="line">[root@graylog ~]<span class="comment"># sudo /etc/init.d/elasticsearch start</span></div><div class="line"></div><div class="line">移动mongo的数据目录</div><div class="line">[root@graylog ~]<span class="comment"># sudo /etc/init.d/mongod stop</span></div><div class="line">[root@graylog ~]<span class="comment"># sudo cp -rp /var/lib/mongo /data/</span></div><div class="line">[root@graylog ~]<span class="comment"># sudo vi /etc/mongod.conf</span></div><div class="line">---</div><div class="line"><span class="number">13</span> dbpath=/<span class="keyword">var</span>/lib/mongo<span class="function"></span></div><div class="line">-&gt;</div><div class="line"><span class="number">13</span> dbpath=/data/mongo</div><div class="line">---</div><div class="line">[mtagent@access2 ~]$ sudo /etc/init.d/mongod start</div></pre></td></tr></table></figure><h4 id="其余参考文档"><a href="#其余参考文档" class="headerlink" title="其余参考文档"></a>其余参考文档</h4><p><a href="http://docs.graylog.org/en/1.3/" target="_blank" rel="external">参考官网</a></p><p><a href="http://blog.topspeedsnail.com/archives/4066" target="_blank" rel="external">Centos7 搭建graylog</a></p><p><a href="https://my.oschina.net/fitnessefan/blog/464351?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="external">Graylog——日志聚合工具中的后起之秀</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;摘要: 提起日志聚合工具，有开源界的ELK，商业界的Splunk，但我要介绍开源的后起之秀Graylog，可以说是龙头老大Splunk的开源版。&lt;/p&gt;
&lt;h1 id=&quot;Graylog——日志分析平台完美代替Elasticsearch&quot;&gt;&lt;a href=&quot;#Graylog—
      
    
    </summary>
    
      <category term="Log Analysis Platform" scheme="http://blog.yancy.cc/categories/Log-Analysis-Platform/"/>
    
    
      <category term="Log analysis platform" scheme="http://blog.yancy.cc/tags/Log-analysis-platform/"/>
    
  </entry>
  
</feed>
