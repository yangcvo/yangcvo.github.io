<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yancy&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/4bb94619692fa63fbfa18343b7c7965c</icon>
  <subtitle>SIMPLICITY IS PREREQUISITE FOR  RELIABILITY</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.yancy.cc/"/>
  <updated>2018-04-21T11:53:28.813Z</updated>
  <id>http://blog.yancy.cc/</id>
  
  <author>
    <name>Yancy</name>
    <email>yangcvo@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>OpenLDAP一主多从复制节点服务的配置-phpldapadmin管理认证</title>
    <link href="http://blog.yancy.cc/2018/03/08/OpenLDAP/OpenLDAP%E4%B8%80%E4%B8%BB%E5%A4%9A%E4%BB%8E%E5%A4%8D%E5%88%B6%E8%8A%82%E7%82%B9%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%85%8D%E7%BD%AE-phpldapadmin%E7%AE%A1%E7%90%86%E8%AE%A4%E8%AF%81/"/>
    <id>http://blog.yancy.cc/2018/03/08/OpenLDAP/OpenLDAP一主多从复制节点服务的配置-phpldapadmin管理认证/</id>
    <published>2018-03-08T03:36:00.000Z</published>
    <updated>2018-04-21T11:53:28.813Z</updated>
    
    <content type="html"><![CDATA[<h4 id="OpenLDAP主从复制节点配置线上版本"><a href="#OpenLDAP主从复制节点配置线上版本" class="headerlink" title="OpenLDAP主从复制节点配置线上版本"></a>OpenLDAP主从复制节点配置线上版本</h4><p>公司服务器上搭建了一个OpenLDAP服务，为了避免出现单点，需要给LDAP做主从要从国外从服务器实时同步。这里我也升级了Openldap 配置一主多从方法。<br>于是上openldap官网上查了一下openldap的复制功能。</p><p><figure class="figure"><img src="https://devopsideas.com/wp-content/uploads/2017/09/Openldap-7-800x445.jpg" alt=""></figure></p><a id="more"></a><h4 id="OpenLDAP软件2-3管理员指南"><a href="#OpenLDAP软件2-3管理员指南" class="headerlink" title="OpenLDAP软件2.3管理员指南"></a><a href="http://www.openldap.org/doc/admin23/" target="_blank" rel="external">OpenLDAP软件2.3管理员指南</a></h4><p><a href=""></a></p><ul><li>OpenLDAP前期配置准备：</li><li>OpenLDAP同步条件：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">一主多从OpenLDAP集群服务器：特意声明下：2.3版本实现不了1主多从，只能实现1主1从。</div><div class="line"></div><div class="line">1.Linux系统最好保持一致：CentOS release 6.7 </div><div class="line">2.LDAP服务器之间需要保持时间同步  /usr/sbin/ntpdate ntp.api.bz</div><div class="line">3.LDAP软件包版本保持一致     openldap-2.4.40</div><div class="line">4.节点之间的域名可以互相解析  </div><div class="line">5.配置LDAP同步复制，需要提供完全一致的配置及目录树信息 （这里我会重点讲如何初始化数据）</div><div class="line">6.数据条目保持一致  （数据和结构目录统一化）</div><div class="line">7.额外的schema文件保持一致</div></pre></td></tr></table></figure><h4 id="openldap支持5种复制方式，分别是"><a href="#openldap支持5种复制方式，分别是" class="headerlink" title="openldap支持5种复制方式，分别是:"></a>openldap支持5种复制方式，分别是:</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Syncrepl：slave服务器从master上拉取数据，缺点是拉取的最小粒度是单条记录</div><div class="line">Delta-syncrepl：与上一条相似，但拉取的最小粒度是属性</div><div class="line">N-Way Multi-Master：多主，支持2个及以上的master</div><div class="line">MirrorMode：双主镜像，不支持3个及以上的master，但可以有slave</div><div class="line">Syncrepl Proxy：代理模式</div></pre></td></tr></table></figure><ul><li>同步需要开启syncrepl模式：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">slave服务器到master服务器以拉的模式同步目录树。当主服务器对某个条目或更多条目</div><div class="line">修改条目属性时，slave服务器会把修改的整个条目进行同步，而不是单独的同步修改的属</div><div class="line">性值。</div></pre></td></tr></table></figure><p>按目前的需求只要配置成MirrorMode即可，编辑<code>/etc/openldap/sldap.conf</code><br>找到<code>“moduleload syncprov.la”</code>，将前面的#号去掉。</p><p>操作系统信息：CentOS_6.x_64   <code>备注：6.0以下系统安装版本会过低，不支持一主多从配置。</code></p><table><thead><tr><th>角色</th><th>主机名</th><th>IP 地址</th></tr></thead><tbody><tr><td>OpenLDAP MAster服务器</td><td>openldap-master</td><td>192.168.17.145</td></tr><tr><td>OpenLDAP slave1服务器</td><td>openldap-slave1</td><td>192.168.3.15</td></tr><tr><td>OpenLDAP slave2服务器</td><td>openldap-slave2</td><td>192.168.3.82</td></tr></tbody></table><p>安装方法我在另外一篇安装配置文档很详细的写出。 这里我快速命令操作 不做太多的命令描述了。</p><h3 id="初始化数据结构"><a href="#初始化数据结构" class="headerlink" title="初始化数据结构"></a>初始化数据结构</h3><p>初始化命令：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">cp -a /var/<span class="class"><span class="keyword">lib</span>/<span class="title">ldap</span> /<span class="title">var</span>/<span class="title">lib</span>/<span class="title">ldap</span>.<span class="title">backup</span></span></div><div class="line">rm -f /var/<span class="class"><span class="keyword">lib</span>/<span class="title">ldap</span>/*</span></div><div class="line">cp /usr/share/openldap-servers/DB_CONFIG.example /var/<span class="class"><span class="keyword">lib</span>/<span class="title">ldap</span>/<span class="title">DB_CONFIG</span></span></div><div class="line">cp -a /etc/openldap/slapd.d/ <span class="regexp">/etc/openldap</span><span class="regexp">/slapd.dbakup</span></div><div class="line">rm -rf /etc<span class="regexp">/openldap/slapd</span>.d/*</div><div class="line">chown ldap.ldap /var/<span class="class"><span class="keyword">lib</span>/<span class="title">ldap</span>/*</span></div><div class="line">chmod -R <span class="number">600</span> /var/<span class="class"><span class="keyword">lib</span>/<span class="title">ldap</span>/*</span></div><div class="line">chown -R <span class="symbol">ldap:</span>ldap /etc/openldap/slapd.d</div><div class="line">cd /var/<span class="class"><span class="keyword">lib</span>/<span class="title">ldap</span>/ &amp;&amp; <span class="title">slapd</span></span></div><div class="line">slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d</div><div class="line">chown -R <span class="symbol">ldap:</span>ldap /etc/openldap/slapd.d</div><div class="line">chown ldap.ldap /var/<span class="class"><span class="keyword">lib</span>/<span class="title">ldap</span>/*</span></div><div class="line">service slapd restart</div></pre></td></tr></table></figure><h3 id="主服务器-Master-Centos6-6安装OpenLDAP"><a href="#主服务器-Master-Centos6-6安装OpenLDAP" class="headerlink" title="主服务器 Master | Centos6.6安装OpenLDAP"></a>主服务器 Master | Centos6.6安装OpenLDAP</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">添加host配置：</div><div class="line"></div><div class="line">1.时间同步：</div><div class="line">因为使用的是xx云主机，默认添加的有时间同步，这里就不在描述。</div><div class="line"></div><div class="line"></div><div class="line">yum快速安装openldap</div><div class="line"></div><div class="line"><span class="comment"># yum install -y vim automake autoconf gcc xz ncurses-devel \ patch python-devel git python-pip gcc-c++  # 安装基本环境，后面依赖</span></div><div class="line"><span class="comment"># yum install -y openldap openldap-servers openldap-clients openldap-devel</span></div><div class="line"></div><div class="line">配置 OpenLDAP 服务器</div><div class="line"></div><div class="line"><span class="comment">#拷贝LDAP配置文件到LDAP目录</span></div><div class="line"></div><div class="line"><span class="comment"># cp /usr/share/openldap-servers/slapd.conf.obsolete /etc/openldap/slapd.conf  ## 该文件是slapd的配置文件</span></div><div class="line"><span class="comment"># cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG    ## 数据库的配置文件</span></div><div class="line"></div><div class="line"><span class="comment"># cd /etc/openldap/</span></div><div class="line"><span class="comment"># cp slapd.conf slapd.conf.bak</span></div></pre></td></tr></table></figure><h4 id="编辑LDAP主配置文件-slapd-conf-文件如下"><a href="#编辑LDAP主配置文件-slapd-conf-文件如下" class="headerlink" title="编辑LDAP主配置文件 slapd.conf 文件如下:"></a>编辑LDAP主配置文件 slapd.conf 文件如下:</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div></pre></td><td class="code"><pre><div class="line">include         /etc/openldap/schema/corba.schema</div><div class="line">include         /etc/openldap/schema/core.schema</div><div class="line">include         /etc/openldap/schema/cosine.schema</div><div class="line">include         /etc/openldap/schema/duaconf.schema</div><div class="line">include         /etc/openldap/schema/dyngroup.schema</div><div class="line">include         /etc/openldap/schema/inetorgperson.schema</div><div class="line">include         /etc/openldap/schema/java.schema</div><div class="line">include         /etc/openldap/schema/misc.schema</div><div class="line">include         /etc/openldap/schema/nis.schema</div><div class="line">include         /etc/openldap/schema/openldap.schema</div><div class="line">include         /etc/openldap/schema/ppolicy.schema</div><div class="line">include         /etc/openldap/schema/collective.schema</div><div class="line"></div><div class="line"><span class="comment"># Allow LDAPv2 client connections.  This is NOT the default.</span></div><div class="line">allow bind_v2</div><div class="line">disallow bind_anon                   <span class="comment">#阻止匿名登陆</span></div><div class="line"><span class="comment"># Do not enable referrals until AFTER you have a working directory</span></div><div class="line"><span class="comment"># service AND an understanding of referrals.</span></div><div class="line"><span class="comment">#referral       ldap://root.openldap.org</span></div><div class="line"></div><div class="line">pidfile         /var/run/openldap/slapd.pid</div><div class="line">argsfile        /var/run/openldap/slapd.args</div><div class="line"></div><div class="line"><span class="comment"># Load dynamic backend modules</span></div><div class="line"><span class="comment"># - modulepath is architecture dependent value (32/64-bit system)</span></div><div class="line"><span class="comment"># - back_sql.la overlay requires openldap-server-sql package</span></div><div class="line"><span class="comment"># - dyngroup.la and dynlist.la cannot be used at the same time</span></div><div class="line"></div><div class="line">modulepath /usr/lib/openldap</div><div class="line">modulepath /usr/lib64/openldap</div><div class="line"></div><div class="line"><span class="comment"># moduleload accesslog.la</span></div><div class="line"><span class="comment"># moduleload auditlog.la</span></div><div class="line"><span class="comment"># moduleload back_sql.la</span></div><div class="line"><span class="comment"># moduleload chain.la</span></div><div class="line"><span class="comment"># moduleload collect.la</span></div><div class="line"><span class="comment"># moduleload constraint.la</span></div><div class="line"><span class="comment"># moduleload dds.la</span></div><div class="line"><span class="comment"># moduleload deref.la</span></div><div class="line"><span class="comment"># moduleload dyngroup.la</span></div><div class="line"><span class="comment"># moduleload dynlist.la</span></div><div class="line"><span class="comment"># moduleload memberof.la</span></div><div class="line"><span class="comment"># moduleload pbind.la</span></div><div class="line"><span class="comment"># moduleload pcache.la</span></div><div class="line"><span class="comment"># moduleload ppolicy.la</span></div><div class="line"><span class="comment"># moduleload refint.la</span></div><div class="line"><span class="comment"># moduleload retcode.la</span></div><div class="line"><span class="comment"># moduleload rwm.la</span></div><div class="line"><span class="comment"># moduleload seqmod.la</span></div><div class="line"><span class="comment"># moduleload seqmod.la</span></div><div class="line"><span class="comment"># moduleload smbk5pwd.la</span></div><div class="line"><span class="comment"># moduleload sssvlv.la</span></div><div class="line">moduleload syncprov.la</div><div class="line"><span class="comment"># moduleload translucent.la</span></div><div class="line"><span class="comment"># moduleload unique.la</span></div><div class="line"><span class="comment"># moduleload valsort.la</span></div><div class="line"></div><div class="line"><span class="comment"># The next three lines allow use of TLS for encrypting connections using a</span></div><div class="line"><span class="comment"># dummy test certificate which you can generate by running</span></div><div class="line"><span class="comment"># /usr/libexec/openldap/generate-server-cert.sh. Your client software may balk</span></div><div class="line"><span class="comment"># at self-signed certificates, however.</span></div><div class="line">TLSCACertificatePath /etc/openldap/certs</div><div class="line">TLSCertificateFile <span class="string">"\"OpenLDAP Server\""</span></div><div class="line">TLSCertificateKeyFile /etc/openldap/certs/password</div><div class="line"></div><div class="line"><span class="comment"># Sample security restrictions</span></div><div class="line"><span class="comment">#       Require integrity protection (prevent hijacking)</span></div><div class="line"><span class="comment">#       Require 112-bit (3DES or better) encryption for updates</span></div><div class="line"><span class="comment">#       Require 63-bit encryption for simple bind</span></div><div class="line"><span class="comment"># security ssf=1 update_ssf=112 simple_bind=64</span></div><div class="line"></div><div class="line"><span class="comment"># Sample access control policy:</span></div><div class="line"><span class="comment">#       Root DSE: allow anyone to read it</span></div><div class="line"><span class="comment">#       Subschema (sub)entry DSE: allow anyone to read it</span></div><div class="line"><span class="comment">#       Other DSEs:</span></div><div class="line"><span class="comment">#               Allow self write access</span></div><div class="line"><span class="comment">#               Allow authenticated users read access</span></div><div class="line"><span class="comment">#               Allow anonymous users to authenticate</span></div><div class="line"><span class="comment">#       Directives needed to implement policy:</span></div><div class="line"><span class="comment"># access to dn.base="" by * read</span></div><div class="line"><span class="comment"># access to dn.base="cn=Subschema" by * read</span></div><div class="line"><span class="comment"># access to *</span></div><div class="line"><span class="comment">#       by self write</span></div><div class="line"><span class="comment">#       by users read</span></div><div class="line"><span class="comment">#       by anonymous auth</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># if no access controls are present, the default policy</span></div><div class="line"><span class="comment"># allows anyone and everyone to read anything but restricts</span></div><div class="line"><span class="comment"># updates to rootdn.  (e.g., "access to * by * read")</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># rootdn can always read and write EVERYTHING!</span></div><div class="line"></div><div class="line">access to dn.subtree=<span class="string">"ou=users,dc=jollychic,dc=com"</span></div><div class="line">       by self write</div><div class="line">       by dn=<span class="string">"cn=Manager,dc=jollychic,dc=com"</span> write</div><div class="line">       by dn=<span class="string">"cn=repl,ou=manager,dc=jollychic,dc=com"</span> write</div><div class="line">       by dn.exact=<span class="string">"cn=zabbix,ou=manager,dc=jollychic,dc=com"</span> <span class="built_in">read</span></div><div class="line">       by users <span class="built_in">read</span></div><div class="line">       by anonymous auth</div><div class="line"></div><div class="line"> access to *</div><div class="line">        by self write</div><div class="line">        by users <span class="built_in">read</span></div><div class="line">        by anonymous auth</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># enable on-the-fly configuration (cn=config)</span></div><div class="line">database config</div><div class="line">access to *</div><div class="line">        by dn.exact=<span class="string">"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth"</span> manage</div><div class="line">        by * none</div><div class="line"></div><div class="line"><span class="comment"># enable server status monitoring (cn=monitor)</span></div><div class="line">database monitor</div><div class="line">access to *</div><div class="line">        by dn.exact=<span class="string">"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth"</span> <span class="built_in">read</span></div><div class="line">        by dn.exact=<span class="string">"cn=Manager,dc=my-domain,dc=com"</span> <span class="built_in">read</span></div><div class="line">        by * none</div><div class="line"></div><div class="line"><span class="comment">#######################################################################</span></div><div class="line"><span class="comment"># database definitions</span></div><div class="line"><span class="comment">#######################################################################</span></div><div class="line"></div><div class="line">database        bdb</div><div class="line">suffix          <span class="string">"dc=jollychic,dc=com"</span> </div><div class="line">checkpoint      1024 15</div><div class="line">rootdn          <span class="string">"cn=Manager,dc=jollychic,dc=com"</span></div><div class="line"><span class="comment"># Cleartext passwords, especially for the rootdn, should</span></div><div class="line"><span class="comment"># be avoided.  See slappasswd(8) and slapd.conf(5) for details.</span></div><div class="line"><span class="comment"># Use of strong authentication encouraged.</span></div><div class="line"><span class="comment"># rootpw                secret</span></div><div class="line"><span class="comment"># rootpw                &#123;crypt&#125;ijFYNcSNctBYg</span></div><div class="line">rootpw          se12pa  <span class="comment">#管理员密码</span></div><div class="line"><span class="comment"># The database directory MUST exist prior to running slapd AND</span></div><div class="line"><span class="comment"># should only be accessible by the slapd and slap tools.</span></div><div class="line"><span class="comment"># Mode 700 recommended.</span></div><div class="line">directory       /var/lib/ldap <span class="comment">#存储目录</span></div><div class="line"></div><div class="line"><span class="comment"># Indices to maintain for this database</span></div><div class="line">index objectClass                       eq,pres</div><div class="line">index ou,cn,mail,surname,givenname      eq,pres,sub</div><div class="line">index uidNumber,gidNumber,loginShell    eq,pres</div><div class="line">index uid,memberUid                     eq,pres,sub</div><div class="line">index nisMapName,nisMapEntry            eq,pres,sub</div><div class="line">index entryCSN,entryUUID eq</div><div class="line"></div><div class="line"><span class="comment"># syncprov配置</span></div><div class="line"><span class="comment">#配置末尾添加如下3行</span></div><div class="line"><span class="comment"># #######################################################################</span></div><div class="line"><span class="comment">#后端工作在overlay模式</span></div><div class="line">overlay syncprov</div><div class="line"><span class="comment">#当满足需改100个entry或者10分钟的条件时主动以推的方式执行</span></div><div class="line">syncprov-checkpoint 100 10</div><div class="line"><span class="comment">#会话日志条目的最大数量</span></div><div class="line">syncprov-sessionlog 100</div></pre></td></tr></table></figure><h4 id="修改系统日志配置文件"><a href="#修改系统日志配置文件" class="headerlink" title="修改系统日志配置文件"></a>修改系统日志配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># vim /etc/rsyslog.conf</span></div><div class="line"> <span class="built_in">local</span>4.*        /var/<span class="built_in">log</span>/ldap.log</div><div class="line"><span class="comment"># local7.*下添加一行</span></div><div class="line"></div><div class="line">在启动服务。</div><div class="line"><span class="comment"># service rsyslog restart</span></div></pre></td></tr></table></figure><h4 id="测试-slapd-conf-设置-slaptest检测、生成数据库"><a href="#测试-slapd-conf-设置-slaptest检测、生成数据库" class="headerlink" title="测试 slapd.conf 设置 slaptest检测、生成数据库"></a>测试 slapd.conf 设置 slaptest检测、生成数据库</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@openldap-master openldap]<span class="comment"># slaptest -u</span></div><div class="line">config file testing succeeded</div></pre></td></tr></table></figure><h4 id="OpenLDAP-的启动与停止"><a href="#OpenLDAP-的启动与停止" class="headerlink" title="OpenLDAP 的启动与停止"></a>OpenLDAP 的启动与停止</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># service slapd stop</span></div><div class="line"><span class="comment"># rm -rf /etc/openldap/slapd.d/*</span></div><div class="line"><span class="comment">#chown ldap.ldap /var/lib/ldap/*</span></div><div class="line"><span class="comment"># slaptest  -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d</span></div><div class="line"><span class="comment"># chown -R ldap:ldap /etc/openldap/slapd.d</span></div><div class="line"><span class="comment"># service slapd restart</span></div></pre></td></tr></table></figure><p>配置管理脚本：可以写成脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#----------------------------------------------------</span></div><div class="line">vim ldap.sh</div><div class="line"><span class="comment">#----------------------------------------------------</span></div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line">/etc/init.d/slapd stop</div><div class="line">rm -rf /etc/openldap/slapd.d/*</div><div class="line">slaptest <span class="_">-f</span> /etc/openldap/slapd.conf -F /etc/openldap/slapd.d</div><div class="line">chown -R ldap.ldap /etc/openldap/slapd.d</div><div class="line">/etc/init.d/slapd start</div><div class="line"><span class="comment">#----------------------------------------------------</span></div></pre></td></tr></table></figure><p>设置开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@Ldap-Server ldap]<span class="comment"># chkconfig slapd on</span></div><div class="line">[root@Ldap-Server ldap]<span class="comment"># chkconfig rsyslog on</span></div></pre></td></tr></table></figure><p>默认使用端口为389 通过ssl协议加密后slapd进程使用663端口号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@Ldap-Server ldap]<span class="comment"># netstat -lntup|grep 389</span></div><div class="line">tcp        0      0 0.0.0.0:389                 0.0.0.0:*                   LISTEN      25358/slapd</div><div class="line">tcp        0      0 :::389                      :::*                        LISTEN      25358/slapd</div></pre></td></tr></table></figure><p>这里创建好以后再参考第一篇创建导入点数据，作为设置同步查看效果。</p><h3 id="使用-phpLDAPadmin"><a href="#使用-phpLDAPadmin" class="headerlink" title="使用 phpLDAPadmin"></a>使用 phpLDAPadmin</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#安装PHP</span></div><div class="line">yum -y install php-gd php-xml php-mbstring php-ldap php-pear php-xmlrpc  php 需要PHP模块支持</div><div class="line"></div><div class="line"><span class="comment">#安装Apache</span></div><div class="line">yum install httpd -y</div><div class="line">vi /etc/httpd/conf/httpd.conf</div><div class="line"></div><div class="line">添加：</div><div class="line">....</div><div class="line">ServerName 192.168.17.145:80</div><div class="line">Listen 80</div><div class="line">.....</div><div class="line"></div><div class="line">service httpd start</div></pre></td></tr></table></figure><p>先通过scp上传phpldapadmin-1.2.3.zip到apache网页目录</p><p>下载：<a href="https://sourceforge.net/projects/phpldapadmin/files/phpldapadmin-php5/1.2.3/phpldapadmin-1.2.3.tgz" target="_blank" rel="external">phpldapadmin</a></p><p>或者这里用我下载好的链接wget.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#1. 下载安装</span></div><div class="line">    <span class="built_in">cd</span> /var/www/html/</div><div class="line">    wget http://oak0aohum.bkt.clouddn.com/phpldapadmin-1.2.3.tgz</div><div class="line">    tar -zxvf phpldapadmin-1.2.3.tgz</div><div class="line">    mv phpldapadmin-1.2.3 phpldapadmin</div><div class="line">    <span class="built_in">cd</span> phpldapadmin/config/</div><div class="line">    cp config.php.example config.php</div><div class="line">    vim config.php</div><div class="line"></div><div class="line"><span class="comment">#2. 修改配置文件：vim /var/www/html/phpldapadmin/config/config.php</span></div><div class="line">/*</div><div class="line"><span class="variable">$servers</span>-&gt;newServer(<span class="string">'ldap_pla'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'name'</span>,<span class="string">'LDAP Server'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'host'</span>,<span class="string">'192.168.17.145'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'port'</span>,389);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'base'</span>,array(<span class="string">'dc=jollychic,dc=com'</span>));</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'login'</span>,<span class="string">'auth_type'</span>,<span class="string">'cookie'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'login'</span>,<span class="string">'bind_id'</span>,<span class="string">'cn=Manager,dc=jollychic,dc=com'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'login'</span>,<span class="string">'bind_pass'</span>,<span class="string">'&#123;SSHA&#125;SlPVguw1zrxCkTnGXLM2jZpDZio9Btyt'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'tls'</span>,<span class="literal">false</span>);</div><div class="line"></div><div class="line"><span class="comment">#apache-http修改</span></div><div class="line"></div><div class="line">    vim /etc/httpd/conf/httpd.conf</div><div class="line">    &lt;Directory <span class="string">"/var/www/html/phpldapadmin"</span>&gt;</div><div class="line">    DirectoryIndex index.html index.html.var index.php</div><div class="line"><span class="comment">#重启服务。</span></div><div class="line"> service httpd restart</div><div class="line"></div><div class="line"><span class="comment">#防火墙端口开启</span></div><div class="line"> -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT</div><div class="line"></div><div class="line"><span class="comment">#重启iptables </span></div><div class="line">service iptables restart</div></pre></td></tr></table></figure><h4 id="登录PHPldapadmin"><a href="#登录PHPldapadmin" class="headerlink" title="登录PHPldapadmin"></a>登录PHPldapadmin</h4><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/openldapsalve1.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/openldapsalve2.png" alt=""></figure></p><h3 id="slave1-Centos6-6安装OpenLDAP"><a href="#slave1-Centos6-6安装OpenLDAP" class="headerlink" title="slave1 | Centos6.6安装OpenLDAP"></a>slave1 | Centos6.6安装OpenLDAP</h3><p><code>OpenLDAP slave1服务器|  openldap-slave1 | 192.168.3.15</code></p><p>前面安装全部一样，只需要在slave配置上面做下修改：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># syncprov配置</span></div><div class="line"><span class="comment"># #######################################################################</span></div><div class="line">overlay syncprov</div><div class="line">syncprov-checkpoint 100 10</div><div class="line">syncprov-sessionlog 100</div><div class="line"></div><div class="line">syncrepl rid=123</div><div class="line">    provider=ldap://192.168.17.145:389</div><div class="line">    <span class="built_in">type</span>=refreshAndPersist</div><div class="line">    searchbase=<span class="string">"dc=jollychic,dc=com"</span></div><div class="line">    interval=00:00:00:10</div><div class="line">    schemachecking=off</div><div class="line">    searchbase=<span class="string">"dc=jollychic,dc=com"</span></div><div class="line">    bindmethod=simple</div><div class="line">    scope=sub</div><div class="line">    binddn=<span class="string">"cn=Manager,dc=jollychic,dc=com"</span></div><div class="line">    retry=<span class="string">"60 +"</span></div><div class="line">    attrs=<span class="string">"*,+"</span></div><div class="line">    credentials=se12pa</div><div class="line">    </div><div class="line">mirrormode on</div></pre></td></tr></table></figure><h3 id="slave2-Centos6-6安装OpenLDAP"><a href="#slave2-Centos6-6安装OpenLDAP" class="headerlink" title="slave2 | Centos6.6安装OpenLDAP"></a>slave2 | Centos6.6安装OpenLDAP</h3><p><code>OpenLDAP slave2服务器|  openldap-slave2 | 192.168.3.82</code></p><p>只需要修改<code>rid=124</code> 可以往后添加ID数字加1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">syncrepl rid=124</div><div class="line">    provider=ldap://192.168.17.145:389</div><div class="line">    <span class="built_in">type</span>=refreshAndPersist</div><div class="line">    searchbase=<span class="string">"dc=jollychic,dc=com"</span></div><div class="line">    interval=00:00:00:00</div><div class="line">    schemachecking=off</div><div class="line">    searchbase=<span class="string">"dc=jollychic,dc=com"</span></div><div class="line">    bindmethod=simple</div><div class="line">    scope=sub</div><div class="line">    binddn=<span class="string">"cn=Manager,dc=jollychic,dc=com"</span></div><div class="line">    retry=<span class="string">"60 +"</span></div><div class="line">    attrs=<span class="string">"*,+"</span></div><div class="line">    credentials=se12pa</div><div class="line">    </div><div class="line">mirrormode on</div></pre></td></tr></table></figure><p>解释说明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># syncrepl特有的索引  </span></div><div class="line">index entryCSN eq  </div><div class="line">index entryUUID eq  </div><div class="line">   </div><div class="line"><span class="comment"># syncrepl参数  </span></div><div class="line">syncrepl rid=203  </div><div class="line">provider=ldap://IP地址:端口 <span class="comment">#提供者的IP和端口号  provider项填写主服务器的ldap地址</span></div><div class="line">bindmethod=simple <span class="comment">#认证方式，默认选择简单认证  </span></div><div class="line">interval=00:00:00:00 <span class="comment">#同步时间间隔 天：小时：分钟：秒  interval表示从服务器多久跟主服务器进行一次数据同步</span></div><div class="line">binddn=<span class="string">"cn=Manager,dc=jollychic,dc=com"</span> <span class="comment">#登陆的ldap账号  </span></div><div class="line">credentials=登陆密码  </div><div class="line">searchbase=<span class="string">"dc=jollychic,dc=com"</span> <span class="comment">#同步的根路径  </span></div><div class="line">filter=<span class="string">"(objectClass=*)"</span>  </div><div class="line">scope=sub  </div><div class="line">attrs=<span class="string">"*,+"</span>  </div><div class="line"><span class="built_in">type</span>=refreshAndPersist <span class="comment">#同步方式：有refreshAndPersist和  </span></div><div class="line">retry=<span class="string">"60 10 600 +"</span>  <span class="comment"># retry表示失败重试策略</span></div></pre></td></tr></table></figure><h3 id="PS-在phpldapadmin添加从LDAP服务器、便于管理"><a href="#PS-在phpldapadmin添加从LDAP服务器、便于管理" class="headerlink" title="PS:在phpldapadmin添加从LDAP服务器、便于管理"></a>PS:在phpldapadmin添加从LDAP服务器、便于管理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">/* A convenient name that will appear <span class="keyword">in</span> the tree viewer and throughout</div><div class="line">   phpLDAPadmin to identify this LDAP server to users. */</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'name'</span>,<span class="string">'My LDAP Server'</span>);</div><div class="line"></div><div class="line"><span class="variable">$servers</span>-&gt;newServer(<span class="string">'ldap_pla'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'name'</span>,<span class="string">'192.168.3.15:389'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'host'</span>,<span class="string">'192.168.3.15'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'port'</span>,<span class="string">'389'</span>);</div><div class="line"></div><div class="line"><span class="variable">$servers</span>-&gt;newServer(<span class="string">'ldap_pla'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'name'</span>,<span class="string">'192.168.3.82:389'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'host'</span>,<span class="string">'192.168.3.82'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'port'</span>,<span class="string">'389'</span>);</div></pre></td></tr></table></figure><h3 id="登录测试数据是否同步："><a href="#登录测试数据是否同步：" class="headerlink" title="登录测试数据是否同步："></a>登录测试数据是否同步：</h3><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/openldapsalve3.png" alt=""></figure></p><h3 id="slave1-数据同步成功图："><a href="#slave1-数据同步成功图：" class="headerlink" title="slave1 数据同步成功图："></a>slave1 数据同步成功图：</h3><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/openldapsalve4.png" alt=""></figure></p><h3 id="slave2-数据同步成功图："><a href="#slave2-数据同步成功图：" class="headerlink" title="slave2 数据同步成功图："></a>slave2 数据同步成功图：</h3><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/openldapsalve5.png" alt=""></figure></p><p>遇到故障问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@ldap-master ~]<span class="comment"># ldapsearch -x -LLL</span></div><div class="line">No such object (32)</div><div class="line"></div><div class="line">打开并修改为如下两行即可</div><div class="line">vim /etc/openldap/ldap.conf</div><div class="line"><span class="comment">#-----------------------------------------------------------------</span></div><div class="line">BASE dc=jollychic,dc=com</div><div class="line">URI ldap://192.168.17.145</div><div class="line"><span class="comment">#-----------------------------------------------------------------</span></div></pre></td></tr></table></figure><p>测试结果，主从配置成功。</p><p>补充：由于在syncrepl中slave是refreshOnly，相当于从节点是只读的，这时不允许在从节点导入或者删除用户，否则会出现错误，如下所示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@LDAP openldap]<span class="comment"># ldapadd -x -D "cn=Manager,dc=jollychic,dc=com" -W -f /tmp/jolly.ldif</span></div><div class="line">Enter LDAP Password:</div><div class="line">adding new entry <span class="string">"dc=jollychic,dc=com"</span></div><div class="line">ldap_add: Server is unwilling to perform (53)</div><div class="line">additional info: shadow context; no update referral</div></pre></td></tr></table></figure><p>参考：<a href="https://itsecureadmin.com/2013/01/ldapmodify-fails-with-server-is-unwilling-to-perform-53/" target="_blank" rel="external">https://itsecureadmin.com/2013/01/ldapmodify-fails-with-server-is-unwilling-to-perform-53/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;OpenLDAP主从复制节点配置线上版本&quot;&gt;&lt;a href=&quot;#OpenLDAP主从复制节点配置线上版本&quot; class=&quot;headerlink&quot; title=&quot;OpenLDAP主从复制节点配置线上版本&quot;&gt;&lt;/a&gt;OpenLDAP主从复制节点配置线上版本&lt;/h4&gt;&lt;p&gt;公司服务器上搭建了一个OpenLDAP服务，为了避免出现单点，需要给LDAP做主从要从国外从服务器实时同步。这里我也升级了Openldap 配置一主多从方法。&lt;br&gt;于是上openldap官网上查了一下openldap的复制功能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://devopsideas.com/wp-content/uploads/2017/09/Openldap-7-800x445.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/categories/OpenLDAP/"/>
    
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/tags/OpenLDAP/"/>
    
  </entry>
  
  <entry>
    <title>OpenLDAP 错误收集</title>
    <link href="http://blog.yancy.cc/2018/02/04/OpenLDAP/OpenLDAP%20%E9%94%99%E8%AF%AF%E6%94%B6%E9%9B%86/"/>
    <id>http://blog.yancy.cc/2018/02/04/OpenLDAP/OpenLDAP 错误收集/</id>
    <published>2018-02-04T03:36:00.000Z</published>
    <updated>2018-04-21T11:36:58.502Z</updated>
    
    <content type="html"><![CDATA[<h4 id="OpenLDAP-错误收集"><a href="#OpenLDAP-错误收集" class="headerlink" title="OpenLDAP 错误收集"></a>OpenLDAP 错误收集</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">启动slapd服务：报错</div><div class="line"></div><div class="line">Checking configuration files <span class="keyword">for</span> slapd:                    [WARNING]</div><div class="line">bdb_db_open: warning – no DB_CONFIG file found <span class="keyword">in</span> directory /var/lib/ldap: (2).</div><div class="line">Expect poor performance <span class="keyword">for</span> suffix “dc=my-domain,dc=com”.</div><div class="line">config file testing succeeded</div><div class="line"></div><div class="line">操作命令：</div><div class="line">rm -rf /var/lib/ldap/*</div><div class="line">cp /usr/share/doc/openldap-servers-2.4.12/DB_CONFIG.example /var/lib/ldap/DB_CONFIG</div><div class="line">chown -R ldap:ldap /var/lib/ldap</div><div class="line">/etc/init.d/ldap restart</div><div class="line"></div><div class="line">2. slaptest <span class="_">-f</span> /etc/openldap/slapd.conf -F /etc/openldap/slapd.d 报错：</div><div class="line"></div><div class="line">[root@yancy ldap]<span class="comment"># slaptest  -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d</span></div><div class="line">59842dbe bdb_db_open: database <span class="string">"dc=jollychic,dc=com"</span>: db_open(/var/lib/ldap/id2entry.bdb) failed: No such file or directory (2).</div><div class="line">59842dbe backend_startup_one (<span class="built_in">type</span>=bdb, suffix=<span class="string">"dc=jollychic,dc=com"</span>): bi_db_open failed! (2)</div><div class="line">slap_startup failed (<span class="built_in">test</span> would succeed using the -u switch)</div><div class="line"></div><div class="line"></div><div class="line">解决操作命令：</div><div class="line">解决方法： </div><div class="line">cp /usr/share/openldap-servers/DB_CONFIG.example  /var/lib/ldap/DB_CONFIG</div><div class="line">chown -R ldap.ldap /var/lib/ldap</div><div class="line"></div><div class="line">继续检查配置文件</div><div class="line">slaptest <span class="_">-f</span> /etc/openldap/slapd.conf -F /etc/openldap/slapd.d</div><div class="line"></div><div class="line">如果还继续报错：</div><div class="line"></div><div class="line">删掉slapd.d下的文件，重新生成 不然待会启动时会报错</div><div class="line">[root@yancy ldap]<span class="comment"># rm -rf  slapd.d/*</span></div><div class="line">[root@yancy ldap]<span class="comment"># cd /var/lib/ldap/</span></div><div class="line">[root@yancy ldap]<span class="comment"># slapd</span></div><div class="line"></div><div class="line">测试 slapd.conf 设置 </div><div class="line">slaptest -u</div><div class="line">cp <span class="_">-a</span> /etc/openldap/slapd.d/ /etc/openldap/slapd.dbakup0810</div><div class="line">rm -rf /etc/openldap/slapd.d/*</div><div class="line">chown ldap.ldap /var/lib/ldap/*</div><div class="line">chmod -R 600 /var/lib/ldap/*</div><div class="line"><span class="built_in">cd</span> /var/lib/ldap/ &amp;&amp; slapd</div><div class="line">slaptest <span class="_">-f</span> /etc/openldap/slapd.conf -F /etc/openldap/slapd.d</div><div class="line"></div><div class="line"></div><div class="line">如果还不行：</div><div class="line">rm -rf /etc/openldap/slapd.d/*</div><div class="line">rm -rf rm -rf /var/lib/ldap/__db.00*</div><div class="line">rm -rf /var/lib/ldap/alock</div><div class="line">/etc/init.d/slapd restart</div><div class="line"></div><div class="line">chown -R ldap:ldap /etc/openldap/slapd.d</div><div class="line">chown ldap.ldap /var/lib/ldap/*</div><div class="line">service slapd restart</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">3.  导入数据报错： </div><div class="line">[root@H5 ldap]<span class="comment">#  ldapadd -x -D "cn=Manager,dc=jollychic,dc=com" -W -f /tmp/base.ldif</span></div><div class="line">Enter LDAP Password:</div><div class="line">adding new entry <span class="string">"dc=jollychic,dc=com"</span></div><div class="line">ldap_add: Server is unwilling to perform (53)</div><div class="line">additional info: shadow context; no update referral</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;OpenLDAP-错误收集&quot;&gt;&lt;a href=&quot;#OpenLDAP-错误收集&quot; class=&quot;headerlink&quot; title=&quot;OpenLDAP 错误收集&quot;&gt;&lt;/a&gt;OpenLDAP 错误收集&lt;/h4&gt;&lt;figure class=&quot;highlight bash
      
    
    </summary>
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/categories/OpenLDAP/"/>
    
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/tags/OpenLDAP/"/>
    
  </entry>
  
  <entry>
    <title>OpenLDAP企业应用方案-PPT认识LDAP+熟悉操作LDAP命令</title>
    <link href="http://blog.yancy.cc/2018/02/03/OpenLDAP/%20OpenLDAP%20%E4%BC%81%E4%B8%9A%E5%BA%94%E7%94%A8%E6%96%B9%E6%A1%88%20-PPT%E8%AE%A4%E8%AF%86LDAP+%E7%86%9F%E6%82%89%E6%93%8D%E4%BD%9CLDAP%E5%91%BD%E4%BB%A4/"/>
    <id>http://blog.yancy.cc/2018/02/03/OpenLDAP/ OpenLDAP 企业应用方案 -PPT认识LDAP+熟悉操作LDAP命令/</id>
    <published>2018-02-03T03:36:00.000Z</published>
    <updated>2018-04-21T09:59:39.559Z</updated>
    
    <content type="html"><![CDATA[<h4 id="认识LDAP熟悉LDAP配置"><a href="#认识LDAP熟悉LDAP配置" class="headerlink" title="认识LDAP熟悉LDAP配置"></a>认识LDAP熟悉LDAP配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">LDAP目录中可以存储各种类型的数据：电子邮件地址、邮件路由信息、人力资源数据、公用密匙、联系人列表，等等</div><div class="line">但是不是关系型数据库。不象被设计成每分钟需要处理成百上千条数据变化的数据库 </div><div class="line">可以把数据“推”到远程的办公室，以增加数据的安全性</div><div class="line">复制功能，数据库产商就会要你支付额外的费用，而且也很难管理</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">LDAP组织-目录树的结构</div><div class="line"></div><div class="line">LDAP目录中的所有记录项都有一个唯一的“Distinguished Name”</div><div class="line">现在为公司的员工设置一个DN。可以用基于cn或uid（User ID），作为典型的用户帐号</div><div class="line">用uid表示“User ID”，不要把它和UNIX的uid号混淆了 ，大多数公司都会给每一个员工唯一的登录名，因此用这个办法可以很好地保存员工的信息</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">LDIF文件格式例子：一个普通用户都要存哪些信息?</div><div class="line"></div><div class="line">你可以用LDAP存储各种类型的数据对象，只要这些对象可以用属性来表示，下面这些是可以在LDAP中存储的一些信息： </div><div class="line">员工信息：员工的姓名、登录名、口令、员工号、他的经理的登录名，邮件服务器，等等。 </div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">lDAP默认端口：端口为389</div><div class="line"></div><div class="line">LDAP同步配置:</div><div class="line"><span class="comment"># Where to store the replica logs for database #1</span></div><div class="line"><span class="comment">#replogfile     /var/lib/ldap/master-slapd.replog</span></div><div class="line"></div><div class="line">replogfile /var/lib/ldap/master-slapd.replog</div><div class="line">replica         host=192.168.7.108:389</div><div class="line">                binddn=<span class="string">"cn=admin,dc=ldap,dc=monkey,dc=com,dc=de"</span></div><div class="line">                bindmethod=simple credentials=<span class="string">'password'</span></div></pre></td></tr></table></figure><h5 id="OpenLDAP-包在服务器上安装了很多程序："><a href="#OpenLDAP-包在服务器上安装了很多程序：" class="headerlink" title="OpenLDAP 包在服务器上安装了很多程序："></a>OpenLDAP 包在服务器上安装了很多程序：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"> 守护进程：</div><div class="line"></div><div class="line"></div><div class="line">- slapd：主 LDAP 服务器</div><div class="line"></div><div class="line">- slurpd：负责与复制 LDAP 服务器保持同步的服务器</div><div class="line"></div><div class="line">- 对网络上的目录进行操作的客户机程序。下面这两个程序是一对儿：</div><div class="line"></div><div class="line">- ldapadd：打开一个到 LDAP 服务器的连接，绑定、修改或增加条目</div><div class="line"></div><div class="line">- ldapsearch：打开一个到 LDAP 服务器的连接，绑定并使用指定的参数进行搜索</div><div class="line"></div><div class="line">- 对本地系统上的数据库进行操作的几个程序：</div><div class="line"></div><div class="line">- slapadd：将以 LDAP 目录交换格式（LDIF）指定的条目添加到 LDAP 数据库中</div><div class="line"></div><div class="line">- slapcat：打开 LDAP 数据库，并将对应的条目输出为 LDIF 格式</div></pre></td></tr></table></figure><h5 id="Openldap命令操作总结："><a href="#Openldap命令操作总结：" class="headerlink" title="Openldap命令操作总结："></a>Openldap命令操作总结：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">解读配置文件：</div><div class="line"></div><div class="line">vim /etc/openldap/slapd.conf</div><div class="line"></div><div class="line"></div><div class="line">例子 </div><div class="line"></div><div class="line">创建用户</div><div class="line"></div><div class="line">ldapadd -x -D <span class="string">"cn=Manager,dc=jollychic,dc=com"</span> -w secret <span class="_">-f</span> /root/test.ldif </div><div class="line">ldapadd -x -D <span class="string">"cn=Manager,dc=jollychic,dc=com"</span> -w secret (这样写就是在命令行添加条目)</div><div class="line"></div><div class="line">-x   进行简单认证</div><div class="line">-D   用来绑定服务器的DN</div><div class="line">-w   绑定DN的密码</div><div class="line">-b   指定要查询的根节点</div><div class="line">-H   制定要查询的服务器</div><div class="line">-S   提示的输入密码</div><div class="line"><span class="_">-s</span> pass 把密码设置为pass</div><div class="line"><span class="_">-a</span> pass 设置old passwd为pass</div><div class="line">-A   提示的设置old passwd</div><div class="line">-I   使用sasl会话方式</div><div class="line"></div><div class="line">这样可以访问整个的活动目录结构.</div><div class="line"></div><div class="line">ldapsearch -x -W -D <span class="string">"cn=Manager,dc=jollychic,dc=com"</span> -b <span class="string">"dc=jollychic,dc=com"</span> </div><div class="line">se12pa</div><div class="line"></div><div class="line">ldapsearch -x -W -D <span class="string">"uid=764,ou=users,dc=jollychic,dc=com"</span> -b <span class="string">"uid=764,ou=users,dc=jollychic,dc=com"</span></div><div class="line">使用简单认证，用 <span class="string">"ou=users,dc=jollychic,dc=com"</span> 进行绑定，要查询的根是 <span class="string">"dc=jollychic,dc=com"</span>。这样会把绑定的用户能访问<span class="string">"uid=764,ou=users,dc=jollychic,dc=com"</span>下的</div><div class="line">所有数据显示出来。</div><div class="line"></div><div class="line"></div><div class="line">ldapdelete -x -D <span class="string">"cn=Manager,dc=jollychic,dc=com"</span> -W <span class="string">"uid=900,ou=users,dc=jollychic,dc=com"</span> -S</div><div class="line">ldapmodify命令,在changetype时输入：delete</div><div class="line"></div><div class="line"></div><div class="line">修改用户密码：</div><div class="line"></div><div class="line">ldappasswd -x -D <span class="string">"cn=Manager,dc=jollychic,dc=com"</span> -W <span class="string">"uid=900,ou=users,dc=jollychic,dc=com"</span> -S</div><div class="line">New password:</div><div class="line">Re-enter new password:</div><div class="line">Enter LDAP Password:</div><div class="line">就可以更改密码了，如果原来记录中没有密码，将会自动生成一个userPassword。</div><div class="line"><span class="comment">##Enter LDAP password" 是 "cn=Manager,dc=jollychic,dc=com"管理员的密码.</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">管理员密码更改</div><div class="line"><span class="comment">#slappasswd</span></div><div class="line">New password</div><div class="line">Re-enter new password</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;认识LDAP熟悉LDAP配置&quot;&gt;&lt;a href=&quot;#认识LDAP熟悉LDAP配置&quot; class=&quot;headerlink&quot; title=&quot;认识LDAP熟悉LDAP配置&quot;&gt;&lt;/a&gt;认识LDAP熟悉LDAP配置&lt;/h4&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/categories/OpenLDAP/"/>
    
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/tags/OpenLDAP/"/>
    
  </entry>
  
  <entry>
    <title>OpenLDAP服务的配置与管理-phpldapadmin管理认证</title>
    <link href="http://blog.yancy.cc/2018/01/29/OpenLDAP/OpenLDAP%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E7%AE%A1%E7%90%86-phpldapadmin%E7%AE%A1%E7%90%86%E8%AE%A4%E8%AF%81/"/>
    <id>http://blog.yancy.cc/2018/01/29/OpenLDAP/OpenLDAP服务的配置与管理-phpldapadmin管理认证/</id>
    <published>2018-01-29T03:36:00.000Z</published>
    <updated>2018-04-21T11:53:22.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用-OpenLDAP-集中管理用户帐号"><a href="#使用-OpenLDAP-集中管理用户帐号" class="headerlink" title="使用 OpenLDAP 集中管理用户帐号"></a>使用 OpenLDAP 集中管理用户帐号</h2><h4 id="1-OpenLDAP-简介"><a href="#1-OpenLDAP-简介" class="headerlink" title="1.OpenLDAP 简介"></a>1.OpenLDAP 简介</h4><p>OpenLDAP可以从 http: //www.openldap.org/ 获得安装包。OpenLDAP 网站主页如图所示。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/openldap3.png" alt=""></figure></p><a id="more"></a><p>2.OpenLDAP 软件包的获取与安装</p><p>用 rpm –ivh 命令进行安装。 </p><h4 id="1-redHat系统rpm手工安装"><a href="#1-redHat系统rpm手工安装" class="headerlink" title="1.redHat系统rpm手工安装"></a>1.redHat系统rpm手工安装</h4><pre><code class="bash"><span class="comment"># rpm -qa |grep ldap</span>openldap-devel-2.3.27-5php-ldap-5.1.6-5.el5openldap-servers-2.3.27-5openldap-2.3.27-5openldap-clients-2.3.27-5nss_ldap-253-3安装rpm -ivh openldap-servers-2.3.27-5.i386.rpmwarning: openldap-servers-2.3.27-5.i386.rpm: Header V3 DSA signature: NOKEY, key ID 37017186Preparing... <span class="comment">########################################### [100%] 1:openldap-servers ########################################### [100%]</span>    [root@localhost teacher]<span class="comment"># rpm -ivh openldap-servers-sql-2.3.27-5.i386.rpm</span>warning: openldap-servers-sql-2.3.27-5.i386.rpm: Header V3 DSA signature: NOKEY, key ID 37017186Preparing... <span class="comment">########################################### [100%]</span>6011:openldap-servers-sql <span class="comment">########################################### [100%]</span><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">若出现以上结果表示 openldap 服务器包安装完毕。</div><div class="line"></div><div class="line">centos上面系统<span class="number">6</span> 或者<span class="number">7</span> 都是只能看到：</div></pre></td></tr></table></figure>[root@openldap ~]<span class="comment"># rpm -qa |grep ldap</span>openldap-2.4.40-5.el6.x86_64<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"><span class="comment">### 2.根据源代码编译安装</span></span></div><div class="line"></div><div class="line">下载源码包：[openldap-stable](ftp://ftp.openldap.org/pub/OpenLDAP/openldap-release)</div><div class="line"></div><div class="line">```bash</div><div class="line"><span class="meta">#</span><span class="bash">tar –zxvf openldap-stable-20070831.tgz</span><span class="meta">#</span><span class="bash"><span class="built_in">cd</span> openldap-stable-20070831</span><span class="meta">#</span><span class="bash">./configure</span><span class="meta">#</span><span class="bash">make depend</span><span class="meta">#</span><span class="bash">make</span><span class="meta">#</span><span class="bash">make <span class="built_in">test</span></span><span class="meta">#</span><span class="bash">make install</span></div></pre></td></tr></table></figure>* Berkeley DB 数据库的安装OpenLDAP 可以支持多种后台数据库,如 LDBM 和 BDB 等,这些轻量级的数据库项目都是 针对(key,value)类型的信息存放的,属于非关系型数据库,采用 <span class="built_in">hash</span> 散列或者 B+树的方式存 储数据,查询效率较高。BDB(Berkeley DB)是由美国 Sleepycat Software 公司开发的一套开放源代码的嵌入式数据库 系统,2006 年被 Oracle 公司收购。它具有良好的可伸缩、高性能的事务处理机制和较好的可扩展 性等优点。OpenLDAP 可以使用 BDB 或 LDBM 作为后台数据库。因此,若需要正常使用 OpenLDAP 服务器,需要安装 BDB 数据库在下载[Berkeley DB](http://www.oracle.com/technology/software/products/berkeley-db/index.html)Berkeley DB 下载页面 以下载并安装 `db-4.6.19.tar.gz` 为例,使用如下命令:```bash<span class="comment"># tar zxvf db-4.6.19.tar.gz</span><span class="comment"># cd db-4.6.19/build_unix</span> <span class="comment"># ../dist/configure</span><span class="comment"># make</span><span class="comment"># make install</span><figure class="highlight clean"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">默认情况下,BDB 被安装在/usr/local/BerkeleyDB<span class="number">.4</span><span class="number">.6</span>/目录中,为了能让 OpenLDAP 使用 BDB 的库文件,还需要将 BDB 库文件所在的目录添加到系统动态链接库的路径中。需要编辑系统动态 链接库的配置文件/etc/ld.so.conf,在文件的末尾加入如下语句:    </div><div class="line">    /usr/local/ BerkeleyDB<span class="number">.4</span><span class="number">.6</span>/lib之后,使用以下命令刷新系统动态链接库缓存:Berkeley DB 数据库安装完毕。</div><div class="line"></div><div class="line">#### <span class="number">3.</span>这里我选择yum一键安装因为centos会自动安装一些需要的依赖</div><div class="line"></div><div class="line"><span class="number">1.</span>LDAP Server Setup:</div><div class="line"></div><div class="line">a.安装LDAP服务（使用YUM本地光盘安装）</div><div class="line"></div><div class="line">会提示安装以下几个必须的包，另外鉴于依赖，可能还会安装一些其他的包，所以我们选择-y：</div><div class="line"></div><div class="line">```bash</div><div class="line"></div><div class="line">openldap-devel<span class="number">-2.4</span><span class="number">.23</span><span class="number">-26.</span>el6.x86_64</div><div class="line">openldap-clients<span class="number">-2.4</span><span class="number">.23</span><span class="number">-26.</span>el6.x86_64</div><div class="line">openldap<span class="number">-2.4</span><span class="number">.23</span><span class="number">-26.</span>el6.x86_64</div><div class="line">openldap-servers<span class="number">-2.4</span><span class="number">.23</span><span class="number">-26.</span>el6.x86_64</div><div class="line"></div><div class="line"># yum install -y vim automake autoconf gcc xz ncurses-devel \ patch python-devel git python-pip gcc-c++  # 安装基本环境，后面依赖</div><div class="line"># yum install -y openldap openldap-servers openldap-clients openldap-devel</div></pre></td></tr></table></figure>前面编译安装也提到：OpenLDAP 使用`Berkely-DB`来作为数据库存储信息，我们可以去官网下载解压到本地安装。But,用Yum的话，应该会帮我们做好这一切事情。 我们只要在安装完后检查一下是否安装了db4*相关的rpm包就可以了。If <span class="literal">true</span>,那恭喜，我们可以进行下一步了。If not,请用yum命令安装。查看rpm安装包的命令：`rpm -qa | grep db4`<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@openldap ~]<span class="comment"># rpm -qa | grep db4</span></div><div class="line">db4-4.7.25-19.el6_6.x86_64</div><div class="line">db4-utils-4.7.25-19.el6_6.x86_64</div></pre></td></tr></table></figure><span class="comment">## 2. 配置 OpenLDAP 服务器</span>OpenLDAP 的主配置文件是`/etc/openldap/slapd.conf`,配置 OpenLDAP 服务器主要是对该文件 进行修改。拷贝LDAP配置文件到LDAP目录<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cp /usr/share/openldap-servers/slapd.conf.obsolete /etc/openldap/slapd.conf  ## 该文件是slapd的配置文件</span></div><div class="line"><span class="comment"># cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG      ## 数据库的配置文件</span></div></pre></td></tr></table></figure>Note: redhat6.0或6.1版本配置文件在主目录有备份：<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cd /etc/openldap/</span></div><div class="line"><span class="comment">#  cp slapd.conf slapd.conf.bak</span></div></pre></td></tr></table></figure><span class="comment">#### 配置 slapd.conf 文件</span>需要注意的是每次修改配置文件的设置后,需重新启动 OpenLDAP 服务后才能使新的配置生效。按照以下方法配置/etc/openldap/slapd.conf 文件。<span class="comment">#### 设置 Schema</span>`Schema(模式)`定义了 LDAP 中的对象类型、属性、语法和匹配规则等,如用户的电子邮件、 联系地址和联系电话等属性,它类似于关系数据库中的表结构。在实际应用中,不同的应用领域 会有不同的 Schema,用户可以通过自定义 Schema 使 LDAP 能够存储各种各样的信息。`OpenLDAP 安装目录的/etc/openldap/schema`目录下包含许多常用的 Schema 定义文件(例如,用于表示单位中 人员的 InetOrgPerson 模式),使用它们已经可以满足一般性应用的需要。<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@openldap openldap]<span class="comment"># ll /etc/openldap/schema/*.schema</span></div><div class="line">-rw-r--r--. 1 root root  6190 5月  11 07:32 /etc/openldap/schema/collective.schema</div><div class="line">-rw-r--r--. 1 root root  8063 5月  11 07:32 /etc/openldap/schema/corba.schema</div><div class="line">-rw-r--r--. 1 root root 20499 5月  11 07:32 /etc/openldap/schema/core.schema</div><div class="line">-rw-r--r--. 1 root root 73994 5月  11 07:32 /etc/openldap/schema/cosine.schema</div><div class="line">-rw-r--r--. 1 root root 10388 5月  11 07:32 /etc/openldap/schema/duaconf.schema</div><div class="line">-rw-r--r--. 1 root root  3289 5月  11 07:32 /etc/openldap/schema/dyngroup.schema</div><div class="line">-rw-r--r--. 1 root root  6267 5月  11 07:32 /etc/openldap/schema/inetorgperson.schema</div><div class="line">-rw-r--r--. 1 root root 13901 5月  11 07:32 /etc/openldap/schema/java.schema</div><div class="line">-rw-r--r--. 1 root root  2387 5月  11 07:32 /etc/openldap/schema/misc.schema</div><div class="line">-rw-r--r--. 1 root root  7640 5月  11 07:32 /etc/openldap/schema/nis.schema</div><div class="line">-rw-r--r--. 1 root root  1514 5月  11 07:32 /etc/openldap/schema/openldap.schema</div><div class="line">-rw-r--r--. 1 root root 20467 5月  11 07:32 /etc/openldap/schema/pmi.schema</div><div class="line">-rw-r--r--. 1 root root 19963 5月  11 07:32 /etc/openldap/schema/ppolicy.schema</div></pre></td></tr></table></figure>编辑 slapd.conf 文件,如下:```bash<span class="comment"># See slapd.conf(5) for details on configuration options.</span><span class="comment"># This file should NOT be world readable.</span><span class="comment">#</span>include         /etc/openldap/schema/corba.schemainclude         /etc/openldap/schema/core.schemainclude         /etc/openldap/schema/cosine.schemainclude         /etc/openldap/schema/duaconf.schemainclude         /etc/openldap/schema/dyngroup.schemainclude         /etc/openldap/schema/inetorgperson.schemainclude         /etc/openldap/schema/java.schemainclude         /etc/openldap/schema/misc.schemainclude         /etc/openldap/schema/nis.schemainclude         /etc/openldap/schema/openldap.schemainclude         /etc/openldap/schema/ppolicy.schemainclude         /etc/openldap/schema/collective.schema</code></pre><ol><li>其中,<code>core.schema</code> 是所有 LDAP 目录所必需的,<code>nis.schema</code> 文件用于在 LDAP 目录文件中提 供网络信息系统数据。一般而言,在不能肯定是否需要这些 schema 文件时,<code>不应删除这些文件</code>。</li></ol><p>2.修改目录树后缀 slapd.conf 中,目录树后缀为:</p><pre><code class="bash">suffix <span class="string">"dc=my-domain,dc=com"</span></code></pre><p>将其修改为:</p><pre><code class="bash">suffix <span class="string">"dc=ihaozhuo,dc=com"</span></code></pre><p>3.修改管理员 DN<br>slapd.conf 中,原 LDAP 修改管理员 DN 为:</p><pre><code class="bash">rootdn <span class="string">"cn=Manager,dc=my-domain,dc=com"</span></code></pre><p>将其改为:</p><pre><code class="bash">rootdn  <span class="string">"cn=admin,dc=ihaozhuo,dc=com"</span><figure class="highlight clean"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="number">4.</span>设置支持数据库 将原先设置的数据库:```bashdatabase ldbm```修改为已安装的BDB: </div><div class="line"></div><div class="line">```bash</div><div class="line">database bdb</div></pre></td></tr></table></figure>5.使用 slappasswd 命令创建加密口令在 slapd.conf 文件中,需要输入修改 OpenLDAP 后台数据库所必需的口令。默认情况下,在 slapd.conf 文件中使用 rootpw 语句以明文字符串定义口令。该口令赋予对后台数据库的完全控制 权限。<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># rootpw secret</span></div></pre></td></tr></table></figure>若希望把后台数据库口令改为“qwe123.com”,则作如下修改:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rootpw 111</div></pre></td></tr></table></figure>但是,明文字符串是不安全的,应当为 OpenLDAP 创建加密口令。创建加密口令使用 slappasswd命令。加密方式有 SSHA、MD5、SMD5、SSH 等。<span class="comment">#### 创建LDAP管理员密码</span>这里我输入的密码是qwe123.com,输入完密码后,返回一串密文，先保存到剪贴板,之后要复制到LDAP配置文件中使用:<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@openldap ]<span class="comment"># slappasswd</span></div><div class="line">New password:</div><div class="line">Re-enter new password:</div><div class="line">&#123;SSHA&#125;k2+3iWauGVNo8k2sKQbGAe/iUeaAq3Hk</div></pre></td></tr></table></figure>则 OpenLDAP 后台数据库将使用加密口令。总结配置：<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">database        bdb</div><div class="line">suffix          <span class="string">"dc=ihaozhuo,dc=com"</span></div><div class="line">checkpoint      1024 15</div><div class="line">rootdn          <span class="string">"cn=admin,dc=ihaozhuo,dc=com"</span></div><div class="line"><span class="comment"># Cleartext passwords, especially for the rootdn, should</span></div><div class="line"><span class="comment"># be avoided.  See slappasswd(8) and slapd.conf(5) for details.</span></div><div class="line"><span class="comment"># Use of strong authentication encouraged.</span></div><div class="line"><span class="comment"># rootpw                secret</span></div><div class="line"><span class="comment"># rootpw                &#123;crypt&#125;ijFYNcSNctBYg</span></div><div class="line">rootpw  &#123;SSHA&#125;k2+3iWauGVNo8k2sKQbGAe/iUeaAq3Hk</div></pre></td></tr></table></figure>到这里就配置完sldap.conf * 说明：<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">       loglevel：设置日志级别</div><div class="line">       suffix：其实就是BaseDN</div><div class="line">       rootdn:超级管理员的dn</div><div class="line">       rootpw:超级管理员的密码</div><div class="line">```    </div><div class="line"></div><div class="line"><span class="comment">#### 修改系统日志配置文件</span></div><div class="line"></div><div class="line">```bash</div><div class="line"><span class="comment"># vim /etc/rsyslog.conf</span></div><div class="line"> <span class="built_in">local</span>4.*        /var/<span class="built_in">log</span>/ldap.log</div><div class="line"><span class="comment"># local7.*下添加一行</span></div><div class="line"></div><div class="line">在启动服务。</div><div class="line"><span class="comment"># service rsyslog restart</span></div></pre></td></tr></table></figure><span class="comment">## 3. 测试 slapd.conf 设置</span>完成 slapd.conf 文件的设置后,可以运行 slaptest 命令进行测试。命令如下:<figure class="highlight applescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@openldap openldap]<span class="comment"># slaptest -u</span></div><div class="line">config <span class="built_in">file</span> testing succeeded</div></pre></td></tr></table></figure>如果lapd.conf文件设置不正确,将出现`“slaptest: bad configuration file!”`的提示,同时提示 错误的行号,按照提示进行修改后,运行 slaptest 命令重新检查,直到配置文件设置正确。可[参考链接](http://www.oschina.net/question/137892_77767)<span class="comment">#### OpenLDAP 的启动与停止</span>启动slapd, 查看启动情况<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">service slapd start</div></pre></td></tr></table></figure>如果跟下面一样报错 error启动 slapd：[失败] 下面是解决方法：<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@openldap openldap]<span class="comment"># service slapd start</span></div><div class="line">/var/lib/ldap/__db.006 is not owned by <span class="string">"ldap"</span>              [警告]</div><div class="line">/var/lib/ldap/__db.005 is not owned by <span class="string">"ldap"</span>              [警告]</div><div class="line">/var/lib/ldap/__db.004 is not owned by <span class="string">"ldap"</span>              [警告]</div><div class="line">/var/lib/ldap/__db.003 is not owned by <span class="string">"ldap"</span>              [警告]</div><div class="line">/var/lib/ldap/alock is not owned by <span class="string">"ldap"</span>                 [警告]</div><div class="line">/var/lib/ldap/__db.001 is not owned by <span class="string">"ldap"</span>              [警告]</div><div class="line">/var/lib/ldap/__db.002 is not owned by <span class="string">"ldap"</span>              [警告]</div><div class="line">正在启动 slapd：                                             [失败]</div></pre></td></tr></table></figure>解决方法：<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">进入 <span class="built_in">cd</span> /var/lib/ldap/  </div><div class="line">slapd        (会生成一些库文件)</div><div class="line">[root@openldap ldap]<span class="comment"># ll</span></div><div class="line">总用量 11312</div><div class="line">-rw-r--r--. 1 root root      2048 9月  28 15:41 alock</div><div class="line">-rw-------. 1 root root     24576 9月  28 15:41 __db.001</div><div class="line">-rw-------. 1 root root   9093120 9月  28 15:41 __db.002</div><div class="line">-rw-------. 1 root root 335552512 9月  28 15:41 __db.003</div><div class="line">-rw-------. 1 root root   2359296 9月  28 15:37 __db.004</div><div class="line">-rw-------. 1 root root    753664 9月  28 15:41 __db.005</div><div class="line">-rw-------. 1 root root     32768 9月  28 15:41 __db.006</div><div class="line">-rw-r--r--. 1 root root       845 9月  28 15:06 DB_CONFIG</div><div class="line">[root@openldap ldap]<span class="comment"># slapd</span></div><div class="line">[root@openldap ldap]<span class="comment"># ll</span></div><div class="line">总用量 11472</div><div class="line">-rw-r--r--. 1 root root      2048 9月  28 15:51 alock</div><div class="line">-rw-------. 1 root root     24576 9月  28 15:51 __db.001</div><div class="line">-rw-------. 1 root root   9093120 9月  28 15:51 __db.002</div><div class="line">-rw-------. 1 root root 335552512 9月  28 15:51 __db.003</div><div class="line">-rw-------. 1 root root   2359296 9月  28 15:51 __db.004</div><div class="line">-rw-------. 1 root root    753664 9月  28 15:51 __db.005</div><div class="line">-rw-------. 1 root root     32768 9月  28 15:51 __db.006</div><div class="line">-rw-r--r--. 1 root root       845 9月  28 15:06 DB_CONFIG</div><div class="line">-rw-------. 1 root root      8192 9月  28 15:51 dn2id.bdb</div><div class="line">-rw-------. 1 root root     32768 9月  28 15:51 id2entry.bdb</div><div class="line">-rw-------. 1 root root  10485760 9月  28 15:51 log.0000000001</div><div class="line">[root@openldap ldap]<span class="comment">#  netstat -ntlp | grep 389</span></div><div class="line">tcp        0      0 0.0.0.0:389                 0.0.0.0:*                   LISTEN      1859/slapd</div><div class="line">tcp        0      0 :::389                      :::*                        LISTEN      1859/slapd</div><div class="line">[root@openldap ldap]<span class="comment"># chown ldap:ldap *</span></div><div class="line">[root@openldap ldap]<span class="comment"># chmod 600 *</span></div><div class="line">[root@openldap ldap]<span class="comment"># ll</span></div><div class="line">总用量 11472</div><div class="line">-rw-------. 1 ldap ldap      2048 9月  28 15:55 alock</div><div class="line">-rw-------. 1 ldap ldap     24576 9月  28 15:55 __db.001</div><div class="line">-rw-------. 1 ldap ldap   9093120 9月  28 16:25 __db.002</div><div class="line">-rw-------. 1 ldap ldap 335552512 9月  28 15:55 __db.003</div><div class="line">-rw-------. 1 ldap ldap   2359296 9月  28 16:10 __db.004</div><div class="line">-rw-------. 1 ldap ldap    753664 9月  28 15:55 __db.005</div><div class="line">-rw-------. 1 ldap ldap     32768 9月  28 16:10 __db.006</div><div class="line">-rw-------. 1 ldap ldap       845 9月  28 15:06 DB_CONFIG</div><div class="line">-rw-------. 1 ldap ldap      8192 9月  28 15:51 dn2id.bdb</div><div class="line">-rw-------. 1 ldap ldap     32768 9月  28 15:51 id2entry.bdb</div><div class="line">-rw-------. 1 ldap ldap  10485760 9月  28 16:10 log.0000000001</div><div class="line">[root@openldap ldap]<span class="comment"># service slapd restart</span></div><div class="line">停止 slapd：                                               [确定]</div><div class="line">正在启动 slapd：                                           [确定]</div><div class="line">[root@openldap ldap]<span class="comment"># service slapd start</span></div><div class="line">正在启动 slapd：ln: 创建硬链接<span class="string">"/var/run/slapd.pid"</span>: 文件已存在</div><div class="line"></div><div class="line">[root@openldap ldap]<span class="comment"># service slapd status</span></div><div class="line">slapd (pid  2197) 正在运行...</div><div class="line">You have new mail <span class="keyword">in</span> /var/spool/mail/root</div></pre></td></tr></table></figure>在下一步执行：    <span class="comment"># rm -rf /etc/openldap/slapd.d/*</span>测试并生成配置文件：    slaptest  <span class="_">-f</span> /etc/openldap/slapd.conf -F /etc/openldap/slapd.d返回config file testing succeeded,则配置成功。赋予生成的配置文件予权限并重启：<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># chown -R ldap:ldap /etc/openldap/slapd.d</span></div><div class="line"><span class="comment"># service slapd restart</span></div><div class="line">netstat -tulnp | grep slapd</div><div class="line">tcp        0      0 0.0.0.0:389                 0.0.0.0:*                   LISTEN      2358/slapd</div><div class="line">tcp        0      0 :::389                      :::*                        LISTEN      2358/slapd</div></pre></td></tr></table></figure><span class="comment">## 4.创建一个账号，以备客户端测试登陆</span><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># useradd ldapuser1</span></div><div class="line"><span class="comment"># passwd ldapuser1</span></div></pre></td></tr></table></figure>至此，这些用户仅仅是系统上存在的用户`（存储在/etc/passwd和/etc/shadow上）`，并没有在LDAP数据库里，所以要把这些用户导入到LDAP里面去。但LDAP只能识别特定格式的文件 即后缀为ldif的文件（也是文本文件），所以不能直接使用`etc/passwd和/etc/shadow。` 需要migrationtools这个工具把这两个文件转变成LDAP能识别的文件。<span class="comment">## 5. 安装配置migrationtools</span>    yum install migrationtools -y<span class="comment">#### 进入migrationtool配置目录</span><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#cd /usr/share/migrationtools/</span></div><div class="line">首先编辑migrate_common.ph</div><div class="line"></div><div class="line"><span class="comment"># vi  migrate_common.ph</span></div><div class="line">...</div><div class="line"><span class="comment"># Default DNS domain</span></div><div class="line"><span class="variable">$DEFAULT_MAIL_DOMAIN</span> = <span class="string">"ihaozhuo.com"</span>;</div><div class="line"></div><div class="line"><span class="comment"># Default base</span></div><div class="line"><span class="variable">$DEFAULT_BASE</span> = <span class="string">"dc=ihaozhuo,dc=com"</span>;</div><div class="line">.....</div><div class="line">保存退出。：-）</div><div class="line">K.下面利用pl脚本将/etc/passwd 和/etc/shadow生成LDAP能读懂的文件格式，保存在/tmp/下</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># ./migrate_base.pl &gt; /tmp/base.ldif</span></div><div class="line"><span class="comment"># ./migrate_passwd.pl  /etc/passwd &gt; /tmp/passwd.ldif</span></div><div class="line"><span class="comment"># ./migrate_group.pl  /etc/group &gt; /tmp/group.ldif</span></div></pre></td></tr></table></figure><span class="comment">#### 说明：</span> 第一次启动生会初始化ldap数据库，在/var/lib/ldap中，如果想删除ldap数据库就删除该目录，保留DB_CONFIG配置文件。新版的ldap使用的是/etc/openldap/slapd.d 下的配置文件，删除原来的配置文件，slaptest是重新生成新的配置文件.<span class="comment">## 6.导入数据 </span>下面就要把这三个文件导入到LDAP，这样LDAP的数据库里就有了我们想要的用户<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ldapadd -x -D "cn=admin,dc=example,dc=com" -W -f /tmp/base.ldif</span></div><div class="line"><span class="comment"># ldapadd -x -D "cn=admin,dc=example,dc=com" -W -f /tmp/passwd.ldif</span></div><div class="line"><span class="comment"># ldapadd -x -D "cn=admin,dc=example,dc=com" -W -f /tmp/group.ldif</span></div><div class="line">过程若无报错，则LDAP服务端配置完毕.</div></pre></td></tr></table></figure>执行成功会出现：<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">[root@openldap migrationtools]<span class="comment"># ldapadd -x -D "cn=admin,dc=ihaozhuo,dc=com" -W -f /tmp/base.ldif</span></div><div class="line">Enter LDAP Password:</div><div class="line">adding new entry <span class="string">"dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"ou=Hosts,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"ou=Rpc,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"ou=Services,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"nisMapName=netgroup.byuser,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">.........</div><div class="line"></div><div class="line">[root@openldap migrationtools]<span class="comment"># ldapadd -x -D "cn=admin,dc=ihaozhuo,dc=com" -W -f /tmp/passwd.ldif</span></div><div class="line">Enter LDAP Password:</div><div class="line">adding new entry <span class="string">"uid=root,ou=People,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"uid=bin,ou=People,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"uid=daemon,ou=People,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"uid=adm,ou=People,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"uid=lp,ou=People,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"uid=sync,ou=People,dc=ihaozhuo,dc=com"</span></div><div class="line">.....</div><div class="line"></div><div class="line">[root@openldap migrationtools]<span class="comment"># ldapadd -x -D "cn=admin,dc=ihaozhuo,dc=com" -W -f /tmp/group.ldif</span></div><div class="line">Enter LDAP Password:</div><div class="line">adding new entry <span class="string">"cn=root,ou=Group,dc=ihaozhuo,dc=com"</span></div><div class="line"></div><div class="line">adding new entry <span class="string">"cn=bin,ou=Group,dc=ihaozhuo,dc=com"</span></div><div class="line">.......</div></pre></td></tr></table></figure>ldapsearch 查看数据<figure class="highlight ldif"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div></pre></td><td class="code"><pre><div class="line">[root@openldap migrationtools]<span class="comment"># ldapsearch -x -b "dc=ihaozhuo,dc=com"</span></div><div class="line"><span class="comment"># extended LDIF</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># LDAPv3</span></div><div class="line"><span class="comment"># base &lt;dc=ihaozhuo,dc=com&gt; with scope subtree</span></div><div class="line"><span class="comment"># filter: (objectclass=*)</span></div><div class="line"><span class="comment"># requesting: ALL</span></div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line"><span class="comment"># ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">dc</span>: ihaozhuo</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: domain</div><div class="line"></div><div class="line"><span class="comment"># Hosts, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Hosts,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Hosts</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># Rpc, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Rpc,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Rpc</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># Services, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Services,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Services</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># netgroup.byuser, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: nisMapName=netgroup.byuser,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">nisMapName</span>: netgroup.byuser</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: nisMap</div><div class="line"></div><div class="line"><span class="comment"># Mounts, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Mounts,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Mounts</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># Networks, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Networks,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Networks</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># People, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=People,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: People</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># Group, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Group,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Group</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># Netgroup, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Netgroup,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Netgroup</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># Protocols, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Protocols,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Protocols</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># Aliases, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: ou=Aliases,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">ou</span>: Aliases</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: organizationalUnit</div><div class="line"></div><div class="line"><span class="comment"># netgroup.byhost, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: nisMapName=netgroup.byhost,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">nisMapName</span>: netgroup.byhost</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: nisMap</div><div class="line"></div><div class="line"><span class="comment"># root, People, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: uid=root,ou=People,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">uid</span>: root</div><div class="line"><span class="attribute">cn</span>: root</div><div class="line"><span class="attribute">objectClass</span>: account</div><div class="line"><span class="attribute">objectClass</span>: posixAccount</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: shadowAccount</div><div class="line"><span class="attribute">userPassword:</span>: e2NyeXB0fSQ2JHJCNmVjYmdEZGtUR2w1c3QkTldDc1NQNXJBYXU3QkhpUWlXVXl</div><div class="line"> LaXl4dW9VNWtFNm0wZ1dILmRzaXhFL2ZPTG1zOC52OTdNdXVuNkZlNjhDOExTYmtvVjdiaW03Lmti</div><div class="line"> ak1ZbURVZC4=</div><div class="line"><span class="attribute">shadowLastChange</span>: 17056</div><div class="line"><span class="attribute">shadowMin</span>: 0</div><div class="line"><span class="attribute">shadowMax</span>: 99999</div><div class="line"><span class="attribute">shadowWarning</span>: 7</div><div class="line"><span class="attribute">loginShell</span>: /bin/bash</div><div class="line"><span class="attribute">uidNumber</span>: 0</div><div class="line"><span class="attribute">gidNumber</span>: 0</div><div class="line"><span class="attribute">homeDirectory</span>: /root</div><div class="line"><span class="attribute">gecos</span>: root</div><div class="line"></div><div class="line"><span class="comment"># bin, People, ihaozhuo.com</span></div><div class="line"><span class="attribute">dn</span>: uid=bin,ou=People,dc=ihaozhuo,dc=com</div><div class="line"><span class="attribute">uid</span>: bin</div><div class="line"><span class="attribute">cn</span>: bin</div><div class="line"><span class="attribute">objectClass</span>: account</div><div class="line"><span class="attribute">objectClass</span>: posixAccount</div><div class="line"><span class="attribute">objectClass</span>: top</div><div class="line"><span class="attribute">objectClass</span>: shadowAccount</div><div class="line"><span class="attribute">userPassword:</span>: e2NyeXB0fSo=</div><div class="line"><span class="attribute">shadowLastChange</span>: 15980</div><div class="line"><span class="attribute">shadowMin</span>: 0</div><div class="line"><span class="attribute">shadowMax</span>: 99999</div><div class="line"><span class="attribute">shadowWarning</span>: 7</div><div class="line"><span class="attribute">loginShell</span>: /sbin/nologin</div><div class="line"><span class="attribute">uidNumber</span>: 1</div><div class="line"><span class="attribute">gidNumber</span>: 1</div><div class="line"><span class="attribute">homeDirectory</span>: /bin</div><div class="line"><span class="attribute">gecos</span>: bin</div><div class="line"></div><div class="line">................</div></pre></td></tr></table></figure>/相关信息导入数据库成功<span class="comment">## 7.实现LDAP用户home目录自动挂载</span><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">centos 6 :</div><div class="line">yum install nfs-utils rpcbind</div><div class="line">服务器端： </div><div class="line"><span class="comment">###(建立挂载的目录，并且挂载目录。)</span></div><div class="line">[root@openldap ~]<span class="comment"># mkdir /opt/centos6</span></div><div class="line">[root@openldap ~]<span class="comment"># cd /opt/centos6/</span></div><div class="line">[root@openldap centos6]<span class="comment">#  mkdir thisISnfsFile</span></div><div class="line">[root@openldap centos6]<span class="comment">#  vi /etc/exports</span></div><div class="line">[root@openldap centos6]<span class="comment"># chkconfig nfs on</span></div><div class="line">[root@openldap centos6]<span class="comment"># /etc/init.d/rpcbind start</span></div><div class="line">正在启动 rpcbind：                                          [确定]</div><div class="line">[root@openldap centos6]<span class="comment"># /etc/init.d/nfs start</span></div><div class="line">启动 NFS 服务：                                              [确定]</div><div class="line">启动 NFS mountd：                                           [确定]</div><div class="line">启动 NFS 守护进程：                                          [确定]</div><div class="line">正在启动 RPC idmapd：                                       [确定]</div><div class="line">[root@openldap centos6]<span class="comment">#  cat /etc/exports</span></div><div class="line">/opt/centos6 *(rw,sync)</div><div class="line"><span class="comment">### 备注：/opt/centos6表示nfs共享的目录 有读写权限。</span></div><div class="line">这个可以不需要写：</div><div class="line"></div><div class="line">/opt/centos6 192.168.1.0/24(ro,no_root_squash)</div><div class="line"><span class="comment">### 备注:/opt/centos6表示nfs共享的目录 192.168.1.0-192.168.1.254区间的IP可以访问，访问权限是自读，root 用户</span></div></pre></td></tr></table></figure>可参考：[Nfs共享同步](http://blog.chinaunix.net/uid-26284318-id-3111651.html)<span class="comment">## 8.使用 phpLDAPadmin</span>OpenLDAP 安装目录的 bin 子目录下提供了管理目录树的客户端命令(如 ldapadd、ldapdelete 和 ldapsearch 等),但对于不熟悉 OpenLDAP 命令的初学者而言,使用它们去管理目录树是一件非 常麻烦的事。可以借助于图形界面的 LDAP管理工具来管理目录树,其中,用 PHP 编写的 phpLDAPadmin 就是一款很好的选择。   1.1 初始化数据为了能使用 phpLDAPadmin 管理目录树,需要为目录树建立初始化数据,方法是将初始化数 据保存在 LDIF 文件中,然后使用 ldapadd 命令导入 LDAP 的数据库中。LDIF 是 LDAP Data Interchange Format (即 LDAP 数据交换格式)的缩写。将 LDAP 保存为 一种简单的文本格式,该格式可以用于在符合 LDAP 标准的目录上完成批量操作,如添加、修改、 删除和导入、导出数据等批量操作。LDIF 可以在来自两个不同的厂家的 LDAP 服务器间交换数据, 即使 LDAP 服务器使用不同的后台数据库或运行在不同的操作系统上。这一步已经在上面导入过数据了。* 下载与安装phpLDAPadmin 是一款开放源代码的,通过它提供的友好用户界面,只要使用 Web 浏览器即 可轻松管理本地或远程的 LDAP 服务器,例如浏览 LDAP 目录树,创建/删除/修改和复制节点, 执行搜索,导入/导出 LDIF 文件,查看服务器 schema。还可以在两个 LDAP 服务器之间复制对象、 恢复删除、复制树节点。phpLDAPadmin 的下载地址是 http://phpldapadmin.sourceforge.net/实现phpldapadmin 网页WEB管理用户需要注意的是`,phpLDAPadmin `是一个基于 `web `的管理工具,因此,若要正常使用 phpLDAPadmin,需要确保` Apache` 服务器运行正常。<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">yum install httpd -y</div><div class="line">vi /etc/httpd/conf/httpd.conf</div><div class="line"></div><div class="line">添加：</div><div class="line">....</div><div class="line">ServerName 192.168.1.186:80</div><div class="line">.....</div><div class="line"></div><div class="line">service httpd start</div></pre></td></tr></table></figure>先通过scp上传phpldapadmin-1.2.3.zip到apache网页目录下载：[phpldapadmin](https://sourceforge.net/projects/phpldapadmin/files/phpldapadmin-php5/1.2.3/phpldapadmin-1.2.3.tgz)或者这里用我下载好的链接wget.    <span class="built_in">cd</span> /var/www/html/    wget http://oak0aohum.bkt.clouddn.com/phpldapadmin-1.2.3.tgz    tar -zxvf phpldapadmin-1.2.3.tgz    mv phpldapadmin-1.2.3 phpldapadmin    <span class="built_in">cd</span> phpldapadmin/config/    cp config.php.example config.php    vim config.php<span class="comment">#### 2.修改配置文件</span>用文本编辑器打开 config.php 文件后发现,尽管文件行数几百行,但实际上很多都是 php 程序的注释语句。因此,只需要修改其中一部分内容,即可完成 phpLDAPadmin 的配置。设置如下:   <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">/*</div><div class="line"><span class="variable">$servers</span>-&gt;newServer(<span class="string">'ldap_pla'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'name'</span>,<span class="string">'LDAP Server'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'host'</span>,<span class="string">'192.168.1.186'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'port'</span>,389);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'base'</span>,array(<span class="string">'dc=ihaozhuo,dc=com'</span>));</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'login'</span>,<span class="string">'auth_type'</span>,<span class="string">'cookie'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'login'</span>,<span class="string">'bind_id'</span>,<span class="string">'cn=admin,dc=ihaozhuo,dc=com'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'login'</span>,<span class="string">'bind_pass'</span>,<span class="string">'qwe123.com'</span>);</div><div class="line"><span class="variable">$servers</span>-&gt;<span class="built_in">set</span>Value(<span class="string">'server'</span>,<span class="string">'tls'</span>,<span class="literal">false</span>);</div></pre></td></tr></table></figure><span class="comment">#### 对以上的设置作如下说明:   LDAP 服务器名</span>    <span class="variable">$ldapservers</span>-&gt;SetValue(<span class="variable">$i</span>,<span class="string">'server'</span>,<span class="string">'name'</span>,<span class="string">'My LDAP Server'</span>);使用以上语句,设置 LDAP 服务器名称为“My LDAP Server”。  &gt; LDAP 服务器 IP    <span class="variable">$ldapservers</span>-&gt;SetValue(<span class="variable">$i</span>,<span class="string">'server'</span>,<span class="string">'host'</span>,<span class="string">'192.168.1.186'</span>);使用以上语句,设置 LDAP 服务器地址为“192.168.1.186”。   &gt;LDAP 服务器管理员 DN    <span class="variable">$ldapservers</span>-&gt;SetValue(<span class="variable">$i</span>,<span class="string">'login'</span>,<span class="string">'dn'</span>,<span class="string">'cn=admin,dc=ihaozhuo,dc=com'</span>);设置 LDAP 服务器管理员 DN为`“cn=Manager,dc=example,dc=com”。`&gt;LDAP 服务器管理员口令    <span class="variable">$ldapservers</span>-&gt;SetValue(<span class="variable">$i</span>,<span class="string">'login'</span>,<span class="string">'pass'</span>,<span class="string">''</span>);此处将 LDAP 服务器管理员口令设置为空。   LDAP 服务器认证方式phpLDAPadmin 提供了 cookie、session 和 config 这三种认证方式。cookie 方式提供一个登录 的界面,让用户输入正确的 LDAP 管理员的 DN 和口令才允许登录到服务器,同时 LDAP 管理员 的 DN 与口令会保存在客户端 Web 浏览器的 cookie 里面。session 方式类似 cookie,不同点在于 LDAP 管理员的 DN 与口令保存在服务器端,而不是客户端。config 是 phpLDAPadmin 缺省的认证 方式,不需要用户登录,但需要将LDAP管理员的DN和口令写在phpLDAPadmin的主配置文件中。以上语句表明 phpLDAPadmin 采用 cookie 认证。在使用 cookie 认证方式时,必须配置一个口 令加密字符串,以便安全地加密和解密一些敏感信息。<span class="string">'blowfish'</span>就是口令加密字符串,可以随意 设置,例如设置为“phpldapadmin”。删除的内容: ‘ 删除的内容: ’ 设置完毕后,保存 config.php 文件使设置生效。<span class="comment">## apache-http修改</span>    vim /etc/httpd/conf/httpd.conf    &lt;Directory <span class="string">"/var/www/html/phpldapadmin"</span>&gt;    DirectoryIndex index.html index.html.var index.php重启服务。    service httpd restart<span class="comment">## 网页访问</span>http://192.168.1.186/phpldapadmin 报错无法正常访问，是由于php,php-ldap依赖包没有安装.安装php可参考我博客上面文档：[PHP编译升级YUM最全面安装部署](http://blog.yangcvo.me/2015/12/21/PHP%E7%BC%96%E8%AF%91%E5%8D%87%E7%BA%A7YUM%E6%9C%80%E5%85%A8%E9%9D%A2%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/)安装完，重启http服务。    service httpd restart设置防火墙访问：    -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT    重启iptables     service iptables restart<span class="comment">#### 1.登录</span>![](http://7xrthw.com1.z0.glb.clouddn.com/openldap4.png)单击左侧“Login”链接,打开如图1.1 所示页面,输入 ldap 服务器认证信息。此处应当输入管理员的 DN 和密码。![](http://7xrthw.com1.z0.glb.clouddn.com/openldap5.png)单击“认证”按钮后,进入 phpLDAPadmin 管理页面,如图 1.2所示。页面右侧提示“LDAP服务器认证成功”,页面左侧是 LDAP 的目录树。在左页面中单击【dc=ihaozhuo,dc=com】链接,在右页面中会出现目录树的各种属性并提供各 种操作链接。&gt;查看对象![](http://7xrthw.com1.z0.glb.clouddn.com/openldap6.png)因为之前导入了 LDIF 文件,初始化的数据在此处可以显示出来。<span class="comment">#### 2.创建对象</span>选择【创建一个子条目】链接,在出现的【创建对象】页面中,选择【Organization Unit】(组 织机构),如图1.3所示。&gt;选择需要创建的对象![](http://7xrthw.com1.z0.glb.clouddn.com/openldap7.png)单击【Organization Unit】前面的单选按钮后,进入创建【Organization Unit】界面,如图&gt;创建新组织机构(OU)![](http://7xrthw.com1.z0.glb.clouddn.com/openldap8.png)在 phpLDAPadmin 中创建 LDAP 目录树中的其他对象,同样使用如上所述的方法,不同的是 需要在图1.3 中选择不同的对象。&gt; 创建 OU 成功![](http://7xrthw.com1.z0.glb.clouddn.com/openldap9.png)<span class="comment">#### 3.导入导出</span>使用导入导出按钮,可以将 LDAP 服务器使用的 LDIF 文件导出,也可以导入需要的 LDIF 文 件。直接导入导出 LDIF 文件是管理 LDAP 目录树一种快捷方法。在该页面中,单击【浏览】按钮,选择相关的 LDIF 文件,或是直接在【Paste your LDIF file】 文件的位置输入 LDIF 文件内容,单击【Proceed】按钮,开始导入。若导入成功,显示如下图所示提示内容。若导入不成功,同样会出现相关错误提示。![](http://7xrthw.com1.z0.glb.clouddn.com/openldap10.png)选择导出操作时,单击页面左侧【<span class="built_in">export</span>】导出链接,出现如图 所示【导出】页面。在 该页面中,选择 DN;导出范围、导出格式及行结束标志等都在该页面中设置。![](http://7xrthw.com1.z0.glb.clouddn.com/openldap11.png)注意：导入用户成功以后，再次导入或者创建  下次不要重复导入。不然会报错，68导入group 和user就行，过滤 、/etc/passwd 、/etc/group新建的用户，导入成功以后，重新刷新。<span class="comment">## 设置 LDIF 文件</span>LDAP 数据交换格式(LDIF)是指存储 LDAP 配置信息及目录内容的标准文本文件格式,之 所以使用文本文件格式来存储这些信息是为了方便读取和修改。LDAP 也是其他大多数服务配置 文件所采取的格式。LDIF 文件常用来向目录导入或更改记录信息,这些信息需要按照 LDAP 中 schema 的格式进行组织,并会接受 schema 的检查,不符合其要求的格式将会出现报错信息。<span class="comment">#### 1.语法格式</span>LDIF 文件中条目的语法结构如下:```bashdn: &lt;distinguished name&gt;&lt;attrdesc&gt;: &lt;attrvalue&gt;&lt;attrdesc&gt;: &lt;attrvalue&gt;&lt;attrdesc&gt;:: &lt;base64-encoded-value&gt;&lt;attrdesc&gt;:&lt; &lt;URL&gt;...<figure class="highlight ldif"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">例如:```bash <span class="comment">#LDIF file example</span><span class="attribute">dn</span>: dc=mydomain,dc=org<span class="attribute">objectClass</span>: domain<span class="attribute">dc</span>: mydomain</div></pre></td></tr></table></figure>以“<span class="comment">#”号开头的为注释行;第二行起的各行中,冒号左边为属性,右边是属性的值,与编程 中的变量及为其所赋的值类似,LDIF 文件中属性可以被重复赋值。</span>以下是一个有三个条目的 LDIF 文件:```bashdn: cn=Barbara J Jensen,dc=example,dc=comcn: Barbara J Jensencn: Babs Jensenobjectclass: persondescription:&lt; file:///tmp/babssn: Jensendn: cn=Bjorn J Jensen,dc=example,dc=comcn: Bjorn J Jensencn: Bjorn Jensenobjectclass: personsn: Jensendn: cn=Jennifer J Jensen,dc=example,dc=comcn: Jennifer J Jensencn: Jennifer Jensenobjectclass: personsn: JensenjpegPhoto:: /9j/4AAQSkZJRgABAAAAAQABAAD/2wBDABALD A4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQ ERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVG...<figure class="highlight ldif"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">#### 2.创建 LDIF 文件</span>可以用任何文本编辑器以 root 用户进行创建 LDIF 文件。当创建 LDIF 文件时需要注意,在每 个 DN 行前应添加空行,确保 LDAP 能够识别是一个新的 DN 的开始。例如,创建 test.ldif 文件。</div><div class="line"> 3.定义基准 </div><div class="line"> </div><div class="line"> DN使用如下内容定义基准 DN。```<span class="attribute">dn</span>: dc=test, dc=net<span class="attribute">objectClass</span>: top<span class="attribute">objectClass</span>: dcobject<span class="attribute">objectClass</span>: organization<span class="attribute">o</span>: Test, Inc.<span class="attribute">dc</span>: test</div></pre></td></tr></table></figure>4.定义管理员<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"> dn: cn=manager, dc=test, dc=net<span class="symbol">objectClass:</span> organizationalRole<span class="symbol">cn:</span> manager<span class="symbol">description:</span> A Manager of Test, <span class="keyword">inc</span>.```<span class="number">5.</span>定义组织机构```<span class="symbol">dn:</span> ou=members, dc=test, dc=net<span class="symbol">objectClass:</span> organizationalUnit<span class="symbol">ou:</span> members</div></pre></td></tr></table></figure>6.定义用户</code></pre><p>dn: cn=Ray D. Jones, ou= members, dc=test, dc=net<br>objectClass: organizationalPerson<br>objectClass: inetOrgPerson<br>cn: Ray D. Jones<br>sn: Jones<br>telephoneNumber: 444-555-6767<br>mail: jonesrd@test.net<br>localityName: Houston<br>dn: cn=Eric S. Woods, ou= members, dc=test, dc=net<br>objectClass: organizationalPerson<br>objectClass: inetOrgPerson<br>cn: Eric S. Woods<br>sn: Woods<br>telephoneNumber: 444-555-6768<br>mail: woodses@test.net<br>localityName: Houston<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">7.</span>保存并导入 LDIF 文件 使用如下命令导入 LDIF 文件</div><div class="line"></div><div class="line">```# ldapadd -x -W -D <span class="string">"cn=manager,dc=test,dc=net"</span> -f test.ldifEnter LDAP Password:</div></pre></td></tr></table></figure></p><p>此处输入的口令是 LDAP 管理员口令,-x 表明使用简单验证。-D 表示使用在 slapd.conf 文件 中所定义的 DN。-W 表示提示输入口令而不是从命令行输入。-f 表明要装载的文件“test.ldif”。<br>ldapadd 命令成功添加了每个条目内容同时,会列出每个条目内容有关的 DN。</p><p>8.搜索目录</p><p>搜索目录使用 ldapsearch 命令,搜索结果如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ldapsearch -x -W -D 'cn=manager,dc=test,dc=net' -b ' dc=test,dc=net ' '(objectClass=*)'</span>    Enter LDAP Password:    <span class="comment"># extended LDIF</span>    <span class="comment">#</span>    <span class="comment"># LDAPv3</span>    <span class="comment"># base &lt; dc=test,dc=net &gt; with scope subtree</span>    <span class="comment"># filter: (objectClass=*)</span>    <span class="comment"># requesting: ALL</span>    <span class="comment">#</span>    <span class="comment"># test. net</span>    dn: dc=<span class="built_in">test</span>, dc=net    objectClass: top    objectClass: dcobject    objectClass: organization    o: Test, Inc.    dc: <span class="built_in">test</span>    <span class="comment"># manager, test. net</span>    dn: cn=manager, dc=<span class="built_in">test</span>, dc=net    objectClass: organizationalRole    cn: manager    description: A Manager of Test, inc.    <span class="comment"># members, test. net</span>    dn: ou=members, dc=<span class="built_in">test</span>, dc=net    objectClass: organizationalUnit    ou: members    ...</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用-OpenLDAP-集中管理用户帐号&quot;&gt;&lt;a href=&quot;#使用-OpenLDAP-集中管理用户帐号&quot; class=&quot;headerlink&quot; title=&quot;使用 OpenLDAP 集中管理用户帐号&quot;&gt;&lt;/a&gt;使用 OpenLDAP 集中管理用户帐号&lt;/h2&gt;&lt;h4 id=&quot;1-OpenLDAP-简介&quot;&gt;&lt;a href=&quot;#1-OpenLDAP-简介&quot; class=&quot;headerlink&quot; title=&quot;1.OpenLDAP 简介&quot;&gt;&lt;/a&gt;1.OpenLDAP 简介&lt;/h4&gt;&lt;p&gt;OpenLDAP可以从 http: //www.openldap.org/ 获得安装包。OpenLDAP 网站主页如图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xrthw.com1.z0.glb.clouddn.com/openldap3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/categories/OpenLDAP/"/>
    
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/tags/OpenLDAP/"/>
    
  </entry>
  
  <entry>
    <title>OpenLDAP入门了解学习笔记(一)</title>
    <link href="http://blog.yancy.cc/2018/01/26/OpenLDAP/OpenLDAP%E5%85%A5%E9%97%A8%E4%BA%86%E8%A7%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
    <id>http://blog.yancy.cc/2018/01/26/OpenLDAP/OpenLDAP入门了解学习笔记(一)/</id>
    <published>2018-01-26T03:36:00.000Z</published>
    <updated>2018-04-21T11:53:26.970Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OpenLDAP入门了解"><a href="#OpenLDAP入门了解" class="headerlink" title="OpenLDAP入门了解"></a>OpenLDAP入门了解</h2><h3 id="LDAP概述"><a href="#LDAP概述" class="headerlink" title="LDAP概述"></a>LDAP概述</h3><p>我的个人理解：LDAP是轻量目录访问协议(Lightweight Directory Access Protocol)的缩写，LDAP是一个统一的账号管理认证平台，使用轻量级目录访问协议（LDAP）构建集中的身份验证系统可以减少管理成本，增强安全性，避免数据复制的问题，并提高数据的一致性。随着 Linux的不断成熟，已经出现了很多工具用来简化用户帐号信息到 LDAP目录的迁移。还开发了一些工具用来在客户机和目录服务器之间启用加密通信配置，并通过复制提供容错性。本文将向您展示如何配置服务器和客户机在Centos上使用OpenLDAP。</p><p><figure class="figure"><img src="https://devopsideas.com/wp-content/uploads/2017/09/Openldap-7-800x445.jpg" alt=""></figure></p><a id="more"></a><h3 id="官方解释："><a href="#官方解释：" class="headerlink" title="官方解释："></a>官方解释：</h3><ul><li>什么是LDAP? </li></ul><p>LDAP的英文全称是Lightweight Directory Access Protocol，一般都简称为LDAP。它是基于X.500标准的，但是简单多了并且可以根据需要定制。与X.500不同，LDAP支持TCP/IP，这对访问Internet是必须的。LDAP的核心规范在RFC中都有定义，所有与LDAP相关的RFC都可以在LDAPman RFC网页中找到。 </p><ul><li>怎么使用LDAP这个术语呢？</li></ul><p>在日常交谈中，你可能会听到有些人这么说：“我们要把那些东西存在LDAP中吗？”，或者“从LDAP数据库中取出那些数据！”，又或者“我们怎么把LDAP和关系型数据库集成在一起？”。严格地说，LDAP根本不是数据库而是用来访问存储在信息目录（也就是LDAP目录）中的信息的协议。更为确切和正式的说法应该是象这样的：“通过使用LDAP，可以在信息目录的正确位置读取（或存储）数据”。但是，也没有必要吹毛求疵，尽管表达得不够准确，我们也都知道对方在说什么。 </p><h3 id="规划-LDAP-目录结构"><a href="#规划-LDAP-目录结构" class="headerlink" title="规划 LDAP 目录结构"></a>规划 LDAP 目录结构</h3><p>要真正了解 LDAP,可以从规划一个 LDAP 目录树开始。以下是一个简单的例子。假设有一个名为 Example 的公司(DNS 名为example.com).</p><p>  <figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/openldap1.png" alt=""></figure></p><p>在规划过程中,首先要为目录树建立一个“根(root)”。根是目录树的最顶层,之后建立的所 有对象都是基于这个根的,也称为基准 DN。具有三种格式表示。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">使用 x.<span class="number">500</span>标准格式<span class="symbol">:o=example</span>,c=CN。直接使用公司的 DNS 域名<span class="symbol">:o=example</span>.com。使用公司的 DNS 域名的不同部分<span class="symbol">:dc=example</span>,dc=com。</div></pre></td></tr></table></figure><p>   一般而言,推荐使用第三种格式,因为其更利于以后目录树的扩展,若将来 Example 公司合并了 Test 公司,只需要将 Test.com 作为根即可,不需要修改原有的结构。<br>公司中的部门作为 OU,如“ou=sales”。 OU 是目录树的分枝节点,OU 之下可以包含其他分枝节点或叶子节点。 用户是目录数的最底层(即叶子节点),可以根据用户所在的部门位于不同的 OU 中,使用 uid或 cn 描述都可,如“uid=tom”或“cn=Kelly King”。<br>因此,可以将 Example 公司的组织结构转化为如图所示的 LDAP 目录树。</p><p>图- LDAP 目录树</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/openldap2.png" alt=""></figure></p><h3 id="LDAP的功能"><a href="#LDAP的功能" class="headerlink" title="LDAP的功能"></a>LDAP的功能</h3><p>在LDAP的功能模型中定义了一系列利用LDAP协议的操作，主要包含以下4部分：</p><ul><li>查询操作 ：允许查询目录和取得数据，其查询性能比关系数据库好。</li><li>更新操作 ：目录的更新操作没关系数据库方便，更新性能较差，但也同样允许进行添加、删除、修改等操作。</li><li>复制操作 ：前面也提到过，LDAP是一种典型的分布式结构，提供复制操作，可将主服务器的数据的更新复制到设置的从服务器中。</li><li><p>认证和管理操作 ：允许客户端在目录中识别自己，并且能够控制一个会话的性质。</p><h3 id="LDAP目录的优势"><a href="#LDAP目录的优势" class="headerlink" title="LDAP目录的优势"></a>LDAP目录的优势</h3></li><li><p>LDAP协议是跨平台的和标准的协议，因此应用程序就不用为LDAP目录放在什么样的服务器上操心了。</p></li><li>LDAP服务器可以用“推”或“拉”的方法复制部分或全部数据，例如：可以把数据“推”到远程的办公室，以增加数据的安全性。复制技术是内置在LDAP服务器中的而且很容易配置。</li></ul><h3 id="LDAP协议的特点"><a href="#LDAP协议的特点" class="headerlink" title="LDAP协议的特点"></a>LDAP协议的特点</h3><ul><li>LDAP是一种目录服务，保存在特殊的数据库中，数据的读取速度远高于写入速度。</li><li>LDAP对查询做了优化，读取速度优于普通关系数据库。</li><li>LDAP不支持事务、不能进行回滚，需要进行这些操作的应用只有选择关系数据库。</li><li>LDAP采用服务器/客户端模式，支持分布式结构。</li><li>LDAP中的条目以树形结构组织和存储。</li><li>LDAP基于Internet协议，直接运行在简单和通用的TCP/IP或其他可靠的传输协议层上，使连接的建立和包的处理简单、快捷，对于互联网和企业网应用都很方便。</li><li>LDAP协议简单，通过使用查找操作实现列表操作和读操作。</li><li>LDAP通过引用机制实现分布式访问，通过客户端API实现分布式操作（对于应用透明），平衡了负载。</li><li>LDAP实现具有低费用、易配置和易管理的特点，并提供了满足应用程序对目录服务所需求的特性。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;OpenLDAP入门了解&quot;&gt;&lt;a href=&quot;#OpenLDAP入门了解&quot; class=&quot;headerlink&quot; title=&quot;OpenLDAP入门了解&quot;&gt;&lt;/a&gt;OpenLDAP入门了解&lt;/h2&gt;&lt;h3 id=&quot;LDAP概述&quot;&gt;&lt;a href=&quot;#LDAP概述&quot; class=&quot;headerlink&quot; title=&quot;LDAP概述&quot;&gt;&lt;/a&gt;LDAP概述&lt;/h3&gt;&lt;p&gt;我的个人理解：LDAP是轻量目录访问协议(Lightweight Directory Access Protocol)的缩写，LDAP是一个统一的账号管理认证平台，使用轻量级目录访问协议（LDAP）构建集中的身份验证系统可以减少管理成本，增强安全性，避免数据复制的问题，并提高数据的一致性。随着 Linux的不断成熟，已经出现了很多工具用来简化用户帐号信息到 LDAP目录的迁移。还开发了一些工具用来在客户机和目录服务器之间启用加密通信配置，并通过复制提供容错性。本文将向您展示如何配置服务器和客户机在Centos上使用OpenLDAP。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://devopsideas.com/wp-content/uploads/2017/09/Openldap-7-800x445.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/categories/OpenLDAP/"/>
    
    
      <category term="OpenLDAP" scheme="http://blog.yancy.cc/tags/OpenLDAP/"/>
    
  </entry>
  
  <entry>
    <title>秋意群山，醉美25KM标毅线，挑战自己😼</title>
    <link href="http://blog.yancy.cc/2017/12/04/%E4%B8%AA%E4%BA%BA%E7%94%9F%E6%B4%BB%E8%AE%B0%E5%BD%95/%E7%A7%8B%E6%84%8F%E7%BE%A4%E5%B1%B1%EF%BC%8C%E9%86%89%E7%BE%8E25KM%E6%A0%87%E6%AF%85%E7%BA%BF%EF%BC%8C%E6%8C%91%E6%88%98%E8%87%AA%E5%B7%B1%F0%9F%98%BC/"/>
    <id>http://blog.yancy.cc/2017/12/04/个人生活记录/秋意群山，醉美25KM标毅线，挑战自己😼/</id>
    <published>2017-12-04T12:32:17.000Z</published>
    <updated>2017-12-03T07:49:50.400Z</updated>
    
    <content type="html"><![CDATA[<p>🍓  再不动弹，我们就要生锈了,在杭四,五年了，还没有真正旅游过西湖周边的景区，年初参加了户外俱乐部这次是第三次的登山活动，也非常有意义纪念。</p><p><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism03.jpeg" alt=""></figure></p><a id="more"></a><h4 id="线路说明"><a href="#线路说明" class="headerlink" title="线路说明"></a>线路说明</h4><p>从浙大玉泉校区或老和山牌坊上山，环绕西湖群山，翻山越岭，跋山涉水，终点吴山广场。此线路被杭州户外爱好者称为标准毅行线。它是一条经典的环西湖登山路线，非常适合户外拉练。体力不支的情况下随时选择下撤点返回市区。</p><p>【标毅线路25公里】浙大/老和山－灵峰山－锅子顶－北高峰（314米）－美人峰－龙门山－石人亭（石人岭256米）－白云峰－云雾台－猢狲岭－天门山（也叫天竺山，杭州第一高峰，412米）－十里郎当－五云山（340米）－林海亭－马鞍山－马儿山－贵人阁（贵人峰，240米）－虎跑－玉皇山－慈云宫－将台山－凤凰山－孔家山－大华山－万松书院－云居山－城隍山（吴山）－吴山广场</p><h4 id="活动强度：中等偏强，累计上升高度1800米，距离25公里"><a href="#活动强度：中等偏强，累计上升高度1800米，距离25公里" class="headerlink" title="活动强度：中等偏强，累计上升高度1800米，距离25公里"></a>活动强度：中等偏强，累计上升高度1800米，距离25公里</h4><h4 id="集合地点：老和山牌坊"><a href="#集合地点：老和山牌坊" class="headerlink" title="集合地点：老和山牌坊"></a>集合地点：老和山牌坊</h4><p><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism01.jpeg" alt=""></figure></p><h4 id="出发-🏃🏻-25km"><a href="#出发-🏃🏻-25km" class="headerlink" title="出发 🏃🏻 25km"></a>出发 🏃🏻 25km</h4><p><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism02.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism04.jpeg" alt=""></figure></p><h4 id="沿途的下撤点"><a href="#沿途的下撤点" class="headerlink" title="沿途的下撤点"></a>沿途的下撤点</h4><p>北高峰（第一个下撤点）：从浙大玉泉出发，经过将军山，一路前行，西湖美景尽收，这里可以看到断桥和苏堤的全景，走到北高峰算是完成了四分之一<br>走到如下路口，左侧可下撤至灵隐；右侧可下撤至法华寺（西溪路）</p><p><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism05.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism06.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism07.jpeg" alt=""></figure></p><h4 id="马上到龙井村"><a href="#马上到龙井村" class="headerlink" title="马上到龙井村"></a>马上到龙井村</h4><p>龙井村位于西湖风景名胜区西南面，四面群山环抱，呈北高南低的趋势，村内常住人口约800多人，拥有近800亩的高山茶园，村的西北面北高峰，狮子峰，天竺峰形成一道天然屏障，挡住<br>西北寒风的侵袭。南面为九溪，溪谷深广，直通钱塘江，春夏季的东南风易入山谷，通风通气的地理条件为龙井茶的生长提供了得天独厚的优势。这里出产的龙井茶位居“狮，龙，云，虎”之首。相传乾隆皇帝下江南时，曾到龙井村狮峰山下的胡公庙品尝西湖龙井茶。饮后赞不绝口，并将庙前十八棵茶树封为“御茶”。<br>“茶乡第一村”——龙井村，因盛产顶级西湖龙井茶而闻名于世。东临西子湖，西依五云山，南靠滔滔东去的钱塘江水，北抵插入云端的南北高峰，四周群山叠翠，云雾环绕，就如一颗镶嵌在西子湖畔的翡翠宝石。</p><p><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism08.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism09.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism10.jpeg" alt=""></figure></p><h4 id="九溪烟树🌲下山"><a href="#九溪烟树🌲下山" class="headerlink" title="九溪烟树🌲下山"></a>九溪烟树🌲下山</h4><p><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism11.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism12.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism13.jpeg" alt=""></figure></p><h4 id="九溪烟树🌲-14km-心里目标已完成😼"><a href="#九溪烟树🌲-14km-心里目标已完成😼" class="headerlink" title="九溪烟树🌲.14km  心里目标已完成😼"></a>九溪烟树🌲.14km  心里目标已完成😼</h4><p><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism14.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism16.jpeg" alt=""></figure><br><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism15.jpeg" alt=""></figure></p><h4 id="装备"><a href="#装备" class="headerlink" title="装备"></a>装备</h4><p>本次活动，你需要携带如下装备：<br>相机、手机、徒步鞋、登山杖、饮用水、中餐</p><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><p>1、穿着适合户外运动，最好请穿登山鞋、徒步鞋，其次是旅游鞋。<br>2、带水2升，零食，干粮，最好带登山杖。<br>3、沿途严禁丢不可降解的垃圾</p><p><figure class="figure"><img src="http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism17.jpeg" alt=""></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;🍓  再不动弹，我们就要生锈了,在杭四,五年了，还没有真正旅游过西湖周边的景区，年初参加了户外俱乐部这次是第三次的登山活动，也非常有意义纪念。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://p0de85ckf.bkt.clouddn.com/hangzhou-tourism03.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="生活" scheme="http://blog.yancy.cc/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="Personal life record" scheme="http://blog.yancy.cc/tags/Personal-life-record/"/>
    
  </entry>
  
  <entry>
    <title>KafKa从0.8.2.1升级到0.9.0.1变化方案与步骤</title>
    <link href="http://blog.yancy.cc/2017/11/04/Bigdata-hadoop/Kafka/KafKa%E4%BB%8E0.8.2.1%E5%8D%87%E7%BA%A7%E5%88%B00.9.0.1%E5%8F%98%E5%8C%96%E6%96%B9%E6%A1%88%E4%B8%8E%E6%AD%A5%E9%AA%A4/"/>
    <id>http://blog.yancy.cc/2017/11/04/Bigdata-hadoop/Kafka/KafKa从0.8.2.1升级到0.9.0.1变化方案与步骤/</id>
    <published>2017-11-04T03:22:00.000Z</published>
    <updated>2017-12-03T06:29:16.077Z</updated>
    
    <content type="html"><![CDATA[<p>🍊我这里是采用,群集升级,全部更新停止老版本zk和kafka更新服务。</p><p>9.0.0有潜在的中断更改风险（在升级之前需要知道），并且与之前版本的broker之间的协议改变。这意味着此次升级可能和客户端旧版本不兼容。因此在升级客户端之前，先升级kafka集群。如果你使用MirrorMaker下游集群，则同样应首先升级。</p><p><figure class="figure"><img src="https://blogs.apache.org/hbase/mediaresource/0b77f435-da5c-4696-a1b5-f35bc4139b7b" alt=""></figure></p><a id="more"></a><h3 id="滚动升级"><a href="#滚动升级" class="headerlink" title="滚动升级"></a>滚动升级</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">升级所有broker的server<span class="selector-class">.properties</span>,并在其中添加</div><div class="line">inter<span class="selector-class">.broker</span><span class="selector-class">.protocol</span><span class="selector-class">.version</span> = <span class="number">0.8</span>.<span class="number">2</span><span class="selector-class">.X</span></div><div class="line">每次升级一个broker：关闭broker，替换新版本，然后重新启动。</div></pre></td></tr></table></figure><h3 id="群集升级"><a href="#群集升级" class="headerlink" title="群集升级"></a>群集升级</h3><p>一旦整个群集升级，通过编辑inter.broker.protocol.version并将其设置为0.9.0.0来转换所有协议。<br>逐个重新启动broker，使新协议版本生效。<br>注意 ：如果你可接受停机，你可以简单地将所有broker关闭，更新版本并重启启动，协议将默认从新版本开始。<br>注意 ：变换协议版本和重启启动可以在broker升级完成后的任何时间去做，不必马上做。</p><h4 id="0-9-0-0潜在的中断变化"><a href="#0-9-0-0潜在的中断变化" class="headerlink" title="0.9.0.0潜在的中断变化"></a>0.9.0.0潜在的中断变化</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">Java 1.6不再支持，需要Jdk1.7版本以上。</div><div class="line">Scala 2.9不再支持。</div><div class="line"></div><div class="line">默认情况下，1000以上的Broker ID为自动分配。如果你的集群高于该阈值，需相应地增加</div><div class="line">reserved.broker.max.id配置。</div><div class="line"></div><div class="line">replica.lag.max.messages配置已经移除。分区leader在决定哪些副本处于同步时将不再考虑落后的消息的数。</div><div class="line"></div><div class="line">配置参数replica.lag.time.max.ms现在不仅指自上次从副本获取请求后经过的时间，还指自副本上次被捕获以来的时间。 副本仍然从leader获取消息，但超过replica.lag.time.max.ms配置的最新消息将被认为不同步的。</div><div class="line"></div><div class="line">压缩的topic不再接受没有key的消息，如果出现，生产者将抛出异常。 在0.8.x中，没有key的消息将导致日志压缩线程退出（并停止所有压缩的topic）。</div><div class="line"></div><div class="line">MirrorMaker不再支持多个目标集群。 它只接受一个--consumer.config。 要镜像多个源集群，每个源集群至少需要一个MirrorMaker实例，每个源集群都有自己的消费者配置。</div><div class="line"></div><div class="line">在org.apache.kafka.clients.tools。包下的Tools已移至org.apache.kafka.tools。。 所有包含的脚本仍将照常工作，只有直接导入这些类的自定义代码将受到影响。</div><div class="line"></div><div class="line">在kafka-run-class.sh中更改了默认的Kafka JVM性能选项（KAFKA_JVM_PERFORMANCE_OPTS）。</div><div class="line"></div><div class="line">kafka-topics.sh脚本（kafka.admin.TopicCommand）现在退出，失败时出现非零退出代码。</div><div class="line"></div><div class="line">kafka-topics.sh脚本（kafka.admin.TopicCommand）现在将在topic名称由于使用“.”或“_”而导致风险度量标准冲突时打印警告。以及冲突的情况下的错误。</div><div class="line"></div><div class="line">kafka-console-producer.sh脚本（kafka.tools.ConsoleProducer）将默认使用新的Java Producer，用户必须指定“old-producer”才能使用旧生产者。</div><div class="line">默认情况下，所有命令行工具都会将所有日志消息打印到stderr而不是stdout。</div></pre></td></tr></table></figure><h4 id="0-9-0-1中的显著变化"><a href="#0-9-0-1中的显著变化" class="headerlink" title="0.9.0.1中的显著变化"></a>0.9.0.1中的显著变化</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">可以通过将broker.id.generation.enable设置为<span class="literal">false</span>来禁用新的broker ID生成功能。</div><div class="line"></div><div class="line">默认情况下，配置参数log.cleaner.enable为<span class="literal">true</span>。 这意味着topic会清理。</div><div class="line">policy = compact现在将被默认压缩，并且128MB的堆（通过log.cleaner.dedupe.buffer.size）分配给清洗进程。你可能需要根据你对压缩topic的使用情况，查看log.cleaner.dedupe.buffer.size和其他log.cleaner配置值。</div><div class="line"></div><div class="line">默认情况下，新消费者的配置参数fetch.min.bytes的默认值为1。</div></pre></td></tr></table></figure><h4 id="0-9-0-0弃用的"><a href="#0-9-0-0弃用的" class="headerlink" title="0.9.0.0弃用的"></a>0.9.0.0弃用的</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">kafka-topics.sh脚本的变更topic配置已弃用（kafka.admin.ConfigCommand），以后将使用kafka-configs.sh(kafka.admin.ConfigCommand) 。</div><div class="line"></div><div class="line">kafka-consumer-offset-checker.sh(kafka.tools.ConsumerOffsetChecker)已弃用，以后将使用kafka-consumer-groups.sh (kafka.admin.ConsumerGroupCommand)</div><div class="line"></div><div class="line">kafka.tools.ProducerPerformance已弃用。以后将使用org.apache.kafka.tools.ProducerPerformance（kafka-producer-perf-test.sh也将使用新类）</div><div class="line"></div><div class="line">生产者的block.on.buffer.full已弃用，并将在以后的版本中移除。目前其默认已经更为<span class="literal">false</span>。KafkaProducer将不再抛出BufferExhaustedException，而是使用max.block.ms来中止，之后将抛出TimeoutException。如果block.on.buffer.full属性明确地设置为<span class="literal">true</span>，它将设置max.block.ms为Long.MAX_VALUE和metadata.fetch.timeout.ms将不执行。</div></pre></td></tr></table></figure><h4 id="升级准备步骤："><a href="#升级准备步骤：" class="headerlink" title="升级准备步骤："></a>升级准备步骤：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">（0）</div><div class="line">wget http://mirror.bit.edu.cn/apache/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz</div><div class="line">tar -xvf kafka_2.10-0.9.0.1.tgz</div><div class="line"></div><div class="line">[jollybi@kafka1 tools]$ ll</div><div class="line">total 88100</div><div class="line">drwxr-xr-x  7 jollybi jollybi     4096 Jul 14 13:07 kafka_2.10-0.8.2.1</div><div class="line">-rw-rw-r--  1 jollybi jollybi 16162559 Jul 14 11:40 kafka_2.10-0.8.2.1.tgz</div><div class="line">drwxr-xr-x  6 jollybi jollybi     4096 Feb 12  2016 kafka_2.10-0.9.0.1</div><div class="line">-rw-rw-r--  1 jollybi jollybi 35725063 Jun 20 20:11 kafka_2.10-0.9.0.1.tgz</div><div class="line">drwxr-xr-x 12 jollybi jollybi     4096 Sep 25 16:41 zookeeper-3.4.5</div><div class="line">-rw-rw-r--  1 jollybi jollybi 38307840 Jul 14 11:40 zookeeper-3.4.5.tar</div><div class="line"></div><div class="line">[jollybi@kafka1 tools]$ mkdir -p /data/tools/kafka_2.10-0.9.0.1/kafka-logs</div><div class="line">[jollybi@kafka1 tools]$ mkdir -p /data/tools/kafka_2.10-0.9.0.1/logs</div><div class="line"></div><div class="line"><span class="comment">###修改配置：</span></div><div class="line"></div><div class="line">(1)</div><div class="line">[jollybi@kafka1 tools]$ vim kafka_2.10-0.9.0.1/config/zookeeper.properties</div><div class="line">第一步修改:</div><div class="line"><span class="comment"># the directory where the snapshot is stored.</span></div><div class="line">dataDir=dataDir=/data/jollybi/tools/zookeeper-3.4.5/tmp</div><div class="line"><span class="comment"># the port at which the clients will connect</span></div><div class="line">clientPort=2281</div><div class="line"><span class="comment"># disable the per-ip limit on the number of connections since this is a non-production config</span></div><div class="line">maxClientCnxns=0</div><div class="line">dataDir,clientPort的意义不需要说明了，对于maxClientCnxns选项，如果不设置或者设置为0，则每个ip连接zookeeper时的连接数没有限制。</div><div class="line">需要注意的是，设置maxClientCnxns的值时需要把kafka server的连接数考虑进去，因为启动kafka server时，kafka server也会连接zookeeper的。</div><div class="line"></div><div class="line">(2)</div><div class="line">[jollybi@kafka1 tools]$ vim kafka_2.10-0.9.0.1/config/consumer.properties</div><div class="line">zookeeper.connect=10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281</div><div class="line">其他参数根据不同集群环境调整</div><div class="line"></div><div class="line">(3)</div><div class="line">[jollybi@kafka1 tools]$ vim kafka_2.10-0.9.0.1/config/producer.properties (修改内网IP)</div><div class="line">metadata.broker.list=10.155.90.153:9292,10.155.90.155:9292,10.155.90.138:9292</div><div class="line">producer.type=sync</div><div class="line">compression.codec=none</div><div class="line">serializer.class=kafka.serializer.DefaultEncoder</div><div class="line">其他参数根据不同集群环境调整</div><div class="line"></div><div class="line">(4) </div><div class="line">[jollybi@kafka1 config]$ cat server.properties | grep -Pv <span class="string">"^#|^$"</span></div><div class="line">broker.id=1</div><div class="line">auto.leader.rebalance.enable = <span class="literal">true</span></div><div class="line">listeners=PLAINTEXT://10.155.90.153:9292</div><div class="line">port=9292</div><div class="line">host.name=10.155.90.153</div><div class="line">zookeeper.connect=10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281</div><div class="line">zookeeper.connection.timeout.ms=6000</div><div class="line">num.network.threads=3</div><div class="line">num.io.threads=8</div><div class="line">socket.send.buffer.bytes=102400</div><div class="line">socket.receive.buffer.bytes=102400</div><div class="line">socket.request.max.bytes=104857600</div><div class="line">log.dirs=/data/tools/kafka_2.10-0.9.0.1/kafka-logs</div><div class="line">log.cleaner.enable=<span class="literal">true</span></div><div class="line">delete.topic.enable=<span class="literal">true</span></div><div class="line">num.partitions=1</div><div class="line">num.recovery.threads.per.data.dir=1</div><div class="line">log.retention.hours=96</div><div class="line">log.segment.bytes=1073741824</div><div class="line">log.retention.check.interval.ms=300000</div><div class="line"></div><div class="line">备注：listeners一定要配置成为IP地址；如果配置为localhost或服务器的hostname,在使用java发送数据时就会抛出异 常：org.apache.kafka.common.errors.TimeoutException: Batch Expired 。因为在没有配置advertised.host.name 的情况下，Kafka并没有像官方文档宣称的那样改为广播我们配置的hostname，而是广播了主机配置的hostname。远端的客户端并没有配置 hosts，所以自然是连接不上这个hostname的</div></pre></td></tr></table></figure><h4 id="kafka与zk内存日志优化"><a href="#kafka与zk内存日志优化" class="headerlink" title="kafka与zk内存日志优化"></a>kafka与zk内存日志优化</h4><p>我个人博客有写优化文档<br><a href="http://blog.yancy.cc/2017/08/27/Bigdata-hadoop/zookeeper/Bigdata-Zookeeper%E9%9B%86%E7%BE%A4%E6%97%A5%E5%BF%97%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%E5%92%8C%E6%B8%85%E7%90%86%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%AF%E5%8A%A8%E5%86%85%E5%AD%98%20/">Bigdata-Zookeeper集群日志配置详解和清理自定义启动内存</a><br><a href="http://blog.yancy.cc/2017/07/04/Bigdata-hadoop/Kafka/kafka%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E2%80%93JVM%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/">Kafka性能优化–JVM参数配置优化</a><br><a href="http://blog.yancy.cc/2017/03/30/Bigdata-hadoop/Kafka/Kafka%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8%E8%A7%A3%E6%9E%90%E4%B8%8E%E5%AE%9E%E8%B7%B5%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%BC%98%E5%8C%96/">Kafka日志存储解析与实践数据存储优化</a></p><h4 id="升级步骤方案："><a href="#升级步骤方案：" class="headerlink" title="升级步骤方案："></a>升级步骤方案：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">(0) mongodb 能存储时间戳4个小时的数据，在4个小时之内升级不会有风险。</div><div class="line">(1) mongotail 先停止生产写入数据到kafka</div><div class="line">(2) kafka 消费组继续消费 topic分区查看消费为零。</div><div class="line">(3) zk需要升级3.4.6版本对应kafka0.9.1版本，如果zk不需要升级，升级之前要先将ZooKeeper中原版本的kafka生成的znode删除，包括：consumers, controller, brokers, controller_epoch等。否则启动kafka会报错。</div><div class="line">(4) 重启新版本zk集群，查看集群服务选举是否正常。</div><div class="line">(5) 重启新版本kafka集群，重启没有报错，查看版本没问题就行。</div><div class="line">(6) zk上面查看命令</div><div class="line"> ./zkCli.sh -server 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281</div><div class="line">[zk: 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281(CONNECTED) 0] ls /</div><div class="line"></div><div class="line">controller_epoch          controller                brokers                   zookeeper                 kafka-manager             admin                     isr_change_notification   consumers                 config</div><div class="line">[zk: 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281(CONNECTED) 0] ls /brokers/ids</div><div class="line">[1, 2, 3]  </div><div class="line">集群三个brokers id正常</div><div class="line">(7)修改监控指标。 完成升级</div><div class="line"></div><div class="line">(8) 另外，新版本的一些命令与原版本的有些相同,增删改查命令。</div></pre></td></tr></table></figure><h4 id="开始升级步骤操作："><a href="#开始升级步骤操作：" class="headerlink" title="开始升级步骤操作："></a>开始升级步骤操作：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">（1)启动zk与kafka</div><div class="line"><span class="comment"># /data/tools/zookeeper-3.4.6/bin/zkServer.sh start</span></div><div class="line"><span class="comment"># /data/tools/kafka_2.10-0.9.0.1/bin/kafka-server-start.sh /data/tools/ kafka_2.10-0.9.0.1/config/server.properties &amp;</span></div><div class="line"></div><div class="line">（2）新建topic</div><div class="line"><span class="comment">#./bin/kafka-topics.sh --create --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --replication-factor 3 --partitions 12  --topic mongotail_lz4_imp </span></div><div class="line"><span class="comment">#./bin/kafka-topics.sh --create --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --replication-factor 3 --partitions 12  --topic mongotail_lz4</span></div><div class="line"></div><div class="line">(3) 查看集群topic分区情况</div><div class="line"><span class="comment">#./bin/kafka-topics.sh  --describe --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --topic mongotail_lz4</span></div><div class="line"></div><div class="line">(4) 更新所有开源监控（KafkaOffsetMonitor）</div><div class="line">vim /home/jollybi/./monitor/monitor.sh</div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line">java -Xms512M -Xmx512M -Xss1024K -XX:PermSize=256m -XX:MaxPermSize=512m -cp KafkaOffsetMonitor-assembly-0.2.0_modify.jar com.quantifind.kafka.offsetapp.OffsetGetterWeb --zk 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --port 8086  --refresh 10.seconds  --retain 7.days &gt;/tmp/stdout.log 2&gt;&amp;1 &amp;</div><div class="line"></div><div class="line">zk地址修改成内网地址 这里的数据存储7天。</div><div class="line">重启服务：</div><div class="line">./monitor/monitor.sh &amp;</div><div class="line"></div><div class="line">（5）更新开源监控（kafka-manager）</div><div class="line">vim kafka-manager-1.3.3.8/conf/application.conf</div><div class="line">kafka-manager.zkhosts=<span class="string">"10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281"</span></div><div class="line">zk地址修改成内网地址.</div><div class="line"></div><div class="line">重启服务：</div><div class="line">./kafka-manager-1.3.3.8/bin/kafka-manager -Dconfig.file=kafka-manager-1.3.3.8/conf/application.conf -Dhttp.port=8080 &amp;</div><div class="line"></div><div class="line">（6）更新开源监控（zabbix）自己写的监控脚本</div><div class="line"></div><div class="line">[jollybi@kafka1 ~]$ /data/tools/kafka_2.10-0.9.0.1/bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeepe 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --group group_ml --topic mongotail_lz4</div><div class="line">[2017-10-31 14:05:24,390] WARN WARNING: ConsumerOffsetChecker is deprecated and will be dropped <span class="keyword">in</span> releases following 0.9.0. Use ConsumerGroupCommand instead. (kafka.tools.ConsumerOffsetChecker$)</div><div class="line">Group           Topic                          Pid Offset          <span class="built_in">log</span>Size         Lag             Owner</div><div class="line">group_ml        mongotail_lz4                  0   1205186         1205257         71              group_ml_Graphsql-test.jollychic.com-1509421110787-1daec53a-0</div><div class="line">group_ml        mongotail_lz4                  1   1206860         1206915         55              group_ml_Graphsql-test.jollychic.com-1509421110787-1daec53a-0</div><div class="line">....</div><div class="line">运行会出现WARN警告，0.9版本更新过了，</div><div class="line"></div><div class="line">注意：在0.9.0.0，kafka.tools.ConsumerOffsetChecker已经不支持了。你应该使用kafka.admin.ConsumerGroupCommand(或bin/kafka-consumer-groups.sh脚本)来管理消费者组，包括用新消费者API创建的消费者。</div><div class="line"></div><div class="line">这里使用kafka.admin.ConsumerGroupCommand 提示命令不对，现在在研究使用。 </div><div class="line">监控脚本 输出会有提示直接过滤掉即可。</div><div class="line"><span class="keyword">function</span> wlj_event_lag &#123;</div><div class="line"><span class="built_in">echo</span> <span class="string">"`sh /etc/zabbix/kafka_topic_monitor.sh 2&gt;/dev/null |  sed -n  '3p'`"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>🎉 kafka升级，调配内网访问地址，内存，日志输出类型调优，监控更新配置。<br>🎉 参考官网文档 <a href="https://kafka.apache.org/0101/documentation.html#upgrade_9" target="_blank" rel="external">Apache Kafka 从 0.8.0, 0.8.1.X 或 0.8.2.X 升级到 0.9.0.0</a></p><h3 id="Communicative-learning"><a href="#Communicative-learning" class="headerlink" title="Communicative learning:"></a>Communicative learning:</h3><p>🐧  Linux shell_ senior operation and maintenance faction: QQ group <code>459096184</code> circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)<br>🐧  BigData-Exchange School:QQ group <code>521621407</code> circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join</p><p>Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;🍊我这里是采用,群集升级,全部更新停止老版本zk和kafka更新服务。&lt;/p&gt;
&lt;p&gt;9.0.0有潜在的中断更改风险（在升级之前需要知道），并且与之前版本的broker之间的协议改变。这意味着此次升级可能和客户端旧版本不兼容。因此在升级客户端之前，先升级kafka集群。如果你使用MirrorMaker下游集群，则同样应首先升级。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blogs.apache.org/hbase/mediaresource/0b77f435-da5c-4696-a1b5-f35bc4139b7b&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/categories/Bigdata-Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>ZABBIX monitoring Flume</title>
    <link href="http://blog.yancy.cc/2017/09/21/%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7/Zabbix/ZABBIX%20monitoring%20Flume/"/>
    <id>http://blog.yancy.cc/2017/09/21/性能监控/Zabbix/ZABBIX monitoring Flume/</id>
    <published>2017-09-21T10:42:58.000Z</published>
    <updated>2018-04-11T12:02:49.662Z</updated>
    
    <content type="html"><![CDATA[<h4 id="ZABBIX-monitoring-Flume"><a href="#ZABBIX-monitoring-Flume" class="headerlink" title="ZABBIX monitoring Flume"></a>ZABBIX monitoring Flume</h4><p>Flume本身提供了http, ganglia的监控服务，而我们目前主要使用zabbix做监控。因此，我们为Flume添加了zabbix监控模块，和sa的监控服务无缝融合。<br>另一方面，净化Flume的metrics。只将我们需要的metrics发送给zabbix，避免 zabbix server造成压力。目前我们最为关心的是Flume能否及时把应用端发送过来的日志写到Hdfs上， 对应关注的metrics为：</p><p>Source : 接收的event数和处理的event数<br>Channel : Channel中拥堵的event数<br>Sink : 已经处理的event数</p><p><figure class="figure"><img src="https://cdn.itzgeek.com/wp-content/uploads/2014/07/Zabbix.jpg" alt=""></figure></p><a id="more"></a><h4 id="zabbix安装-amp-JVM性能监控"><a href="#zabbix安装-amp-JVM性能监控" class="headerlink" title="zabbix安装&amp;JVM性能监控"></a>zabbix安装&amp;JVM性能监控</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">zabbix安装</div><div class="line">http://my.oschina.net/yunnet/blog/173161</div><div class="line"></div><div class="line">JDK1.8</div><div class="line">[jollybi@countly1 conf]$ java -version</div><div class="line">java version <span class="string">"1.8.0_65"</span></div><div class="line">Java(TM) SE Runtime Environment (build 1.8.0_65-b17)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#JVM性能监控</span></div><div class="line">Young GC  counts</div><div class="line"></div><div class="line">/usr/<span class="built_in">local</span>/jdk/bin/jstat  -gcutil 87007 | tail -1 | awk <span class="string">'&#123;print  $6&#125;'</span></div><div class="line">95.16</div><div class="line"></div><div class="line">Full GC counts</div><div class="line">/usr/<span class="built_in">local</span>/jdk/bin/jstat  -gcutil 87007 | tail -1 | awk <span class="string">'&#123;print  $8&#125;'</span></div><div class="line">436.252</div><div class="line"></div><div class="line">JVM total  memory usage</div><div class="line">/usr/<span class="built_in">local</span>/jdk/bin/jmap -histo $(pgrep java)|grep Total | sed -n <span class="string">'$p'</span> | awk <span class="string">'&#123;print  $3&#125;'</span></div><div class="line"></div><div class="line"></div><div class="line">JVM total  instances usage</div><div class="line"> /usr/<span class="built_in">local</span>/jdk/bin/jmap -histo $(pgrep java)|grep Total | sed -n <span class="string">'$p'</span> | awk <span class="string">'&#123;print  $2&#125;'</span></div></pre></td></tr></table></figure><h4 id="flume应用参数监控"><a href="#flume应用参数监控" class="headerlink" title="flume应用参数监控"></a>flume应用参数监控</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">启动时加上JSON repoting参数,这样就可以通过http://localhost:34545/metrics访问</div><div class="line">bin/flume-ng agent --conf conf --conf-file conf/flume-conf-test.properties --name agent -Dflume.root.logger=INFO,console -Dflume.monitoring.type=http -Dflume.monitoring.port=34545 &amp;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">[root@localhost apache-flume-1.8.0-bin]<span class="comment"># curl http://localhost:34545/metrics 2&gt;/dev/null|sed -e  's/\([,]\)\s*/\1\n/g' -e 's/[&#123;&#125;]/\n/g' -e 's/[",]//g'</span></div><div class="line"></div><div class="line">SOURCE.source1:</div><div class="line">EventReceivedCount:4871</div><div class="line">AppendBatchAcceptedCount:52</div><div class="line">Type:SOURCE</div><div class="line">EventAcceptedCount:4871</div><div class="line">AppendReceivedCount:0</div><div class="line">StartTime:1511251310062</div><div class="line">OpenConnectionCount:0</div><div class="line">AppendAcceptedCount:0</div><div class="line">AppendBatchReceivedCount:52</div><div class="line">StopTime:0</div><div class="line"></div><div class="line">SINK.sink1:</div><div class="line">ConnectionCreatedCount:0</div><div class="line">BatchCompleteCount:0</div><div class="line">BatchEmptyCount:43</div><div class="line">EventDrainAttemptCount:0</div><div class="line">StartTime:1511251311047</div><div class="line">BatchUnderflowCount:1</div><div class="line">ConnectionFailedCount:0</div><div class="line">ConnectionClosedCount:0</div><div class="line">Type:SINK</div><div class="line">RollbackCount:0</div><div class="line">EventDrainSuccessCount:4871</div><div class="line">KafkaEventSendTimer:24748</div><div class="line">StopTime:0</div><div class="line"></div><div class="line">CHANNEL.channel1:</div><div class="line">ChannelCapacity:1000</div><div class="line">ChannelFillPercentage:0.0</div><div class="line">Type:CHANNEL</div><div class="line">ChannelSize:0</div><div class="line">EventTakeSuccessCount:4871</div><div class="line">EventTakeAttemptCount:4915</div><div class="line">StartTime:1511251309391</div><div class="line">EventPutAttemptCount:4871</div><div class="line">EventPutSuccessCount:4871</div><div class="line">StopTime:0</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">/opt/jdk1.8.0_101/bin/jstat</div></pre></td></tr></table></figure><h4 id="配置监控flume的脚本文件"><a href="#配置监控flume的脚本文件" class="headerlink" title="配置监控flume的脚本文件"></a>配置监控flume的脚本文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">vim /etc/zabbix/monitor_flume.sh</div><div class="line">event=EventDrainSuccessCount</div><div class="line"><span class="comment">#curl  http://localhost:34545/metrics 2&gt;/dev/null|sed -e 's/\([,]\)\s*/\1\n/g' -e  's/[&#123;&#125;]/\n/g' -e 's/[",]//g' |grep $1|awk -F: '&#123;print $2&#125;'</span></div><div class="line"><span class="keyword">function</span> EventDrainSuccessCount  &#123;</div><div class="line">curl  http://localhost:34545/metrics 2&gt;/dev/null|sed <span class="_">-e</span> <span class="string">'s/\([,]\)\s*/\1\n/g'</span> <span class="_">-e</span>  <span class="string">'s/[&#123;&#125;]/\n/g'</span> <span class="_">-e</span> <span class="string">'s/[",]//g'</span> |grep <span class="variable">$event</span>|awk -F: <span class="string">'&#123;print $2&#125;'</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> StartTime  &#123;</div><div class="line">curl  http://localhost:34545/metrics 2&gt;/dev/null|sed <span class="_">-e</span> <span class="string">'s/\([,]\)\s*/\1\n/g'</span> <span class="_">-e</span>  <span class="string">'s/[&#123;&#125;]/\n/g'</span> <span class="_">-e</span> <span class="string">'s/[",]//g'</span> |grep StartTim |awk -F: <span class="string">'&#123;print $2&#125;'</span> |sed -n <span class="string">"2p"</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">function</span> Total &#123;</div><div class="line">curl  http://localhost:34545/metrics 2&gt;/dev/null|sed <span class="_">-e</span> <span class="string">'s/\([,]\)\s*/\1\n/g'</span> <span class="_">-e</span>  <span class="string">'s/[&#123;&#125;]/\n/g'</span> <span class="_">-e</span> <span class="string">'s/[",]//g'</span> |grep Total|awk -F: <span class="string">'&#123;print $2&#125;'</span></div><div class="line">&#125;</div><div class="line"><span class="comment">#Run the requested function</span></div><div class="line"><span class="variable">$1</span></div></pre></td></tr></table></figure><h4 id="在zabbix-agent配置文件进行部署"><a href="#在zabbix-agent配置文件进行部署" class="headerlink" title="在zabbix agent配置文件进行部署"></a>在zabbix agent配置文件进行部署</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">vim zabbix_flume_jdk.conf</div><div class="line">UserParameter=ygc.counts,sudo /opt/jdk1.8.0_101/bin/jstat -gcutil $(pgrep java|head -1)|tail -1|awk  <span class="string">'&#123;print $6&#125;'</span></div><div class="line">UserParameter=fgc.counts,sudo /opt/jdk1.8.0_101/bin/jstat  -gcutil $(pgrep java|head -1)|tail -1|awk  <span class="string">'&#123;print $8&#125;'</span></div><div class="line">UserParameter=jvm.memory.usage,sudo /opt/jdk1.8.0_101/bin/jmap -histo $(pgrep java|sed -n <span class="string">'$p'</span>)|grep Total | sed -n <span class="string">'$p'</span> |awk  <span class="string">'&#123;print $3&#125;'</span></div><div class="line">UserParameter=jvm.instances.usage,sudo /opt/jdk1.8.0_101/bin/jmap -histo $(pgrep java|sed -n <span class="string">'$p'</span>)|grep Total | sed -n <span class="string">'$p'</span> |awk  <span class="string">'&#123;print $2&#125;'</span></div><div class="line">UserParameter=flume.monitor[*],sudo /bin/bash /etc/zabbix/monitor_flume.sh <span class="variable">$1</span></div></pre></td></tr></table></figure><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ZABBIX-monitoring-Flume.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/ZABBIX-monitoring-Flume1.png" alt=""></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;ZABBIX-monitoring-Flume&quot;&gt;&lt;a href=&quot;#ZABBIX-monitoring-Flume&quot; class=&quot;headerlink&quot; title=&quot;ZABBIX monitoring Flume&quot;&gt;&lt;/a&gt;ZABBIX monitoring Flume&lt;/h4&gt;&lt;p&gt;Flume本身提供了http, ganglia的监控服务，而我们目前主要使用zabbix做监控。因此，我们为Flume添加了zabbix监控模块，和sa的监控服务无缝融合。&lt;br&gt;另一方面，净化Flume的metrics。只将我们需要的metrics发送给zabbix，避免 zabbix server造成压力。目前我们最为关心的是Flume能否及时把应用端发送过来的日志写到Hdfs上， 对应关注的metrics为：&lt;/p&gt;
&lt;p&gt;Source : 接收的event数和处理的event数&lt;br&gt;Channel : Channel中拥堵的event数&lt;br&gt;Sink : 已经处理的event数&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.itzgeek.com/wp-content/uploads/2014/07/Zabbix.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="monitoring" scheme="http://blog.yancy.cc/categories/monitoring/"/>
    
    
      <category term="Zabbix monitoring" scheme="http://blog.yancy.cc/tags/Zabbix-monitoring/"/>
    
  </entry>
  
  <entry>
    <title>ELK架构梳理-之ES2.4双实例平滑升级至5.2.1踩坑并supervisor管理笔记</title>
    <link href="http://blog.yancy.cc/2017/09/14/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%10%E5%B9%B3%E5%8F%B0/Elasticsearch/ELK%E6%9E%B6%E6%9E%84%E6%A2%B3%E7%90%86-%E4%B9%8BES%202.4%E5%8F%8C%E5%AE%9E%E4%BE%8B%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7%E8%87%B35.2.1%E8%B8%A9%E5%9D%91%E5%B9%B6supervisor%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0/"/>
    <id>http://blog.yancy.cc/2017/09/14/日志分析平台/Elasticsearch/ELK架构梳理-之ES 2.4双实例平滑升级至5.2.1踩坑并supervisor管理笔记/</id>
    <published>2017-09-14T09:56:03.000Z</published>
    <updated>2018-04-21T11:55:23.532Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ELK架构梳理-之ES-2-4双实例平滑升级至5-2-1踩坑并supervisor管理笔记"><a href="#ELK架构梳理-之ES-2-4双实例平滑升级至5-2-1踩坑并supervisor管理笔记" class="headerlink" title="ELK架构梳理-之ES 2.4双实例平滑升级至5.2.1踩坑并supervisor管理笔记"></a>ELK架构梳理-之ES 2.4双实例平滑升级至5.2.1踩坑并supervisor管理笔记</h3><h4 id="ELK架构梳理："><a href="#ELK架构梳理：" class="headerlink" title="ELK架构梳理："></a>ELK架构梳理：</h4><p>实时日志分析作为掌握业务情况、故障分析排查的一个重要手段，目前使用最多最成熟的莫过于ELK方案，整体方案也有各种架构组合，像<code>rsyslog-&gt;ES-&gt;kibana、rsyslog-&gt;Redis-&gt;Logstash-&gt;ES-&gt;kibana、rsyslog-&gt;kafka-&gt;Logstash-&gt;ES-&gt;kibana</code>等等，复杂点的有spark的引用。</p><p>每种方案适合不同的应用场景，没有优劣之分，我目前用的是<code>rsyslog-&gt;kafka-&gt;Logstash-&gt;ES-&gt;kibana和rsyslog-&gt;rsyslog中继-&gt;kafka-&gt;Logstash-&gt;ES-&gt;kibana</code>方案，共5台ES（12核、64G、机械盘）每天索引10多亿条日志，包含<code>nginx、uwsgi、redis、php</code>开发日志等，运行比较健壮，每条索引日志精简后在10个字段左右，每天Primary Shard的索引量大概在600个G，考虑到性能问题，我们没要复制分片，同时着重做了ES集群的调优，日志保留7天。</p><p>从整体架构进行抽象总结，其实就是采集-&gt;清洗-&gt;索引-&gt;展现四个环节，再去考虑各环节中缓存、队列的使用，每个环节点用不同的软件来实现。下面介绍一下我目前方案集群的搭建和配置。</p><h4 id="ES集群方案平滑："><a href="#ES集群方案平滑：" class="headerlink" title="ES集群方案平滑："></a>ES集群方案平滑：</h4><p>ES老集群用的2.4.1版本，跑的比较好就一直没动，最近看资料ES5.X已经稳定，并且性能有较大提升，心里就发痒了，但由于业务要保持高可用的属性，就得想一个平滑升级的方案，最后想到了多实例过度的办法，5.X版本网上介绍配置变化较大，也做好了踩坑准备，确定好要升级后，立刻动手。</p><p>一、对应升级改造方案</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="number">1</span>. 使用端口<span class="number">9220</span>和<span class="number">9330</span> 安装并配置好新的ES5<span class="meta">.2</span><span class="meta">.1</span>实例</div><div class="line"><span class="number">2</span>. 关掉logstash并将ES2<span class="meta">.4</span><span class="meta">.1</span>实例堆栈调小重启（kafka保留<span class="number">3</span>个小时日志所以不会丢失</div><div class="line"><span class="number">3</span>. 启动ES5<span class="meta">.2</span><span class="meta">.1</span>并将logstash开启指向ES5<span class="meta">.2</span><span class="meta">.1</span></div><div class="line"><span class="number">4</span>. 安装新版kibana实例做好指向，老数据用http://host/old访问——&gt;ES5<span class="meta">.2</span><span class="meta">.1</span>配置调优。</div></pre></td></tr></table></figure><p>二、升级后统一用<a href="github：https://github.com/mlazarov/supervisord-monitor">supervisord-monitor管理</a><br>三、周末跑了一天ES的cpu、IO、heap内存使用率，es磁盘情况，集群健康监测和thread_pool的监控数据（需要了解的添加QQ群）<br>四、升级过程——编写了ES5.2.1的安装脚本如下</p><h4 id="集群脚本化部署："><a href="#集群脚本化部署：" class="headerlink" title="集群脚本化部署："></a>集群脚本化部署：</h4><p>之前用的rpm包，后考虑直接使用tar包安装，对于需要系统做的调优操作，直接编写自动化安装脚本，一键将所有系统参数配置后，将环境搭建好。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#/bin/sh</span></div><div class="line">id elasticsearch || useradd elasticsearch <span class="_">-s</span> /sbin/nologin   <span class="comment">#添加用户</span></div><div class="line">grep <span class="string">"* - nofile 512000"</span> /etc/security/limits.conf || <span class="built_in">echo</span>  <span class="string">"* - nofile 512000"</span>  &gt;&gt; /etc/security/limits.conf  <span class="comment">#修改文件描述符数量</span></div><div class="line">grep <span class="string">"elasticsearch - nproc unlimited"</span> /etc/security/limits.conf || <span class="built_in">echo</span> <span class="string">"elasticsearch - nproc unlimited"</span>   &gt;&gt; /etc/security/limits.conf  <span class="comment">#修改最大打开进程数数量</span></div><div class="line">grep <span class="string">"fs.file-max = 1024000"</span> /etc/sysctl.conf || <span class="built_in">echo</span> <span class="string">"fs.file-max = 1024000"</span>  &gt;&gt; /etc/sysctl.conf  <span class="comment">#修改系统文件描述符</span></div><div class="line">grep <span class="string">"vm.max_map_count = 262144"</span> /etc/sysctl.conf || <span class="built_in">echo</span> <span class="string">"vm.max_map_count = 262144"</span>  &gt;&gt;  /etc/sysctl.conf  <span class="comment">#修改程序最大管理的vm</span></div><div class="line">sysctl -p</div><div class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/src</div><div class="line">[ ! <span class="_">-f</span> /usr/<span class="built_in">local</span>/src/elasticsearch-5.2.1.zip ] &amp;&amp; wget </div><div class="line">https://artifacts.elastic.co/dow ... ticsearch-5.2.1.zip</div><div class="line">[ ! <span class="_">-d</span> /usr/<span class="built_in">local</span>/src/elasticsearch-5.2.1 ] &amp;&amp; unzip elasticsearch-5.2.1.zip</div><div class="line">mv elasticsearch-5.2.1 /usr/<span class="built_in">local</span>/</div><div class="line">chown -R elasticsearch:elasticsearch /usr/<span class="built_in">local</span>/elasticsearch-5.2.1  <span class="comment">#修改拥有者所有组</span></div><div class="line">sed -i <span class="string">'s/-XX:+UseConcMarkSweepGC/-XX:+UseG1GC/'</span> /usr/<span class="built_in">local</span>/elasticsearch-5.2.1/config/jvm.options    <span class="comment">#GC方式修改为G1</span></div><div class="line">sed -i <span class="string">'s/-XX:CMSInitiatingOccupancyFraction=75/-XX:MaxGCPauseMillis=200/'</span> /usr/<span class="built_in">local</span>/elasticsearch-5.2.1/config/jvm.options</div><div class="line">sed -i <span class="string">'s/-XX:+UseCMSInitiatingOccupancyOnly/#-XX:+UseCMSInitiatingOccupancyOnly/'</span> /usr/<span class="built_in">local</span>/elasticsearch-5.2.1/config/jvm.options</div></pre></td></tr></table></figure><h4 id="五、升级过程——配置文件、索引相关的更新调优"><a href="#五、升级过程——配置文件、索引相关的更新调优" class="headerlink" title="五、升级过程——配置文件、索引相关的更新调优"></a>五、升级过程——配置文件、索引相关的更新调优</h4><p>   升级期间着实踩了不少坑，老版ES索引配置可以直接写到配置文件里，新版是不行的，必须使用api去设置，另外ES2.X版本的进程数调优，在ES5.X我发现调整与否没有影响。配置文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">cluster.name: yz-5search</div><div class="line">path.data:  /data1/LogData5/</div><div class="line">path.logs:  /data1/LogData5/logs</div><div class="line">bootstrap.memory_lock: <span class="literal">false</span>   <span class="comment">#centos6内核不支持，必须要关闭</span></div><div class="line">bootstrap.system_call_filter: <span class="literal">false</span></div><div class="line">network.host: 10.39.40.94</div><div class="line">http.port: 9220</div><div class="line">transport.tcp.port: 9330</div><div class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"10.39.40.94:9330"</span>,<span class="string">"10.39.40.95:9330"</span>,<span class="string">"10.39.40.96:9330"</span>,<span class="string">"10.39.40.97:9330"</span>]</div><div class="line">discovery.zen.minimum_master_nodes: 2</div><div class="line">http.cors.enabled: <span class="literal">true</span></div><div class="line">http.cors.allow-origin: <span class="string">"*"</span></div></pre></td></tr></table></figure><p>为了加快索引效率，编写index的模板配置（index配置不允许写到配置文件了），将参数put到es的里，当然模板也可以通过前端logstash指定（要改logtash觉得麻烦），template脚本如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#/bin/sh</span></div><div class="line"><span class="comment">#index template</span></div><div class="line">curl -XPUT <span class="string">'http://10.39.40.94:9220/_template/cms_logs?pretty'</span> <span class="_">-d</span> <span class="string">'&#123;</span></div><div class="line">     "order": 6,                                    #优先级</div><div class="line">      "template": "logstash-cms*",                  #正则匹配索引</div><div class="line">      "settings": &#123;</div><div class="line">             "index.refresh_interval" : "60s",  #索引刷新时间</div><div class="line">             "index.number_of_replicas" : "0",  #副本数设置为0</div><div class="line">             "index.number_of_shards" : "8",    #分片数设置为8，共4台服务器</div><div class="line">             "index.translog.flush_threshold_size" : "768m",  #translog触发flush的阀值</div><div class="line">             "index.store.throttle.max_bytes_per_sec" : "500m", #存储的阀值</div><div class="line">             "index.translog.durability": "async",              #设置translog异步刷新到硬盘，更注重性能</div><div class="line">             "index.merge.scheduler.max_thread_count": "1",     #机械盘设置为1</div><div class="line">             "index.routing.allocation.total_shards_per_node": "2"  #每个节点上两个分片</div><div class="line">      &#125;</div><div class="line">&#125;'</div></pre></td></tr></table></figure><p>备：如果是更改，将PUT改为POST</p><p>日志保留7天，清除的脚本如下，写入计划任务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line">DATE=`date +%Y.%m.%d.%I`</div><div class="line">DATA2=`date +%Y.%m.%d <span class="_">-d</span><span class="string">'-7 day'</span>`</div><div class="line">curl -XDELETE <span class="string">"http://10.39.40.97:9220/logstash-*-<span class="variable">$&#123;DATA2&#125;</span>*?pretty"</span></div></pre></td></tr></table></figure><p>   由于单个索引达到了35G甚至40G以上，于是在logstash层面对建索引数量进行修改，把每天12个索引修改为每天24个索引：</p><p>logstash的修改如下：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">index</span> =&gt; <span class="string">"logstash-cms-front-nginx-%&#123;+YYYY.MM.dd.hh&#125;"</span>  修改为</div><div class="line"><span class="attr">index</span> =&gt; <span class="string">"logstash-cms-front-nginx-%&#123;+YYYY.MM.dd.HH&#125;"</span></div></pre></td></tr></table></figure><p><em>*更新自动化搭建es集群，架构梳理详解-与实现es监控服务</em></p><p>参考： Logstash分享,online生产环境的使用,online日志规范。</p><p><strong>☺待整理续写~~</strong> </p><h3 id="Communicative-learning"><a href="#Communicative-learning" class="headerlink" title="Communicative learning:"></a>Communicative learning:</h3><p>🐧  Linux shell_ senior operation and maintenance faction: QQ group <code>459096184</code> circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)<br>🐧  BigData-Exchange School:QQ group <code>521621407</code> circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join</p><p>Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ELK架构梳理-之ES-2-4双实例平滑升级至5-2-1踩坑并supervisor管理笔记&quot;&gt;&lt;a href=&quot;#ELK架构梳理-之ES-2-4双实例平滑升级至5-2-1踩坑并supervisor管理笔记&quot; class=&quot;headerlink&quot; title=&quot;EL
      
    
    </summary>
    
      <category term="Log Analysis Platform" scheme="http://blog.yancy.cc/categories/Log-Analysis-Platform/"/>
    
    
      <category term="logstash" scheme="http://blog.yancy.cc/tags/logstash/"/>
    
  </entry>
  
  <entry>
    <title>Logstash分享,online生产环境的使用,online日志规范。</title>
    <link href="http://blog.yancy.cc/2017/09/13/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%10%E5%B9%B3%E5%8F%B0/Elasticsearch/Logstash%E5%88%86%E4%BA%AB,online%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84%E4%BD%BF%E7%94%A8,online%E6%97%A5%E5%BF%97%E8%A7%84%E8%8C%83%E3%80%82/"/>
    <id>http://blog.yancy.cc/2017/09/13/日志分析平台/Elasticsearch/Logstash分享,online生产环境的使用,online日志规范。/</id>
    <published>2017-09-13T09:56:03.000Z</published>
    <updated>2017-12-28T13:56:50.461Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logstash分享-online生产环境的使用-online日志规范。"><a href="#Logstash分享-online生产环境的使用-online日志规范。" class="headerlink" title="Logstash分享,online生产环境的使用,online日志规范。"></a>Logstash分享,online生产环境的使用,online日志规范。</h3><p>写这篇文章，主要分享几点: 因为学所有学，既然学何不深度去了解~</p><ol><li>什么是Logstash？</li><li>logstash运行在什么环境下对应的版本是多少？</li><li>logstash工作原理？</li><li>online日志现在是如何规范？</li><li>如何写logstash收集conf文件？ </li></ol><h4 id="1-什么是Logstash？"><a href="#1-什么是Logstash？" class="headerlink" title="1. 什么是Logstash？"></a>1. 什么是Logstash？</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Logstash 是有管道输送能力的开源数据收集引擎。它可以动态地从分散的数据源收集数据，并且标准化数据输送到你选择的目的地。它是一款日志而不仅限于日志的搜集处理框架，将分散多样的数据搜集自定义处理并输出到指定位置。</div></pre></td></tr></table></figure><h4 id="2-logstash运行在什么环境下对应的版本是多少？"><a href="#2-logstash运行在什么环境下对应的版本是多少？" class="headerlink" title="2. logstash运行在什么环境下对应的版本是多少？"></a>2. logstash运行在什么环境下对应的版本是多少？</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">logstash更新比较快，跟es一样，2.4以上直接升级到5.0 </div><div class="line"></div><div class="line">5.0x以下版本运行环境 需要JDK1.7以上版本.</div><div class="line">5.0x版本运行环境 需要JDK1.8以上版本。</div><div class="line"></div><div class="line">安装方法很多：yum,rpm,tar.gz源码， 支持Docker镜像运行。</div></pre></td></tr></table></figure><h4 id="3-logstash工作原理？"><a href="#3-logstash工作原理？" class="headerlink" title="3. logstash工作原理？"></a>3. logstash工作原理？</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Logstash使用管道方式进行日志的搜集处理和输出。有点类似linux系统的管道命令 xxx | ccc | ddd，xxx执行完了会执行ccc，然后执行ddd。</div><div class="line"></div><div class="line">logstash收集日志基本流程: input--&gt;codec--&gt;filter--&gt;codec--&gt;output </div><div class="line">1.input:从哪里收集日志。 </div><div class="line">2.filter:发出去前进行过滤  （不是必须的）</div><div class="line">3.output:输出至Elasticsearch或Redis消息队列 </div><div class="line">4.codec:输出至前台，方便边实践边测试 </div><div class="line">5.数据量不大日志按照月来进行收集</div></pre></td></tr></table></figure><p><figure class="figure"><img src="https://www.elastic.co/guide/en/logstash/current/static/images/basic_logstash_pipeline.png" alt=""></figure></p><h4 id="4-日志现在收集规范："><a href="#4-日志现在收集规范：" class="headerlink" title="4.日志现在收集规范："></a>4.日志现在收集规范：</h4><p>是记录用户访问行为和服务运行状态的信息，是应用软件基本的输出单元，做到日志输出位置、命名、格式规范，可以大大方便后续应用服务监控和数据分析。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">1. 日志目录结构</div><div class="line">2. 日志类型</div><div class="line">3. 日志要求配置</div><div class="line">4. 日志级别</div><div class="line">5. 日志分割与周期</div><div class="line">6. 日志保留要求</div><div class="line"></div><div class="line">现在我们online 日志规范架构：</div><div class="line"></div><div class="line"><span class="comment">###之前应用日志规范：</span></div><div class="line"></div><div class="line">一个Tomcat服务logs目录下面的日志：定期对catalina.out几个G按两个小时进行压缩一次，保留7天，每天备份到<span class="built_in">log</span>-server服务器。</div><div class="line"></div><div class="line">logstash收集catalina.out所有日志。</div><div class="line"></div><div class="line"><span class="comment">###现在应用日志规范:</span></div><div class="line"></div><div class="line">一个Tomcat服务logs目录下面的日志：定期对catalina.out每天1M多日志进行压缩一次，保留7天，每天备份到<span class="built_in">log</span>-server服务器。</div><div class="line"></div><div class="line">logstash收集每台应用输出应用日志：error.log &amp; info.log</div><div class="line"></div><div class="line">好处分别为四个： </div><div class="line">1.对索引的要求细分和kibana查询日志速度无疑会变更快。</div><div class="line">2.查询日志快速定位。</div><div class="line">3.不会对catalina.out日志进行大级别日志写入，那里只存放系统日志，例如：发布日志，请求第三方地址日志。</div><div class="line">4.日志开发可以在Java代码<span class="built_in">log</span>4j文件大小指定压缩，每天定时清空，不需要我们写脚本处理。多个脚本定时在运行，特别乱。</div></pre></td></tr></table></figure><h4 id="5-如何写logstash收集conf文件？"><a href="#5-如何写logstash收集conf文件？" class="headerlink" title="5.如何写logstash收集conf文件？"></a>5.如何写logstash收集conf文件？</h4><p>下面是我写好的online logstash收集代码，根据之前日志收集方式，现在修过几个地方：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"> input &#123;</div><div class="line">       stdin&#123;&#125;  <span class="comment">#可以标准输入读数据 （可以放可以不放）</span></div><div class="line">file &#123;</div><div class="line">  <span class="built_in">type</span> =&gt; <span class="string">"tms-task-info"</span></div><div class="line">  path =&gt; [<span class="string">"/data/tms-task_new/logs/info.log"</span>]</div><div class="line">  start_position =&gt; <span class="string">"beginning"</span> <span class="comment">#从文件开始处读写</span></div><div class="line">     &#125;</div><div class="line">file &#123;</div><div class="line">  <span class="built_in">type</span> =&gt; <span class="string">"tms-task-error"</span></div><div class="line">  path =&gt; [<span class="string">"/data/tms-task_new/logs/error.log"</span>]</div><div class="line">  start_position =&gt; <span class="string">"beginning"</span> <span class="comment">#从文件开始处读写</span></div><div class="line">     &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">filter &#123; <span class="comment">#过滤方式</span></div><div class="line"></div><div class="line">        multiline &#123;</div><div class="line">                        pattern =&gt; <span class="string">"^\d+-\d+-\d+"</span></div><div class="line">                        negate =&gt; <span class="literal">true</span></div><div class="line">                        what =&gt; <span class="string">"previous"</span></div><div class="line">                &#125;</div><div class="line"></div><div class="line">       &#125;</div><div class="line">output &#123;</div><div class="line"><span class="keyword">if</span> [<span class="built_in">type</span>] == <span class="string">"tms-task-info"</span> &#123;</div><div class="line">  elasticsearch &#123;</div><div class="line">          hosts =&gt; [<span class="string">"10.155.90.141:9200"</span>,<span class="string">"10.155.90.176:9200"</span>]</div><div class="line">     index =&gt; <span class="string">"log-tms-task-info-%&#123;+YYYY.MM.dd&#125;"</span></div><div class="line">   document_type =&gt; <span class="string">"log"</span></div><div class="line">      template_overwrite =&gt; <span class="literal">true</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"> <span class="keyword">if</span> [<span class="built_in">type</span>] == <span class="string">"tms-task-error"</span> &#123;</div><div class="line">          elasticsearch &#123;</div><div class="line">          hosts =&gt; [<span class="string">"10.155.90.141:9200"</span>,<span class="string">"10.155.90.176:9200"</span>]</div><div class="line">          index =&gt; <span class="string">"log-tms-task-error-%&#123;+YYYY.MM.dd&#125;"</span></div><div class="line">          document_type =&gt; <span class="string">"log"</span></div><div class="line">          template_overwrite =&gt; <span class="literal">true</span></div><div class="line">        &#125;</div><div class="line">&#125;</div><div class="line">stdout&#123;</div><div class="line">    codec=&gt;rubydebug  <span class="comment">#控制台输出 (不建议配置，测试阶段可以调试使用)</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">有一些比较有用的配置项，可以用来指定 FileWatch 库的行为：</div><div class="line"></div><div class="line">discover_interval</div><div class="line">logstash 每隔多久去检查一次被监听的 path 下是否有新文件。默认值是 15 秒。</div><div class="line"></div><div class="line">exclude</div><div class="line">不想被监听的文件可以排除出去，这里跟 path 一样支持 glob 展开。</div><div class="line"></div><div class="line">sincedb_path</div><div class="line">如果你不想用默认的 <span class="variable">$HOME</span>/.sincedb(Windows 平台上在 C:\Windows\System32\config\systemprofile\.sincedb)，可以通过这个配置定义 sincedb 文件到其他位置。</div><div class="line"></div><div class="line">sincedb_write_interval</div><div class="line">logstash 每隔多久写一次 sincedb 文件，默认是 15 秒。</div><div class="line"></div><div class="line">stat_interval</div><div class="line">logstash 每隔多久检查一次被监听文件状态（是否有更新），默认是 1 秒。</div><div class="line"></div><div class="line">start_position</div><div class="line">logstash 从什么位置开始读取文件数据，默认是结束位置，也就是说 logstash 进程会以类似 tail -F 的形式运行。如果你是要导入原有数据，把这个设定改成 <span class="string">"beginning"</span>，logstash 进程就从头开始读取，有点类似 cat，但是读到最后一行不会终止，而是继续变成 tail -F。</div></pre></td></tr></table></figure><p>配置详解：参考中文文档<a href="https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/input/file.html" target="_blank" rel="external">logstash-best-practice-cn</a><br>官网详细说明：<a href="https://www.elastic.co/guide/en/logstash/current/multiple-input-output-plugins.html" target="_blank" rel="external">multiple-input-output-plugins</a></p><h3 id="Communicative-learning"><a href="#Communicative-learning" class="headerlink" title="Communicative learning:"></a>Communicative learning:</h3><p>🐧  Linux shell_ senior operation and maintenance faction: QQ group <code>459096184</code> circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)<br>🐧  BigData-Exchange School:QQ group <code>521621407</code> circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join</p><p>Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Logstash分享-online生产环境的使用-online日志规范。&quot;&gt;&lt;a href=&quot;#Logstash分享-online生产环境的使用-online日志规范。&quot; class=&quot;headerlink&quot; title=&quot;Logstash分享,online生产环
      
    
    </summary>
    
      <category term="Log Analysis Platform" scheme="http://blog.yancy.cc/categories/Log-Analysis-Platform/"/>
    
    
      <category term="logstash" scheme="http://blog.yancy.cc/tags/logstash/"/>
    
  </entry>
  
  <entry>
    <title>KafKa不懂就学就问就解答笔记</title>
    <link href="http://blog.yancy.cc/2017/09/11/Bigdata-hadoop/Kafka/KafKa%E4%B8%8D%E6%87%82%E5%B0%B1%E5%AD%A6%E5%B0%B1%E9%97%AE%E5%B0%B1%E8%A7%A3%E7%AD%94%E7%AC%94%E8%AE%B0/"/>
    <id>http://blog.yancy.cc/2017/09/11/Bigdata-hadoop/Kafka/KafKa不懂就学就问就解答笔记/</id>
    <published>2017-09-11T03:22:00.000Z</published>
    <updated>2017-12-03T06:44:53.305Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗"><a href="#1-部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗" class="headerlink" title="1. 部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗?"></a>1. 部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗?</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">答案：可以是可以，但为了容错还是部署zookeeper集群比较好。broker和zookeeper的对应比例倒是没什么，都是独立集群。</div></pre></td></tr></table></figure><p><figure class="figure"><img src="https://www.confluent.io/wp-content/uploads/2016/09/Event-sourced-based-architecture.jpeg" alt=""></figure></p><a id="more"></a><h5 id="2-kafka集群为什么需要zookeeper来配合？"><a href="#2-kafka集群为什么需要zookeeper来配合？" class="headerlink" title="2. kafka集群为什么需要zookeeper来配合？"></a>2. kafka集群为什么需要zookeeper来配合？</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">答案：zookeeper 是作为性能协调工具的角色存在。存储着你Kafka服务的一些些元数据（partitions、offset等等）。zookeeper集群的作用在于保证Zookeeper服务的高可用。因此你可以根据你的需要来选择是否构建zookeeper集群。</div></pre></td></tr></table></figure><h5 id="3-查看kafka-topic-消费记录报错WARN-Session-0x0-for-server-null-unexpected-error-closing-socket-connection-and-attempting-reconnect-org-apache-zookeeper-ClientCnxn"><a href="#3-查看kafka-topic-消费记录报错WARN-Session-0x0-for-server-null-unexpected-error-closing-socket-connection-and-attempting-reconnect-org-apache-zookeeper-ClientCnxn" class="headerlink" title="3. 查看kafka topic 消费记录报错WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)"></a>3. 查看kafka topic 消费记录报错WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)</h5><p>java.net.ConnectException: Connection timed out</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">问题：</div><div class="line">1.是kafka连不上zookeeper了。请检查 zk 集群是否正常能Telnet，kafka集群是否正常。</div><div class="line">2.检查server.properties 中zookeeper.connect是否配置正确，如果都没有问题，重新启动服务。</div></pre></td></tr></table></figure><h5 id="4-kafka-支持压缩传输吗？"><a href="#4-kafka-支持压缩传输吗？" class="headerlink" title="4. kafka 支持压缩传输吗？"></a>4. kafka 支持压缩传输吗？</h5><h5 id="5-Kafka-如何在开启数据压缩的情况下-consumer维护自己的offset"><a href="#5-Kafka-如何在开启数据压缩的情况下-consumer维护自己的offset" class="headerlink" title="5. Kafka 如何在开启数据压缩的情况下, consumer维护自己的offset?"></a>5. Kafka 如何在开启数据压缩的情况下, consumer维护自己的offset?</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">问题：</div><div class="line">kafka这边数据传输消费跨aws机房较慢。会有网络瓶颈,我们kafka在国外的一dalasi机房，消费端在dalasi。</div><div class="line"></div><div class="line">官网解答：</div><div class="line">Offset management on the consumer</div><div class="line"></div><div class="line">The data received by a consumer <span class="keyword">for</span> a topic might contain both compressed as well as uncompressed messages. The consumer iterator transparently decompresses compressed data and only returns an uncompressed message. The offset maintenance <span class="keyword">in</span> the consumer gets a little tricky. In the zookeeper consumer, the consumed offset is updated each time a message is returned. This consumed offset should be a valid fetch offset <span class="keyword">for</span> correct failure recovery. Since data is stored <span class="keyword">in</span> compressed format on the broker, valid fetch offsets are the compressed message boundaries. Hence, <span class="keyword">for</span> compressed data, the consumed offset will be advanced one compressed message at a time. This has the side effect of possible duplicates <span class="keyword">in</span> the event of a consumer failure. For uncompressed data, consumed offset will be advanced one message at a time.</div><div class="line"></div><div class="line">这段话不是很理解: producer将100条message压缩成1条发给broker后, broker是如何存储的，并且consumer是如何取出这压缩后的数据, 并维护offset的?</div><div class="line"></div><div class="line"><span class="comment">### 消息压缩（message compression）</span></div><div class="line"></div><div class="line">考虑到网络带宽的瓶颈，Kafka提供了消息组压缩特性。Kafka通过递归消息集来支持高效压缩。高效压缩需要多个消息同时压缩，而不是对每个消息单独压缩。一批消息压缩在一起发送给broker。压缩消息集降低了网络的负载，但是解压缩也带来了一些额外的开销。消息集的解压缩是由broker处理消息offset时完成的。</div><div class="line"></div><div class="line">每个消息可通过一个不可比较的、递增的逻辑offset访问，这个逻辑offset在每个分区内是唯一的。接收到压缩数据后，lead broker将消息集解压缩，为每个消息分配offset。offset分配完成后，leader再次将消息集压缩并写入磁盘。</div><div class="line"></div><div class="line">在Kafka中，数据的压缩由producer完成，可使用GZIP或Snappy压缩协议。同时需要在producer端配置相关的参数：</div><div class="line"></div><div class="line">compression.codec：指定压缩格式，默认为none，可选的值还有gzip和snappy。</div><div class="line">compressed.topics：设置对指定的topic开启压缩，默认为null。当compression.codec不为none时，对指定的topic开启压缩；如果compressed.topics为null则对所有topic开启压缩。</div><div class="line">消息集ByteBufferMessageSet可能既包含压缩数据也包含非压缩数据，为了区分开来，消息头中添加了压缩属性字节。在该字节中，最低位的两位表示压缩格式，如果都是0表示非压缩数据。</div></pre></td></tr></table></figure><h3 id="Communicative-learning"><a href="#Communicative-learning" class="headerlink" title="Communicative learning:"></a>Communicative learning:</h3><p>🐧  Linux shell_ senior operation and maintenance faction: QQ group <code>459096184</code> circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)<br>🐧  BigData-Exchange School:QQ group <code>521621407</code> circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join</p><p>Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;1-部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗&quot;&gt;&lt;a href=&quot;#1-部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗&quot; class=&quot;headerlink&quot; title=&quot;1. 部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗?&quot;&gt;&lt;/a&gt;1. 部署生产环境，打算部署三个broker实例，但zookeeper部署一个可以吗?&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;答案：可以是可以，但为了容错还是部署zookeeper集群比较好。broker和zookeeper的对应比例倒是没什么，都是独立集群。&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;https://www.confluent.io/wp-content/uploads/2016/09/Event-sourced-based-architecture.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/categories/Bigdata-Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-Zookeeper集群日志配置详解和清理自定义启动内存</title>
    <link href="http://blog.yancy.cc/2017/08/27/Bigdata-hadoop/zookeeper/Bigdata-Zookeeper%E9%9B%86%E7%BE%A4%E6%97%A5%E5%BF%97%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%E5%92%8C%E6%B8%85%E7%90%86%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%AF%E5%8A%A8%E5%86%85%E5%AD%98%20/"/>
    <id>http://blog.yancy.cc/2017/08/27/Bigdata-hadoop/zookeeper/Bigdata-Zookeeper集群日志配置详解和清理自定义启动内存 /</id>
    <published>2017-08-27T06:46:00.000Z</published>
    <updated>2017-09-26T09:41:46.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Zookeeper集群日志配置详解和清理自定义启动内存"><a href="#Zookeeper集群日志配置详解和清理自定义启动内存" class="headerlink" title="Zookeeper集群日志配置详解和清理自定义启动内存"></a>Zookeeper集群日志配置详解和清理自定义启动内存</h4><h3 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h3><p>搭建zookeeper和kafka集群运行大数据处理数据消费，公司dubbo使用zookeeper做服务端的服务发现管理及配置中心，在使用时都出现过由于zk的日志大小过大塞满磁盘的情况 ，遇到了Zookeeper日志问题输出路径的问题, 发现zookeeper设置log4j.properties不能解决日志路径问题, 发现解决方案如下。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/zookeeper_cartoon.jpg" alt=""></figure></p><h3 id="zookeeper日志说明"><a href="#zookeeper日志说明" class="headerlink" title="zookeeper日志说明"></a>zookeeper日志说明</h3><p>ZooKeeper使用<code>SLF4J(the Simple Logging Facade for Java)</code>作为日志的抽象层，默认使用<code>Log4J</code>来做实际的日志工作。使用2层日志抽象看起来真是够呛，这里简要的说明如何来配置<code>Log4J</code>。尽管Log4J非常灵活且强大，但它也有一些复杂，可以用一整本书来描述它，这里只是简要的介绍一下基本的用法。</p><a id="more"></a><p>Log4J的配置文件名为<code>log4j.properties</code>，从classpath中查找。如果没有找到log4j.properties文件，会输出如下警告信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">log</span>4j:WARN No appenders could be found <span class="keyword">for</span> logger (org.apache.zookeeper.serv ...  </div><div class="line"><span class="built_in">log</span>4j:WARN Please initialize the <span class="built_in">log</span>4j system properly.</div></pre></td></tr></table></figure><p>它说的是所有后续的日志消息会被丢弃，通常<code>log4j.properties</code>文件会放在<code>conf</code>文件夹，并放在<code>classpath</code>下。<br>来看看<code>ZooKeeper</code>使用的<code>log4j.properties</code>的主要部分：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka2 conf]$ cat <span class="built_in">log</span>4j.properties | grep -Pv <span class="string">"^$|^#"</span></div><div class="line">zookeeper.root.logger=INFO, CONSOLE  <span class="comment">#（1）</span></div><div class="line">zookeeper.console.threshold=INFO</div><div class="line">zookeeper.log.dir=.</div><div class="line">zookeeper.log.file=zookeeper.log</div><div class="line">zookeeper.log.threshold=DEBUG</div><div class="line">zookeeper.tracelog.dir=.                                                                     </div><div class="line">zookeeper.tracelog.file=zookeeper_trace.log</div><div class="line"><span class="built_in">log</span>4j.rootLogger=<span class="variable">$&#123;zookeeper.root.logger&#125;</span> <span class="comment">#（2）</span></div><div class="line"><span class="built_in">log</span>4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender  <span class="comment">#（3）</span></div><div class="line"><span class="built_in">log</span>4j.appender.CONSOLE.Threshold=<span class="variable">$&#123;zookeeper.console.threshold&#125;</span> <span class="comment">#（4）</span></div><div class="line"><span class="built_in">log</span>4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout <span class="comment">#（5）</span></div><div class="line"><span class="built_in">log</span>4j.appender.CONSOLE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L] - %m%n</div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender <span class="comment">#（6）</span></div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.Threshold=<span class="variable">$&#123;zookeeper.log.threshold&#125;</span> <span class="comment">#（7）</span></div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.File=<span class="variable">$&#123;zookeeper.log.dir&#125;</span>/<span class="variable">$&#123;zookeeper.log.file&#125;</span></div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.MaxFileSize=10MB</div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout</div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L] - %m%n</div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE=org.apache.log4j.FileAppender</div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE.Threshold=TRACE</div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE.File=<span class="variable">$&#123;zookeeper.tracelog.dir&#125;</span>/<span class="variable">$&#123;zookeeper.tracelog.file&#125;</span></div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayout</div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L][%x] - %m%n</div></pre></td></tr></table></figure><ul><li><p>(1) <code>zookeeper.root.logger=INFO, CONSOLE</code><br>第一组设置以<code>zookeeper</code>开头，它们实际上是<code>Java system property</code>，可以被<code>-D</code>形式的命令行参数覆盖。<br>第一行配置了日志级别，默认的设置是说在INFO级别以下的日志会被丢弃，并且日志会使用<code>CONSOLE appender</code>输出。你可以指定多个<code>appender</code>，例如如果你想使用<code>CONSOLE appender</code>和<code>ROLLINGFILE appender</code>输出日志，那么可以配置<code>zookeeper.logger</code>为<code>INFO,CONSOLE,ROLLINGFILE</code>。</p></li><li><p>(2) <code>rootLogger</code>处理所有日志的<code>logger</code>，因为我们没有定义其他<code>logger</code>。</p></li><li><p>(3) <code>log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender</code>这一行把CONSOLE appender和实际上处理日志输出的类绑定在一起，这里是ConsoleAppender类。</p></li><li><p>(4) <code>appender</code>也可以过滤日志。这一行将过滤任何在INFO级别之下的日志，因为这是在<code>zookeeper.root.logger设置的threshold</code>。</p></li><li><p>(5) <code>appender</code>使用一个布局(layout)类在输出前对日志进行格式化。我们使用pattern layout来记录日志的级别，日期，线程信息和调用位置信息以及消息本身。</p></li><li><p>(6) RollingFileAppender实现了rolling日志文件的功能，而不是持续的写到一个单独的文件或者控制台。如果rootLogger没有关联ROLLINGFILE，则此appender会被忽略。</p></li><li><p>(7) <code>ROLLINGFILE</code>的threshold设置成<code>DEBUG</code>。因为<code>rootLogger</code>过滤了所有在<code>INFO</code>级别之下的日志，没有DEBUG日志会输出到<code>ROLLINGFILE</code>。如果你想要看到<code>DEBUG</code>日志，你必须把<code>zookeeper.root.logger</code>从<code>INFO改成DEBUG</code>。</p></li></ul><p>打日志会影响到进程的性能，尤其是在DEBUG级别下。同时日志会提供有价值的信息为诊断错误提供线索。一个平衡性能开销的有效方式是把appender的threshold设成DEBUG，并把rootLogger设成WARN级别，这在一般的情况都适用，一般只需要关注WARNING和它之上的日志。当你需要诊断问题时可以使用JMX动态设置为INFO或DEBUG级别，这样可以更方便定位问题。</p><h3 id="快照事物日志-修改日志输出目录"><a href="#快照事物日志-修改日志输出目录" class="headerlink" title="快照事物日志,修改日志输出目录"></a>快照事物日志,修改日志输出目录</h3><p>之前出现<code>zookeeper</code>在bin目录下出现了<code>zookeeper.out</code>的日志文件，经分析发现此文件是由于<code>nohup</code>命令打印的控制台日志。</p><p>但是，我们在<code>zoo.cfg</code>配置文件中，对日志文件进行了配置（截取部分）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataDir=/data/tools/zookeeper-3.4.5/tmp</div><div class="line">dataLogDir=/data/tools/zookeeper-3.4.5/tmp/logs</div></pre></td></tr></table></figure><p>其中，<code>dataDir和dataLogDir</code>是针对数据信息及数据信息日志的位置配置。但是在zookeeper内部集成了<code>log4j.properties</code>（对应配置文件在conf路径下）。</p><h3 id="参考log4j配置说明"><a href="#参考log4j配置说明" class="headerlink" title="参考log4j配置说明"></a>参考log4j配置说明</h3><p>打开<code>log4j.properties</code>文件，我们会发现有这样的配置，它在说明关于<code>zookeeper</code>本身的一些默认设置，但是可以被系统配置文件所覆盖。那么，在<code>log4j</code>中，<code>root是log4j</code>记录的原始起点，而这部分参数又可以被系统修改，那么系统配置在什么地方呢？</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define some default values that can be overridden by system properties</span></div><div class="line">zookeeper.root.logger=INFO, CONSOLE</div><div class="line">zookeeper.console.threshold=INFO</div><div class="line">zookeeper.log.dir=.</div><div class="line">zookeeper.log.file=zookeeper.log</div><div class="line">zookeeper.log.threshold=DEBUG</div><div class="line">zookeeper.tracelog.dir=.</div><div class="line">zookeeper.tracelog.file=zookeeper_trace.log</div></pre></td></tr></table></figure><p>zk日志.out及log4j日志路径配置 ：首先修改<code>bin/zkEnv.sh</code>，配置<code>ZOO_LOG_DIR</code>的环境变量，<code>ZOO_LOG_DIR</code>是zookeeper日志输出目录，<code>ZOO_LOG4J_PROP</code>是log4j日志输出的配置：</p><ul><li>默认配置：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> = <span class="string">"x"</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    ZOO_LOG_DIR=<span class="string">"."</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> = <span class="string">"x"</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    ZOO_LOG4J_PROP=<span class="string">"INFO,CONSOLE"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><ul><li>生产环境修改：</li></ul><p>在zk目录下面创建logs目录 给予bi组操作权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo mkdir /data/tools/zookeeper-3.4.5/logs/</div><div class="line">sudo chown -R jollybi:jollybi /data/tools/zookeeper-3.4.5/logs/</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> = <span class="string">"x"</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    ZOO_LOG_DIR=<span class="string">"<span class="variable">$ZOOBINDIR</span>/../logs"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> = <span class="string">"x"</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    ZOO_LOG4J_PROP=<span class="string">"INFO,ROLLINGFILE"</span>  //ROLLINGFILE —— 日志轮转，避免单一文件过大</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><h3 id="Zk设置定期自动清理日志"><a href="#Zk设置定期自动清理日志" class="headerlink" title="Zk设置定期自动清理日志."></a>Zk设置定期自动清理日志.</h3><p>从<code>3.4.0</code>开始，zookeeper提供了自动清理<code>snapshot</code>和事务日志的功能，通过配置 <code>autopurge.snapRetainCount</code> 和 <code>autopurge.purgeInterval</code> 这两个参数能够实现定时清理了。这两个参数都是在zoo.cfg中配置的：</p><ul><li>在zoo.cfg中配置：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">autopurge.purgeInterval: 24*2     </div><div class="line"><span class="comment">##这个参数指定了清理频率，单位是小时。默认是0，表示不开启自己清理功能。</span></div><div class="line">autopurge.snapRetainCount:  2</div><div class="line"><span class="comment">##这个参数和上面的参数搭配使用，这个参数指定了需要保留的文件数目。默认是保留3个。</span></div></pre></td></tr></table></figure><h3 id="配置zookeeper-out的位置及log4j日志输出"><a href="#配置zookeeper-out的位置及log4j日志输出" class="headerlink" title="配置zookeeper.out的位置及log4j日志输出"></a>配置zookeeper.out的位置及log4j日志输出</h3><h6 id="1-zookeeper-out由nohup的输出，也就是zookeeper的stdout和stdeer输出。"><a href="#1-zookeeper-out由nohup的输出，也就是zookeeper的stdout和stdeer输出。" class="headerlink" title="1. zookeeper.out由nohup的输出，也就是zookeeper的stdout和stdeer输出。"></a>1. <code>zookeeper.out</code>由nohup的输出，也就是<code>zookeeper</code>的<code>stdout和stdeer</code>输出。</h6><p>在zkServer.sh中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ ! -w <span class="string">"<span class="variable">$ZOO_LOG_DIR</span>"</span> ] ; <span class="keyword">then</span>  </div><div class="line">mkdir -p <span class="string">"<span class="variable">$ZOO_LOG_DIR</span>"</span>  </div><div class="line"><span class="keyword">fi</span>  </div><div class="line">  </div><div class="line">_ZOO_DAEMON_OUT=<span class="string">"<span class="variable">$ZOO_LOG_DIR</span>/zookeeper.out"</span>  <span class="comment">#日志输出文件路径</span></div><div class="line"></div><div class="line"><span class="comment">#nohup日志输出</span></div><div class="line">nohup <span class="variable">$JAVA</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \  </div><div class="line">-cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$JVMFLAGS</span> <span class="variable">$ZOOMAIN</span> <span class="string">"<span class="variable">$ZOOCFG</span>"</span> &gt; <span class="string">"<span class="variable">$_ZOO_DAEMON_OUT</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</div></pre></td></tr></table></figure><h6 id="2-log4j日志输出配置-conf-log4j-properties中："><a href="#2-log4j日志输出配置-conf-log4j-properties中：" class="headerlink" title="2.log4j日志输出配置 conf/log4j.properties中："></a>2.log4j日志输出配置 conf/log4j.properties中：</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Add ROLLINGFILE to rootLogger to get log file output  </span></div><div class="line"><span class="comment">#    Log DEBUG level and above messages to a log file  </span></div><div class="line"></div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender  //日志轮转，DaliyRollingFileAppender —— 按天轮转</div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.Threshold=<span class="variable">$&#123;zookeeper.log.threshold&#125;</span>  </div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.File=<span class="variable">$&#123;zookeeper.log.dir&#125;</span>/<span class="variable">$&#123;zookeeper.log.file&#125;</span></div></pre></td></tr></table></figure><p>轮转前提需要将(1)里bin/zkEnv.sh中的轮转配置好</p><p>4.zk事务日志查看</p><p>zookeeper的事务日志通过zoo.cfg文件中的dataLogDir配置项配置：</p><pre><code># the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/tmp/zookeeper    </code></pre><p>查看事务日志方法：<br>(需要下载slf4j-api-1.6.1.jar包) </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -classpath .:slf4j-api-1.6.1.jar:zookeeper-3.4.5.jar org.apache.zookeeper.server.LogFormatter /tmp/zookeeper/version-2/xxx.xxx</div></pre></td></tr></table></figure><h3 id="怎么自定义zookeeper的启动内存"><a href="#怎么自定义zookeeper的启动内存" class="headerlink" title="怎么自定义zookeeper的启动内存"></a>怎么自定义zookeeper的启动内存</h3><p>运行zookeeper时，使用jmap -heap <pid> 命令查看内存情况如下:</pid></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/zookeeper_01.png" alt=""></figure></p><p>解决：分配内存文件路径：<code>zookeeper/bin/zkEnv.sh</code></p><p>该文件已经明确说明有独立JVM内存的设置文件，路径是<code>zookeeper/conf/java.env</code><br>安装的时候这个路径下没有有<code>java.env</code>文件，需要自己新建一个：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># vim java.env</span></div><div class="line"></div><div class="line"><span class="meta">#!/bin/sh</span></div><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk</div><div class="line"><span class="comment"># heap size MUST be modified according to cluster environment</span></div><div class="line"><span class="built_in">export</span> JVMFLAGS=<span class="string">"-Xms4g -Xmx4g <span class="variable">$JVMFLAGS</span>"</span></div><div class="line"></div><div class="line">对于内存的分配，还是根据项目和机器情况而定。如果内存够用，适当的大点可以提升zk性能。</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Zookeeper集群日志配置详解和清理自定义启动内存&quot;&gt;&lt;a href=&quot;#Zookeeper集群日志配置详解和清理自定义启动内存&quot; class=&quot;headerlink&quot; title=&quot;Zookeeper集群日志配置详解和清理自定义启动内存&quot;&gt;&lt;/a&gt;Zookeeper集群日志配置详解和清理自定义启动内存&lt;/h4&gt;&lt;h3 id=&quot;问题：&quot;&gt;&lt;a href=&quot;#问题：&quot; class=&quot;headerlink&quot; title=&quot;问题：&quot;&gt;&lt;/a&gt;问题：&lt;/h3&gt;&lt;p&gt;搭建zookeeper和kafka集群运行大数据处理数据消费，公司dubbo使用zookeeper做服务端的服务发现管理及配置中心，在使用时都出现过由于zk的日志大小过大塞满磁盘的情况 ，遇到了Zookeeper日志问题输出路径的问题, 发现zookeeper设置log4j.properties不能解决日志路径问题, 发现解决方案如下。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xrthw.com1.z0.glb.clouddn.com/zookeeper_cartoon.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;zookeeper日志说明&quot;&gt;&lt;a href=&quot;#zookeeper日志说明&quot; class=&quot;headerlink&quot; title=&quot;zookeeper日志说明&quot;&gt;&lt;/a&gt;zookeeper日志说明&lt;/h3&gt;&lt;p&gt;ZooKeeper使用&lt;code&gt;SLF4J(the Simple Logging Facade for Java)&lt;/code&gt;作为日志的抽象层，默认使用&lt;code&gt;Log4J&lt;/code&gt;来做实际的日志工作。使用2层日志抽象看起来真是够呛，这里简要的说明如何来配置&lt;code&gt;Log4J&lt;/code&gt;。尽管Log4J非常灵活且强大，但它也有一些复杂，可以用一整本书来描述它，这里只是简要的介绍一下基本的用法。&lt;/p&gt;
    
    </summary>
    
      <category term="Bigdata" scheme="http://blog.yancy.cc/categories/Bigdata/"/>
    
    
      <category term="ZooKeeper" scheme="http://blog.yancy.cc/tags/ZooKeeper/"/>
    
  </entry>
  
  <entry>
    <title>Monitor Kafka with Prometheus +Grafana</title>
    <link href="http://blog.yancy.cc/2017/08/04/Bigdata-hadoop/Kafka/Monitor%20Kafka%20with%20Prometheus%20+Grafana/"/>
    <id>http://blog.yancy.cc/2017/08/04/Bigdata-hadoop/Kafka/Monitor Kafka with Prometheus +Grafana/</id>
    <published>2017-08-04T03:22:00.000Z</published>
    <updated>2017-10-20T06:02:32.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Monitoring-Kafka-with-Prometheus"><a href="#Monitoring-Kafka-with-Prometheus" class="headerlink" title="Monitoring Kafka with Prometheus"></a>Monitoring Kafka with Prometheus</h3><p>We’ve previously looked at how to monitor Cassandra with <a href="https://www.robustperception.io/monitoring-cassandra-with-prometheus/" target="_blank" rel="external">Prometheus</a>. Let’s see the process for getting metrics from another popular Java application, <a href="https://kafka.apache.org/" target="_blank" rel="external">Kafka.</a></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-overview.png" alt=""></figure></p><a id="more"></a><p>we download Kafka, the JMX exporter and the config file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">wget http://ftp.heanet.ie/mirrors/www.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz</div><div class="line">tar -xzf kafka _ *。tgz</div><div class="line"><span class="built_in">cd</span> kafka_ *</div><div class="line"></div><div class="line">wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.6/jmx_prometheus_javaagent-0.6.jar</div><div class="line">wget https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/kafka-0-8-2.yml</div><div class="line"></div><div class="line">Download the good files and put them under the Kafka directory:</div><div class="line">[hadoop@hadoop6 kafka_2.10-0.9.0.1]$ ll</div><div class="line">drwxr-xr-x 3 hadoop hadoop    4096 9月  27 13:34 bin</div><div class="line">drwxr-xr-x 2 hadoop hadoop    4096 9月  27 21:18 config</div><div class="line">-rw-rw-r-- 1 hadoop hadoop 1225418 2月   5 2016 jmx_prometheus_javaagent-0.6.jar</div><div class="line">-rw-rw-r-- 1 hadoop hadoop    2824 9月  26 17:48 kafka-0-8-2.yml</div><div class="line">drwxr-xr-x 2 hadoop hadoop    4096 9月  27 13:31 libs</div><div class="line">-rw-r--r-- 1 hadoop hadoop   11358 2月  12 2016 LICENSE</div><div class="line">drwxrwxr-x 2 hadoop hadoop  266240 9月  27 21:19 logs</div><div class="line">-rw-r--r-- 1 hadoop hadoop     162 2月  12 2016 NOTICE</div><div class="line">drwxrwxr-x 5 hadoop hadoop    4096 9月  27 15:56 prometheus-1.2.1.linux-amd64</div><div class="line">drwxr-xr-x 2 hadoop hadoop    4096 2月  12 2016 site-docs</div></pre></td></tr></table></figure><p>We start a Zookeeper (a Kafka dependency) and Kafka with the JMX exporter running as a Java agent:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">First step:</div><div class="line">Configure the zookeeper zoo.zoo.cfg port: 2181</div><div class="line">Start zookeeper:</div><div class="line">./zookeeper-3.4.6/bin/zkServer.sh start</div><div class="line"></div><div class="line">Second steps:</div><div class="line">Configuration modification: kafka_2.10-0.9.0.1/config/zookeeper.properties</div><div class="line">Port modification: 2182 <span class="keyword">do</span> not conflict with the zookeeper service port</div><div class="line"></div><div class="line"></div><div class="line">dataDir=/data2/zookeeper-3.4.6/zookeeper-data</div><div class="line"><span class="comment"># the port at which the clients will connect</span></div><div class="line">clientPort=2181</div><div class="line"><span class="comment"># disable the per-ip limit on the number of connections since this is a non-production config</span></div><div class="line">maxClientCnxns=0</div><div class="line"></div><div class="line">./bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</div><div class="line"></div><div class="line">KAFKA_OPTS=<span class="string">"<span class="variable">$KAFKA_OPTS</span> -javaagent:<span class="variable">$PWD</span>/jmx_prometheus_javaagent-0.6.jar=7071:<span class="variable">$PWD</span>/kafka-0-8-2.yml"</span> \</div><div class="line">./bin/kafka-server-start.sh config/server.properties &amp;</div></pre></td></tr></table></figure><h4 id="If-you-visit-http-localhost-7071-metrics-you’ll-see-the-metrics"><a href="#If-you-visit-http-localhost-7071-metrics-you’ll-see-the-metrics" class="headerlink" title="If you visit http://localhost:7071/metrics you’ll see the metrics."></a>If you visit <a href="http://localhost:7071/metrics" target="_blank" rel="external">http://localhost:7071/metrics</a> you’ll see the metrics.</h4><p>Let’s setup a quick Prometheus server:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">wget https://github.com/prometheus/prometheus/releases/download/v1.2.1/prometheus-1.2.1.linux-amd64.tar.gz</div><div class="line">tar -xzf prometheus-*.tar.gz</div><div class="line"><span class="built_in">cd</span> prometheus-*</div><div class="line">cat &lt;&lt;<span class="string">'EOF'</span> &gt; prometheus.yml</div><div class="line">global:</div><div class="line"> scrape_interval: 10s</div><div class="line"> evaluation_interval: 10s</div><div class="line">scrape_configs:</div><div class="line"> - job_name: <span class="string">'kafka'</span></div><div class="line">   static_configs:</div><div class="line">    - targets:</div><div class="line">      - localhost:7071</div><div class="line">EOF</div><div class="line">./prometheus</div></pre></td></tr></table></figure><p>Following run access: <a href="http://localhost:9090/graph" target="_blank" rel="external">http://localhost:9090/graph</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop8 prometheus-1.2.1.linux-amd64]$ ./prometheus</div><div class="line">INFO[0000] Starting prometheus (version=1.2.1, branch=master, revision=dd66f2e94b2b662804b9aa1b6a50587b990ba8b7)  <span class="built_in">source</span>=main.go:75</div><div class="line">INFO[0000] Build context (go=go1.7.1, user=root@fd9b0daff6bd, date=20161010-15:58:23)  <span class="built_in">source</span>=main.go:76</div><div class="line">INFO[0000] Loading configuration file prometheus.yml     <span class="built_in">source</span>=main.go:247</div><div class="line">INFO[0000] Loading series map and head chunks...         <span class="built_in">source</span>=storage.go:354</div><div class="line">INFO[0000] 49 series loaded.                             <span class="built_in">source</span>=storage.go:359</div><div class="line">WARN[0000] No AlertManagers configured, not dispatching any alerts  <span class="built_in">source</span>=notifier.go:176</div><div class="line">INFO[0000] Listening on :9090                            <span class="built_in">source</span>=web.go:240</div><div class="line">INFO[0000] Starting target manager...                    <span class="built_in">source</span>=targetmanager.go:76</div></pre></td></tr></table></figure><p>The Prometheus platform monitors all data from the Kafka index:<br>This function is very powerful, late will explain what each representative means:</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-monitor02.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-monitor03.png" alt=""></figure></p><p>Install <a href="http://docs.grafana.org/installation/rpm/#install-stable" target="_blank" rel="external">open source Grafana monitoring</a>, combine <a href="https://prometheus.io/docs/visualization/grafana/" target="_blank" rel="external">prometheus.io</a> to obtain Prometheus platform data.</p><h3 id="Installing-Grafana"><a href="#Installing-Grafana" class="headerlink" title="Installing Grafana"></a>Installing Grafana</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Download and unpack Grafana from binary tar (adjust version as appropriate).</span></div><div class="line">curl -L -O https://grafanarel.s3.amazonaws.com/builds/grafana-2.5.0.linux-x64.tar.gz</div><div class="line">tar zxf grafana-2.5.0.linux-x64.tar.gz</div><div class="line"></div><div class="line"><span class="comment"># Start Grafana.</span></div><div class="line"><span class="built_in">cd</span> grafana-2.5.0/</div><div class="line">./bin/grafana-server web</div></pre></td></tr></table></figure><h3 id="Using"><a href="#Using" class="headerlink" title="Using"></a>Using</h3><p>By default, Grafana will be listening on <a href="http://localhost:3000" target="_blank" rel="external">http://localhost:3000</a>. The default login is “admin” / “admin”.</p><h4 id="Creating-a-Prometheus-data-source"><a href="#Creating-a-Prometheus-data-source" class="headerlink" title="Creating a Prometheus data source"></a>Creating a Prometheus data source</h4><p>To create a Prometheus data source:</p><ol><li>Click on the Grafana logo to open the sidebar menu.</li><li>Click on “Data Sources” in the sidebar.</li><li>Click on “Add New”.</li><li>Select “Prometheus” as the type.</li><li>Set the appropriate Prometheus server URL (for example, <a href="http://localhost:9090/" target="_blank" rel="external">http://localhost:9090/</a>)</li><li>Adjust other data source settings as desired (for example, turning the proxy access off).</li><li>Click “Add” to save the new data source.</li><li>The following shows an example data source configuration:</li></ol><p>Reference address: <a href="https://prometheus.io/docs/visualization/grafana/" target="_blank" rel="external">GRAFANA SUPPORT FOR PROMETHEUS</a></p><p><figure class="figure"><img src="https://prometheus.io/assets/grafana_configuring_datasource-cb0e78b7cfa.png" alt=""></figure></p><p>Finally load the <a href="https://grafana.com/dashboards/721" target="_blank" rel="external">Kafka Overview</a> dashboard from grafana.net into your Grafana to get the above console!</p><p>If you want to run Kafka inside docker, there’s <a href="https://www.robustperception.io/monitoring-kafka-in-a-docker-container-using-prometheus/" target="_blank" rel="external">another blog post</a> covering that.</p><p>Reference address: <a href="https://www.robustperception.io/monitoring-kafka-with-prometheus/" target="_blank" rel="external">monitoring-kafka-with-prometheus</a></p><h3 id="Communicative-learning"><a href="#Communicative-learning" class="headerlink" title="Communicative learning:"></a>Communicative learning:</h3><p>🐧  Linux shell_ senior operation and maintenance faction: QQ group <code>459096184</code> circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)<br>🐧  BigData-Exchange School:QQ group <code>521621407</code> circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join</p><p>Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Monitoring-Kafka-with-Prometheus&quot;&gt;&lt;a href=&quot;#Monitoring-Kafka-with-Prometheus&quot; class=&quot;headerlink&quot; title=&quot;Monitoring Kafka with Prometheus&quot;&gt;&lt;/a&gt;Monitoring Kafka with Prometheus&lt;/h3&gt;&lt;p&gt;We’ve previously looked at how to monitor Cassandra with &lt;a href=&quot;https://www.robustperception.io/monitoring-cassandra-with-prometheus/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Prometheus&lt;/a&gt;. Let’s see the process for getting metrics from another popular Java application, &lt;a href=&quot;https://kafka.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Kafka.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xrthw.com1.z0.glb.clouddn.com/kafka-overview.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/categories/Bigdata-Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
      <category term="Prometheus" scheme="http://blog.yancy.cc/tags/Prometheus/"/>
    
      <category term="Grafana" scheme="http://blog.yancy.cc/tags/Grafana/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-Kafka-node模块实现调用js发送数据</title>
    <link href="http://blog.yancy.cc/2017/08/01/Bigdata-hadoop/Kafka/Kafka-node%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E8%B0%83%E7%94%A8js%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE/"/>
    <id>http://blog.yancy.cc/2017/08/01/Bigdata-hadoop/Kafka/Kafka-node模块实现调用js发送数据/</id>
    <published>2017-08-01T06:46:00.000Z</published>
    <updated>2017-11-21T02:04:41.471Z</updated>
    
    <content type="html"><![CDATA[<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/streams-interactive-queries-02.png" alt=""></figure></p><p>mongodb写到kafka 指定topic消费。为了保证数据稳定可靠性。<br>配合大数据在countly 使用开源<code>Kafka-node</code>是一个Node.js客户端 写js程序让countly三台集群分别数据到kafka 做新的topic主题备份。</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install kafka-node</div></pre></td></tr></table></figure><p>进入kafka-node目录: vim kafka_test.js</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">var kafka = require(<span class="string">'kafka-node'</span>),</div><div class="line">HighLevelProducer = kafka.HighLevelProducer,</div><div class="line">    //Producer = kafka.Producer,</div><div class="line">    client = new kafka.Client(<span class="string">'169.44.62.139:2281,169.44.59.138:2281,169.44.62.137:2281'</span>),</div><div class="line">    //producer = new Producer(client);</div><div class="line">producer = new HighLevelProducer(client);</div><div class="line"></div><div class="line">console.log(<span class="string">'连接kafka中'</span>);</div><div class="line"></div><div class="line">var argv = &#123;</div><div class="line">    topic: <span class="string">"test1"</span></div><div class="line">&#125;;</div><div class="line">var topic = argv.topic || <span class="string">'test1'</span>;</div><div class="line">var p = argv.p || 0;</div><div class="line">var a = argv.a || 0;</div><div class="line">var producer = new HighLevelProducer(client, &#123;</div><div class="line">    requireAcks: 1,</div><div class="line">    partitionerType: 3</div><div class="line">&#125;);</div><div class="line"></div><div class="line">console.log(producer);</div><div class="line"></div><div class="line">producer.on(<span class="string">'ready'</span>, <span class="function"><span class="title">function</span></span>() &#123;</div><div class="line">    var args = &#123;</div><div class="line">        appid: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        createTime: <span class="string">'222-ddd'</span>,</div><div class="line">        toUserName: <span class="string">'222-wx238c28839a133d0e'</span>,</div><div class="line">        fromUserName: <span class="string">'222-wx238c28839a133d0e'</span></div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    producer.send([&#123;</div><div class="line">        topic: topic,</div><div class="line">        partition: p,</div><div class="line">        messages: [JSON.stringify(args)],</div><div class="line">        attributes: a</div><div class="line">    &#125;], <span class="keyword">function</span>(err, result) &#123;</div><div class="line">        console.log(err || result);</div><div class="line">        process.exit();</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    console.log(args);</div><div class="line">&#125;);</div></pre></td></tr></table></figure><h4 id="官网地址：https-www-npmjs-com-package-kafka-node-install-kafka"><a href="#官网地址：https-www-npmjs-com-package-kafka-node-install-kafka" class="headerlink" title="官网地址：https://www.npmjs.com/package/kafka-node#install-kafka"></a>官网地址：<a href="https://www.npmjs.com/package/kafka-node#install-kafka" target="_blank" rel="external">https://www.npmjs.com/package/kafka-node#install-kafka</a></h4><h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p><p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xrthw.com1.z0.glb.clouddn.com/streams-interactive-queries-02.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;mongodb写到kafka 指定topic消费。为了保证数据稳定可靠性。&lt;br&gt;配合大数据在countly 使用开源&lt;code&gt;Kafka-node&lt;/code&gt;是一个Node.js客户端 写js程序让countly三台集群分别数据到kafka 做新的topic主题备份。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-Cloudera Manager5及CDH5安装指导</title>
    <link href="http://blog.yancy.cc/2017/07/29/Bigdata-hadoop/countly/Cloudera%20Manager5%E5%8F%8ACDH5%E5%AE%89%E8%A3%85%E6%8C%87%E5%AF%BC/"/>
    <id>http://blog.yancy.cc/2017/07/29/Bigdata-hadoop/countly/Cloudera Manager5及CDH5安装指导/</id>
    <published>2017-07-29T06:46:00.000Z</published>
    <updated>2017-09-27T17:09:29.000Z</updated>
    
    <content type="html"><![CDATA[<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Bigdata-Cloudera-Manager.png" alt=""></figure></p><h3 id="问题导读："><a href="#问题导读：" class="headerlink" title="问题导读："></a>问题导读：</h3><p>1.什么是cloudera CM 、CDH?<br>2.CDH、CM有哪些版本？<br>3.CDH、CM有哪些安装方式？<br>4.CDH如何开发？</p><a id="more"></a><h3 id="什么是CDH"><a href="#什么是CDH" class="headerlink" title="什么是CDH"></a>什么是CDH</h3><p>hadoop是一个开源项目，所以很多公司在这个基础进行商业化，Cloudera对hadoop做了相应的改变。<br>Cloudera公司的发行版，我们将该版本称为CDH。</p><p>很多新手问的最多的问题是，哪个是收费的，那个是免费的。<br>Cloudera Express版本是免费的<br>Cloudera Enterprise是需要购买注册码的</p><p>更多内容：<br>Cloudera Hadoop什么是CDH及CDH版本介绍 :<a href="http://www.aboutyun.com/thread-6788-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-6788-1-1.html</a> </p><p>CDH（Cloudera）与hadoop（apache）对比 : <a href="http://www.aboutyun.com/thread-9225-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-9225-1-1.html</a></p><p>大数据架构师基础：hadoop家族，Cloudera产品系列等各种技术 : <a href="http://www.aboutyun.com/thread-6842-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-6842-1-1.html</a> </p><h3 id="官网介绍"><a href="#官网介绍" class="headerlink" title="官网介绍"></a>官网介绍</h3><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera1.png" alt=""></figure></p><p><strong>主页：</strong><br><a href="https://www.cloudera.com/downloads/manager/5-12-0.html" target="_blank" rel="external">https://www.cloudera.com/downloads/manager/5-12-0.html</a></p><h3 id="CM-Cloudera-Manager-有三种安装方式："><a href="#CM-Cloudera-Manager-有三种安装方式：" class="headerlink" title="CM(Cloudera Manager)有三种安装方式："></a>CM(Cloudera Manager)有三种安装方式：</h3><p><strong>1.第一种使用cloudera-manager-installer.bin安装</strong></p><p>这种安装方式，只要从官网下载<code>cloudera-manager-installer.bin</code> 然后执行这个<code>bin</code>文件，剩下的就是等待下载和安装。但是这个时间不是一般的长，最好吃个饭，睡个觉，最后看到还在安装过程中。此帖安装步骤及遇到问题记录很详细，可参考<br><a href="http://www.aboutyun.com/thread-9303-1-1.html" target="_blank" rel="external">Cloudera Manager5及CDH5在线（cloudera-manager-installer.bin）安装详细文档</a><br><a href="http://www.aboutyun.com/thread-9075-1-1.html" target="_blank" rel="external">Cloudera Manager5及CDH5安装指导（终极在线安装）</a></p><h3 id="问题导读：-1"><a href="#问题导读：-1" class="headerlink" title="问题导读："></a>问题导读：</h3><p>1.Cloudera Manager5安装需要哪些环境要求？<br>2.哪些Linux系统上，可以安装Cloudera Manager5？<br>3.在安装cdh的过程中，该如何选择版本？</p><p><strong>安装环境要求</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">集群中的四台主机必须满足以下要求：</div><div class="line">主机必须至少有16 GB的RAM</div><div class="line">对于RAM，我们或许没有太多的概念，下面可以参考</div><div class="line">RAM容量是运行中的程序所占用的空间，他运行需要的空间、</div><div class="line">ROM容量是你的系统可以存放，占用的空间，你的所有系统文件，程序都在这里存放。</div><div class="line"></div><div class="line">必须使用root用户，或则使用sudo无密码访问</div><div class="line">（也就是说当你使用其它用户，使用sudo的时候，不能输入密码）</div><div class="line"></div><div class="line">如果使用root用户，必须使用相同的密码</div><div class="line">主机必须能上网，允许安装向导访问cdm.jollychic.com</div><div class="line">可以从下面选择一个系统</div><div class="line"></div><div class="line">RHEL-兼容系统</div><div class="line">Red Hat Enterprise Linux and CentOS 5, 64-bit</div><div class="line">Red Hat Enterprise Linux and CentOS 6, 64-bit</div><div class="line">Red Hat Enterprise Linux and CentOS 7 <span class="keyword">in</span> SE Linux Mode</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">SLES - SUSE Linux Enterprise Server 11, 64-bit. Service Pack 2或则更高版本. 更新存储库必须是激活的和 SUSE Linux Enterprise 软件开发包11 SP1 .</div><div class="line">Debian - Debian 7.0 and 7.1, 64-bit</div><div class="line">Ubuntu - Ubuntu 12.04, 64-bit</div><div class="line"></div><div class="line">如果要求不能上面满足，安装会不成功。关于Cloudera Manager安装选项和安装要求的详细信息可以查看Cloudera Manager安装向导（英文版）</div><div class="line">补充和强调一些内容</div><div class="line">1.关闭防火墙</div><div class="line">2.配置host，如下形式</div><div class="line">3.swapoff <span class="_">-a</span> 关闭swap分区</div><div class="line">添加开机启动生效：</div><div class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/defrag</div><div class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled</div></pre></td></tr></table></figure><h3 id="环境："><a href="#环境：" class="headerlink" title="环境："></a>环境：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">系统： CentOS Linux release 7.3.1611 (Core)  </div><div class="line">2台Namenode</div><div class="line">10台DataNode</div><div class="line">1台cdm 安装Cloudera Manager</div><div class="line">1台gateway</div></pre></td></tr></table></figure><h3 id="安装步骤："><a href="#安装步骤：" class="headerlink" title="安装步骤："></a>安装步骤：</h3><ul><li>准备工作</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">配置hosts</div><div class="line"></div><div class="line">14台服务器需要配置hostname及hosts如下</div><div class="line">（这里一定要配置正确否则，会面会出通信问题）</div><div class="line"></div><div class="line">vim /etc/hosts</div><div class="line">127.0.0.1 localhost.localdomain localhost</div><div class="line">10.155.90.132  cdm  cdm.jollychic.com</div><div class="line">10.155.90.134  gateway gateway.jollychic.com</div><div class="line">10.155.90.165  namenode1  namenode1.jollychic.com</div><div class="line">10.155.90.146  namenode2  namenode2.jollychic.com</div><div class="line">10.155.90.177  datanode1  datanode1.jollychic.com</div><div class="line">10.155.90.184  datanode2  datanode2.jollychic.com</div><div class="line">10.155.90.166  datanode3  datanode3.jollychic.com</div><div class="line">10.155.90.188  datanode4  datanode4.jollychic.com</div><div class="line">10.155.90.156  datanode5  datanode5.jollychic.com</div><div class="line">10.155.90.172  datanode6  datanode6.jollychic.com</div><div class="line">10.155.90.182  datanode7  datanode7.jollychic.com</div><div class="line">10.155.90.142  datanode8  datanode8.jollychic.com</div><div class="line">10.155.90.151  datanode9  datanode9.jollychic.com</div><div class="line">10.155.90.159  datanode10  datanode10.jollychic.com</div><div class="line"></div><div class="line">关闭SELinux</div><div class="line">[root@cdm ~]<span class="comment"># setenforce 0</span></div><div class="line">[root@cdm ~]<span class="comment"># getenforce</span></div><div class="line">Disabled</div></pre></td></tr></table></figure><ul><li>下载安装：</li></ul><figure class="highlight armasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">单击下载 Cloudera Express 或则 Download Cloudera Enterprise. 查看 Cloudera Express <span class="keyword">and </span>Cloudera Enterprise Features.</div><div class="line">选择注册和单击<span class="keyword">Submit </span>或则直接单击下载页链接（ download page），下载 cloudera-manager-installer.<span class="keyword">bin文件</span></div><div class="line"></div><div class="line"><span class="symbol">Pre</span>-requisites: <span class="keyword">multiple, </span>Internet-connected Linux machines, with SSH access, <span class="keyword">and </span>significant free <span class="meta">space</span> in /var <span class="keyword">and </span>/<span class="meta">opt</span>.</div><div class="line"></div><div class="line"><span class="number">1</span>. 下载cloudera-manager-installer.<span class="keyword">bin</span></div><div class="line">$ wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.<span class="keyword">bin</span></div><div class="line"></div><div class="line"><span class="number">2</span>.改变 cloudera-manager-installer.<span class="keyword">bin </span>的执行权限</div><div class="line"></div><div class="line">$ chmod u+x cloudera-manager-installer.<span class="keyword">bin</span></div><div class="line">复制代码</div><div class="line"></div><div class="line"><span class="number">3</span>.执行 cloudera-manager-installer.<span class="keyword">bin</span></div><div class="line"></div><div class="line">$ sudo ./cloudera-manager-installer.<span class="keyword">bin</span></div><div class="line">复制代码</div><div class="line"></div><div class="line"><span class="number">4</span>.按照cloudera-manager的README 来安装</div><div class="line"><span class="number">5</span>.阅读Cloudera Manager Express License，然后按照提示选择YES来确定接受授权（license）</div><div class="line"><span class="number">6</span>.读取 Oracle 二进制 <span class="meta">Code</span> 授权许可协议，然后安装 </div><div class="line"><span class="number">7</span>.当安装完成，Cloudera Manager 管理控制台会提供一个完成的url包括默认端口<span class="number">7180</span></div><div class="line"></div><div class="line">我这种方法是在线安装方式，就说直接用.<span class="keyword">bin让系统自动下载需要的jdk和cm管理包。 </span></div><div class="line">如果网络差建议用离线安装方式。</div></pre></td></tr></table></figure><p>先在本地搭建一个<a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_yumrepo_local_create.html" target="_blank" rel="external">Creating a Local Yum Repository:</a><br>下载独立的包路径：<a href="http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/5/RPMS/x86_64/" target="_blank" rel="external">cm need package</a></p><p>安装完成以后服务也启动。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera2.png" alt=""></figure></p><p>这里显示安装成功，访问地址，如果离线安装不到这一步不能操作，会提示报错指示。先手动下载好包rpm安装上即可。</p><h4 id="安装目录详情："><a href="#安装目录详情：" class="headerlink" title="安装目录详情："></a>安装目录详情：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">默认安装目录：/opt/cloudera</div><div class="line">日志目录：/var/<span class="built_in">log</span>/cloudera-scm-server</div><div class="line">启动服务：[root@yancy cloudera-scm-server]<span class="comment"># /etc/init.d/cloudera-scm-server status</span></div><div class="line">cloudera-scm-server (pid  2723) 正在运行...</div></pre></td></tr></table></figure><h4 id="查看启动进程端口："><a href="#查看启动进程端口：" class="headerlink" title="查看启动进程端口："></a>查看启动进程端口：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@cdm opt]<span class="comment"># netstat -ntulp | grep java</span></div><div class="line">tcp        0      0 0.0.0.0:7180            0.0.0.0:*               LISTEN      25780/java</div><div class="line">tcp        0      0 0.0.0.0:7182            0.0.0.0:*               LISTEN      25780/java   </div><div class="line"></div><div class="line">访问地址：http://cdm.jollychic.com:7180 </div><div class="line">登录Cloudera Manager Admin 控制</div><div class="line">Username: admin </div><div class="line">Password: admin.</div></pre></td></tr></table></figure><h3 id="使用Cloudera-Manager-向导安装和配置软件"><a href="#使用Cloudera-Manager-向导安装和配置软件" class="headerlink" title="使用Cloudera Manager 向导安装和配置软件"></a>使用Cloudera Manager 向导安装和配置软件</h3><h4 id="cloudera首页："><a href="#cloudera首页：" class="headerlink" title="cloudera首页："></a>cloudera首页：</h4><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera3.png" alt=""></figure></p><h5 id="1-添加服务器集群："><a href="#1-添加服务器集群：" class="headerlink" title="1. 添加服务器集群："></a>1. 添加服务器集群：</h5><p>在集群主机上安装和配置Cloudera Manager ，CDH，和管理服务软件包括以下三个主要步骤<br>选择 Cloudera Manager 版本 和指定主机</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera5.png" alt=""></figure></p><h5 id="2-选择免费版，add继续"><a href="#2-选择免费版，add继续" class="headerlink" title="2. 选择免费版，add继续"></a>2. 选择免费版，add继续</h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera6.png" alt=""></figure></p><h5 id="3-显示的是安装Cloudera-Manager后面可以安装的服务的软件包"><a href="#3-显示的是安装Cloudera-Manager后面可以安装的服务的软件包" class="headerlink" title="3. 显示的是安装Cloudera Manager后面可以安装的服务的软件包"></a>3. 显示的是安装Cloudera Manager后面可以安装的服务的软件包</h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera7.png" alt=""></figure></p><h5 id="4-这里需要说明的是指定主机安装有多种方式："><a href="#4-这里需要说明的是指定主机安装有多种方式：" class="headerlink" title="4. 这里需要说明的是指定主机安装有多种方式："></a>4. 这里需要说明的是指定主机安装有多种方式：</h5><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">1.</span>直接列出ip或则host，多台以逗号、分号、制表符、空格或放置在单独的行。</div><div class="line"><span class="number">2.</span>指定ip的地址范围例如：<span class="number">10.1</span><span class="number">.1</span>.[<span class="number">1</span><span class="number">-4</span>] 或则 host[<span class="number">1</span><span class="number">-3</span>].hadoop.com<span class="number">.3</span>.记得指定的主机需要关闭防火墙，（如果遇到不能安装，最好使用安装cloudera manager虚拟机进行复制）</div><div class="line">选择需要安装的Hadoop datanode服务器：这里我们线上访问端口不是<span class="number">22</span>是<span class="number">58958</span></div></pre></td></tr></table></figure><p>显示如下图说明机器通信没问题。</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera8.png" alt=""></figure></p><h5 id="5-安装CDH"><a href="#5-安装CDH" class="headerlink" title="5. 安装CDH"></a>5. 安装CDH</h5><p>选择安装方式<br>当我们选择安装的host之后，我们需要选择CDH的安装方式（方法），如下图所示</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera9.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera10.png" alt=""></figure></p><h5 id="6-这里点击安装Java工具包"><a href="#6-这里点击安装Java工具包" class="headerlink" title="6. 这里点击安装Java工具包"></a>6. 这里点击安装Java工具包</h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera11.png" alt=""></figure></p><h5 id="7-这里把需要安装的Hadoop服务器root密码设置一样，或者使用其他用户也可以，只要有sudo权限都可以。"><a href="#7-这里把需要安装的Hadoop服务器root密码设置一样，或者使用其他用户也可以，只要有sudo权限都可以。" class="headerlink" title="7. 这里把需要安装的Hadoop服务器root密码设置一样，或者使用其他用户也可以，只要有sudo权限都可以。"></a>7. 这里把需要安装的Hadoop服务器root密码设置一样，或者使用其他用户也可以，只要有sudo权限都可以。</h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera12.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera13.png" alt=""></figure></p><h5 id="8-安装这里如果提示出现报错：查看好文-Cloudera-Manager5-在线bin安装遇到-无法检测到-Agent-发出的检测信号-总结"><a href="#8-安装这里如果提示出现报错：查看好文-Cloudera-Manager5-在线bin安装遇到-无法检测到-Agent-发出的检测信号-总结" class="headerlink" title="8.安装这里如果提示出现报错：查看好文 Cloudera Manager5 在线bin安装遇到 无法检测到 Agent 发出的检测信号 总结"></a>8.安装这里如果提示出现报错：查看好文 <a href="http://www.aboutyun.com/thread-9293-1-1.html" target="_blank" rel="external">Cloudera Manager5 在线bin安装遇到 无法检测到 Agent 发出的检测信号 总结</a></h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera14.png" alt=""></figure><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">这里只需要卸载：</div><div class="line">yum remove cloudera-manager-agent </div><div class="line"></div><div class="line">如果想重新安装可以卸载所有：</div><div class="line">yum remove cloudera-manager-repository cloudera-manager-agent cloudera-manager-daemons cloudera-manager-server-db cloudera-manager-server -y</div></pre></td></tr></table></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera15.png" alt=""></figure></p><h5 id="9-集群安装"><a href="#9-集群安装" class="headerlink" title="9.集群安装"></a>9.集群安装</h5><p><strong>图1</strong></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera16.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera17.png" alt=""></figure><br><strong>这是一个很漫长的过程，不幸的是在下载完毕，执行分发的时候，这个过程被打断，安装被终止。导致回不到图2，这个该如何解决？</strong><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera18.png" alt=""></figure></p><p>难道真的要重装吗？花费了大半天功夫，又不得不重来。<br>这里面的问题是找不到cloudera manager5所管理的节点了？</p><p>记得about云有这篇文章<a href="http://www.aboutyun.com/thread-8992-1-1.html" target="_blank" rel="external">卸载 Cloudera Manager 5.1.x.和 相关软件</a>【官网翻译：高可用】<br>但是这篇文章是卸载Cloudera Manager以及CDH的，由于cloudera-scm-server和cloudera-scm-agent查看都是运行正常的，如何查看状态，可以参考：</p><p><a href="http://www.aboutyun.com/thread-9096-1-1.html" target="_blank" rel="external">Cloudera Manager Server5及Cloudera Manager Agents5命令整理（about云）</a>，所以cloudera manager不需要卸载，卸载的是CDH的相关内容。<br>于是执行下面命令：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">sudo apt-<span class="keyword">update</span> remove avro-tools crunch flume-ng hadoop-hdfs-fuse hadoop-hdfs-nfs3 hadoop-httpfs hbase-solr hive-hbase hive-webhcat hue-beeswax hue-hbase hue-impala hue-pig hue-plugins hue-rdbms hue-<span class="keyword">search</span> hue-spark hue-sqoop hue-zookeeper impala impala-<span class="keyword">shell</span> kite llama mahout oozie pig pig-udf-datafu <span class="keyword">search</span> sentry solr-mapreduce spark-python sqoop sqoop2 whirr</div><div class="line">复制代码</div><div class="line">sudo apt-get clean</div><div class="line">复制代码</div><div class="line">sudo <span class="keyword">rm</span> -Rf /<span class="keyword">var</span>/lib/flume-ng /<span class="keyword">var</span>/lib/hadoop* /<span class="keyword">var</span>/lib/hue /<span class="keyword">var</span>/lib/navigator /<span class="keyword">var</span>/lib/oozie /<span class="keyword">var</span>/lib/solr /<span class="keyword">var</span>/lib/sqoop* /<span class="keyword">var</span>/lib/zookeeper</div><div class="line">复制代码</div><div class="line">sudo <span class="keyword">rm</span> -Rf /dfs /mapred /yarn</div><div class="line">复制代码</div><div class="line">通过上面终于找到所管理的三个节点。然后从新登录，选择三个主机，然后继续继续，最后终于进入了这个界面。但是细心的同学会发现这里已经更换为中文版本。因为这是通过宿主主机访问的。而前面是在虚拟机里使用firefox访问的。</div></pre></td></tr></table></figure><h4 id="成功到这一步：集群安装"><a href="#成功到这一步：集群安装" class="headerlink" title="成功到这一步：集群安装"></a>成功到这一步：集群安装</h4><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera19.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera20.png" alt=""></figure></p><h5 id="10-选择集群设置"><a href="#10-选择集群设置" class="headerlink" title="10.选择集群设置"></a>10.选择集群设置</h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera21.png" alt=""></figure></p><h5 id="10-1-集群设置：自定义角色分配"><a href="#10-1-集群设置：自定义角色分配" class="headerlink" title="10.1 集群设置：自定义角色分配"></a>10.1 集群设置：自定义角色分配</h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera22.png" alt=""></figure><br>设置好角色分配查看主机：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera23.png" alt=""></figure></p><h5 id="11-数据库设置"><a href="#11-数据库设置" class="headerlink" title="11. 数据库设置"></a>11. 数据库设置</h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera24.png" alt=""></figure></p><p>✨✨ <strong>这里使用默认设置，填写MySQL服务器IP，给予MySQL 用户名和密码all权限，不然会提示权限不足。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">Create the Oozie Database and Oozie MySQL User</div><div class="line">For example, using the MySQL mysql <span class="built_in">command</span>-line tool:</div><div class="line"></div><div class="line">$ mysql -u root -p</div><div class="line">Enter password:</div><div class="line"></div><div class="line">mysql&gt; create database oozie default character <span class="built_in">set</span> utf8;</div><div class="line">Query OK, 1 row affected (0.00 sec)</div><div class="line"></div><div class="line">mysql&gt;  grant all privileges on oozie.* to <span class="string">'oozie'</span>@<span class="string">'localhost'</span> identified by <span class="string">'oozie'</span>;</div><div class="line">Query OK, 0 rows affected (0.00 sec)</div><div class="line"></div><div class="line">mysql&gt;  grant all privileges on oozie.* to <span class="string">'oozie'</span>@<span class="string">'%'</span> identified by <span class="string">'oozie'</span>;</div><div class="line">Query OK, 0 rows affected (0.00 sec)</div><div class="line"></div><div class="line">mysql&gt; <span class="built_in">exit</span></div><div class="line">Bye</div></pre></td></tr></table></figure><p>选择然后单击测试连接即可。</p><p><strong>测试 Activity Monitor 的数据库连接</strong><br><strong>JDBC driver cannot be found. Unable to find the JDBC database jar on host : wlj-cdm.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">如果出现需要MySQL驱动,下载对应版本驱动，copy到相应目录/usr/share/java/ 统一去掉版本号.</div><div class="line">参考Installing the MySQL JDBC Driver官网文档：https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_mysql.html</div><div class="line"></div><div class="line"><span class="comment">#### Installing the MySQL JDBC Driver </span></div><div class="line"></div><div class="line"></div><div class="line">Download the MySQL JDBC driver from http://www.mysql.com/downloads/connector/j/5.1.html.</div><div class="line">Extract the JDBC driver JAR file from the downloaded file. For example:</div><div class="line"></div><div class="line"><span class="comment">### $ wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.44.tar.gz</span></div><div class="line"><span class="comment">### $ tar zxvf mysql-connector-java-5.1.31.tar.gz</span></div><div class="line">Copy the JDBC driver, renamed, to the relevant host. For example:</div><div class="line"><span class="comment">### $ sudo cp mysql-connector-java-5.1.31/mysql-connector-java-5.1.31-bin.jar /usr/share/java/mysql-connector-java.jar</span></div><div class="line">If the target directory does not yet exist on this host, you can create it before copying the JAR file. For example:</div><div class="line"></div><div class="line"><span class="comment">### $ sudo mkdir -p /usr/share/java/</span></div><div class="line"><span class="comment">### $ sudo cp mysql-connector-java-5.1.31/mysql-connector-java-5.1.31-bin.jar /usr/share/java/mysql-connector-java.jar</span></div></pre></td></tr></table></figure><h5 id="12-审核更改-这里选择默认路径"><a href="#12-审核更改-这里选择默认路径" class="headerlink" title="12. 审核更改(这里选择默认路径)"></a>12. 审核更改(这里选择默认路径)</h5><p>等待安装：<br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera25.png" alt=""></figure></p><h5 id="13-安装成功"><a href="#13-安装成功" class="headerlink" title="13. 安装成功"></a>13. 安装成功</h5><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera25.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera26.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera27.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera28.png" alt=""></figure></p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/bidata-cloudera29.png" alt=""></figure></p><h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p><p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xrthw.com1.z0.glb.clouddn.com/Bigdata-Cloudera-Manager.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;问题导读：&quot;&gt;&lt;a href=&quot;#问题导读：&quot; class=&quot;headerlink&quot; title=&quot;问题导读：&quot;&gt;&lt;/a&gt;问题导读：&lt;/h3&gt;&lt;p&gt;1.什么是cloudera CM 、CDH?&lt;br&gt;2.CDH、CM有哪些版本？&lt;br&gt;3.CDH、CM有哪些安装方式？&lt;br&gt;4.CDH如何开发？&lt;/p&gt;
    
    </summary>
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/categories/Bigdata-Hadoop/"/>
    
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/tags/Bigdata-Hadoop/"/>
    
      <category term="Countly" scheme="http://blog.yancy.cc/tags/Countly/"/>
    
  </entry>
  
  <entry>
    <title>Zabbix 3.0 监控推送rabbitmq队列-消息堆积</title>
    <link href="http://blog.yancy.cc/2017/07/29/Bigdata-hadoop/RabbitMQ/Zabbix%203.0%20%E7%9B%91%E6%8E%A7%E6%8E%A8%E9%80%81rabbitmq%E9%98%9F%E5%88%97/"/>
    <id>http://blog.yancy.cc/2017/07/29/Bigdata-hadoop/RabbitMQ/Zabbix 3.0 监控推送rabbitmq队列/</id>
    <published>2017-07-29T06:46:00.000Z</published>
    <updated>2018-01-13T06:20:32.435Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Zabbix-3-0-监控推送rabbitmq队列"><a href="#Zabbix-3-0-监控推送rabbitmq队列" class="headerlink" title="Zabbix 3.0 监控推送rabbitmq队列"></a>Zabbix 3.0 监控推送rabbitmq队列</h4><p>对于RabbitMQ的监控，除了服务器基本信息<code>（硬盘、CPU、内存、IO等）</code>以及MQ的进程和端口，我们也可以通过请求url访问管理API监控其集群和队列的情况。在java api 3.6.0以后，channel接口为我们提供了如下接口：</p><ul><li>messageCount：查询队列未消费的消息数，可以监控消息堆积的情况。 </li><li>consumerCount：队列的消费者个数，可以对消费者进行监控 </li></ul><p>1.监控告警需求问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">message.bi队列  积压&gt; 300 或者 消费者数&lt;=2</div><div class="line">message.push.cart队列  积压 &gt;10000 或者消费者数&lt;5</div><div class="line">message.user.related队列 积压&gt;2000 或者 消费者数&lt;=2</div><div class="line">message.cart队列 积压&gt;10000 或者 消费者数&lt;=2</div></pre></td></tr></table></figure><p>2.编写Python脚本监控获取消费者数监控，队列。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">vim rabbitmq-monitor.py</div><div class="line"><span class="comment">#!/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></div><div class="line">import sys, urllib2, base64, json, re,time</div><div class="line">ip = <span class="string">"169.23.73.22"</span></div><div class="line">keys = (<span class="string">'messages_ready'</span>,)</div><div class="line">def GetRabbitmqData():</div><div class="line">        request = urllib2.Request(<span class="string">"http://%s:15672/api/queues"</span> % ip)</div><div class="line">        base64string = base64.b64encode(<span class="string">'guest:guest'</span>)</div><div class="line">        request.add_header(<span class="string">"Authorization"</span>, <span class="string">"Basic %s"</span> % base64string)</div><div class="line">        result = urllib2.urlopen(request)</div><div class="line">        data = json.loads(result.read())</div><div class="line">        <span class="built_in">return</span> data</div><div class="line"></div><div class="line"></div><div class="line">data=GetRabbitmqData()</div><div class="line"><span class="comment">#print data</span></div><div class="line"><span class="keyword">for</span> queue <span class="keyword">in</span> data:</div><div class="line">    try:</div><div class="line">        <span class="built_in">print</span> <span class="string">"消费者数量:"</span>,queue[<span class="string">'consumers'</span>],<span class="string">"队列:"</span>,queue[<span class="string">'name'</span>],<span class="string">"消息积压数:"</span>,int(queue[keys[0]])</div><div class="line">    except:</div><div class="line">        pass</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[root@message-center-mq zabbix]<span class="comment"># python rabbitmq-monitor.py</span></div><div class="line">消费者数量: 0 队列: 79d02dde-2007-4a49-b94b-0d4bee67b19c 消息积压数: 0</div><div class="line">消费者数量: 0 队列: aliveness-test 消息积压数: 0</div><div class="line">消费者数量: 0 队列: cartService.orderCancel.update 消息积压数: 0</div><div class="line">消费者数量: 0 队列: cartService.virtualOrderCancel.update 消息积压数: 0</div><div class="line">消费者数量: 0 队列: e9de65bd-be59-4c1c-b4a8-7312f382ac59 消息积压数: 0</div><div class="line">消费者数量: 3 队列: message.bi 消息积压数: 0</div><div class="line">消费者数量: 3 队列: message.cart 消息积压数: 0</div><div class="line">消费者数量: 3 队列: message.console 消息积压数: 0</div><div class="line">消费者数量: 3 队列: message.logistics 消息积压数: 0</div><div class="line">消费者数量: 3 队列: message.order.info 消息积压数: 0</div><div class="line">消费者数量: 30 队列: message.push.cart 消息积压数: 0</div><div class="line">消费者数量: 3 队列: message.style 消息积压数: 0</div><div class="line">消费者数量: 3 队列: message.user.related 消息积压数: 0</div><div class="line">消费者数量: 0 队列: payment.virtual.notify.success 消息积压数: 0</div><div class="line">消费者数量: 1 队列: 61b73745-4c74-475b-803e-bf2d48d2fa50 消息积压数: 0</div><div class="line">消费者数量: 1 队列: a984d00a-8bbe-43c1-aa20-c1dc788ddd97 消息积压数: 0</div><div class="line">消费者数量: 1 队列: e569e8a1-b7c9-4736-8bf8-13d76ecf7577 消息积压数: 0</div><div class="line">消费者数量: 3 队列: push.station.task.status 消息积压数: 0</div></pre></td></tr></table></figure><p>编写zabbix-agentd 监控：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@message-center-mq zabbix]<span class="comment"># vim zabbix_agentd.conf</span></div><div class="line">UserParameter=rabbitmq.consumer.bi,python /etc/zabbix/rabbitmq-monitor.py | grep message.bi |awk -F<span class="string">'[ :]'</span> <span class="string">'&#123;print $3&#125;'</span></div><div class="line">UserParameter=rabbitmq.overstock.bi,python /etc/zabbix/rabbitmq-monitor.py | grep message.bi |awk -F<span class="string">'[ :]'</span> <span class="string">'&#123;print $NF&#125;'</span></div><div class="line">UserParameter=rabbitmq.consumer.push.cart,python /etc/zabbix/rabbitmq-monitor.py | grep message.push.cart |awk -F<span class="string">'[ :]'</span> <span class="string">'&#123;print $3&#125;'</span></div><div class="line">UserParameter=rabbitmq.overstock.push.cart,python /etc/zabbix/rabbitmq-monitor.py | grep message.push.cart |awk -F<span class="string">'[ :]'</span> <span class="string">'&#123;print $NF&#125;'</span></div><div class="line">UserParameter=rabbitmq.consumer.user.related,python /etc/zabbix/rabbitmq-monitor.py | grep message.user.related |awk -F<span class="string">'[ :]'</span> <span class="string">'&#123;print $3&#125;'</span></div><div class="line">UserParameter=rabbitmq.overstock.user.related,python /etc/zabbix/rabbitmq-monitor.py | grep message.user.related |awk -F<span class="string">'[ :]'</span> <span class="string">'&#123;print $NF&#125;'</span></div><div class="line">UserParameter=rabbitmq.consumer.cart,python /etc/zabbix/rabbitmq-monitor.py | grep message.cart |awk -F<span class="string">'[ :]'</span> <span class="string">'&#123;print $3&#125;'</span></div><div class="line">UserParameter=rabbitmq.overstock.cart,python /etc/zabbix/rabbitmq-monitor.py | grep message.cart |awk -F<span class="string">'[ :]'</span> <span class="string">'&#123;print $NF&#125;'</span></div></pre></td></tr></table></figure><p>监控模板下载：在我github上面.</p><p>weixin监控效果图：</p><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/rabbitmq.png" alt=""></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Zabbix-3-0-监控推送rabbitmq队列&quot;&gt;&lt;a href=&quot;#Zabbix-3-0-监控推送rabbitmq队列&quot; class=&quot;headerlink&quot; title=&quot;Zabbix 3.0 监控推送rabbitmq队列&quot;&gt;&lt;/a&gt;Zabbix 3.0 
      
    
    </summary>
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/categories/Bigdata-Hadoop/"/>
    
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/tags/Bigdata-Hadoop/"/>
    
      <category term="Rabbitmq" scheme="http://blog.yancy.cc/tags/Rabbitmq/"/>
    
  </entry>
  
  <entry>
    <title>Kafka动态增加Topic的副本(Replication)</title>
    <link href="http://blog.yancy.cc/2017/07/21/Bigdata-hadoop/Kafka/Kafka%E5%8A%A8%E6%80%81%E5%A2%9E%E5%8A%A0Topic%E7%9A%84%E5%89%AF%E6%9C%AC(Replication)/"/>
    <id>http://blog.yancy.cc/2017/07/21/Bigdata-hadoop/Kafka/Kafka动态增加Topic的副本(Replication)/</id>
    <published>2017-07-21T03:22:00.000Z</published>
    <updated>2018-04-11T06:29:09.238Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kafka动态增加Topic的副本-Replication"><a href="#Kafka动态增加Topic的副本-Replication" class="headerlink" title="Kafka动态增加Topic的副本(Replication)"></a>Kafka动态增加Topic的副本(Replication)</h2><p>当前我的topic ：countly_event 只有1个副本，如果集群其中一台机器出现问题，就会丢失数据。所以这里以后新建新的topic 最少2个副本，如果资源非常充足可以考虑副本3个。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka_2.10-0.9.0.1]$ ./bin/kafka-topics.sh --zookeeper 172.31.2.6:2182,172.31.2.7:2182,172.31.2.8:2182  -describe -topic countly_event</div><div class="line">Topic:countly_eventPartitionCount:12ReplicationFactor:1Configs:</div><div class="line">Topic: countly_eventPartition: 0Leader: 1Replicas: 1Isr: 1</div><div class="line">Topic: countly_eventPartition: 1Leader: 2Replicas: 2Isr: 2</div><div class="line">Topic: countly_eventPartition: 2Leader: 3Replicas: 3Isr: 3</div><div class="line">Topic: countly_eventPartition: 3Leader: 4Replicas: 4Isr: 4</div><div class="line">Topic: countly_eventPartition: 4Leader: 1Replicas: 1Isr: 1</div><div class="line">Topic: countly_eventPartition: 5Leader: 2Replicas: 2Isr: 2</div><div class="line">Topic: countly_eventPartition: 6Leader: 3Replicas: 3Isr: 3</div><div class="line">Topic: countly_eventPartition: 7Leader: 4Replicas: 4Isr: 4</div><div class="line">Topic: countly_eventPartition: 8Leader: 1Replicas: 1Isr: 1</div><div class="line">Topic: countly_eventPartition: 9Leader: 2Replicas: 2Isr: 2</div><div class="line">Topic: countly_eventPartition: 10Leader: 3Replicas: 3Isr: 3</div><div class="line">Topic: countly_eventPartition: 11Leader: 4Replicas: 4Isr: 4</div></pre></td></tr></table></figure><p>增加现有分区的复制因子很容易。只需在自定义重新分配json文件中指定额外副本，并将其与–execute选项一起使用即可增加指定分区的复制因子。</p><p>例如，以下示例将主题countly_event的分区0的复制因子从1增加到3.在增加复制因子之前，该分区的唯一副本存在于代理1上。作为增加复制因子的一部分，我们将添加更多副本经纪人2和3和4。</p><p>第一步是在json文件中手工制作自定义重新分配计划 - 这个自己定义每个分区设置副本，例如：0分区设置在3和2和4. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">"version"</span>:1,</div><div class="line"> <span class="string">"partitions"</span>:</div><div class="line"> [&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[3,2,4]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:1,<span class="string">"replicas"</span>:[3,2,4]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:2,<span class="string">"replicas"</span>:[4,3,1]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:3,<span class="string">"replicas"</span>:[2,1,3]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[3,4,1]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:5,<span class="string">"replicas"</span>:[4,1,2]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:6,<span class="string">"replicas"</span>:[1,2,3]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:7,<span class="string">"replicas"</span>:[2,3,4]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[3,1,2]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:9,<span class="string">"replicas"</span>:[4,2,3]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:10,<span class="string">"replicas"</span>:[1,3,4]&#125;,</div><div class="line"> &#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:11,<span class="string">"replicas"</span>:[2,4,1]&#125;]</div><div class="line"> &#125;</div></pre></td></tr></table></figure><p>然后，使用带有–execute选项的json文件启动重新分配过程 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"> ./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281  --reassignment-json-file increase-replication-factor.json --execute</div><div class="line"></div><div class="line"></div><div class="line">[jollybi@kafka1 kafka_2.10-0.9.0.1]$ ./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281  --reassignment-json-file increase-replication-factor.json --execute</div><div class="line">Current partition replica assignment</div><div class="line"></div><div class="line">&#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:[&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:6,<span class="string">"replicas"</span>:[4]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:5,<span class="string">"replicas"</span>:[3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:7,<span class="string">"replicas"</span>:[1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:3,<span class="string">"replicas"</span>:[1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:9,<span class="string">"replicas"</span>:[3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:11,<span class="string">"replicas"</span>:[1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:10,<span class="string">"replicas"</span>:[4]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:2,<span class="string">"replicas"</span>:[4]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:1,<span class="string">"replicas"</span>:[3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[2]&#125;]&#125;</div><div class="line"></div><div class="line">Save this to use as the --reassignment-json-file option during rollback</div><div class="line">Successfully started reassignment of partitions &#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:[&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[1,2,3,4]&#125;]&#125;</div></pre></td></tr></table></figure><p>–verify选项可与该工具一起使用，以检查分区重新分配的状态。请注意，与–verify选项一起使用相同的increase-replication-factor.json（与–execute选项一起使用）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="string">[jollybi@kafka1</span> <span class="string">kafka_2.10-0.9.0.1]$</span> <span class="string">./bin/kafka-reassign-partitions.sh</span> <span class="bullet">--zookeeper</span> <span class="number">10.155</span><span class="number">.90</span><span class="number">.153</span><span class="string">:2281,10.155.90.155:2281,10.155.90.138:2281</span>  <span class="bullet">--reassignment-json-file</span> <span class="string">increase-replication-factor.json</span> <span class="bullet">--verify</span></div><div class="line"><span class="string">Status</span> <span class="string">of</span> <span class="string">partition</span> <span class="attr">reassignment:</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,0]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,1]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,2]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,3]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,4]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,5]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,6]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,7]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,8]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,9]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,10]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"><span class="string">Reassignment</span> <span class="string">of</span> <span class="string">partition</span> <span class="string">[countly_event,11]</span> <span class="string">is</span> <span class="string">still</span> <span class="string">in</span> <span class="string">progress</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">您还可以使用kafka-topics工具验证复制因子的增加情况</span> </div><div class="line"></div><div class="line"><span class="string">[root@kafka1</span> <span class="string">kafka_2.10-0.9.0.1]#</span> <span class="string">./bin/kafka-topics.sh</span> <span class="bullet">--zookeeper</span> <span class="number">10.155</span><span class="number">.90</span><span class="number">.153</span><span class="string">:2281,10.155.90.155:2281,10.155.90.138:2281</span> <span class="bullet">-describe</span> <span class="bullet">-topic</span> <span class="string">countly_event</span></div><div class="line"><span class="attr">Topic:</span><span class="string">countly_event</span><span class="attr">PartitionCount:12</span><span class="attr">ReplicationFactor:3</span><span class="attr">Configs:</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">0</span><span class="attr">Leader:</span> <span class="number">2</span><span class="attr">Replicas:</span> <span class="number">3</span><span class="string">,2,4</span><span class="attr">Isr:</span> <span class="number">2</span><span class="string">,3,4</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">1</span><span class="attr">Leader:</span> <span class="number">3</span><span class="attr">Replicas:</span> <span class="number">3</span><span class="string">,2,4</span><span class="attr">Isr:</span> <span class="number">3</span><span class="string">,2,4</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">2</span><span class="attr">Leader:</span> <span class="number">4</span><span class="attr">Replicas:</span> <span class="number">4</span><span class="string">,3,1</span><span class="attr">Isr:</span> <span class="number">4</span><span class="string">,1,3</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">3</span><span class="attr">Leader:</span> <span class="number">1</span><span class="attr">Replicas:</span> <span class="number">2</span><span class="string">,1,3</span><span class="attr">Isr:</span> <span class="number">1</span><span class="string">,2,3</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">4</span><span class="attr">Leader:</span> <span class="number">3</span><span class="attr">Replicas:</span> <span class="number">3</span><span class="string">,4,1</span><span class="attr">Isr:</span> <span class="number">4</span><span class="string">,1,3</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">5</span><span class="attr">Leader:</span> <span class="number">4</span><span class="attr">Replicas:</span> <span class="number">4</span><span class="string">,1,2</span><span class="attr">Isr:</span> <span class="number">1</span><span class="string">,4,2</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">6</span><span class="attr">Leader:</span> <span class="number">1</span><span class="attr">Replicas:</span> <span class="number">1</span><span class="string">,2,3</span><span class="attr">Isr:</span> <span class="number">2</span><span class="string">,1,3</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">7</span><span class="attr">Leader:</span> <span class="number">2</span><span class="attr">Replicas:</span> <span class="number">2</span><span class="string">,3,4</span><span class="attr">Isr:</span> <span class="number">4</span><span class="string">,2,3</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">8</span><span class="attr">Leader:</span> <span class="number">2</span><span class="attr">Replicas:</span> <span class="number">3</span><span class="string">,1,2</span><span class="attr">Isr:</span> <span class="number">2</span><span class="string">,1,3</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">9</span><span class="attr">Leader:</span> <span class="number">3</span><span class="attr">Replicas:</span> <span class="number">4</span><span class="string">,2,3</span><span class="attr">Isr:</span> <span class="number">3</span><span class="string">,2,4</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">10</span><span class="attr">Leader:</span> <span class="number">4</span><span class="attr">Replicas:</span> <span class="number">1</span><span class="string">,3,4</span><span class="attr">Isr:</span> <span class="number">4</span><span class="string">,1,3</span></div><div class="line"><span class="attr">Topic:</span> <span class="string">countly_event</span><span class="attr">Partition:</span> <span class="number">11</span><span class="attr">Leader:</span> <span class="number">1</span><span class="attr">Replicas:</span> <span class="number">2</span><span class="string">,4,1</span><span class="attr">Isr:</span> <span class="number">1</span><span class="string">,4,2</span></div></pre></td></tr></table></figure><p>参考地址：<a href="http://kafka.apache.org/documentation/" target="_blank" rel="external">http://kafka.apache.org/documentation/</a></p><p>Communicative learning:<br>🐧 Linux shell_ senior operation and maintenance faction: QQ group 459096184 circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)<br>🐧 BigData-Exchange School:QQ group 521621407 circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join</p><p>Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Kafka动态增加Topic的副本-Replication&quot;&gt;&lt;a href=&quot;#Kafka动态增加Topic的副本-Replication&quot; class=&quot;headerlink&quot; title=&quot;Kafka动态增加Topic的副本(Replication)&quot;&gt;&lt;/
      
    
    </summary>
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/categories/Bigdata-Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>KafKa动态扩容群集-(Topic.partitions迁移)</title>
    <link href="http://blog.yancy.cc/2017/07/11/Bigdata-hadoop/Kafka/KafKa%E6%89%A9%E5%AE%B9%E7%BE%A4%E9%9B%86(Topic.partitions%E8%BF%81%E7%A7%BB)/"/>
    <id>http://blog.yancy.cc/2017/07/11/Bigdata-hadoop/Kafka/KafKa扩容群集(Topic.partitions迁移)/</id>
    <published>2017-07-11T03:22:00.000Z</published>
    <updated>2018-04-17T07:08:36.086Z</updated>
    
    <content type="html"><![CDATA[<h4 id="kafka的扩容难点："><a href="#kafka的扩容难点：" class="headerlink" title="kafka的扩容难点："></a>kafka的扩容难点：</h4><p>1）主要在于增加机器之后，数据需要rebalance到新增的空闲节点，即把partitions迁移到空闲机器上。<br>kafka提供<code>bin/kafka-reassign-partitions.sh</code>工具，完成parttition的迁移。</p><p>2）kafka的集群的数据量加大，数据rebalance的时间较长。解决办法是把<code>log.retention.hours=1</code>设置一小时（生产参数24小时）。<br>修改参数之后，重启kakfa节点，kafka会主动purge 1小时之前的log数据。<br>以下是kafka_0.8.1.1版本kafkka集群扩容操作记录，从3台物理机扩容到4台物理，partition数量由24个增加到28个。</p><a id="more"></a><p>参考：<a href="http://kafka.apache.org/081/documentation.html#basic_ops_modify_topic" target="_blank" rel="external">http://kafka.apache.org/081/documentation.html#basic_ops_modify_topic</a></p><h4 id="kafka的复制副本："><a href="#kafka的复制副本：" class="headerlink" title="kafka的复制副本："></a>kafka的复制副本：</h4><p>将服务器添加到Kafka集群非常简单，只需为其分配唯一的代理ID，然后在新服务器上启动Kafka。但是，这些新的服务器不会自动分配任何数据分区，除非将分区移动到这些分区，否则直到创建新主题时才会执行任何工作。所以通常当你将机器添加到你的群集中时，你会想把一些现有的数据迁移到这些机器上。<br>数据迁移过程是手动启动的，但是完全自动化。下面介绍的是，Kafka会将新服务器添加为正在迁移的分区的跟随者，并允许其完全复制该分区中的现有数据。当新服务器完全复制了此分区的内容并加入了同步副本时，其中一个现有副本将删除其分区的数据。</p><p>分区重新分配工具可用于跨代理移动分区。理想的分区分布将确保所有代理的数据加载和分区大小。分区重新分配工具不具备自动研究Kafka集群中的数据分布并移动分区以实现均匀负载分配的功能。因此，管理员必须找出哪些主题或分区应该移动。</p><p>分区重新分配工具可以运行在3个互斥的模式中：</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="literal">-</span><span class="literal">-</span><span class="comment">生成：在此模式下，给定主题列表和经纪人列表，该工具会生成候选重新​​分配，以将指定主题的所有分区移至新经纪人。此选项仅提供了一种便捷的方式，可以根据主题和目标代理列表生成分区重新分配计划。</span></div><div class="line"></div><div class="line"><span class="literal">-</span><span class="literal">-</span><span class="comment">execute：在此模式下，该工具根据用户提供的重新分配计划启动分区重新分配。（使用</span><span class="literal">-</span><span class="literal">-</span><span class="comment">reassignment</span><span class="literal">-</span><span class="comment">json</span><span class="literal">-</span><span class="comment">file选项）。这可以是由管理员制作的自定义重新分配计划，也可以是使用</span><span class="literal">-</span><span class="literal">-</span><span class="comment">generate选项提供的自定义重新分配计划。</span></div><div class="line"></div><div class="line"><span class="literal">-</span><span class="literal">-</span><span class="comment">verify：在此模式下，该工具会验证上次执行过程中列出的所有分区的重新分配状态。状态可以是成功完成，失败或正在进行</span></div></pre></td></tr></table></figure><p>自动将数据迁移到新机器</p><p>分区重新分配工具可用于将当前一组经纪人的一些主题移到新增的经纪人。这在扩展现有集群时通常很有用，因为将整个主题移动到新的代理集比移动一个分区更容易。用于这样做时，用户应该提供应该移动到新的经纪人集合和新的经纪人的目标列表的主题列表。然后，该工具在新的代理集合上均匀分配给定主题列表的所有分区。在此过程中，主题的复制因子保持不变。有效地，主题输入列表的所有分区副本都从旧的代理集合移动到新添加的代理。<br>例如，以下示例将把主题<code>countly_apppush，countly_event...</code>的所有分区移动到新的代理集4 。在本次移动结束时，主题<code>countly_apppush，countly_event...</code>的所有分区将仅存在于代理4上。</p><h3 id="1-kafka-扩容"><a href="#1-kafka-扩容" class="headerlink" title="1.kafka 扩容"></a>1.kafka 扩容</h3><p>首先按照搭建步骤，在其他机器上搭建集群，kafka的配置文件中 zkconnect 要保持与原kafka一致<br>kafka版本一致，配置跟之前kafka集群一致，只需要修改本地kafka的地址.</p><h3 id="2-验证kafka新节点是否加入集群成功，这个应该去zookeeper的zkCli-sh-去查看"><a href="#2-验证kafka新节点是否加入集群成功，这个应该去zookeeper的zkCli-sh-去查看" class="headerlink" title="2.验证kafka新节点是否加入集群成功，这个应该去zookeeper的zkCli.sh 去查看"></a>2.验证kafka新节点是否加入集群成功，这个应该去zookeeper的zkCli.sh 去查看</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@kafka1 bin]<span class="comment"># ./zkCli.sh -server  10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281</span></div><div class="line">[zk: 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281(CONNECTED) 0] ls /brokers/ids/</div><div class="line">1,2,3,4</div></pre></td></tr></table></figure><h5 id="3-由于该工具接受主题的输入列表作为json文件，因此首先需要确定要移动的主题并创建json文件，如下所示："><a href="#3-由于该工具接受主题的输入列表作为json文件，因此首先需要确定要移动的主题并创建json文件，如下所示：" class="headerlink" title="3.由于该工具接受主题的输入列表作为json文件，因此首先需要确定要移动的主题并创建json文件，如下所示："></a>3.由于该工具接受主题的输入列表作为json文件，因此首先需要确定要移动的主题并创建json文件，如下所示：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">"topics"</span>: [&#123;<span class="string">"topic"</span>: <span class="string">"countly_apppush"</span>&#125;,</div><div class="line">            &#123;<span class="string">"topic"</span>: <span class="string">"countly_event"</span>&#125;,</div><div class="line">            &#123;<span class="string">"topic"</span>: <span class="string">"countly_imp"</span>&#125;,</div><div class="line">            &#123;<span class="string">"topic"</span>: <span class="string">"countly_metrics"</span>&#125;,</div><div class="line">            &#123;<span class="string">"topic"</span>: <span class="string">"countly_pv"</span>&#125;,</div><div class="line">            &#123;<span class="string">"topic"</span>: <span class="string">"countly_session"</span>&#125;,</div><div class="line">            &#123;<span class="string">"topic"</span>: <span class="string">"mongotail_lz4"</span>&#125;,</div><div class="line">            &#123;<span class="string">"topic"</span>: <span class="string">"mongotail_lz4_imp"</span>&#125;</div><div class="line">            ],</div><div class="line"> <span class="string">"version"</span>:1</div><div class="line">&#125;</div></pre></td></tr></table></figure><h5 id="4-一旦json文件准备就绪，使用分区重新分配工具来生成候选分配："><a href="#4-一旦json文件准备就绪，使用分区重新分配工具来生成候选分配：" class="headerlink" title="4.一旦json文件准备就绪，使用分区重新分配工具来生成候选分配："></a>4.一旦json文件准备就绪，使用分区重新分配工具来生成候选分配：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --topics-to-move-json-file topics-to-move.json --broker-list <span class="string">"1,2,3,4"</span> --generate</div><div class="line"></div><div class="line"></div><div class="line">[jollybi@kafka3 kafka_2.10-0.9.0.1]$ bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281 --topics-to-move-json-file topics-to-move.json --broker-list <span class="string">"1,2,3,4"</span> --generate</div><div class="line">Current partition replica assignment</div><div class="line"></div><div class="line">&#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:[&#123;<span class="string">"topic"</span>:<span class="string">"countly_metrics"</span>,<span class="string">"partition"</span>:7,<span class="string">"replicas"</span>:[1,3,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:6,<span class="string">"replicas"</span>:[1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_pv"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[3,2,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_pv"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[1,3,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:5,<span class="string">"replicas"</span>:[1,3,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[1,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_apppush"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[2,3,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_session"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[1,3,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:9,<span class="string">"replicas"</span>:[2,1,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_metrics"</span>,<span class="string">"partition"</span>:2,<span class="string">"replicas"</span>:[2,1,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_session"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[3,1,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_pv"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[2,3,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_apppush"</span>,<span class="string">"partition"</span>:6,<span class="string">"replicas"</span>:[2,3,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:11,<span class="string">"replicas"</span>:[1,3,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_pv"</span>,<span class="string">"partition"</span>:3,<span class="string">"replicas"</span>:[1,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_metrics"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[1,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_metrics"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[2,1,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:10,<span class="string">"replicas"</span>:[3,2,1]&#125;,</div><div class="line">.....</div><div class="line"></div><div class="line">Proposed partition reassignment configuration</div><div class="line"></div><div class="line">&#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:[&#123;<span class="string">"topic"</span>:<span class="string">"countly_event"</span>,<span class="string">"partition"</span>:6,<span class="string">"replicas"</span>:[3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_metrics"</span>,<span class="string">"partition"</span>:7,<span class="string">"replicas"</span>:[4,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_pv"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[2,1,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_pv"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[2,3,4]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:5,<span class="string">"replicas"</span>:[3,4,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[2,4,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_apppush"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[1,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:9,<span class="string">"replicas"</span>:[3,1,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_session"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[1,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_metrics"</span>,<span class="string">"partition"</span>:2,<span class="string">"replicas"</span>:[3,4,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_session"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[1,4,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_pv"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[2,4,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_apppush"</span>,<span class="string">"partition"</span>:6,<span class="string">"replicas"</span>:[3,1,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_imp"</span>,<span class="string">"partition"</span>:11,<span class="string">"replicas"</span>:[1,3,4]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"countly_pv"</span>,<span class="string">"partition"</span>:3,<span class="string">"replicas"</span>:[1,2,3]&#125;</div><div class="line">......</div></pre></td></tr></table></figure><p>其中的<code>Current partition replica assignment</code>指的是迁移前的partition replica；Proposed partition reassignment configuration 指的就是迁移分配规则json。需要将该<code>[Proposed partition reassignment configuration]</code>json文件保存到json文件中(如expand-cluster-reassignment.json)</p><p>该工具会生成一个候选分配，将所有分区从主题<code>countly_apppush，countly_event...</code>移动到<code>brokers 1,2,3,4</code>但是，请注意，在这一点上，分区运动还没有开始，它只是告诉你当前的任务和建议的新任务。应该保存当前的分配，以防你想要回滚到它。新的任务应该保存在一个json文件中<code>（例如expand-cluster-reassignment.json）</code>，并用<code>--execute</code>选项输入到工具中，如下所示:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281  --reassignment-json-file expand-cluster-reassignment.json --execute</div><div class="line">Current partition replica assignment</div><div class="line"></div><div class="line">&#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:[&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:9,<span class="string">"replicas"</span>:[2,3,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:7,<span class="string">"replicas"</span>:[3,2,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:6,<span class="string">"replicas"</span>:[2,1,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:5,<span class="string">"replicas"</span>:[1,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[1,3,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:2,<span class="string">"replicas"</span>:[1,3,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:1,<span class="string">"replicas"</span>:[3,2,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[3,1,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[2,1,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:10,<span class="string">"replicas"</span>:[3,1,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:3,<span class="string">"replicas"</span>:[2,3,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:11,<span class="string">"replicas"</span>:[1,2,3]&#125;]&#125;</div><div class="line"></div><div class="line">Save this to use as the --reassignment-json-file option during rollback</div><div class="line">Successfully started reassignment of partitions &#123;<span class="string">"version"</span>:1,<span class="string">"partitions"</span>:[&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:3,<span class="string">"replicas"</span>:[3,4,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:8,<span class="string">"replicas"</span>:[4,3,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[4,1,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:6,<span class="string">"replicas"</span>:[2,4,1]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:11,<span class="string">"replicas"</span>:[3,2,4]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:1,<span class="string">"replicas"</span>:[1,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:10,<span class="string">"replicas"</span>:[2,1,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:4,<span class="string">"replicas"</span>:[4,2,3]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:9,<span class="string">"replicas"</span>:[1,4,2]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:2,<span class="string">"replicas"</span>:[2,3,4]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:5,<span class="string">"replicas"</span>:[1,3,4]&#125;,&#123;<span class="string">"topic"</span>:<span class="string">"mongotail_lz4_imp"</span>,<span class="string">"partition"</span>:7,<span class="string">"replicas"</span>:[3,1,2]&#125;]&#125;</div><div class="line">``` </div><div class="line"></div><div class="line"></div><div class="line">最后，可以使用`--verify`选项来检查分区重新分配的状态。</div><div class="line">请注意，相同的`expand-cluster-reassignment.json`（与`--execute`选项一起使用）应该与--verify选项一起使用</div><div class="line"></div><div class="line">```BASH</div><div class="line">/bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281  --reassignment-json-file expand-cluster-reassignment.json  --verify</div><div class="line"></div><div class="line">[jollybi@kafka4 kafka_2.10-0.9.0.1]$ ./bin/kafka-reassign-partitions.sh --zookeeper 10.155.90.153:2281,10.155.90.155:2281,10.155.90.138:2281  --reassignment-json-file expand-cluster-reassignment.json  --verify</div><div class="line">Status of partition reassignment:</div><div class="line">Reassignment of partition [countly_event,2] completed successfully</div><div class="line">Reassignment of partition [countly_session,7] completed successfully</div><div class="line">Reassignment of partition [countly_pv,5] completed successfully</div><div class="line">Reassignment of partition [countly_apppush,1] completed successfully</div><div class="line">Reassignment of partition [countly_event,0] completed successfully</div><div class="line">Reassignment of partition [countly_session,10] completed successfully</div><div class="line">Reassignment of partition [countly_apppush,4] completed successfully</div><div class="line">Reassignment of partition [countly_event,7] is still <span class="keyword">in</span> progress</div><div class="line">Reassignment of partition [countly_metrics,7] completed successfully</div><div class="line">Reassignment of partition [countly_imp,4] is still <span class="keyword">in</span> progress</div><div class="line">Reassignment of partition [countly_apppush,10] completed successfully</div><div class="line">Reassignment of partition [countly_imp,5] is still <span class="keyword">in</span> progress</div><div class="line">.....</div></pre></td></tr></table></figure><p>注意：在迁移过程中不能人为的结束或停止kafka服务，不然会有数据不一致的问题.</p><h3 id="Communicative-learning"><a href="#Communicative-learning" class="headerlink" title="Communicative learning:"></a>Communicative learning:</h3><p>🐧  Linux shell_ senior operation and maintenance faction: QQ group <code>459096184</code> circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)<br>🐧  BigData-Exchange School:QQ group <code>521621407</code> circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join</p><p>Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;kafka的扩容难点：&quot;&gt;&lt;a href=&quot;#kafka的扩容难点：&quot; class=&quot;headerlink&quot; title=&quot;kafka的扩容难点：&quot;&gt;&lt;/a&gt;kafka的扩容难点：&lt;/h4&gt;&lt;p&gt;1）主要在于增加机器之后，数据需要rebalance到新增的空闲节点，即把partitions迁移到空闲机器上。&lt;br&gt;kafka提供&lt;code&gt;bin/kafka-reassign-partitions.sh&lt;/code&gt;工具，完成parttition的迁移。&lt;/p&gt;
&lt;p&gt;2）kafka的集群的数据量加大，数据rebalance的时间较长。解决办法是把&lt;code&gt;log.retention.hours=1&lt;/code&gt;设置一小时（生产参数24小时）。&lt;br&gt;修改参数之后，重启kakfa节点，kafka会主动purge 1小时之前的log数据。&lt;br&gt;以下是kafka_0.8.1.1版本kafkka集群扩容操作记录，从3台物理机扩容到4台物理，partition数量由24个增加到28个。&lt;/p&gt;
    
    </summary>
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/categories/Bigdata-Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka性能优化–JVM参数配置优化</title>
    <link href="http://blog.yancy.cc/2017/07/04/Bigdata-hadoop/Kafka/kafka%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E2%80%93JVM%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/"/>
    <id>http://blog.yancy.cc/2017/07/04/Bigdata-hadoop/Kafka/kafka性能优化–JVM参数配置优化/</id>
    <published>2017-07-04T03:22:00.000Z</published>
    <updated>2017-11-30T13:55:40.566Z</updated>
    
    <content type="html"><![CDATA[<p><figure class="figure"><img src="https://cdn2.hubspot.net/hubfs/540072/Kafka_Connect_graphic.png" alt="img-w650"><figcaption class="figure__caption">img-w650</figcaption></figure></p><h3 id="Kafka集群稳定"><a href="#Kafka集群稳定" class="headerlink" title="Kafka集群稳定"></a>Kafka集群稳定</h3><p>GC调优<br>　　调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent，具体参数设置可以参考Google，这里不赘述。</p><p>　　Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent 分别设置为 20毫秒和 35。Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入。<br><a id="more"></a></p><h3 id="Kafka-Broker个数决定因素"><a href="#Kafka-Broker个数决定因素" class="headerlink" title="Kafka Broker个数决定因素"></a>Kafka Broker个数决定因素</h3><p>　　磁盘容量：首先考虑的是所需保存的消息所占用的总磁盘容量和每个broker所能提供的磁盘空间。如果Kafka集群需要保留 10 TB数据，单个broker能存储 2 TB，那么我们需要的最小Kafka集群大小5个broker。此外，如果启用副本参数，则对应的存储空间需至少增加一倍（取决于副本参数）。这意味着对应的Kafka集群至少需要 10 个broker。</p><p>　　请求量：另外一个要考虑的是Kafka集群处理请求的能力。这主要取决于对Kafka client请求的网络处理能力，特别是，有多个consumer或者网路流量不稳定。如果，高峰时刻，单个broker的网络流量达到80%，这时是撑不住两个consumer的，除非有两个broker。再者，如果启用了副本参数，则需要考虑副本这个额外的consumer。也可以扩展多个broker来减少磁盘的吞吐量和系统内存。</p><h3 id="主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。"><a href="#主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。" class="headerlink" title="主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。"></a>主要是启动脚本和log4j基本参数的设置和优化，这些参数藏的比较深。</h3><h3 id="1、JVM参数配置优化"><a href="#1、JVM参数配置优化" class="headerlink" title="1、JVM参数配置优化"></a>1、JVM参数配置优化</h3><p>如果使用的CMS GC算法，建议JVM Heap不要太大，在4GB以内就可以。JVM太大，导致Major GC或者Full GC产生的“stop the world”时间过长，导致broker和zk之间的session超时，比如重新选举controller节点和提升follow replica为leader replica。<br>JVM也不能过小，否则会导致频繁地触发gc操作，也影响Kafka的吞吐量。另外，需要避免CMS GC过程中的发生promotion failure和concurrent failure问题。CMSInitiatingOccupancyFraction=70可以预防concurrent failure问题，提前出发Major GC。<br>Kafka JVM参数可以直接修改启动脚本<code>bin/kafka-server-start.sh</code><br>中的变量值。下面是一些基本参数，也可以根据实际的gc状况和调试GC需要增加一些相关的参数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx8G -Xms8G -Xmn4G -XX:PermSize=64m -XX:MaxPermSize=128m  -XX:SurvivorRatio=6  -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly"</span></div></pre></td></tr></table></figure><p>需要关注gc日志中的YGC时间以及CMS GC里面的CMS-initial-mark和CMS-remark两个阶段的时间，这些GC过程是“stop the world”方式完成的。</p><blockquote><p>jdk1.8 优化的话会提示MaxPermSize=128m,PermSize=64m 字面意思是MaxPermSize不需要我们配置了,jdk1.8 版本功能其实已不需要这个优化参数：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka3 kafka_2.10-0.8.2.1]$ /data/tools/kafka_2.10-0.8.2.1/bin/kafka-server-start.sh /data/tools/kafka_2.10-0.8.2.1/config/server.properties &amp;</div><div class="line">[1] 10312</div><div class="line">[jollybi@kafka3 kafka_2.10-0.8.2.1]$ Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=64m; support was removed <span class="keyword">in</span> 8.0</div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed <span class="keyword">in</span> 8.0</div><div class="line"></div><div class="line">优化参数：</div><div class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx4G -Xms4G -Xmn2G  -XX:SurvivorRatio=6  -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly"</span></div><div class="line"><span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span>  <span class="comment">### Kafka Manager监控监听jmx端口,如果没有可不设置。</span></div></pre></td></tr></table></figure><h3 id="2、打开JMX端口"><a href="#2、打开JMX端口" class="headerlink" title="2、打开JMX端口"></a>2、打开JMX端口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">主要是为了通过JMX端口监控Kafka Broker信息。可以在bin/kafka-server-start.sh中打开JMX端口变量。</div><div class="line"><span class="built_in">export</span> JMX_PORT=9999</div></pre></td></tr></table></figure><h3 id="3、调整log4j的日志级别"><a href="#3、调整log4j的日志级别" class="headerlink" title="3、调整log4j的日志级别"></a>3、调整log4j的日志级别</h3><p>如果集群中topic和partition数量较大时，因为log4j的日志级别太低，导致进程持续很长的时间在打印日志。日志量巨大，导致很多额外的性能开销。特别是contoller日志级别为trace级别，这点比较坑。<br>Tips 通过JMX端口设置log4j日志级别，不用重启broker节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">设置日志级别：</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController <span class="built_in">set</span>LogLevel=kafka.controller,INFO</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController <span class="built_in">set</span>LogLevel=state.change.logger,INFO</div><div class="line"> </div><div class="line">检查日志级别：</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController getLogLevel=kafka.controller</div><div class="line">java -jar cmdline-jmxclient-0.10.3.jar - localhost:9999 kafka:<span class="built_in">type</span>=kafka.Log4jController getLogLevel=state.change.logger</div></pre></td></tr></table></figure><h3 id="4、性能优化技巧"><a href="#4、性能优化技巧" class="headerlink" title="4、性能优化技巧"></a>4、性能优化技巧</h3><p>4.1、配置合适的partitons数量。</p><p>这似乎是kafka新手必问得问题。首先，我们必须理解，partiton是kafka的并行单元。从producer和broker的视角看，向不同的partition写入是完全并行的；而对于consumer，并发数完全取决于partition的数量，即，如果consumer数量大于partition数量，则必有consumer闲置。所以，我们可以认为kafka的吞吐与partition时线性关系。partition的数量要根据吞吐来推断，假定p代表生产者写入单个partition的最大吞吐，c代表消费者从单个partition消费的最大吞吐，我们的目标吞吐是t，那么partition的数量应该是t/p和t/c中较大的那一个。实际情况中，p的影响因素有批处理的规模，压缩算法，确认机制和副本数等，然而，多次benchmark的结果表明，单个partition的最大写入吞吐在10MB/sec左右；c的影响因素是逻辑算法，需要在不同场景下实测得出。</p><p>这个结论似乎太书生气和不实用。我们通常建议partition的数量一定要大于等于消费者的数量来实现最大并发。官方曾测试过1万个partition的情况，所以不需要太担心partition过多的问题。下面的知识会有助于读者在生产环境做出最佳的选择：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">a、一个partition就是一个存储kafka-log的目录。</div><div class="line">b、一个partition只能寄宿在一个broker上。</div><div class="line">c、单个partition是可以实现消息的顺序写入的。</div><div class="line">d、单个partition只能被单个消费者进程消费，与该消费者所属于的消费组无关。这样做，有助于实现顺序消费。</div><div class="line">e、单个消费者进程可同时消费多个partition，即partition限制了消费端的并发能力。</div><div class="line">f、partition越多则file和memory消耗越大，要在服务器承受服务器设置。</div><div class="line">g、每个partition信息都存在所有的zk节点中。</div><div class="line">h、partition越多则失败选举耗时越长。</div><div class="line">k、offset是对每个partition而言的，partition越多，查询offset就越耗时。</div><div class="line">i、partition的数量是可以动态增加的（只能加不能减）。</div></pre></td></tr></table></figure><p>我们建议的做法是，如果是3个broker的集群，有5个消费者，那么建议partition的数量是15，也就是broker和consumer数量的最小公倍数。当然，也可以是一个大于消费者的broker数量的倍数，比如6或者9，还请读者自行根据实际环境裁定。</p><h3 id="Communicative-learning"><a href="#Communicative-learning" class="headerlink" title="Communicative learning:"></a>Communicative learning:</h3><p>🐧  Linux shell_ senior operation and maintenance faction: QQ group <code>459096184</code> circle (system operation and maintenance - application operation and maintenance - automation operation and maintenance - virtualization technology research, welcome to join)<br>🐧  BigData-Exchange School:QQ group <code>521621407</code> circles (big data Yun Wei) (Hadoop developer) (big data research enthusiasts) welcome to join</p><p>Bidata have internal WeChat exchange group, learn from each other, join QQ group has links.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn2.hubspot.net/hubfs/540072/Kafka_Connect_graphic.png&quot; alt=&quot;img-w650&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Kafka集群稳定&quot;&gt;&lt;a href=&quot;#Kafka集群稳定&quot; class=&quot;headerlink&quot; title=&quot;Kafka集群稳定&quot;&gt;&lt;/a&gt;Kafka集群稳定&lt;/h3&gt;&lt;p&gt;GC调优&lt;br&gt;　　调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent，具体参数设置可以参考Google，这里不赘述。&lt;/p&gt;
&lt;p&gt;　　Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，MaxGCPauseMillis 和 InitiatingHeapOccupancyPercent 分别设置为 20毫秒和 35。Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入。&lt;br&gt;
    
    </summary>
    
      <category term="Bigdata Hadoop" scheme="http://blog.yancy.cc/categories/Bigdata-Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Bigdata-如何手动更新Kafka中某个Topic的偏移量</title>
    <link href="http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/Kafka/%E5%A6%82%E4%BD%95%E6%89%8B%E5%8A%A8%E6%9B%B4%E6%96%B0Kafka%E4%B8%AD%E6%9F%90%E4%B8%AATopic%E7%9A%84%E5%81%8F%E7%A7%BB%E9%87%8F/"/>
    <id>http://blog.yancy.cc/2017/06/29/Bigdata-hadoop/Kafka/如何手动更新Kafka中某个Topic的偏移量/</id>
    <published>2017-06-29T06:46:00.000Z</published>
    <updated>2017-10-30T10:31:35.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="如何手动更新Kafka中某个Topic的偏移量"><a href="#如何手动更新Kafka中某个Topic的偏移量" class="headerlink" title="如何手动更新Kafka中某个Topic的偏移量"></a>如何手动更新Kafka中某个Topic的偏移量</h3><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/kafka-streams-poc-old-solution.jpg" alt=""></figure></p><p>我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如iteblog主题分区10的偏移量获取如下：<br>在有些场景下，这个工具不满足我们的需求，我们需要的是能够手动设置分区的偏移量为任何有意义的值，而不仅仅是earliest或者latest。那咋办？</p><a id="more"></a><p>我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为<code>/consumers/[groupId]/offsets/[topic]/[partitionId]</code>，比如<code>mongotail_lz4</code>主题分区10的偏移量获取如下：　　</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 2]  get /consumers/ibm_event/offsets/mongotail_lz4/10</div><div class="line">293894</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x6000018c9</div><div class="line">mtime = Wed Jul 26 18:18:27 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 20</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 6</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure><p>所以，我们可以通过set命令来设置某个分区的偏移量，如下；</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[zk: 127.0.0.1:2281(CONNECT get /consumers/ibm_event/offsets/mongotail_lz4/10 0 </div><div class="line">0</div><div class="line">cZxid = 0x6000011f3</div><div class="line">ctime = Wed Jul 26 17:57:27 CST 2017</div><div class="line">mZxid = 0x60000204c</div><div class="line">mtime = Wed Jul 26 18:37:21 CST 2017</div><div class="line">pZxid = 0x6000011f3</div><div class="line">cversion = 0</div><div class="line">dataVersion = 21</div><div class="line">aclVersion = 0</div><div class="line">ephemeralOwner = 0x0</div><div class="line">dataLength = 1</div><div class="line">numChildren = 0</div></pre></td></tr></table></figure><p>12个分区分别更新过去。</p><h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p><p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;如何手动更新Kafka中某个Topic的偏移量&quot;&gt;&lt;a href=&quot;#如何手动更新Kafka中某个Topic的偏移量&quot; class=&quot;headerlink&quot; title=&quot;如何手动更新Kafka中某个Topic的偏移量&quot;&gt;&lt;/a&gt;如何手动更新Kafka中某个Topic的偏移量&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://7xrthw.com1.z0.glb.clouddn.com/kafka-streams-poc-old-solution.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们都知道，Kafka topic的偏移量一般都是存储在Zookeeper中，具体的路径为&lt;code&gt;/consumers/[groupId]/offsets/[topic]/[partitionId]&lt;/code&gt;，比如iteblog主题分区10的偏移量获取如下：&lt;br&gt;在有些场景下，这个工具不满足我们的需求，我们需要的是能够手动设置分区的偏移量为任何有意义的值，而不仅仅是earliest或者latest。那咋办？&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://blog.yancy.cc/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Kafka" scheme="http://blog.yancy.cc/tags/Kafka/"/>
    
  </entry>
  
</feed>
