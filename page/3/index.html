<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Yancy&#39;s blog</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Yancy&#39;s blog" type="application/atom+xml">
    
</head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/" class="header__link">Home</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/archives" class="header__link">ARCHIVES</a>
			
				<a href="/about/About" class="header__link">ABOUT</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Yancy&#39;s blog</a></h1>
		<h2 class="header__subtitle">SIMPLICITY IS PREREQUISITE FOR  RELIABILITY</h2>
	</header>

	<main>
		
	<span class="different-posts different-posts_earlier">📖 <a href="/page/2">earlier posts</a> 📖</span>




	<article>
	
		<h1><a href="/2017/04/27/Bigdata-hadoop/zookeeper/Bigdata-Zookeeper集群日志配置详解和清理自定义启动内存 /">Bigdata-Zookeeper集群日志配置详解和清理自定义启动内存</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-04-27</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Bigdata/">Bigdata</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/ZooKeeper/">ZooKeeper</a>
			</span>
		
	</div>

	

	
		<h4 id="Zookeeper集群日志配置详解和清理自定义启动内存"><a href="#Zookeeper集群日志配置详解和清理自定义启动内存" class="headerlink" title="Zookeeper集群日志配置详解和清理自定义启动内存"></a>Zookeeper集群日志配置详解和清理自定义启动内存</h4><h3 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h3><p>搭建zookeeper和kafka集群运行大数据处理数据消费，公司dubbo使用zookeeper做服务端的服务发现管理及配置中心，在使用时都出现过由于zk的日志大小过大塞满磁盘的情况 ，遇到了Zookeeper日志问题输出路径的问题, 发现zookeeper设置log4j.properties不能解决日志路径问题, 发现解决方案如下。</p>
<h3 id="zookeeper日志说明"><a href="#zookeeper日志说明" class="headerlink" title="zookeeper日志说明"></a>zookeeper日志说明</h3><p>ZooKeeper使用<code>SLF4J(the Simple Logging Facade for Java)</code>作为日志的抽象层，默认使用<code>Log4J</code>来做实际的日志工作。使用2层日志抽象看起来真是够呛，这里简要的说明如何来配置<code>Log4J</code>。尽管Log4J非常灵活且强大，但它也有一些复杂，可以用一整本书来描述它，这里只是简要的介绍一下基本的用法。</p>
<p>Log4J的配置文件名为<code>log4j.properties</code>，从classpath中查找。如果没有找到log4j.properties文件，会输出如下警告信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">log</span>4j:WARN No appenders could be found <span class="keyword">for</span> logger (org.apache.zookeeper.serv ...  </div><div class="line"><span class="built_in">log</span>4j:WARN Please initialize the <span class="built_in">log</span>4j system properly.</div></pre></td></tr></table></figure>
<p>它说的是所有后续的日志消息会被丢弃，通常<code>log4j.properties</code>文件会放在<code>conf</code>文件夹，并放在<code>classpath</code>下。<br>来看看<code>ZooKeeper</code>使用的<code>log4j.properties</code>的主要部分：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka2 conf]$ cat <span class="built_in">log</span>4j.properties | grep -Pv <span class="string">"^$|^#"</span></div><div class="line">zookeeper.root.logger=INFO, CONSOLE  <span class="comment">#（1）</span></div><div class="line">zookeeper.console.threshold=INFO</div><div class="line">zookeeper.log.dir=.</div><div class="line">zookeeper.log.file=zookeeper.log</div><div class="line">zookeeper.log.threshold=DEBUG</div><div class="line">zookeeper.tracelog.dir=.                                                                     </div><div class="line">zookeeper.tracelog.file=zookeeper_trace.log</div><div class="line"><span class="built_in">log</span>4j.rootLogger=<span class="variable">$&#123;zookeeper.root.logger&#125;</span> <span class="comment">#（2）</span></div><div class="line"><span class="built_in">log</span>4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender  <span class="comment">#（3）</span></div><div class="line"><span class="built_in">log</span>4j.appender.CONSOLE.Threshold=<span class="variable">$&#123;zookeeper.console.threshold&#125;</span> <span class="comment">#（4）</span></div><div class="line"><span class="built_in">log</span>4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout <span class="comment">#（5）</span></div><div class="line"><span class="built_in">log</span>4j.appender.CONSOLE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L] - %m%n</div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender <span class="comment">#（6）</span></div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.Threshold=<span class="variable">$&#123;zookeeper.log.threshold&#125;</span> <span class="comment">#（7）</span></div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.File=<span class="variable">$&#123;zookeeper.log.dir&#125;</span>/<span class="variable">$&#123;zookeeper.log.file&#125;</span></div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.MaxFileSize=10MB</div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout</div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L] - %m%n</div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE=org.apache.log4j.FileAppender</div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE.Threshold=TRACE</div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE.File=<span class="variable">$&#123;zookeeper.tracelog.dir&#125;</span>/<span class="variable">$&#123;zookeeper.tracelog.file&#125;</span></div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayout</div><div class="line"><span class="built_in">log</span>4j.appender.TRACEFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L][%x] - %m%n</div></pre></td></tr></table></figure>
<ul>
<li><p>(1) <code>zookeeper.root.logger=INFO, CONSOLE</code><br>第一组设置以<code>zookeeper</code>开头，它们实际上是<code>Java system property</code>，可以被<code>-D</code>形式的命令行参数覆盖。<br>第一行配置了日志级别，默认的设置是说在INFO级别以下的日志会被丢弃，并且日志会使用<code>CONSOLE appender</code>输出。你可以指定多个<code>appender</code>，例如如果你想使用<code>CONSOLE appender</code>和<code>ROLLINGFILE appender</code>输出日志，那么可以配置<code>zookeeper.logger</code>为<code>INFO,CONSOLE,ROLLINGFILE</code>。</p>
</li>
<li><p>(2) <code>rootLogger</code>处理所有日志的<code>logger</code>，因为我们没有定义其他<code>logger</code>。</p>
</li>
<li><p>(3) <code>log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender</code>这一行把CONSOLE appender和实际上处理日志输出的类绑定在一起，这里是ConsoleAppender类。</p>
</li>
<li><p>(4) <code>appender</code>也可以过滤日志。这一行将过滤任何在INFO级别之下的日志，因为这是在<code>zookeeper.root.logger设置的threshold</code>。</p>
</li>
<li><p>(5) <code>appender</code>使用一个布局(layout)类在输出前对日志进行格式化。我们使用pattern layout来记录日志的级别，日期，线程信息和调用位置信息以及消息本身。</p>
</li>
<li><p>(6) RollingFileAppender实现了rolling日志文件的功能，而不是持续的写到一个单独的文件或者控制台。如果rootLogger没有关联ROLLINGFILE，则此appender会被忽略。</p>
</li>
<li><p>(7) <code>ROLLINGFILE</code>的threshold设置成<code>DEBUG</code>。因为<code>rootLogger</code>过滤了所有在<code>INFO</code>级别之下的日志，没有DEBUG日志会输出到<code>ROLLINGFILE</code>。如果你想要看到<code>DEBUG</code>日志，你必须把<code>zookeeper.root.logger</code>从<code>INFO改成DEBUG</code>。</p>
</li>
</ul>
<p>打日志会影响到进程的性能，尤其是在DEBUG级别下。同时日志会提供有价值的信息为诊断错误提供线索。一个平衡性能开销的有效方式是把appender的threshold设成DEBUG，并把rootLogger设成WARN级别，这在一般的情况都适用，一般只需要关注WARNING和它之上的日志。当你需要诊断问题时可以使用JMX动态设置为INFO或DEBUG级别，这样可以更方便定位问题。</p>
<h3 id="修改日志输出目录"><a href="#修改日志输出目录" class="headerlink" title="修改日志输出目录"></a>修改日志输出目录</h3><p>之前出现<code>zookeeper</code>在bin目录下出现了<code>zookeeper.out</code>的日志文件，经分析发现此文件是由于<code>nohup</code>命令打印的控制台日志。</p>
<p>但是，我们在<code>zoo.cfg</code>配置文件中，对日志文件进行了配置（截取部分）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataDir=/data/tools/zookeeper-3.4.5/tmp</div><div class="line">dataLogDir=/data/tools/zookeeper-3.4.5/tmp/logs</div></pre></td></tr></table></figure>
<p>其中，<code>dataDir和dataLogDir</code>是针对数据信息及数据信息日志的位置配置。但是在zookeeper内部集成了<code>log4j.properties</code>（对应配置文件在conf路径下）。</p>
<h3 id="参考log4j配置说明"><a href="#参考log4j配置说明" class="headerlink" title="参考log4j配置说明"></a>参考log4j配置说明</h3><p>打开<code>log4j.properties</code>文件，我们会发现有这样的配置，它在说明关于<code>zookeeper</code>本身的一些默认设置，但是可以被系统配置文件所覆盖。那么，在<code>log4j</code>中，<code>root是log4j</code>记录的原始起点，而这部分参数又可以被系统修改，那么系统配置在什么地方呢？</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define some default values that can be overridden by system properties</span></div><div class="line">zookeeper.root.logger=INFO, CONSOLE</div><div class="line">zookeeper.console.threshold=INFO</div><div class="line">zookeeper.log.dir=.</div><div class="line">zookeeper.log.file=zookeeper.log</div><div class="line">zookeeper.log.threshold=DEBUG</div><div class="line">zookeeper.tracelog.dir=.</div><div class="line">zookeeper.tracelog.file=zookeeper_trace.log</div></pre></td></tr></table></figure>
<p>zk日志.out及log4j日志路径配置 ：首先修改<code>bin/zkEnv.sh</code>，配置<code>ZOO_LOG_DIR</code>的环境变量，<code>ZOO_LOG_DIR</code>是zookeeper日志输出目录，<code>ZOO_LOG4J_PROP</code>是log4j日志输出的配置：</p>
<ul>
<li>默认配置：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> = <span class="string">"x"</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    ZOO_LOG_DIR=<span class="string">"."</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> = <span class="string">"x"</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    ZOO_LOG4J_PROP=<span class="string">"INFO,CONSOLE"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<ul>
<li>生产环境修改：</li>
</ul>
<p>在zk目录下面创建logs目录 给予bi组操作权限。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo mkdir /data/tools/zookeeper-3.4.5/logs/</div><div class="line">sudo chown -R jollybi:jollybi /data/tools/zookeeper-3.4.5/logs/</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> = <span class="string">"x"</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    ZOO_LOG_DIR=<span class="string">"<span class="variable">$ZOOBINDIR</span>/../logs"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> = <span class="string">"x"</span> ]</div><div class="line"><span class="keyword">then</span></div><div class="line">    ZOO_LOG4J_PROP=<span class="string">"INFO,ROLLINGFILE"</span>  //ROLLINGFILE —— 日志轮转，避免单一文件过大</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<h3 id="Zookeeper版本3-4-0以上的zk可以设置定期自动清理-参考链接"><a href="#Zookeeper版本3-4-0以上的zk可以设置定期自动清理-参考链接" class="headerlink" title="Zookeeper版本3.4.0以上的zk可以设置定期自动清理.参考链接"></a>Zookeeper版本3.4.0以上的zk可以设置定期自动清理.<a href="http://nileader.blog.51cto.com/1381108/932156" target="_blank" rel="external">参考链接</a></h3><ul>
<li>在zoo.cfg中配置：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">autopurge.purgeInterval: 24*2     <span class="comment">##这个参数指定了清理频率，单位是小时。默认是0，表示不开启自己清理功能。</span></div><div class="line">autopurge.snapRetainCount:  2	    <span class="comment">##这个参数和上面的参数搭配使用，这个参数指定了需要保留的文件数目。默认是保留3个。</span></div></pre></td></tr></table></figure>
<h3 id="配置zookeeper-out的位置及log4j日志输出"><a href="#配置zookeeper-out的位置及log4j日志输出" class="headerlink" title="配置zookeeper.out的位置及log4j日志输出"></a>配置zookeeper.out的位置及log4j日志输出</h3><p>(1)<code>zookeeper.out</code>由nohup的输出，也就是<code>zookeeper</code>的<code>stdout和stdeer</code>输出。</p>
<p>在zkServer.sh中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ ! -w <span class="string">"<span class="variable">$ZOO_LOG_DIR</span>"</span> ] ; <span class="keyword">then</span>  </div><div class="line">mkdir -p <span class="string">"<span class="variable">$ZOO_LOG_DIR</span>"</span>  </div><div class="line"><span class="keyword">fi</span>  </div><div class="line">  </div><div class="line">_ZOO_DAEMON_OUT=<span class="string">"<span class="variable">$ZOO_LOG_DIR</span>/zookeeper.out"</span>  <span class="comment">#日志输出文件路径</span></div><div class="line"></div><div class="line"><span class="comment">#nohup日志输出</span></div><div class="line">nohup <span class="variable">$JAVA</span> <span class="string">"-Dzookeeper.log.dir=<span class="variable">$&#123;ZOO_LOG_DIR&#125;</span>"</span> <span class="string">"-Dzookeeper.root.logger=<span class="variable">$&#123;ZOO_LOG4J_PROP&#125;</span>"</span> \  </div><div class="line">-cp <span class="string">"<span class="variable">$CLASSPATH</span>"</span> <span class="variable">$JVMFLAGS</span> <span class="variable">$ZOOMAIN</span> <span class="string">"<span class="variable">$ZOOCFG</span>"</span> &gt; <span class="string">"<span class="variable">$_ZOO_DAEMON_OUT</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</div></pre></td></tr></table></figure>
<p>(2)log4j日志输出配置 conf/log4j.properties中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Add ROLLINGFILE to rootLogger to get log file output  </span></div><div class="line"><span class="comment">#    Log DEBUG level and above messages to a log file  </span></div><div class="line"></div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender  //日志轮转，DaliyRollingFileAppender —— 按天轮转</div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.Threshold=<span class="variable">$&#123;zookeeper.log.threshold&#125;</span>  </div><div class="line"><span class="built_in">log</span>4j.appender.ROLLINGFILE.File=<span class="variable">$&#123;zookeeper.log.dir&#125;</span>/<span class="variable">$&#123;zookeeper.log.file&#125;</span></div></pre></td></tr></table></figure>
<p>轮转前提需要将(1)里bin/zkEnv.sh中的轮转配置好</p>
<p>4.zk事务日志查看</p>
<p>zookeeper的事务日志通过zoo.cfg文件中的dataLogDir配置项配置：</p>
<pre><code># the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
dataDir=/tmp/zookeeper    
</code></pre><p>查看事务日志方法：<br>(需要下载slf4j-api-1.6.1.jar包) </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -classpath .:slf4j-api-1.6.1.jar:zookeeper-3.4.5.jar org.apache.zookeeper.server.LogFormatter /tmp/zookeeper/version-2/xxx.xxx</div></pre></td></tr></table></figure>
<h3 id="怎么自定义zookeeper的启动内存"><a href="#怎么自定义zookeeper的启动内存" class="headerlink" title="怎么自定义zookeeper的启动内存"></a>怎么自定义zookeeper的启动内存</h3><p>运行zookeeper时，使用jmap -heap <pid> 命令查看内存情况如下:</pid></p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/zookeeper_01.png" alt=""></figure></p>
<p>解决：分配内存文件路径：<code>zookeeper/bin/zkEnv.sh</code></p>
<p>该文件已经明确说明有独立JVM内存的设置文件，路径是<code>zookeeper/conf/java.env</code><br>安装的时候这个路径下没有有<code>java.env</code>文件，需要自己新建一个：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># vim java.env</span></div><div class="line"></div><div class="line"><span class="meta">#!/bin/sh</span></div><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk</div><div class="line"><span class="comment"># heap size MUST be modified according to cluster environment</span></div><div class="line"><span class="built_in">export</span> JVMFLAGS=<span class="string">"-Xms4g -Xmx4g <span class="variable">$JVMFLAGS</span>"</span></div><div class="line"></div><div class="line">对于内存的分配，还是根据项目和机器情况而定。如果内存够用，适当的大点可以提升zk性能。</div></pre></td></tr></table></figure>

	

	

</article>




	<article>
	
		<h1><a href="/2017/04/05/运维笔记/【翻译】监控不只是为了处理故障/">【翻译】监控不只是为了处理故障</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-04-05</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/运维笔记/">运维笔记</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Performance-monitoring/">Performance monitoring</a>
			</span>
		
	</div>

	

	
		<h3 id="【翻译】监控不只是为了处理故障"><a href="#【翻译】监控不只是为了处理故障" class="headerlink" title="【翻译】监控不只是为了处理故障"></a>【翻译】监控不只是为了处理故障</h3><p>有一种普遍的想法认为监控只是在系统出错的时候能够发出报警，我们一直相信，监控系统的方方面面给予我们洞察业务内部机制的能力，并驱动我们做出决策。<br>不同的人有不同的监控方式，手动查看日志，接受报警或者24x7值守查看各种图表。不管你用那种方式，最重要的是及时收到报错和警告。<br>但是监控其实能做的更多。</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Prometheus-Overview.png" alt=""></figure></p>
<p>监控不止用于故障时候发报警</p>
<p>监控系统不应该只是告诉你“这有一个问题”，它也要帮助你排查问题出现的原因,你不只是想在服务变慢的时候收到报警，你其实想知道过去几小时流量是否增加了，机器是不是宕机了，后端服务器是不是变慢了或者内部执行时间变长了。</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Prometheus-Overview-1.png" alt=""></figure><br>在你的应用中集成监控模块</p>
<p>做为报警和排错重要手段，监控要在你的架构体系中拥有核心位置。要集成监控工具在你的所有代码逻辑中，而不仅仅是在边缘区域添加一点。跟踪每个请求提交api需要多少组件，以便你在设计新的存储系统是否能够知道要处理多少数据，跟踪内存的命中率，以便你能度量它们随着流量模式变化的规律，甚至分解到memcached中来节省内存和提高命中率。跟踪用户命中你的业务逻辑的”慢路径“，以便你知道它什么时候可以更快，并且当新的功能使用”慢路径“的时候，你已经准备好了让它”延迟命中“。<br>在你的代码内部，做这样大量的细节工作，可以让你前进的更快，技术和产品决策会受益良多，根本上提升了你的效率。这就是监控的意义所在，监控不只是为了处理故障。</p>
<blockquote>
<p>原文 <a href="https://www.robustperception.io/monitoring-not-just-for-outages/" target="_blank" rel="external">https://www.robustperception.io/monitoring-not-just-for-outages/</a></p>
</blockquote>
<h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/03/30/日志分析平台/Elasticsearch/研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？/">研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-03-30</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Log-Analysis-Platform/">Log Analysis Platform</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Elasticsearch/">Elasticsearch</a>
			</span>
		
	</div>

	

	
		<p><img src="http://img13.deviantart.net/1fb9/i/2013/051/6/1/ella_vs__elastic_yoko_by_gamepal-d5vmtjm.png" alt=""></p>
<p>ES 现在普遍大公司都已经在使用，有的用来做数据存储，也有做性能监控，也有做日志收集，跟大数据结合做日志分析。<br>今天在家无聊研究官网学习了下Elasticsearch5.0.0 功能新增哪些，个人觉得ES也是现在开源日志分析平台比较火的，从14年开始陆续使用频率不断提升。<br>之前也有了解过Graylog  现在比ES 做数据存储分析 不错的是Graylog 比ES性能好。<br>废话不多说了哈哈</p>
		<p><a class="article__read-more-link" href="/2017/03/30/日志分析平台/Elasticsearch/研究学习 Elasticsearch 5.0.0 功能提升哪些,ES如何弹性迁移日志数据，安装需要什么环境？/">...read more</a></p>
	

	

</article>




	<article>
	
		<h1><a href="/2017/03/30/Bigdata-hadoop/Kafka/Kafka日志存储解析与实践数据存储优化/">Kafka日志存储解析与实践数据存储优化</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-03-30</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Bigdata-Hadoop/">Bigdata Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<p><img src="https://cdn-images-1.medium.com/max/640/1*kV768OHeCILNIYVCNPUyuQ.jpeg" alt=""></p>
<h4 id="Kafka的名词解释"><a href="#Kafka的名词解释" class="headerlink" title="Kafka的名词解释"></a>Kafka的名词解释</h4><p>kafka是一款分布式消息发布和订阅的系统，具有高性能和高吞吐率。 </p>
<ul>
<li>1，<code>Broker</code>： 一个单独的kafka机器节点就称为一个broker，多个broker组成的集群，称为kafka集群</li>
<li>2，<code>Topic</code>：类似数据库中的一个表，我们将数据存储在Topic里面，当然这只是逻辑上的，在物理上，一个Topic 可能被多个Broker分区存储，这对用户是透明的，用户只需关注消息的产生于消费即可.</li>
<li>3，<code>Partition</code>：类似分区表，每个Topic可根据设置将数据存储在多个整体有序的Partition中，每个顺序化partition会生成2个文件，一个是index文件一个是log文件，index文件存储索引和偏移量，log文件存储具体的数据.</li>
<li>4，<code>Producer</code>：生产者，向Topic里面发送消息的角色 </li>
<li>5，<code>Consumer</code>：消费者，从Topic里面读取消息的角色 </li>
<li>6，<code>Consumer Group</code>：每个Consumer属于一个特定的消费者组，可为Consumer指定group name，如果不指定默认属于group </li>
</ul>
		<p><a class="article__read-more-link" href="/2017/03/30/Bigdata-hadoop/Kafka/Kafka日志存储解析与实践数据存储优化/">...read more</a></p>
	

	

</article>




	<article>
	
		<h1><a href="/2017/03/29/Bigdata-hadoop/countly/Bigdata-countly需要迁移/">Bigdata-countly需要迁移</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-03-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Bigdata-Hadoop/">Bigdata Hadoop</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Bigdata-Hadoop/">Bigdata Hadoop</a> <a class="article__tag-link" href="/tags/Countly/">Countly</a>
			</span>
		
	</div>

	

	
		<h4 id="官方安装文档：http-resources-count-ly-docs-installing-countly-server"><a href="#官方安装文档：http-resources-count-ly-docs-installing-countly-server" class="headerlink" title="官方安装文档：http://resources.count.ly/docs/installing-countly-server"></a>官方安装文档：<a href="http://resources.count.ly/docs/installing-countly-server" target="_blank" rel="external">http://resources.count.ly/docs/installing-countly-server</a></h4><p>目前<code>countly</code>需要迁移，所需<code>countly</code>版本于官方提供的安装方案有冲突，所以如下安装：</p>
<p>安装官方<code>countly</code>让其设置所需环境变量及其启动脚本，手动指定安装nojs版本，拷贝原countly文件，具体如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1、<span class="built_in">cd</span> /data <span class="comment">#countly安装在data目录 我看了安装脚本，是当前在哪个目录，安装文件就在哪个目录</span></div><div class="line">wget -qO- http://c.ly/install | bash</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2、rpm -qa | grep -i nodejs | xargs -I&#123;&#125;  yum remove &#123;&#125; -y</div><div class="line">卸载掉官网安装的最新nodejs 然后新建如下yum源，用于安装旧版所需nodejs，也可以到nodejs官网下载所需nodejs</div><div class="line">cat /etc/yum.repos.d/nodesource-el.repo </div><div class="line">[nodesource]</div><div class="line">name=Node.js Packages <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span></div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/<span class="variable">$basearch</span></div><div class="line">failovermethod=priority</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">[nodesource-source]</div><div class="line">name=Node.js <span class="keyword">for</span> Enterprise Linux 7 - <span class="variable">$basearch</span> - Source</div><div class="line">baseurl=https://rpm.nodesource.com/pub_5.x/el/7/SRPMS</div><div class="line">failovermethod=priority</div><div class="line">enabled=0</div><div class="line">gpgkey=file:///etc/pki/rpm-gpg/NODESOURCE-GPG-SIGNING-KEY-EL</div><div class="line">gpgcheck=1</div></pre></td></tr></table></figure>
<h3 id="安装老版本所需nodejs"><a href="#安装老版本所需nodejs" class="headerlink" title="安装老版本所需nodejs"></a>安装老版本所需nodejs</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum <span class="keyword">install</span> nodejs</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">3、拷贝源countly文件到/data/countly目录</div><div class="line">修改 /data/countly/api/config.js 和 /data/countly/frontend/express/config.js      </div><div class="line">3001端口和 6001端口监听地址换成 本地私有地址   <span class="comment">#源文件是监听的原来机器的内网地址，不修改的话，服务启动不了。</span></div></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="number">4</span><span class="string">、拷贝mongo数据目录到/data/mongo目录，修改mongo配置文件.</span></div><div class="line"></div><div class="line"><span class="string">cat</span> <span class="string">/etc/mongod.conf</span></div><div class="line"><span class="attr">systemLog:</span></div><div class="line"><span class="attr">       destination:</span> <span class="string">file</span></div><div class="line"><span class="attr">       logAppend:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       path:</span> <span class="string">/data/mongodb/mongod.log</span></div><div class="line"><span class="attr">storage:</span></div><div class="line"><span class="attr">       dbPath:</span> <span class="string">/data/mongo</span></div><div class="line"><span class="attr">       journal:</span></div><div class="line"><span class="attr">             enabled:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       engine:</span> <span class="string">mmapv1</span></div><div class="line"><span class="attr">processManagement:</span></div><div class="line"><span class="attr">       fork:</span> <span class="literal">true</span></div><div class="line"><span class="attr">       pidFilePath:</span> <span class="string">/data/mongodb/mongod.pid</span></div><div class="line"><span class="attr">net:</span></div><div class="line"><span class="attr">       port:</span> <span class="number">27017</span></div><div class="line"><span class="attr">       bindIp:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></div><div class="line"><span class="attr">security:</span></div><div class="line"><span class="attr">       authorization:</span> <span class="string">enabled</span></div><div class="line"><span class="attr">operationProfiling:</span></div><div class="line"><span class="attr">       slowOpThresholdMs:</span> <span class="number">40960</span></div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Bigdatacountly01.jpeg" alt=""></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">5、修改硬盘block：</div><div class="line">     blockdev --setra 256 /dev/mapper/xvdc--vg-xvdc–lv    <span class="comment">##按照mongo提示操作</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">6、修改nginx配置文件  conf.d/default.conf</div><div class="line">      将127.0.0.1修改为本地私有地址</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">7、重启countly   mongodb  nginx</div><div class="line"></div><div class="line">countly restart</div><div class="line"></div><div class="line">/etc/init.d/mongod restart</div><div class="line"></div><div class="line">service nginx restart</div><div class="line">迁移完毕</div></pre></td></tr></table></figure>
<h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/03/28/Bigdata-hadoop/Kafka/Bigdata-Kafka集群快速搭建与增删改查命令讲解/">Bigdata-Kafka集群快速搭建与命令讲解</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-03-28</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Bigdata-Hadoop/">Bigdata Hadoop</a> <a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h1 id="Bigdata-Kafka集群快速搭建与命令讲解"><a href="#Bigdata-Kafka集群快速搭建与命令讲解" class="headerlink" title="Bigdata-Kafka集群快速搭建与命令讲解"></a>Bigdata-Kafka集群快速搭建与命令讲解</h1><h3 id="kafka集群和zookeeper集群规范"><a href="#kafka集群和zookeeper集群规范" class="headerlink" title="kafka集群和zookeeper集群规范"></a>kafka集群和zookeeper集群规范</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">kafka集群和zookeeper集群规范</div><div class="line">kafka版本：kafka_2.10-0.8.2.1</div><div class="line">zookeeper版本：zookeeper-3.4.5</div><div class="line"></div><div class="line">启动用户：jollybi</div><div class="line">kafka安装目录：/data/tools/</div><div class="line">kafka新建消息目录：/data/tools/kafka_2.10-0.8.2.1/kafka-logs</div><div class="line"></div><div class="line">启动用户：jollybi</div><div class="line">zookeeper安装目录：/data/tools/</div><div class="line">zookeeper新建日志目录：/data/tools/zookeeper-3.4.5/tmp/logs</div><div class="line"></div><div class="line">统一配置hosts</div><div class="line"></div><div class="line"></div><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure>
<p>本文使用了3台机器部署Kafka集群，IP和主机名对应关系如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure>
<h3 id="Step1-配置-etc-hosts-（3台一致）"><a href="#Step1-配置-etc-hosts-（3台一致）" class="headerlink" title="Step1: 配置/etc/hosts （3台一致）"></a>Step1: 配置/etc/hosts （3台一致）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">127.0.0.1 localhost.localdomain localhost</div><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure>
<h3 id="Step2-环境KafKa集群环境"><a href="#Step2-环境KafKa集群环境" class="headerlink" title="Step2: 环境KafKa集群环境"></a>Step2: 环境KafKa集群环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">下载地址：http://mirrors.hust.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.8.2.1.tgz</div><div class="line">[jollybi@kafka1 ]<span class="comment"># tar -xvf kafka_2.10-0.8.2.1.tgz -C /data/tools/</span></div><div class="line"><span class="built_in">cd</span> /data/tools/kafka_2.10-0.8.2.1/config</div></pre></td></tr></table></figure>
<h3 id="设置data目录，最好不要用默认的-tmp-kafka-logs"><a href="#设置data目录，最好不要用默认的-tmp-kafka-logs" class="headerlink" title="设置data目录，最好不要用默认的/tmp/kafka-logs"></a>设置data目录，最好不要用默认的/tmp/kafka-logs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mkdir -p /data/tools/kafka_2.10-0.8.2.1/kafka-logs/</div></pre></td></tr></table></figure>
<p><em>修改kafka配置文件</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@kafka_01 config]<span class="comment"># vim server.properties</span></div><div class="line"></div><div class="line"></div><div class="line"> 设置brokerid（从0开始，3个节点分别设为1,2,3，不能重复）在这里id=0跟zookeeper id设置一样就行。 集群机器：按顺序写1</div><div class="line">broker.id=1  </div><div class="line"></div><div class="line">auto.leader.rebalance.enable=<span class="literal">true</span> </div><div class="line"></div><div class="line"><span class="comment">#修改本地IP地址：</span></div><div class="line">listeners=PLAINTEXT://10.46.72.172:9092</div><div class="line">port=9292 //设置访问端口</div><div class="line">host.name=10.155.90.153   kafka本机IP</div><div class="line">log.dirs=/data/tools/kafka_2.10-0.8.2.1/kafka-logs</div><div class="line"></div><div class="line"><span class="comment">#设置注册地址（重要，默认会把本机的hostanme注册到zk中，客户端连接时需要解析该hostanme，所以这里直接注册本机的IP地址，避免hostname解析失败，报错java.nio.channels.UnresolvedAddressException或java.io.IOException: Can not resolve address）</span></div><div class="line"><span class="comment">#设置zookeeper地址</span></div><div class="line"><span class="comment">#zookeeper.connect=10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181</span></div><div class="line">zookeeper.connect=kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281</div><div class="line"></div><div class="line">//这里我设置hosts代替IP</div></pre></td></tr></table></figure>
<p><em>配置zookeeper地址</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">vim zookeeper.properties </div><div class="line">dataDir=/data/tools/zookeeper-3.4.5/tmp</div><div class="line"><span class="comment"># the port at which the clients will connect</span></div><div class="line">clientPort=2281</div><div class="line"><span class="comment"># disable the per-ip limit on the number of connections since this is a non-production config</span></div><div class="line">maxClientCnxns=0</div><div class="line">~</div></pre></td></tr></table></figure>
<p><em>配置kafka访问地址</em></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">vim</span> producer.properties </div><div class="line">metadata.broker.list=<span class="number">169.44.62.139:9292</span>,<span class="number">169.44.59.138:9292</span>,<span class="number">169.44.62.137:9292</span>   </div><div class="line"><span class="comment"># name of the partitioner class for partitioning events; default partition spreads data randomly</span></div><div class="line"><span class="comment">#partitioner.class=</span></div><div class="line"></div><div class="line"><span class="comment"># specifies whether the messages are sent asynchronously (async) or synchronously (sync)</span></div><div class="line">producer.type=sync</div><div class="line"></div><div class="line"><span class="comment"># specify the compression codec for all data generated: none, gzip, snappy, lz4.</span></div><div class="line"><span class="comment"># the old config values work as well: 0, 1, 2, 3 for none, gzip, snappy, lz4, respectively</span></div><div class="line">compression.codec=<span class="literal">none</span></div><div class="line"></div><div class="line"><span class="comment"># message encoder</span></div><div class="line">serializer.class=kafka.serializer.DefaultEncoder</div></pre></td></tr></table></figure>
<h3 id="Step3-集群机器快速拷贝配置"><a href="#Step3-集群机器快速拷贝配置" class="headerlink" title="Step3: 集群机器快速拷贝配置:"></a>Step3: 集群机器快速拷贝配置:</h3><p>远程复制分发安装文件<br>最好是把文件打包scp过去<br>接下来将上面的安装文件拷贝到集群中的其他机器上对应的目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scp -P58958 -r kafka_2.10-0.8.2.1 jollybi@169.44.62.137:/home/jollybi/tools/. </div><div class="line">scp -P58958 -r kafka_2.10-0.8.2.1 jollybi@169.44.62.138:/home/jollybi/tools/.</div></pre></td></tr></table></figure>
<p>统一修改分别其他两台kafka <code>server.properties</code></p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">broker.id=<span class="number">2</span>  </div><div class="line">host.name=<span class="number">169.44</span><span class="number">.62</span><span class="number">.137</span>   kafka2本机IP</div><div class="line"></div><div class="line">broker.id=<span class="number">3</span> </div><div class="line">host.name=<span class="number">169.44</span><span class="number">.62</span><span class="number">.138</span>   kafka3本机IP</div></pre></td></tr></table></figure>
<h3 id="Step4-启动-kafka集群"><a href="#Step4-启动-kafka集群" class="headerlink" title="Step4:启动 kafka集群"></a>Step4:启动 kafka集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">./kafka-server-start.sh -daemon ../config/server.properties</div><div class="line"></div><div class="line">[jollybi@kafka1 config]$ jps</div><div class="line">3443 QuorumPeerMain</div><div class="line">16280 Jps</div><div class="line">3628 Kafka</div><div class="line">Step5: Kafka常用命令(普及)</div></pre></td></tr></table></figure>
<h4 id="Step5-测试集群基本命令"><a href="#Step5-测试集群基本命令" class="headerlink" title="Step5:测试集群基本命令"></a>Step5:测试集群基本命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### 默认创建kafka Topic:  1个副本备份 1个分区消费</span></div><div class="line"></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 1 --partitions 1 --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 1 --partitions 1 --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 大数据这边需要12个分区这里我删除Topic重新创建Topic：</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 删除kafka Topic:</span></div><div class="line"></div><div class="line"></div><div class="line">第一步：删除kafka topic</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --delete --topic  mongotail_lz4 </div><div class="line"></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --delete --topic  mongotail_lz4_imp</div><div class="line"></div><div class="line"><span class="comment">### 这是由于删除topic没删干净会报错：</span></div><div class="line"></div><div class="line">org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 删除线程执行删除操作的真正逻辑是：</span></div><div class="line">1. 它首先会给当前所有broker发送更新元数据信息的请求，告诉这些broker说这个topic要删除了，你们可以把它的信息从缓存中删掉了</div><div class="line">2. 开始删除这个topic的所有分区</div><div class="line">2.1 给所有broker发请求，告诉它们这些分区要被删除。broker收到后就不再接受任何在这些分区上的客户端请求了</div><div class="line">2.2 把每个分区下的所有副本都置于OfflineReplica状态，这样ISR就不断缩小，当leader副本最后也被置于OfflineReplica状态时leader信息将被更新为-1</div><div class="line">2.3 将所有副本置于ReplicaDeletionStarted状态</div><div class="line">2.4 副本状态机捕获状态变更，然后发起StopReplicaRequest给broker，broker接到请求后停止所有fetcher线程、移除缓存，然后删除底层<span class="built_in">log</span>文件</div><div class="line">2.5 关闭所有空闲的Fetcher线程</div><div class="line">    </div><div class="line"><span class="comment">### 第二步：删除zookeeper相关的路径：</span></div><div class="line">[jollybi@kafka1 zookeeper-3.4.5]$ ./bin/zkCli.sh -server 127.0.0.1:2281 </div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /brokers/topics/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 0] rmr /consumers/group_ml_general/offsets/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /consumers/group_ml_general/owners/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 3] rmr  /config/topics/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 4] rmr /admin/delete_topics/mongotail_lz4</div><div class="line"></div><div class="line"><span class="comment">### 删除topic上面消费组 </span></div><div class="line"></div><div class="line">删除 ZooKeeper 下面的 /consumers/[group_id] 路径就可以了。ZooKeeper 原生API只支持删除空的路径，所以建议你使用 curator framework 进行这个删除操作，ZkPaths.deleteChildren 会递归式地删除整个路径（包括子路径）。</div><div class="line"></div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 0] rmr /consumers/kafka-node1-imp-group</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /consumers/kafka-node1-group</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 2] rmr /consumers/console-consumer-59053</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 3] rmr /consumers/</div><div class="line"></div><div class="line"></div><div class="line">1、设置配置文件允许删除</div><div class="line">delete.topic.enable=<span class="literal">true</span> 配置添加到 config/server.properties</div><div class="line">2、执行删除命令</div><div class="line">./bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic <span class="built_in">test</span>-topic</div><div class="line"></div><div class="line">在zookeeper中确认：</div><div class="line"></div><div class="line"> 删除zookeeper下/brokers/topics/<span class="built_in">test</span>-topic节点</div><div class="line"> 删除zookeeper下/config/topics/<span class="built_in">test</span>-topic节点</div><div class="line"> 删除zookeeper下/admin/delete_topics/<span class="built_in">test</span>-topic节点</div><div class="line"> </div><div class="line">consumer 的话 删除groupid</div><div class="line"> </div><div class="line"></div><div class="line"><span class="comment">### 重启kafka集群</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 大数据kafka建topic，不能建一个分区，那样吞吐量上不去，不够用，我们以前建了12个分区的</span></div><div class="line"><span class="comment">### 创建新 kafka Topic: 3个副本，12个分区</span></div><div class="line"></div><div class="line"> ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 3 --partitions 12  --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 3 --partitions 12  --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 查看创建的Topic:</span></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 查看集群情况：</span></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 模拟生产者（producer）kafka生产客户端生产数据命令：</span></div><div class="line"></div><div class="line">./bin/kafka-console-producer.sh --broker-list 75.126.39.124:9292,75.126.5.162:9292,75.126.5.178:9292 --topic mongotail_lz4_imp                      </div><div class="line">......</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line">^C</div><div class="line"><span class="comment">### 模拟消费者（consumer）kafka消费客户端数据命令 现在我们来看看消息：</span></div><div class="line"></div><div class="line"> ./bin/kafka-console-consumer.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --fm-beginning --topic mongotail_lz4_imp</div><div class="line">......</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line"></div><div class="line">^C</div></pre></td></tr></table></figure>
<h3 id="Kafka常用命令"><a href="#Kafka常用命令" class="headerlink" title="Kafka常用命令"></a>Kafka常用命令</h3><p>以下是kafka常用命令行总结：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">1、 list topic 显示所有topic</div><div class="line">./bin/kafka-topics.sh --list --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281</div><div class="line"></div><div class="line">2、 查看topic的详细信息  </div><div class="line"> ./bin/kafka-topics.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  -describe -topic mongotail_lz4_imp</div><div class="line"></div><div class="line">3、为topic增加partition分区 参数：--alter --partitions</div><div class="line">./bin/kafka-topics.sh  --alter --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --partitions 20 --topic mongotail_lz4_imp</div><div class="line"></div><div class="line">4、kafka生产者客户端命令  </div><div class="line">./bin/kafka-console-producer.sh --broker-list 75.126.39.124:9292,75.126.5.162:9292,75.126.5.178:9292 --topic mongotail_lz4_imp                        </div><div class="line"></div><div class="line">5、kafka消费者客户端命令  </div><div class="line">./bin/kafka-console-consumer.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --fm-beginning --topic </div><div class="line"></div><div class="line">6、kafka服务启动  </div><div class="line">./kafka-server-start.sh -daemon ../config/server.properties   </div><div class="line"></div><div class="line">7、下线broker  </div><div class="line">./kafka-run-class.sh kafka.admin.ShutdownBroker --zookeeper 127.0.0.1:2181 --broker <span class="comment">#brokerId# --num.retries 3 --retry.interval.ms 60  </span></div><div class="line">shutdown broker  </div><div class="line"></div><div class="line">8、删除topic  </div><div class="line">./kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic <span class="built_in">test</span>KJ1 --zookeeper 127.0.0.1:2181  </div><div class="line">./kafka-topics.sh --zookeeper localhost:2181 --delete --topic <span class="built_in">test</span>KJ1  </div><div class="line"></div><div class="line">9、查看consumer组内消费的offset  </div><div class="line">./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group <span class="built_in">test</span> --topic <span class="built_in">test</span>KJ1</div></pre></td></tr></table></figure>
<h4 id="Step6-Kafka存储"><a href="#Step6-Kafka存储" class="headerlink" title="Step6:Kafka存储"></a>Step6:Kafka存储</h4><p>每个replica一个目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka_2.10-0.8.2.0]<span class="comment"># cd /data/tools/kafka_2.10-0.8.2.0/kafka-logs/</span></div><div class="line">[root@master kafka-logs]<span class="comment"># ls</span></div><div class="line">__consumer_offsets-0   __consumer_offsets-20  __consumer_offsets-32  __consumer_offsets-44   my-replicatedtopic1-0</div><div class="line">__consumer_offsets-1   __consumer_offsets-21  __consumer_offsets-33  __consumer_offsets-45   my-replicated-topic1-1</div></pre></td></tr></table></figure>
<p>二级结构</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]<span class="comment"># cd my-replicated-topic1-0/</span></div><div class="line">[jollybi@kafka1  my-replicated-topic1-0]<span class="comment"># ls</span></div><div class="line">00000000000000000000.index  00000000000000000000.log</div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="http://static.zybuluo.com/sasaki/f71rtngyz2y8n3m6uxq1z2ec/%E5%9B%BE%E7%89%871.png" alt=""></figure></p>
<h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/03/15/运维笔记/APP自动化部署:开发部署-测试部署-灰度发布:蓝绿部署-生产环境等部署流程方案总结/">APP自动化部署:开发部署-测试部署-灰度发布/蓝绿部署-生产环境等部署流程方案总结</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-03-15</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/运维笔记/">运维笔记</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Automation-Deploy/">Automation Deploy</a>
			</span>
		
	</div>

	

	
		<h1 id="APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结"><a href="#APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结" class="headerlink" title="APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结"></a>APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结</h1><h4 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h4><p>个人现在在一个医疗行业公司：美年大这边负责体检APP的运维相关工作，今天主要是整理了下现在团队APP发布流程<br>方案：</p>
<p>因为在项目迭代的过程中，不可避免需要”上线”。上线对应着部署，或者重新部署；部署对应着修改；修改则意味着风险。<br>目前有很多用于部署的技术，有的简单，有的复杂；有的得停机，有的不需要停机即可完成部署。</p>
<p>个人整理下部署流程说明其实现在很多部署方法，现在我们用目前比较流行的几种部署方案，或者说策略方案对比总结简单讨论一下目前比较流行的几种部署方案，或者说策略。如有不足之处请指出，如有谬误，请指正^_^。</p>
<p>我们有自己开发环境和测试环境 ：</p>
<h4 id="开发环境部署："><a href="#开发环境部署：" class="headerlink" title="开发环境部署："></a>开发环境部署：</h4><ul>
<li>问题：</li>
</ul>
<p>开发人员每个人在自己电脑环境写代码自己电脑本机测试代码是否run成功，每个开发人员都在自己本地写完测试出现问题是各自环境不统一导致遇到坑阻碍到测试人员测试，基础的bug也会浪费太多时间。</p>
<ul>
<li>解决后：</li>
</ul>
<p>主要是解决了给予开发团队在写代码或者修改bug可以第一时间更新部署。大家统一一个开发环境这个是为了在开发阶段能立马呈现效果。</p>
<h4 id="测试环境部署："><a href="#测试环境部署：" class="headerlink" title="测试环境部署："></a>测试环境部署：</h4><ul>
<li>问题：</li>
</ul>
<p>一开始没有测试环境，直接开发环境当作测试环境去跑，发现很多问题，就是测试环境数据跟生产不一样，导致很多bug问题，测试发现测试的时候，开发在更新代码发布，耽误了测试人员的测试过程，环境不能独立都互相占用，刀子效率没有提高，bug每个星期都不断提升。</p>
<ul>
<li>解决后：</li>
</ul>
<p>主要解决了开发环境和测试环境独立，不会互相影响，测试人员有单独环境去测试生产能准确的数据对比。</p>
<h4 id="选择灰度环境部署方案："><a href="#选择灰度环境部署方案：" class="headerlink" title="选择灰度环境部署方案："></a>选择灰度环境部署方案：</h4><p>先贴个百度百科：</p>
<p>灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。AB test就是一种灰度发布方式，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。</p>
<p>灰度环境部署:</p>
<p>个人理解灰度部署是增量发布的一种类型，它的执行方式是在原有软件生产版本可用的情况下，同时部署一个新的版本。同时运行同一个软件产品的多个版本需要软件针对配置和完美自动化部署进行特别设计。</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">(1)</span> 准备好部署各个阶段的工件，包括：构建工件，测试脚本，配置文件和部署清单文件。</div><div class="line"><span class="comment">(2)</span> 从负载均衡列表中移除掉“部署灰度环境”服务器。</div><div class="line"><span class="comment">(3)</span> 升级“灰度部署”应用（排掉原有流量并进行部署）。</div><div class="line"><span class="comment">(4)</span> 对应用进行自动化测试。</div><div class="line"><span class="comment">(5)</span> 将“灰度环境”服务器重新添加到负载均衡列表中（连通性和健康检查）。</div><div class="line"><span class="comment">(6)</span> 如果“灰度环境”在线使用测试成功，升级剩余的其他服务器。（否则就回滚）</div><div class="line">灰度发布中，常常按照用户设置路由权重，例如<span class="number">90</span><span class="meta">%</span>的用户维持使用老版本，<span class="number">10</span><span class="meta">%</span>的用户尝鲜新版本。不同版本应用共存，经常与A/B测试一起使用，用于测试选择多种方案。灰度发布比较典型的例子</div></pre></td></tr></table></figure>
<p>第1步: 把绿色集群的状态改为’备用’. 从负载均衡的池里把这些地址去掉,这样,绿色的集群就不再回接收到来自用户的请求了.转而进入备用负载均衡的池里.</p>
<h3 id="选择蓝绿环境部署方案："><a href="#选择蓝绿环境部署方案：" class="headerlink" title="选择蓝绿环境部署方案："></a>选择蓝绿环境部署方案：</h3><h5 id="蓝绿发布的意义"><a href="#蓝绿发布的意义" class="headerlink" title="蓝绿发布的意义"></a>蓝绿发布的意义</h5><p>整个发布过程，用户没有感受到任何宕机或者服务重启。</p>
<h5 id="蓝绿发布的过程"><a href="#蓝绿发布的过程" class="headerlink" title="蓝绿发布的过程"></a>蓝绿发布的过程</h5><ul>
<li>第0步:部署以前的配置<br><figure class="figure"><img src="http://1.bp.blogspot.com/-K8gbbN7S_xo/UmbijDyvS4I/AAAAAAAAE5M/3DCOJRrrbIY/s1600/Blue+Green+Deployment+for+Zero+Downtime+(8" alt=""></figure>.png)</li>
<li>第1步: 把绿色集群的状态改为’备用’. 从负载均衡的池里把这些地址去掉,这样,绿色的集群就不再回接收到来自用户的请求了.转而进入备用负载均衡的池里.<br><figure class="figure"><img src="http://1.bp.blogspot.com/-nITj_nPp5IY/UmbiuMblJNI/AAAAAAAAE5U/ovn4sSMcFco/s1600/Blue+Green+Deployment+for+Zero+Downtime+(9" alt=""></figure>.png)</li>
<li>第2步:在绿色集群里部署新的代码,直到应用启动成功<br><figure class="figure"><img src="http://3.bp.blogspot.com/-2F2dB-vux4g/UmbheBwtcAI/AAAAAAAAE5E/1l9il1igplQ/s1600/Blue+Green+Deployment+for+Zero+Downtime+(4" alt=""></figure>.png)</li>
<li>第3步:使用备用负载均衡简单测试一下备用集群的部署情况.理想状态下是全自动的.</li>
<li>第4步:把绿色备用集群的状态改成存货,于是进入了存活负载均衡的池里<br><figure class="figure"><img src="http://3.bp.blogspot.com/-2F2dB-vux4g/UmbheBwtcAI/AAAAAAAAE5E/1l9il1igplQ/s1600/Blue+Green+Deployment+for+Zero+Downtime+(4" alt=""></figure>.png)<br>看到 蓝色运行v1版本,绿色运行v2版本,都连接的是相同的数据库.这意味着v2版本也要在老的数据模型上运行.如果数据库有变更,要等到所有的集群升级到新的代码上.</li>
<li><p>第5步: 对蓝色集群也进行同样的操作.<br><figure class="figure"><img src="http://4.bp.blogspot.com/-aTz1EdP6pb0/UmbjAx46vrI/AAAAAAAAE5c/qqxolMnae44/s640/Blue+Green+Deployment+for+Zero+Downtime+(5" alt=""></figure>.png)<br><figure class="figure"><img src="http://3.bp.blogspot.com/-95YNOBeYoUU/UmbjFq711dI/AAAAAAAAE5k/Q5Naa80xk64/s640/Blue+Green+Deployment+for+Zero+Downtime+(6" alt=""></figure>.png)</p>
</li>
<li><p>最终v2代码完成部署.<br><figure class="figure"><img src="http://2.bp.blogspot.com/-eh1gR1sBPNc/UmbjMkDCixI/AAAAAAAAE5s/vJOeYjV03vI/s640/Blue+Green+Deployment+for+Zero+Downtime+(7" alt=""></figure>.png)</p>
</li>
<li><p>第6步:根据情况.运行数据库迁移</p>
</li>
</ul>
<p>参考：tks   <a href="http://sunitspace.blogspot.jp/2013/10/blue-green-deployment.html" target="_blank" rel="external">green-deployment</a></p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>1) 蓝绿部署：不停止老版本，额外搞一套新版本，等测试发现新版本OK后，删除老版本。其实这里删除老版本也就是重新部署了老版本成为新版本一起放上去，等需要更新发布迁下期中一个版本环境部署：现在我们公司使用蓝绿部署方案。</p>
<p>3) 灰度发布：不停止老版本，额外搞一套新版本，常常按照用户设置路由权重，例如90%的用户维持使用老版本，10%的用户尝鲜新版本。不同版本应用共存，经常与A/B测试一起使用，用于测试选择多种方案。</p>
<h3 id="其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。"><a href="#其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。" class="headerlink" title="其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。"></a>其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。</h3><h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>





	<span class="different-posts">📖 <a href="/page/4">more posts</a> 📖</span>



	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<h5>Hi there,</h5>
	<p style="text-align:justify;">my name is Yancy, they know me as Radiant🐑🐑. This blog is about Bi-data and my live.<br> I like 🌳🌳🌳</p>
</div>


	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2017 Yancy | Powered by <a href="http://blog.yancy.cc/">Yancy</a> | Theme <a href="https://github.com/yangcvo">Yancy_GitHub</a></span>
		</div>

	</div>


</footer>



</body>

</html>
