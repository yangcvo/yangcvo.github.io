<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Yancy&#39;s blog</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Yancy&#39;s blog" type="application/atom+xml">
    
</head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/" class="header__link">Home</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/archives" class="header__link">ARCHIVES</a>
			
				<a href="/about/About" class="header__link">ABOUT</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Yancy&#39;s blog</a></h1>
		<h2 class="header__subtitle">SIMPLICITY IS PREREQUISITE FOR  RELIABILITY</h2>
	</header>

	<main>
		
	<span class="different-posts different-posts_earlier">📖 <a href="/page/3">earlier posts</a> 📖</span>




	<article>
	
		<h1><a href="/2017/03/28/Bigdata-hadoop/Kafka/Bigdata-Kafka集群快速搭建与增删改查命令讲解/">Bigdata-Kafka集群快速搭建与命令讲解</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-03-28</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/大数据/">大数据</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Kafka/">Kafka</a>
			</span>
		
	</div>

	

	
		<h1 id="Bigdata-Kafka集群快速搭建与命令讲解"><a href="#Bigdata-Kafka集群快速搭建与命令讲解" class="headerlink" title="Bigdata-Kafka集群快速搭建与命令讲解"></a>Bigdata-Kafka集群快速搭建与命令讲解</h1><h3 id="kafka集群和zookeeper集群规范"><a href="#kafka集群和zookeeper集群规范" class="headerlink" title="kafka集群和zookeeper集群规范"></a>kafka集群和zookeeper集群规范</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">kafka集群和zookeeper集群规范</div><div class="line">kafka版本：kafka_2.10-0.8.2.1</div><div class="line">zookeeper版本：zookeeper-3.4.5</div><div class="line"></div><div class="line">启动用户：jollybi</div><div class="line">kafka安装目录：/data/tools/</div><div class="line">kafka新建消息目录：/data/tools/kafka_2.10-0.8.2.1/kafka-logs</div><div class="line"></div><div class="line">启动用户：jollybi</div><div class="line">zookeeper安装目录：/data/tools/</div><div class="line">zookeeper新建日志目录：/data/tools/zookeeper-3.4.5/tmp/logs</div><div class="line"></div><div class="line">统一配置hosts</div><div class="line"></div><div class="line"></div><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure>
<p>本文使用了3台机器部署Kafka集群，IP和主机名对应关系如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure>
<h3 id="Step1-配置-etc-hosts-（3台一致）"><a href="#Step1-配置-etc-hosts-（3台一致）" class="headerlink" title="Step1: 配置/etc/hosts （3台一致）"></a>Step1: 配置/etc/hosts （3台一致）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">127.0.0.1 localhost.localdomain localhost</div><div class="line">10.155.90.153 kafka1.jollychic.com kafka1</div><div class="line">10.155.90.155 kafka2.jollychic.com kafka2</div><div class="line">10.155.90.138 kafka3.jollychic.com kafka3</div></pre></td></tr></table></figure>
<h3 id="Step2-环境KafKa集群环境"><a href="#Step2-环境KafKa集群环境" class="headerlink" title="Step2: 环境KafKa集群环境"></a>Step2: 环境KafKa集群环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">下载地址：http://mirrors.hust.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.8.2.1.tgz</div><div class="line">[jollybi@kafka1 ]<span class="comment"># tar -xvf kafka_2.10-0.8.2.1.tgz -C /data/tools/</span></div><div class="line"><span class="built_in">cd</span> /data/tools/kafka_2.10-0.8.2.1/config</div></pre></td></tr></table></figure>
<h3 id="设置data目录，最好不要用默认的-tmp-kafka-logs"><a href="#设置data目录，最好不要用默认的-tmp-kafka-logs" class="headerlink" title="设置data目录，最好不要用默认的/tmp/kafka-logs"></a>设置data目录，最好不要用默认的/tmp/kafka-logs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mkdir -p /data/tools/kafka_2.10-0.8.2.1/kafka-logs/</div></pre></td></tr></table></figure>
<p><em>修改kafka配置文件</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@kafka_01 config]<span class="comment"># vim server.properties</span></div><div class="line"></div><div class="line"></div><div class="line"> 设置brokerid（从0开始，3个节点分别设为1,2,3，不能重复）在这里id=0跟zookeeper id设置一样就行。 集群机器：按顺序写1</div><div class="line">broker.id=1  </div><div class="line"></div><div class="line">auto.leader.rebalance.enable=<span class="literal">true</span> </div><div class="line"></div><div class="line"><span class="comment">#修改本地IP地址：</span></div><div class="line">listeners=PLAINTEXT://10.46.72.172:9092</div><div class="line">port=9292 //设置访问端口</div><div class="line">host.name=10.155.90.153   kafka本机IP</div><div class="line">log.dirs=/data/tools/kafka_2.10-0.8.2.1/kafka-logs</div><div class="line"></div><div class="line"><span class="comment">#设置注册地址（重要，默认会把本机的hostanme注册到zk中，客户端连接时需要解析该hostanme，所以这里直接注册本机的IP地址，避免hostname解析失败，报错java.nio.channels.UnresolvedAddressException或java.io.IOException: Can not resolve address）</span></div><div class="line"><span class="comment">#设置zookeeper地址</span></div><div class="line"><span class="comment">#zookeeper.connect=10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181</span></div><div class="line">zookeeper.connect=kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281</div><div class="line"></div><div class="line">//这里我设置hosts代替IP</div></pre></td></tr></table></figure>
<p><em>配置zookeeper地址</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">vim zookeeper.properties </div><div class="line">dataDir=/data/tools/zookeeper-3.4.5/tmp</div><div class="line"><span class="comment"># the port at which the clients will connect</span></div><div class="line">clientPort=2281</div><div class="line"><span class="comment"># disable the per-ip limit on the number of connections since this is a non-production config</span></div><div class="line">maxClientCnxns=0</div><div class="line">~</div></pre></td></tr></table></figure>
<p><em>配置kafka访问地址</em></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">vim</span> producer.properties </div><div class="line">metadata.broker.list=<span class="number">169.44.62.139:9292</span>,<span class="number">169.44.59.138:9292</span>,<span class="number">169.44.62.137:9292</span>   </div><div class="line"><span class="comment"># name of the partitioner class for partitioning events; default partition spreads data randomly</span></div><div class="line"><span class="comment">#partitioner.class=</span></div><div class="line"></div><div class="line"><span class="comment"># specifies whether the messages are sent asynchronously (async) or synchronously (sync)</span></div><div class="line">producer.type=sync</div><div class="line"></div><div class="line"><span class="comment"># specify the compression codec for all data generated: none, gzip, snappy, lz4.</span></div><div class="line"><span class="comment"># the old config values work as well: 0, 1, 2, 3 for none, gzip, snappy, lz4, respectively</span></div><div class="line">compression.codec=<span class="literal">none</span></div><div class="line"></div><div class="line"><span class="comment"># message encoder</span></div><div class="line">serializer.class=kafka.serializer.DefaultEncoder</div></pre></td></tr></table></figure>
<h3 id="Step3-集群机器快速拷贝配置"><a href="#Step3-集群机器快速拷贝配置" class="headerlink" title="Step3: 集群机器快速拷贝配置:"></a>Step3: 集群机器快速拷贝配置:</h3><p>远程复制分发安装文件<br>最好是把文件打包scp过去<br>接下来将上面的安装文件拷贝到集群中的其他机器上对应的目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scp -P58958 -r kafka_2.10-0.8.2.1 jollybi@169.44.62.137:/home/jollybi/tools/. </div><div class="line">scp -P58958 -r kafka_2.10-0.8.2.1 jollybi@169.44.62.138:/home/jollybi/tools/.</div></pre></td></tr></table></figure>
<p>统一修改分别其他两台kafka <code>server.properties</code></p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">broker.id=<span class="number">2</span>  </div><div class="line">host.name=<span class="number">169.44</span><span class="number">.62</span><span class="number">.137</span>   kafka2本机IP</div><div class="line"></div><div class="line">broker.id=<span class="number">3</span> </div><div class="line">host.name=<span class="number">169.44</span><span class="number">.62</span><span class="number">.138</span>   kafka3本机IP</div></pre></td></tr></table></figure>
<h3 id="Step4-启动-kafka集群"><a href="#Step4-启动-kafka集群" class="headerlink" title="Step4:启动 kafka集群"></a>Step4:启动 kafka集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">./kafka-server-start.sh -daemon ../config/server.properties</div><div class="line"></div><div class="line">[jollybi@kafka1 config]$ jps</div><div class="line">3443 QuorumPeerMain</div><div class="line">16280 Jps</div><div class="line">3628 Kafka</div><div class="line">Step5: Kafka常用命令(普及)</div></pre></td></tr></table></figure>
<h4 id="Step5-测试集群基本命令"><a href="#Step5-测试集群基本命令" class="headerlink" title="Step5:测试集群基本命令"></a>Step5:测试集群基本命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### 默认创建kafka Topic:  1个副本备份 1个分区消费</span></div><div class="line"></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 1 --partitions 1 --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 1 --partitions 1 --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 大数据这边需要12个分区这里我删除Topic重新创建Topic：</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 删除kafka Topic:</span></div><div class="line"></div><div class="line"></div><div class="line">第一步：删除kafka topic</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --delete --topic  mongotail_lz4 </div><div class="line"></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --delete --topic  mongotail_lz4_imp</div><div class="line"></div><div class="line"><span class="comment">### 这是由于删除topic没删干净会报错：</span></div><div class="line"></div><div class="line">org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 删除线程执行删除操作的真正逻辑是：</span></div><div class="line">1. 它首先会给当前所有broker发送更新元数据信息的请求，告诉这些broker说这个topic要删除了，你们可以把它的信息从缓存中删掉了</div><div class="line">2. 开始删除这个topic的所有分区</div><div class="line">2.1 给所有broker发请求，告诉它们这些分区要被删除。broker收到后就不再接受任何在这些分区上的客户端请求了</div><div class="line">2.2 把每个分区下的所有副本都置于OfflineReplica状态，这样ISR就不断缩小，当leader副本最后也被置于OfflineReplica状态时leader信息将被更新为-1</div><div class="line">2.3 将所有副本置于ReplicaDeletionStarted状态</div><div class="line">2.4 副本状态机捕获状态变更，然后发起StopReplicaRequest给broker，broker接到请求后停止所有fetcher线程、移除缓存，然后删除底层<span class="built_in">log</span>文件</div><div class="line">2.5 关闭所有空闲的Fetcher线程</div><div class="line">    </div><div class="line"><span class="comment">### 第二步：删除zookeeper相关的路径：</span></div><div class="line">[jollybi@kafka1 zookeeper-3.4.5]$ ./bin/zkCli.sh -server 127.0.0.1:2281 </div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /brokers/topics/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 0] rmr /consumers/group_ml_general/offsets/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /consumers/group_ml_general/owners/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 3] rmr  /config/topics/mongotail_lz4</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 4] rmr /admin/delete_topics/mongotail_lz4</div><div class="line"></div><div class="line"><span class="comment">### 删除topic上面消费组 </span></div><div class="line"></div><div class="line">删除 ZooKeeper 下面的 /consumers/[group_id] 路径就可以了。ZooKeeper 原生API只支持删除空的路径，所以建议你使用 curator framework 进行这个删除操作，ZkPaths.deleteChildren 会递归式地删除整个路径（包括子路径）。</div><div class="line"></div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 0] rmr /consumers/kafka-node1-imp-group</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 1] rmr /consumers/kafka-node1-group</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 2] rmr /consumers/console-consumer-59053</div><div class="line">[zk: 127.0.0.1:2281(CONNECTED) 3] rmr /consumers/</div><div class="line"></div><div class="line"></div><div class="line">1、设置配置文件允许删除</div><div class="line">delete.topic.enable=<span class="literal">true</span> 配置添加到 config/server.properties</div><div class="line">2、执行删除命令</div><div class="line">./bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic <span class="built_in">test</span>-topic</div><div class="line"></div><div class="line">在zookeeper中确认：</div><div class="line"></div><div class="line"> 删除zookeeper下/brokers/topics/<span class="built_in">test</span>-topic节点</div><div class="line"> 删除zookeeper下/config/topics/<span class="built_in">test</span>-topic节点</div><div class="line"> 删除zookeeper下/admin/delete_topics/<span class="built_in">test</span>-topic节点</div><div class="line"> </div><div class="line">consumer 的话 删除groupid</div><div class="line"> </div><div class="line"></div><div class="line"><span class="comment">### 重启kafka集群</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 大数据kafka建topic，不能建一个分区，那样吞吐量上不去，不够用，我们以前建了12个分区的</span></div><div class="line"><span class="comment">### 创建新 kafka Topic: 3个副本，12个分区</span></div><div class="line"></div><div class="line"> ./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 3 --partitions 12  --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh --create --zookeeper  kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281 --replication-factor 3 --partitions 12  --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 查看创建的Topic:</span></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4</div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4_imp</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 查看集群情况：</span></div><div class="line">./kafka_2.10-0.8.2.1/bin/kafka-topics.sh  --describe --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281  --topic mongotail_lz4</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">### 模拟生产者（producer）kafka生产客户端生产数据命令：</span></div><div class="line"></div><div class="line">./bin/kafka-console-producer.sh --broker-list 75.126.39.124:9292,75.126.5.162:9292,75.126.5.178:9292 --topic mongotail_lz4_imp                      </div><div class="line">......</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line">^C</div><div class="line"><span class="comment">### 模拟消费者（consumer）kafka消费客户端数据命令 现在我们来看看消息：</span></div><div class="line"></div><div class="line"> ./bin/kafka-console-consumer.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --fm-beginning --topic mongotail_lz4_imp</div><div class="line">......</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line"></div><div class="line">^C</div></pre></td></tr></table></figure>
<h3 id="Kafka常用命令"><a href="#Kafka常用命令" class="headerlink" title="Kafka常用命令"></a>Kafka常用命令</h3><p>以下是kafka常用命令行总结：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">1、 list topic 显示所有topic</div><div class="line">./bin/kafka-topics.sh --list --zookeeper kafka1.jollychic.com:2281,kafka2.jollychic.com:2281,kafka3.jollychic.com:2281</div><div class="line"></div><div class="line">2、 查看topic的详细信息  </div><div class="line"> ./bin/kafka-topics.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  -describe -topic mongotail_lz4_imp</div><div class="line"></div><div class="line">3、为topic增加partition分区 参数：--alter --partitions</div><div class="line">./bin/kafka-topics.sh  --alter --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --partitions 20 --topic mongotail_lz4_imp</div><div class="line"></div><div class="line">4、kafka生产者客户端命令  </div><div class="line">./bin/kafka-console-producer.sh --broker-list 75.126.39.124:9292,75.126.5.162:9292,75.126.5.178:9292 --topic mongotail_lz4_imp                        </div><div class="line"></div><div class="line">5、kafka消费者客户端命令  </div><div class="line">./bin/kafka-console-consumer.sh --zookeeper  75.126.39.124:2281,75.126.5.162:2281,75.126.5.178:2281  --fm-beginning --topic </div><div class="line"></div><div class="line">6、kafka服务启动  </div><div class="line">./kafka-server-start.sh -daemon ../config/server.properties   </div><div class="line"></div><div class="line">7、下线broker  </div><div class="line">./kafka-run-class.sh kafka.admin.ShutdownBroker --zookeeper 127.0.0.1:2181 --broker <span class="comment">#brokerId# --num.retries 3 --retry.interval.ms 60  </span></div><div class="line">shutdown broker  </div><div class="line"></div><div class="line">8、删除topic  </div><div class="line">./kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic <span class="built_in">test</span>KJ1 --zookeeper 127.0.0.1:2181  </div><div class="line">./kafka-topics.sh --zookeeper localhost:2181 --delete --topic <span class="built_in">test</span>KJ1  </div><div class="line"></div><div class="line">9、查看consumer组内消费的offset  </div><div class="line">./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group <span class="built_in">test</span> --topic <span class="built_in">test</span>KJ1</div></pre></td></tr></table></figure>
<h4 id="Step6-Kafka存储"><a href="#Step6-Kafka存储" class="headerlink" title="Step6:Kafka存储"></a>Step6:Kafka存储</h4><p>每个replica一个目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka_2.10-0.8.2.0]<span class="comment"># cd /data/tools/kafka_2.10-0.8.2.0/kafka-logs/</span></div><div class="line">[root@master kafka-logs]<span class="comment"># ls</span></div><div class="line">__consumer_offsets-0   __consumer_offsets-20  __consumer_offsets-32  __consumer_offsets-44   my-replicatedtopic1-0</div><div class="line">__consumer_offsets-1   __consumer_offsets-21  __consumer_offsets-33  __consumer_offsets-45   my-replicated-topic1-1</div></pre></td></tr></table></figure>
<p>二级结构</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[jollybi@kafka1 kafka-logs]<span class="comment"># cd my-replicated-topic1-0/</span></div><div class="line">[jollybi@kafka1  my-replicated-topic1-0]<span class="comment"># ls</span></div><div class="line">00000000000000000000.index  00000000000000000000.log</div></pre></td></tr></table></figure>
<p><figure class="figure"><img src="http://static.zybuluo.com/sasaki/f71rtngyz2y8n3m6uxq1z2ec/%E5%9B%BE%E7%89%871.png" alt=""></figure></p>
<h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2017/03/15/运维笔记/APP自动化部署:开发部署-测试部署-灰度发布:蓝绿部署-生产环境等部署流程方案总结/">APP自动化部署:开发部署-测试部署-灰度发布/蓝绿部署-生产环境等部署流程方案总结</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2017-03-15</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/运维笔记/">运维笔记</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Automation-Deploy/">Automation Deploy</a>
			</span>
		
	</div>

	

	
		<h1 id="APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结"><a href="#APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结" class="headerlink" title="APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结"></a>APP自动化部署：开发部署、测试部署、灰度发布等部署方案对比与总结</h1><h4 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h4><p>个人现在在一个医疗行业公司：美年大这边负责体检APP的运维相关工作，今天主要是整理了下现在团队APP发布流程<br>方案：</p>
<p>因为在项目迭代的过程中，不可避免需要”上线”。上线对应着部署，或者重新部署；部署对应着修改；修改则意味着风险。<br>目前有很多用于部署的技术，有的简单，有的复杂；有的得停机，有的不需要停机即可完成部署。</p>
<p>个人整理下部署流程说明其实现在很多部署方法，现在我们用目前比较流行的几种部署方案，或者说策略方案对比总结简单讨论一下目前比较流行的几种部署方案，或者说策略。如有不足之处请指出，如有谬误，请指正^_^。</p>
<p>我们有自己开发环境和测试环境 ：</p>
<h4 id="开发环境部署："><a href="#开发环境部署：" class="headerlink" title="开发环境部署："></a>开发环境部署：</h4><ul>
<li>问题：</li>
</ul>
<p>开发人员每个人在自己电脑环境写代码自己电脑本机测试代码是否run成功，每个开发人员都在自己本地写完测试出现问题是各自环境不统一导致遇到坑阻碍到测试人员测试，基础的bug也会浪费太多时间。</p>
<ul>
<li>解决后：</li>
</ul>
<p>主要是解决了给予开发团队在写代码或者修改bug可以第一时间更新部署。大家统一一个开发环境这个是为了在开发阶段能立马呈现效果。</p>
<h4 id="测试环境部署："><a href="#测试环境部署：" class="headerlink" title="测试环境部署："></a>测试环境部署：</h4><ul>
<li>问题：</li>
</ul>
<p>一开始没有测试环境，直接开发环境当作测试环境去跑，发现很多问题，就是测试环境数据跟生产不一样，导致很多bug问题，测试发现测试的时候，开发在更新代码发布，耽误了测试人员的测试过程，环境不能独立都互相占用，刀子效率没有提高，bug每个星期都不断提升。</p>
<ul>
<li>解决后：</li>
</ul>
<p>主要解决了开发环境和测试环境独立，不会互相影响，测试人员有单独环境去测试生产能准确的数据对比。</p>
<h4 id="选择灰度环境部署方案："><a href="#选择灰度环境部署方案：" class="headerlink" title="选择灰度环境部署方案："></a>选择灰度环境部署方案：</h4><p>先贴个百度百科：</p>
<p>灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。AB test就是一种灰度发布方式，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。</p>
<p>灰度环境部署:</p>
<p>个人理解灰度部署是增量发布的一种类型，它的执行方式是在原有软件生产版本可用的情况下，同时部署一个新的版本。同时运行同一个软件产品的多个版本需要软件针对配置和完美自动化部署进行特别设计。</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">(1)</span> 准备好部署各个阶段的工件，包括：构建工件，测试脚本，配置文件和部署清单文件。</div><div class="line"><span class="comment">(2)</span> 从负载均衡列表中移除掉“部署灰度环境”服务器。</div><div class="line"><span class="comment">(3)</span> 升级“灰度部署”应用（排掉原有流量并进行部署）。</div><div class="line"><span class="comment">(4)</span> 对应用进行自动化测试。</div><div class="line"><span class="comment">(5)</span> 将“灰度环境”服务器重新添加到负载均衡列表中（连通性和健康检查）。</div><div class="line"><span class="comment">(6)</span> 如果“灰度环境”在线使用测试成功，升级剩余的其他服务器。（否则就回滚）</div><div class="line">灰度发布中，常常按照用户设置路由权重，例如<span class="number">90</span><span class="meta">%</span>的用户维持使用老版本，<span class="number">10</span><span class="meta">%</span>的用户尝鲜新版本。不同版本应用共存，经常与A/B测试一起使用，用于测试选择多种方案。灰度发布比较典型的例子</div></pre></td></tr></table></figure>
<p>第1步: 把绿色集群的状态改为’备用’. 从负载均衡的池里把这些地址去掉,这样,绿色的集群就不再回接收到来自用户的请求了.转而进入备用负载均衡的池里.</p>
<h3 id="选择蓝绿环境部署方案："><a href="#选择蓝绿环境部署方案：" class="headerlink" title="选择蓝绿环境部署方案："></a>选择蓝绿环境部署方案：</h3><h5 id="蓝绿发布的意义"><a href="#蓝绿发布的意义" class="headerlink" title="蓝绿发布的意义"></a>蓝绿发布的意义</h5><p>整个发布过程，用户没有感受到任何宕机或者服务重启。</p>
<h5 id="蓝绿发布的过程"><a href="#蓝绿发布的过程" class="headerlink" title="蓝绿发布的过程"></a>蓝绿发布的过程</h5><ul>
<li>第0步:部署以前的配置<br><figure class="figure"><img src="http://1.bp.blogspot.com/-K8gbbN7S_xo/UmbijDyvS4I/AAAAAAAAE5M/3DCOJRrrbIY/s1600/Blue+Green+Deployment+for+Zero+Downtime+(8" alt=""></figure>.png)</li>
<li>第1步: 把绿色集群的状态改为’备用’. 从负载均衡的池里把这些地址去掉,这样,绿色的集群就不再回接收到来自用户的请求了.转而进入备用负载均衡的池里.<br><figure class="figure"><img src="http://1.bp.blogspot.com/-nITj_nPp5IY/UmbiuMblJNI/AAAAAAAAE5U/ovn4sSMcFco/s1600/Blue+Green+Deployment+for+Zero+Downtime+(9" alt=""></figure>.png)</li>
<li>第2步:在绿色集群里部署新的代码,直到应用启动成功<br><figure class="figure"><img src="http://3.bp.blogspot.com/-2F2dB-vux4g/UmbheBwtcAI/AAAAAAAAE5E/1l9il1igplQ/s1600/Blue+Green+Deployment+for+Zero+Downtime+(4" alt=""></figure>.png)</li>
<li>第3步:使用备用负载均衡简单测试一下备用集群的部署情况.理想状态下是全自动的.</li>
<li>第4步:把绿色备用集群的状态改成存货,于是进入了存活负载均衡的池里<br><figure class="figure"><img src="http://3.bp.blogspot.com/-2F2dB-vux4g/UmbheBwtcAI/AAAAAAAAE5E/1l9il1igplQ/s1600/Blue+Green+Deployment+for+Zero+Downtime+(4" alt=""></figure>.png)<br>看到 蓝色运行v1版本,绿色运行v2版本,都连接的是相同的数据库.这意味着v2版本也要在老的数据模型上运行.如果数据库有变更,要等到所有的集群升级到新的代码上.</li>
<li><p>第5步: 对蓝色集群也进行同样的操作.<br><figure class="figure"><img src="http://4.bp.blogspot.com/-aTz1EdP6pb0/UmbjAx46vrI/AAAAAAAAE5c/qqxolMnae44/s640/Blue+Green+Deployment+for+Zero+Downtime+(5" alt=""></figure>.png)<br><figure class="figure"><img src="http://3.bp.blogspot.com/-95YNOBeYoUU/UmbjFq711dI/AAAAAAAAE5k/Q5Naa80xk64/s640/Blue+Green+Deployment+for+Zero+Downtime+(6" alt=""></figure>.png)</p>
</li>
<li><p>最终v2代码完成部署.<br><figure class="figure"><img src="http://2.bp.blogspot.com/-eh1gR1sBPNc/UmbjMkDCixI/AAAAAAAAE5s/vJOeYjV03vI/s640/Blue+Green+Deployment+for+Zero+Downtime+(7" alt=""></figure>.png)</p>
</li>
<li><p>第6步:根据情况.运行数据库迁移</p>
</li>
</ul>
<p>参考：tks   <a href="http://sunitspace.blogspot.jp/2013/10/blue-green-deployment.html" target="_blank" rel="external">green-deployment</a></p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>1) 蓝绿部署：不停止老版本，额外搞一套新版本，等测试发现新版本OK后，删除老版本。其实这里删除老版本也就是重新部署了老版本成为新版本一起放上去，等需要更新发布迁下期中一个版本环境部署：现在我们公司使用蓝绿部署方案。</p>
<p>3) 灰度发布：不停止老版本，额外搞一套新版本，常常按照用户设置路由权重，例如90%的用户维持使用老版本，10%的用户尝鲜新版本。不同版本应用共存，经常与A/B测试一起使用，用于测试选择多种方案。</p>
<h3 id="其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。"><a href="#其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。" class="headerlink" title="其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。"></a>其实两种含义个人认为相差不是特别大，都特别适合现在发布部署流程方案。</h3><h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2016/12/29/日志分析平台/Elasticsearch/搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台/">搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6 Kafka ) Kafka为消息中心的ELK日志平台</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2016-12-29</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Log-Analysis-Platform/">Log Analysis Platform</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Elasticsearch/">Elasticsearch</a>
			</span>
		
	</div>

	

	
		<h3 id="搭建-ElasticSearch-2-x-Logstash-2-x-Kibana-4-5-x-zookeeper3-4-6-Kafka为消息中心的ELK日志平台"><a href="#搭建-ElasticSearch-2-x-Logstash-2-x-Kibana-4-5-x-zookeeper3-4-6-Kafka为消息中心的ELK日志平台" class="headerlink" title="搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台"></a>搭建(ElasticSearch-2.x Logstash-2.x Kibana-4.5.x zookeeper3.4.6) Kafka为消息中心的ELK日志平台</h3><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>ELK是业界标准的日志采集,存储索引,展示分析系统解决方案</p>
<p>logstash提供了灵活多样的插件支持不同的input/output</p>
<p>主流使用redis/kafka作为日志/消息的中间环节</p>
<p>如果已有kafka的环境了,使用kafka比使用redis更佳</p>
<p>以下是一个最简化的配置做个笔记,elastic官网提供了非常丰富的文档</p>
<p>不要用搜索引擎去搜索,没多少结果的,请直接看官网文档</p>
<h3 id="版本及连接"><a href="#版本及连接" class="headerlink" title="版本及连接"></a>版本及连接</h3><p>elasticseearch版本: 2.4.3</p>
<h3 id="系统要求"><a href="#系统要求" class="headerlink" title="系统要求"></a>系统要求</h3><p>如果仅作测试用, 不需要两天机器, 可以将两个节点部署在同一台机器上, 对磁盘/cpu要求不高, 内存大于2g基本足够了</p>
<p>如果是正式环境, 需要根据日志量进行评估, 例如, 每天日志量占硬盘约约10G, 且保留30天日志, 则磁盘会占用约300g, es设定的阈值是磁盘空间占满85%则日志开始告警. 所以, 需要至少 300/0.85=354g.</p>
<p>准备4台机器, 在同一个局域网内(可ping通), 分别在每台机器上部署相应es节点, 搭建一套日志集群.</p>
<p>4台机器, 最少的资源了, 但是没法做到高可用, 所以, 还需要再加一台机器, 防止脑裂, 具体见最后(两台主力机器+一台稳定的机器就行)</p>
<p>集群节点: 最少4台机器<br>内存: 8G及以上<br>cpu: 4核及以上<br>硬盘: 800G及以上, 建议1T, 集群容量约10亿级(取决于对应日志大小)<br>操作系统: centos</p>
<h3 id="准备工作-应用-网络-环境"><a href="#准备工作-应用-网络-环境" class="headerlink" title="准备工作: 应用/网络 环境"></a>准备工作: 应用/网络 环境</h3><p>SLB： 阿里云做负载均衡&amp; 或者自己搭建nginx</p>
<blockquote>
<p>ELK服务端集群：</p>
</blockquote>
<p>系统centos 6.7  JDK1.8  版本：Elasticsearch-2.4.0</p>
<p>es_01 10.47.88.206<br>es_02 10.47.88.188</p>
<blockquote>
<p>Kibana服务端集群：</p>
</blockquote>
<p>系统centos 6.7  JDK1.8  版本：kibana-4.5.1</p>
<p>es_01 10.47.88.206<br>es_02 10.47.88.188</p>
<blockquote>
<p>KafKa集群</p>
</blockquote>
<p>系统centos 6.7  JDK1.8  版本：kafka_2.10-0.9</p>
<p>kafka_01 10.46.72.172<br>kafka_02 10.47.88.103<br>kafka_03 10.47.102.137</p>
<blockquote>
<p>zookeeper集群</p>
</blockquote>
<p>系统centos 6.7  JDK1.8  版本：zookeeper-3.4.6</p>
<p>kafka_01 10.46.72.172<br>kafka_02 10.47.88.103<br>kafka_03 10.47.102.137</p>
<blockquote>
<p>logstash-2.4</p>
</blockquote>
<p>客户端：系统centos 6.7  JDK1.8  版本： logstash-2.4</p>
<p>tomcat-account_01: 10.27.232.85</p>
<p>都要jdk1.8支持。</p>
<h3 id="整体说明"><a href="#整体说明" class="headerlink" title="整体说明"></a>整体说明</h3><h4 id="数据流向-gt-日志-消息整体流向"><a href="#数据流向-gt-日志-消息整体流向" class="headerlink" title="数据流向=&gt;日志/消息整体流向"></a>数据流向=&gt;日志/消息整体流向</h4><p>logstash =&gt; kafka =&gt; logstash =&gt; elasticsearch =&gt; kibana</p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="1-确认JDK版本及安装"><a href="#1-确认JDK版本及安装" class="headerlink" title="1. 确认JDK版本及安装"></a>1. 确认JDK版本及安装</h4><p>es依赖java的版本最小为1.7</p>
<p>如果系统中未安装JDK<br>则命令返回<code>bash: java: command not found,</code> 需要安装<code>JDK</code></p>
<p>如果系统中安装了JDK, 需确认版本是否大于java 1.7, 否则需要升级</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">java -version</div><div class="line">java version <span class="string">"1.7.0_51"</span> Java(TM) SE Runtime Environment (build 1.7.0_51-b13) Java HotSpot(TM) Server VM (build 24.51-b03, mixed mode)</div><div class="line"></div><div class="line">安装及升级java(注意根据系统不同运行对应安装命令)</div><div class="line"></div><div class="line"><span class="comment"># Redhat/Centos/Fedora</span></div><div class="line">sudo yum install java-1.7.0-openjdk</div><div class="line"></div><div class="line">或者到官网, 下载最新的jdk的rpm包, 然后安装</div><div class="line"></div><div class="line">wget http://download.oracle.com/otn-pub/java/jdk/8u91-b14/jdk-8u91-linux-x64.rpm</div><div class="line">rpm -Uvh jdk-8u91-linux-x64.rpm</div></pre></td></tr></table></figure>
<p>再次确认安装成功</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -version</div></pre></td></tr></table></figure>
<h4 id="基本配置设置FQDN："><a href="#基本配置设置FQDN：" class="headerlink" title="基本配置设置FQDN："></a>基本配置设置FQDN：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#修改hostname</span></div><div class="line">cat /etc/hostname</div><div class="line">es_01</div><div class="line"></div><div class="line"><span class="comment">#修改hosts</span></div><div class="line">cat /etc/hosts</div><div class="line">127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4</div><div class="line">::1 localhost localhost.localdomain localhost6 localhost6.localdomain6</div><div class="line">10.47.88.206 es.ihaozhuo.com es_01</div><div class="line"></div><div class="line"><span class="comment">#刷新环境</span></div><div class="line">hostname -F /etc/hostname</div><div class="line"></div><div class="line"><span class="comment">#复查结果</span></div><div class="line">hostname <span class="_">-f</span></div><div class="line">es.ihaozhuo.com</div><div class="line"></div><div class="line">hostname</div><div class="line">es_01</div></pre></td></tr></table></figure>
<h3 id="防火墙配置"><a href="#防火墙配置" class="headerlink" title="防火墙配置"></a>防火墙配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#service iptables stop</span></div><div class="line"><span class="comment">#setenforce 0</span></div><div class="line"></div><div class="line">不过这里我防火墙是开启的，后期添加出去端口即可。</div><div class="line">或者可以不关闭防火墙，但是要在iptables中打开相关的端口：</div><div class="line"></div><div class="line"><span class="comment"># vim /etc/sysconfig/iptables</span></div><div class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT</div><div class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 9200 -j ACCEPT</div><div class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 9292 -j ACCEPT</div><div class="line"><span class="comment"># service iptables restart</span></div></pre></td></tr></table></figure>
<h4 id="RPM快速安装"><a href="#RPM快速安装" class="headerlink" title="RPM快速安装"></a>RPM快速安装</h4><p>elk所有安装都可以使用rpm二进制包的方式,增加<code>elastic官网</code>的仓库repo就可以用yum安装了</p>
<p>elasticsearch看这里 —– <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html" target="_blank" rel="external">elasticsearch-rpm官方文档</a></p>
<p>logstash看这里 —-<a href="https://www.elastic.co/guide/en/logstash/current/installing-logstash.html" target="_blank" rel="external">logstash-rpm官网文档</a></p>
<p>kibana看这里 —<a href="https://www.elastic.co/guide/en/kibana/current/setup.html" target="_blank" rel="external">kibana-rpm官网文档</a></p>
<h3 id="es-01服务端源码安装"><a href="#es-01服务端源码安装" class="headerlink" title="es_01服务端源码安装"></a>es_01服务端源码安装</h3><p>这里我是源码安装的<br>下载ElasticSearch ElasticSearch默认的对外服务的HTTP端口是9200，节点间交互的TCP端口是9300。</p>
<p>下载地址：<a href="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="external">Elasticsearch</a></p>
<p>2.4版本：<br><a href="https://www.elastic.co/downloads/past-releases/elasticsearch-2-4-3" target="_blank" rel="external">Elasticsearch2.4.3</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">解压源码包：</div><div class="line">[root@es_01 ~]<span class="comment"># tar -zxvf elasticsearch-2.4.3.tar.gz -C /usr/local/</span></div><div class="line">然后给目录做个软链接：</div><div class="line">[root@es_01 <span class="built_in">local</span>]<span class="comment"># ln -s  /usr/local/elasticsearch-2.4.3/ /usr/local/elasticsearch</span></div><div class="line"></div><div class="line">这里需要修改配置文件：</div><div class="line">配置前先创建几个目录文件</div><div class="line">新建目录, 假设/data/目录挂载的硬盘最大(500G以上)</div><div class="line">[root@es_01 srv]]<span class="comment"># mkdir /srv/data/es-data -p</span></div><div class="line">[root@es_01 srv]<span class="comment"># mkdir /srv/data/es-work </span></div><div class="line"></div><div class="line">[root@es_01 <span class="built_in">local</span>]<span class="comment"># mkdir /usr/local/elasticsearch/logs</span></div><div class="line">[root@es_01 <span class="built_in">local</span>]<span class="comment"># mkdir /usr/local/elasticsearch/config/plugins</span></div><div class="line"></div><div class="line">新建用户</div><div class="line">修改源码目录属性属组：</div><div class="line">[root@es_01 elasticsearch]<span class="comment"># useradd  -s /sbin/nologin elasticsearch</span></div><div class="line">[root@es_01 elasticsearch]<span class="comment"># chown -R elasticsearch:elasticsearch /usr/local/elasticsearch</span></div><div class="line">[root@es_01 elasticsearch]<span class="comment"># chown -R elasticsearch:elasticsearch /srv/data/</span></div><div class="line"></div><div class="line"></div><div class="line">切换用户</div><div class="line">切换到elasticsearch用户, 并进入elasticsearch目录</div><div class="line"></div><div class="line">su elasticsearch</div><div class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/elasticsearch/</div></pre></td></tr></table></figure>
<h3 id="配置Elasticsearch："><a href="#配置Elasticsearch：" class="headerlink" title="配置Elasticsearch："></a>配置Elasticsearch：</h3><p>以用户es的身份进行操作</p>
<p>文件路径: <code>config/elasticsearch.yml</code><br>修改该文件中配置项: (注意, 原始文件中都是被#号注释掉了, 需要去掉对应注释并修改配置值)</p>
<ul>
<li>集群名: cluster.name, 注意: 两台机器配置一致</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cluster.name: elk_cluster</div></pre></td></tr></table></figure>
<ul>
<li>节点名: node.name, 注意: 两台机器配置不同, 一台为01, 另一台为02</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># 第一台机器</span></div><div class="line"> </div><div class="line">node.name: inner_es_node_01</div><div class="line"></div><div class="line"><span class="comment"># 第二台机器</span></div><div class="line">node.name: inner_es_node_02</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">[root@es_01 config]<span class="comment"># vim elasticsearch.yml</span></div><div class="line"></div><div class="line"><span class="comment"># Use a descriptive name for your cluster:</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#</span></div><div class="line">cluster.name: elk_cluster</div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># ------------------------------------ Node ------------------------------------</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Use a descriptive name for the node:</span></div><div class="line"><span class="comment">#</span></div><div class="line">node.name: es_01</div><div class="line"></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Add custom attributes to the node:</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># node.rack: r1</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># ----------------------------------- Paths ------------------------------------</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Path to directory where to store the data (separate multiple locations by comma):</span></div><div class="line"><span class="comment">#</span></div><div class="line">path.data: /srv/data/es-data</div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Path to log files:</span></div><div class="line"><span class="comment">#</span></div><div class="line">path.logs: /usr/<span class="built_in">local</span>/elasticsearch/logs</div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># ----------------------------------- Memory -----------------------------------</span></div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Lock the memory on startup:</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># bootstrap.memory_lock: true</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Make sure that the `ES_HEAP_SIZE` environment variable is set to about half the memory</span></div><div class="line"><span class="comment"># available on the system and that the owner of the process is allowed to use this limit.</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Elasticsearch performs poorly when the system is swapping the memory.</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># ---------------------------------- Network -----------------------------------</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Set the bind address to a specific IP (IPv4 or IPv6):</span></div><div class="line"><span class="comment">#</span></div><div class="line">network.host: 10.47.88.206</div></pre></td></tr></table></figure>
<p>切换到elasticsearch用户启动服务。</p>
<p>源码安装启动需要执行 ：<code>/usr/local/elasticsearch/bin/elasticsearch &amp;</code><br>才能启动；</p>
<h3 id="测试访问服务正常："><a href="#测试访问服务正常：" class="headerlink" title="测试访问服务正常："></a>测试访问服务正常：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@es_01 elasticsearch]$  curl http://10.47.88.206:9200</div><div class="line">&#123;</div><div class="line">  <span class="string">"name"</span> : <span class="string">"es_01"</span>,</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elk_cluster"</span>,</div><div class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"mspLZT5nTL-d124suNbBBQ"</span>,</div><div class="line">  <span class="string">"version"</span> : &#123;</div><div class="line">    <span class="string">"number"</span> : <span class="string">"2.4.3"</span>,</div><div class="line">    <span class="string">"build_hash"</span> : <span class="string">"d38a34e7b75af4e17ead16f156feffa432b22be3"</span>,</div><div class="line">    <span class="string">"build_timestamp"</span> : <span class="string">"2016-12-07T16:28:56Z"</span>,</div><div class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</div><div class="line">    <span class="string">"lucene_version"</span> : <span class="string">"5.5.2"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下面是写开机启动脚本，不写的直接切换es用户到目录启动 -d后台启动。<br>这里需要/etc/init.d/创建启动脚本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@ELK ~]<span class="comment"># git clone https://github.com/elastic/elasticsearch-servicewrapper.git</span></div><div class="line">Initialized empty Git repository <span class="keyword">in</span> /root/elasticsearch-servicewrapper/.git/</div><div class="line">remote: Counting objects: 184, done.</div><div class="line">remote: Total 184 (delta 0), reused 0 (delta 0), pack-reused 184</div><div class="line">Receiving objects: 100% (184/184), 4.55 MiB | 511 KiB/s, done.</div><div class="line">Resolving deltas: 100% (53/53), done.</div><div class="line">[root@ELK elasticsearch-servicewrapper]<span class="comment"># mv service/ /usr/local/elasticsearch/bin/</span></div><div class="line">[root@ELK elasticsearch-servicewrapper]<span class="comment"># cd /usr/local/elasticsearch</span></div><div class="line">[root@ELK elasticsearch]<span class="comment"># /usr/local/elasticsearch/bin/service/elasticsearch install    这里是安装es</span></div><div class="line">Detected RHEL or Fedora:</div><div class="line">Installing the Elasticsearch daemon..</div><div class="line">[root@ELK elasticsearch]<span class="comment"># vim /etc/init.d/elasticsearch   查看安装es启动配置文件</span></div><div class="line">[root@ELK elasticsearch]<span class="comment"># service elastic search start  启动es </span></div><div class="line">Starting Elasticsearch...</div><div class="line">Waiting <span class="keyword">for</span> Elasticsearch......</div><div class="line">running: PID:31360   服务已启动了。</div><div class="line"></div><div class="line">启动相关服务</div><div class="line">service elasticsearch start</div><div class="line">service elasticsearch status</div><div class="line"></div><div class="line">配置 elasticsearch 服务随系统自动启动</div><div class="line"><span class="comment"># chkconfig --add elasticsearch</span></div><div class="line"></div><div class="line">测试ElasticSearch服务是否正常，预期返回200的状态码</div><div class="line"><span class="comment"># curl -X GET http://localhost:9200</span></div></pre></td></tr></table></figure>
<h3 id="es-02服务端节点："><a href="#es-02服务端节点：" class="headerlink" title="es_02服务端节点："></a>es_02服务端节点：</h3><p>第一步基础配置都是一样的，跟es_01节点一样。  其他只需要到es_01拷贝过来,然后创建下es用户，修改下配置。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">/usr/<span class="built_in">local</span>/elasticsearch  目录拷贝到es_02机器。</div><div class="line">这里需要修改配置文件：</div><div class="line">配置前先创建几个目录文件</div><div class="line">[root@es_01 srv]]<span class="comment"># mkdir /srv/data/es-data -p</span></div><div class="line">[root@es_01 srv]<span class="comment"># mkdir /srv/data/es-work </span></div><div class="line">修改源码目录属性属组：</div><div class="line">[root@es_01 elasticsearch]<span class="comment"># useradd  -s /sbin/nologin elasticsearch</span></div><div class="line">[root@es_01 elasticsearch]<span class="comment"># chown -R elasticsearch:elasticsearch /usr/local/elasticsearch/*</span></div><div class="line">[root@es_01 elasticsearch]<span class="comment"># chown -R elasticsearch:elasticsearch /srv/data/</span></div><div class="line"></div><div class="line">修改配置文件</div><div class="line">vim elasticsearch.yml</div><div class="line">node.name: es_02</div><div class="line">network.host: 10.47.88.188</div><div class="line"></div><div class="line">其他不需要修改</div></pre></td></tr></table></figure>
<h3 id="集群节点es-02测试："><a href="#集群节点es-02测试：" class="headerlink" title="集群节点es_02测试："></a>集群节点es_02测试：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@es_02 home]<span class="comment"># curl http://10.47.88.188:9200</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"name"</span> : <span class="string">"es_02"</span>,</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elk_cluster"</span>,</div><div class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"-4Rqn4IzS1GfnsodqZD8Tg"</span>,</div><div class="line">  <span class="string">"version"</span> : &#123;</div><div class="line">    <span class="string">"number"</span> : <span class="string">"2.4.3"</span>,</div><div class="line">    <span class="string">"build_hash"</span> : <span class="string">"d38a34e7b75af4e17ead16f156feffa432b22be3"</span>,</div><div class="line">    <span class="string">"build_timestamp"</span> : <span class="string">"2016-12-07T16:28:56Z"</span>,</div><div class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</div><div class="line">    <span class="string">"lucene_version"</span> : <span class="string">"5.5.2"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>elk集群已安装配置完毕，我这里配置了nginx做下反向代理，走80端口出去。然后在nginx设置下内部公司访问不对外开放。</p>
<h3 id="安装-head、marvel、bigdesk插件"><a href="#安装-head、marvel、bigdesk插件" class="headerlink" title="安装 head、marvel、bigdesk插件:"></a>安装 head、marvel、bigdesk插件:</h3><p>es1.5插件安装是<code>./plugin -install xxx</code>,而es2.4插件安装没有减号<code>./plugin install xxx</code></p>
<p>1.5版本方法：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">* head插件</div><div class="line"></div><div class="line">插件安装方法1：</div><div class="line">/usr/<span class="built_in">local</span>/elasticsearch/bin/plugin -install mobz/elasticsearch-head</div><div class="line">重启es 即可。</div><div class="line">打开http://localhost:9200/_plugin/head/</div><div class="line"></div><div class="line">插件安装方法2：</div><div class="line">1.https://github.com/mobz/elasticsearch-head下载zip 解压</div><div class="line">2.建立/usr/<span class="built_in">local</span>/elasticsearch/plugins/head/文件</div><div class="line">3.将解压后的elasticsearch-head-master文件夹下的文件copy到/usr/<span class="built_in">local</span>/elasticsearch/plugins/head/</div><div class="line">重启es 即可。</div><div class="line"></div><div class="line">打开http://localhost:9200/_plugin/head/</div></pre></td></tr></table></figure>
<p>2.4版本以上安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">* head插件</div><div class="line"></div><div class="line">插件安装方法1：</div><div class="line">/usr/<span class="built_in">local</span>/elasticsearch/bin/plugin install mobz/elasticsearch-head</div><div class="line">重启es 即可。</div><div class="line">打开http://localhost:9200/_plugin/head/</div><div class="line"></div><div class="line">插件安装方法2：</div><div class="line">1.https://github.com/mobz/elasticsearch-head下载zip 解压</div><div class="line">2.建立elasticsearch-1.0.0\plugins\head\_site文件</div><div class="line">3.将解压后的elasticsearch-head-master文件夹下的文件copy到_site</div><div class="line">重启es 即可。</div><div class="line"></div><div class="line">打开http://localhost:9200/_plugin/head/</div></pre></td></tr></table></figure>
<p>为了保障搜索服务的稳定性，增加了一台机器，将Elasticsearch部署成了集群模式， 部署到生产环境时发现，新的节点并不能被发现，后台发现阿里云并不支持多播，最后只能改为单播的方式配置了，好在之后一切顺利。</p>
<p>下面附上测试环境配置示例：添加下下面监听集群IP和端口。</p>
<ul>
<li>es_01</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@es_01 config]<span class="comment"># vim elasticsearch.yml</span></div><div class="line"></div><div class="line">discovery.zen.ping.multicast.enabled: <span class="literal">false</span></div><div class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"10.47.88.206:9300"</span>,<span class="string">"10.47.88.188:9300"</span>]</div></pre></td></tr></table></figure>
<ul>
<li>es_02</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@es_02 config]<span class="comment"># vim elasticsearch.yml</span></div><div class="line"></div><div class="line">discovery.zen.ping.multicast.enabled: <span class="literal">false</span></div><div class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"10.47.88.206:9300"</span>,<span class="string">"10.47.88.188:9300"</span>]</div></pre></td></tr></table></figure>
<p>然后重启服务，查看集群节点。</p>
<h3 id="es-02安装Kibana"><a href="#es-02安装Kibana" class="headerlink" title="es_02安装Kibana:"></a>es_02安装Kibana:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div></pre></td><td class="code"><pre><div class="line">到https://www.elastic.co/downloads/kibana  找合适的版本。</div><div class="line"></div><div class="line">wget https://download.elastic.co/kibana/kibana/kibana-4.5.1-linux-x64.tar.gz</div><div class="line"></div><div class="line"><span class="comment">#解压</span></div><div class="line"></div><div class="line">＃tar zxvf kibana-4.1.2-linux-x64.tar.gz -C /usr/<span class="built_in">local</span> </div><div class="line">＃<span class="built_in">cd</span>  /usr/<span class="built_in">local</span>/ &amp;&amp; mv kibana-4.1.2-linux-x64 kibana</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#创建kibana启动脚本服务</span></div><div class="line">vi /etc/rc.d/init.d/kibana</div><div class="line"></div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment">### BEGIN INIT INFO</span></div><div class="line"><span class="comment"># Provides:          kibana</span></div><div class="line"><span class="comment"># Default-Start:     2 3 4 5</span></div><div class="line"><span class="comment"># Default-Stop:      0 1 6</span></div><div class="line"><span class="comment"># Short-Description: Runs kibana daemon</span></div><div class="line"><span class="comment"># Description: Runs the kibana daemon as a non-root user</span></div><div class="line"><span class="comment">### END INIT INFO</span></div><div class="line"></div><div class="line"><span class="comment"># Process name</span></div><div class="line">NAME=kibana</div><div class="line">DESC=<span class="string">"Kibana4"</span></div><div class="line">PROG=<span class="string">"/etc/init.d/kibana"</span></div><div class="line"></div><div class="line"><span class="comment"># Configure location of Kibana bin</span></div><div class="line">KIBANA_BIN=/usr/<span class="built_in">local</span>/kibana/bin</div><div class="line"></div><div class="line"><span class="comment"># PID Info</span></div><div class="line">PID_FOLDER=/var/run/kibana/</div><div class="line">PID_FILE=/var/run/kibana/<span class="variable">$NAME</span>.pid</div><div class="line">LOCK_FILE=/var/lock/subsys/<span class="variable">$NAME</span></div><div class="line">PATH=/bin:/usr/bin:/sbin:/usr/sbin:<span class="variable">$KIBANA_BIN</span></div><div class="line">DAEMON=<span class="variable">$KIBANA_BIN</span>/<span class="variable">$NAME</span></div><div class="line"></div><div class="line"><span class="comment"># Configure User to run daemon process</span></div><div class="line">DAEMON_USER=root</div><div class="line"><span class="comment"># Configure logging location</span></div><div class="line">KIBANA_LOG=/var/<span class="built_in">log</span>/kibana.log</div><div class="line"></div><div class="line"><span class="comment"># Begin Script</span></div><div class="line">RETVAL=0</div><div class="line"></div><div class="line"><span class="keyword">if</span> [ `id -u` <span class="_">-ne</span> 0 ]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"You need root privileges to run this script"</span></div><div class="line">        <span class="built_in">exit</span> 1</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment"># Function library</span></div><div class="line">. /etc/init.d/<span class="built_in">functions</span></div><div class="line"> </div><div class="line"><span class="function"><span class="title">start</span></span>() &#123;</div><div class="line">        <span class="built_in">echo</span> -n <span class="string">"Starting <span class="variable">$DESC</span> : "</span></div><div class="line"></div><div class="line">pid=`pidofproc -p <span class="variable">$PID_FILE</span> kibana`</div><div class="line">        <span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$pid</span>"</span> ] ; <span class="keyword">then</span></div><div class="line">                <span class="built_in">echo</span> <span class="string">"Already running."</span></div><div class="line">                <span class="built_in">exit</span> 0</div><div class="line">        <span class="keyword">else</span></div><div class="line">        <span class="comment"># Start Daemon</span></div><div class="line"><span class="keyword">if</span> [ ! <span class="_">-d</span> <span class="string">"<span class="variable">$PID_FOLDER</span>"</span> ] ; <span class="keyword">then</span></div><div class="line">                        mkdir <span class="variable">$PID_FOLDER</span></div><div class="line">                <span class="keyword">fi</span></div><div class="line">daemon --user=<span class="variable">$DAEMON_USER</span> --pidfile=<span class="variable">$PID_FILE</span> <span class="variable">$DAEMON</span> 1&gt;<span class="string">"<span class="variable">$KIBANA_LOG</span>"</span> 2&gt;&amp;1 &amp;</div><div class="line">                sleep 2</div><div class="line">                pidofproc node &gt; <span class="variable">$PID_FILE</span></div><div class="line">                RETVAL=$?</div><div class="line">                [[ $? <span class="_">-eq</span> 0 ]] &amp;&amp; success || failure</div><div class="line"><span class="built_in">echo</span></div><div class="line">                [ <span class="variable">$RETVAL</span> = 0 ] &amp;&amp; touch <span class="variable">$LOCK_FILE</span></div><div class="line">                <span class="built_in">return</span> <span class="variable">$RETVAL</span></div><div class="line">        <span class="keyword">fi</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="title">reload</span></span>()</div><div class="line">&#123;</div><div class="line">    <span class="built_in">echo</span> <span class="string">"Reload command is not implemented for this service."</span></div><div class="line">    <span class="built_in">return</span> <span class="variable">$RETVAL</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="title">stop</span></span>() &#123;</div><div class="line">        <span class="built_in">echo</span> -n <span class="string">"Stopping <span class="variable">$DESC</span> : "</span></div><div class="line">        killproc -p <span class="variable">$PID_FILE</span> <span class="variable">$DAEMON</span></div><div class="line">        RETVAL=$?</div><div class="line"><span class="built_in">echo</span></div><div class="line">        [ <span class="variable">$RETVAL</span> = 0 ] &amp;&amp; rm <span class="_">-f</span> <span class="variable">$PID_FILE</span> <span class="variable">$LOCK_FILE</span></div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="keyword">case</span> <span class="string">"<span class="variable">$1</span>"</span> <span class="keyword">in</span></div><div class="line">  start)</div><div class="line">        start</div><div class="line">;;</div><div class="line">  stop)</div><div class="line">        stop</div><div class="line">        ;;</div><div class="line">  status)</div><div class="line">        status -p <span class="variable">$PID_FILE</span> <span class="variable">$DAEMON</span></div><div class="line">        RETVAL=$?</div><div class="line">        ;;</div><div class="line">  restart)</div><div class="line">        stop</div><div class="line">        start</div><div class="line">        ;;</div><div class="line">  reload)</div><div class="line">reload</div><div class="line">;;</div><div class="line">  *)</div><div class="line"><span class="comment"># Invalid Arguments, print the following message.</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"Usage: <span class="variable">$0</span> &#123;start|stop|status|restart&#125;"</span> &gt;&amp;2</div><div class="line"><span class="built_in">exit</span> 2</div><div class="line">        ;;</div><div class="line"><span class="keyword">esac</span></div><div class="line"></div><div class="line"></div><div class="line">修改启动权限</div><div class="line">chmod +x /etc/rc.d/init.d/kibana</div></pre></td></tr></table></figure>
<p>配置Kibana：</p>
<h3 id="编辑kibana-yaml-修改端口，设置host-可以设置本地服务器IP"><a href="#编辑kibana-yaml-修改端口，设置host-可以设置本地服务器IP" class="headerlink" title="编辑kibana.yaml  修改端口，设置host 可以设置本地服务器IP"></a>编辑kibana.yaml  修改端口，设置host 可以设置本地服务器IP</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">vim /usr/<span class="built_in">local</span>/kibana/config/kibana.yml</div><div class="line"></div><div class="line"><span class="comment"># Kibana is served by a back end server. This controls which port to use.</span></div><div class="line">server.port: 5601</div><div class="line"></div><div class="line"><span class="comment"># The host to bind the server to.</span></div><div class="line">server.host: <span class="string">"10.47.88.188"</span></div><div class="line"></div><div class="line"><span class="comment"># If you are running kibana behind a proxy, and want to mount it at a path,</span></div><div class="line"><span class="comment"># specify that path here. The basePath can't end in a slash.</span></div><div class="line"><span class="comment"># server.basePath: ""</span></div><div class="line"></div><div class="line"><span class="comment"># The maximum payload size in bytes on incoming server requests.</span></div><div class="line"><span class="comment"># server.maxPayloadBytes: 1048576</span></div><div class="line"></div><div class="line"><span class="comment"># The Elasticsearch instance to use for all your queries.</span></div><div class="line">elasticsearch.url: <span class="string">"http://10.47.88.188:9200"</span></div><div class="line"></div><div class="line"><span class="comment"># preserve_elasticsearch_host true will send the hostname specified in `elasticsearch`. If you set it to false,</span></div><div class="line"><span class="comment"># then the host you use to connect to *this* Kibana instance will be sent.</span></div><div class="line"></div><div class="line">elasticsearch.preserveHost: <span class="literal">true</span></div><div class="line"></div><div class="line"><span class="comment"># Kibana uses an index in Elasticsearch to store saved searches, visualizations</span></div><div class="line"><span class="comment"># and dashboards. It will create a new index if it doesn't already exist.</span></div><div class="line"><span class="comment"># kibana.index: ".kibana"</span></div><div class="line"></div><div class="line"><span class="comment"># The default application to load.</span></div><div class="line">kibana.defaultAppId: <span class="string">"discover"</span></div><div class="line"></div><div class="line"><span class="comment"># If your Elasticsearch is protected with basic auth, these are the user credentials</span></div><div class="line"><span class="comment"># used by the Kibana server to perform maintenance on the kibana_index at startup. Your Kibana</span></div><div class="line"><span class="comment"># users will still need to authenticate with Elasticsearch (which is proxied through</span></div><div class="line"><span class="comment"># the Kibana server)</span></div><div class="line"></div><div class="line"><span class="comment"># elasticsearch.ssl.key: /path/to/your/client.key</span></div><div class="line"></div><div class="line"><span class="comment"># If you need to provide a CA certificate for your Elasticsearch instance, put</span></div><div class="line"><span class="comment"># the path of the pem file here.</span></div><div class="line"><span class="comment"># elasticsearch.ssl.ca: /path/to/your/CA.pem</span></div><div class="line"></div><div class="line"><span class="comment"># Set to false to have a complete disregard for the validity of the SSL</span></div><div class="line"><span class="comment"># certificate.</span></div><div class="line"><span class="comment"># elasticsearch.ssl.verify: true</span></div><div class="line"></div><div class="line"><span class="comment"># Time in milliseconds to wait for elasticsearch to respond to pings, defaults to</span></div><div class="line"><span class="comment"># request_timeout setting</span></div><div class="line"><span class="comment"># elasticsearch.pingTimeout: 1500</span></div><div class="line"></div><div class="line"><span class="comment"># Time in milliseconds to wait for responses from the back end or elasticsearch.</span></div><div class="line"><span class="comment"># This must be &gt; 0</span></div><div class="line"></div><div class="line">elasticsearch.requestTimeout: 30000</div><div class="line"></div><div class="line"><span class="comment"># Time in milliseconds for Elasticsearch to wait for responses from shards.</span></div><div class="line"><span class="comment"># Set to 0 to disable.</span></div><div class="line"><span class="comment"># elasticsearch.shardTimeout: 0</span></div></pre></td></tr></table></figure>
<h3 id="启动kibana服务"><a href="#启动kibana服务" class="headerlink" title="启动kibana服务"></a>启动kibana服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">service kibana start</div><div class="line">service kibana status</div></pre></td></tr></table></figure>
<h3 id="查看端口"><a href="#查看端口" class="headerlink" title="查看端口"></a>查看端口</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">netstat -nltp</div><div class="line">[root@es_02 config]<span class="comment"># netstat -nltp</span></div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name</div><div class="line">tcp        0      0 127.0.0.1:32000             0.0.0.0:*                   LISTEN      2517/java</div><div class="line">tcp        0      0 10.47.88.188:5601           0.0.0.0:*                   LISTEN      6474/node</div><div class="line">tcp        0      0 10.47.88.188:10050          0.0.0.0:*                   LISTEN      305/zabbix_agentd</div><div class="line">tcp        0      0 10.47.88.188:9200           0.0.0.0:*                   LISTEN      5198/java</div><div class="line">tcp        0      0 10.47.88.188:9300           0.0.0.0:*                   LISTEN      5198/java</div><div class="line">tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      25265/sshd</div></pre></td></tr></table></figure>
<p>到我Github上面下载kabana启动脚本。</p>
<p>es_01机器从es_02机器拷贝过去修改下配置就可以。 </p>
<h3 id="kibana安装插件参考："><a href="#kibana安装插件参考：" class="headerlink" title="kibana安装插件参考："></a>kibana安装插件参考：</h3><p><a href="https://www.elastic.co/guide/en/marvel/current/installing-marvel.html#installing-marvel" target="_blank" rel="external">Installing Marvel</a></p>
<p>这里kibana我做了nginx反向代理，集群代理。</p>
<h3 id="nginx配置kibana反向代理："><a href="#nginx配置kibana反向代理：" class="headerlink" title="nginx配置kibana反向代理："></a>nginx配置kibana反向代理：</h3><p>这里我只允许我公司IP访问：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">  upstream kibana.ihaozhuo.com &#123;</div><div class="line">        server 10.47.88.206:5601 weight=1;</div><div class="line">        server 10.47.88.188:5601 weight=1;</div><div class="line">&#125;</div><div class="line">  server &#123;</div><div class="line">        listen       80;</div><div class="line">        server_name  kibana.ihaozhuo.com;</div><div class="line">        location / &#123;</div><div class="line">             index        index.html index.php index.jsp index.htm;</div><div class="line">             allow 202.107.202.82/32;</div><div class="line">             deny all;</div><div class="line">             proxy_pass           http://kibana.ihaozhuo.com;</div><div class="line">             proxy_ignore_client_abort on;</div><div class="line">             proxy_redirect               off;</div><div class="line">             proxy_set_header     Host    <span class="variable">$host</span>;</div><div class="line">             proxy_set_header     X-Real-IP       <span class="variable">$remote_addr</span>;</div><div class="line">             proxy_set_header     X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</div><div class="line">             &#125;</div><div class="line">      &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="KafKa集群搭建"><a href="#KafKa集群搭建" class="headerlink" title="KafKa集群搭建"></a>KafKa集群搭建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">下载地址：http://mirrors.hust.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.9.0.0.tgz</div><div class="line">[root@kafka_01 srv]<span class="comment"># tar -xvf kafka_2.10-0.9.0.0.tgz</span></div><div class="line">[root@kafka_01 srv]<span class="comment"># mv kafka_2.10-0.9.0.0 kafka</span></div></pre></td></tr></table></figure>
<p><em>修改kafka配置文件</em> </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@kafka_01 config]<span class="comment"># vim /srv/kafka/config/server.properties</span></div><div class="line"></div><div class="line"><span class="comment">#设置brokerid（从0开始，3个节点分别设为0,1,2，不能重复）在这里id=0跟zookeeper id设置一样就行。 集群机器：按顺序写1</span></div><div class="line">broker.id=0  </div><div class="line"></div><div class="line"><span class="comment">#设置data目录，最好不要用默认的/tmp/kafka-logs</span></div><div class="line">mkdir -p /srv/kafka/data/kafka-logs</div><div class="line"> </div><div class="line"></div><div class="line"><span class="comment">#修改本地IP地址：</span></div><div class="line">listeners=PLAINTEXT://10.46.72.172:9092</div><div class="line"> </div><div class="line">log.dirs=/srv/kafka/data/kafka-logs</div><div class="line"><span class="comment">#设置注册地址（重要，默认会把本机的hostanme注册到zk中，客户端连接时需要解析该hostanme，所以这里直接注册本机的IP地址，避免hostname解析失败，报错java.nio.channels.UnresolvedAddressException或java.io.IOException: Can not resolve address）</span></div><div class="line"><span class="comment">#设置zookeeper地址</span></div><div class="line">zookeeper.connect=10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181</div></pre></td></tr></table></figure>
<p><em>配置zookeeper地址</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">vim zookeeper.properties </div><div class="line">dataDir=/home/jollybi/tools/zookeeper-3.4.5/tmp</div><div class="line"><span class="comment"># the port at which the clients will connect</span></div><div class="line">clientPort=2281</div><div class="line"><span class="comment"># disable the per-ip limit on the number of connections since this is a non-production config</span></div><div class="line">maxClientCnxns=0</div><div class="line">~</div></pre></td></tr></table></figure>
<p><em>配置kafka访问地址</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">vim producer.properties </div><div class="line">metadata.broker.list=169.44.62.139:9292,169.44.59.138:9292,169.44.62.137:9292   </div><div class="line"><span class="comment"># name of the partitioner class for partitioning events; default partition spreads data randomly</span></div><div class="line"><span class="comment">#partitioner.class=</span></div><div class="line"></div><div class="line"><span class="comment"># specifies whether the messages are sent asynchronously (async) or synchronously (sync)</span></div><div class="line">producer.type=sync</div><div class="line"></div><div class="line"><span class="comment"># specify the compression codec for all data generated: none, gzip, snappy, lz4.</span></div><div class="line"><span class="comment"># the old config values work as well: 0, 1, 2, 3 for none, gzip, snappy, lz4, respectively</span></div><div class="line">compression.codec=none</div><div class="line"></div><div class="line"><span class="comment"># message encoder</span></div><div class="line">serializer.class=kafka.serializer.DefaultEncoder</div></pre></td></tr></table></figure>
<h3 id="Kafka常用命令-普及"><a href="#Kafka常用命令-普及" class="headerlink" title="Kafka常用命令(普及)"></a>Kafka常用命令(普及)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">Kafka常用命令</div><div class="line">以下是kafka常用命令行总结：  </div><div class="line">1.查看topic的详细信息  </div><div class="line">./kafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic <span class="built_in">test</span>KJ1  </div><div class="line">2、为topic增加副本  </div><div class="line">./kafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute  </div><div class="line">3、创建topic </div><div class="line">./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic <span class="built_in">test</span>KJ1  </div><div class="line">4、为topic增加partition  </div><div class="line">./bin/kafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic <span class="built_in">test</span>KJ1  </div><div class="line">5、kafka生产者客户端命令  </div><div class="line">./kafka-console-producer.sh --broker-list localhost:9092 --topic <span class="built_in">test</span>KJ1  </div><div class="line">6、kafka消费者客户端命令  </div><div class="line">./kafka-console-consumer.sh -zookeeper localhost:2181 --from-beginning --topic <span class="built_in">test</span>KJ1  </div><div class="line">7、kafka服务启动  </div><div class="line">./kafka-server-start.sh -daemon ../config/server.properties   </div><div class="line">8、下线broker  </div><div class="line">./kafka-run-class.sh kafka.admin.ShutdownBroker --zookeeper 127.0.0.1:2181 --broker <span class="comment">#brokerId# --num.retries 3 --retry.interval.ms 60  </span></div><div class="line">shutdown broker  </div><div class="line">9、删除topic  </div><div class="line">./kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic <span class="built_in">test</span>KJ1 --zookeeper 127.0.0.1:2181  </div><div class="line">./kafka-topics.sh --zookeeper localhost:2181 --delete --topic <span class="built_in">test</span>KJ1  </div><div class="line">10、查看consumer组内消费的offset  </div><div class="line">./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group <span class="built_in">test</span> --topic <span class="built_in">test</span>KJ1</div></pre></td></tr></table></figure>
<h3 id="Kafka群集新建一个Topic"><a href="#Kafka群集新建一个Topic" class="headerlink" title="Kafka群集新建一个Topic"></a>Kafka群集新建一个Topic</h3><p>叫做logstash  Topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#查看tocpic列表（--zookeeper指定任意一个zk节点即可，用于获取集群信息）</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-topics.sh --zookeeper zk1.yazuoyw.com:2181 --describe</div><div class="line"> </div><div class="line"><span class="comment">#创建topic（--replication-factor表示复制到多少个节点，--partitions表示分区数，一般都设置为2或与节点数相等，不能大于总节点数）</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-topics.sh --zookeeper zk1.yazuoyw.com:2181 --create --topic topic1 --replication-factor 2 --partitions 2</div><div class="line"> </div><div class="line"><span class="comment">#发送消息（--topic 指定topic）</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-console-producer.sh --broker-list kafka1.yazuoyw.com:9092,kafka2.yazuoyw.com:9092,kafka3.yazuoyw.com:9092 --topic topic1</div><div class="line">message1</div><div class="line">message2</div><div class="line"> </div><div class="line"><span class="comment">#消费消息</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-console-consumer.sh --zookeeper zk1.yazuoyw.com:2181 --topic topic1</div><div class="line"></div><div class="line"><span class="comment">#replica检查</span></div><div class="line">/usr/<span class="built_in">local</span>/kafka/bin/kafka-replica-verification.sh --broker-list kafka1.yazuoyw.com:9092,kafka2.yazuoyw.com:9092,kafka3.yazuoyw.com:9092</div></pre></td></tr></table></figure>
<p>每条发布到Kafka集群的消息都有一个类别，这个类别被称为topic。（物理上不同topic的消息分开存储，逻辑上一个topic的消息虽然保存于一个或多个broker上但用户只需指定消息的topic即可生产或消费数据而不必关心数据存于何处）</p>
<p><code>ElasticSearch</code>机器<code>logstash</code>把数据从<code>kafka</code>存到<code>elasticsearch</code>的配置</p>
<p>其中选取kafka群集任意一个有zk的ip做连接使用</p>
<p><code>topic_id</code>就是kafka中设置的<code>topic logstash</code></p>
<p>在es上面安装logstash配置<br>/usr/local/logstash/config/kafka_to_es.conf</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"> input &#123;</div><div class="line">    kafka &#123;</div><div class="line">                zk_connect =&gt; <span class="string">"10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181/kafka"</span></div><div class="line">                group_id =&gt; <span class="string">"logstash"</span></div><div class="line">                topic_id =&gt; <span class="string">"logstash"</span></div><div class="line">                reset_beginning =&gt; <span class="literal">false</span> <span class="comment"># boolean (optional)， default: false</span></div><div class="line">                consumer_threads =&gt; 2  <span class="comment"># number (optional)， default: 1</span></div><div class="line">                decorate_events =&gt; <span class="literal">false</span> <span class="comment"># boolean (optional)， default: false</span></div><div class="line">        &#125;</div><div class="line">  &#125;</div><div class="line">  output &#123;</div><div class="line">    elasticsearch &#123;</div><div class="line">      hosts =&gt; [<span class="string">"10.47.88.206:9200"</span>,<span class="string">"10.47.88.188:9200"</span>]</div><div class="line">      index =&gt; <span class="string">"%&#123;host&#125;-%&#123;+YYYY.MM.dd&#125;"</span></div><div class="line">&#125;</div><div class="line">   <span class="comment"># stdout &#123; codec =&gt; rubydebug &#125;</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>新建了个测试的，测试下发送是否成功：/usr/local/logstash/config/stdin_to_es.conf </p>
<figure class="highlight puppet"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">input</span> &#123;</div><div class="line"> stdin &#123;&#125;</div><div class="line">&#125;</div><div class="line">  <span class="keyword">output</span> &#123;</div><div class="line">    elasticsearch &#123;</div><div class="line">      <span class="attr">hosts</span> =&gt; <span class="string">"10.47.88.206"</span>&#125;</div><div class="line">    <span class="keyword">stdout</span> &#123;</div><div class="line">      <span class="attr">codec</span> =&gt; rubydebug &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="Step-2-启动服务"><a href="#Step-2-启动服务" class="headerlink" title="Step 2: 启动服务"></a>Step 2: 启动服务</h3><figure class="highlight dts"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Kafka用到了Zookeeper，所有首先启动Zookper，下面简单的启用一个单实例的Zookkeeper服务。可以在命令的结尾加个&amp;符号，这样就可以启动后离开控制台。</div><div class="line"></div><div class="line"><span class="meta">#现在启动Kafka:</span></div><div class="line"></div><div class="line"><span class="meta-keyword">/srv/</span>kafka<span class="meta-keyword">/bin/</span>kafka-server-start.sh -daemon config/server.properties</div><div class="line"></div><div class="line"><span class="meta">#添加开机启动</span></div><div class="line">echo ‘</div><div class="line"><span class="meta"># start kafka</span></div><div class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/kafka/</span>bin/kafka-server-start.sh -daemon <span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/kafka/</span>config/server.properties</div><div class="line">‘ &gt;&gt; <span class="meta-keyword">/etc/</span>rc.local</div><div class="line"> </div><div class="line"><span class="meta">#关闭</span></div><div class="line"><span class="meta-keyword">/usr/</span>local<span class="meta-keyword">/kafka/</span>bin/kafka-server-stop.sh</div></pre></td></tr></table></figure>
<h3 id="kafka配置防火墙："><a href="#kafka配置防火墙：" class="headerlink" title="kafka配置防火墙："></a>kafka配置防火墙：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-A INPUT -i eth0 -p tcp -m state --state NEW -m tcp --dport 4888 -j ACCEPT</div></pre></td></tr></table></figure>
<h3 id="zookeeper集群"><a href="#zookeeper集群" class="headerlink" title="zookeeper集群"></a>zookeeper集群</h3><p>查看我之前写的这篇文档 <a href="http://blog.yangcvo.me/2016/05/28/%E5%A4%A7%E6%95%B0%E6%8D%AEhadoop/zookeeper/ZooKeeper%E9%9B%86%E7%BE%A4%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BC%98%E5%8C%96/" target="_blank" rel="external">ZooKeeper的集群快速搭建与优化</a> </p>
<h3 id="走kafka查看是否所有节点都启动："><a href="#走kafka查看是否所有节点都启动：" class="headerlink" title="走kafka查看是否所有节点都启动："></a>走kafka查看是否所有节点都启动：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@kafka_03 bin]<span class="comment"># sh zkCli.sh</span></div><div class="line">Connecting to localhost:2181</div><div class="line">2017-01-04 19:20:24,849 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT</div><div class="line">2017-01-04 19:20:24,853 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=kafka_03</div><div class="line">2017-01-04 19:20:24,853 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_66</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/srv/jdk1.8.0_66/jre</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/srv/zookeeper-3.4.6/bin/../build/classes:/srv/zookeeper-3.4.6/bin/../build/lib/*.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/srv/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/srv/zookeeper-3.4.6/bin/../lib/<span class="built_in">log</span>4j-1.2.16.jar:/srv/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/srv/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/srv/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/srv/zookeeper-3.4.6/bin/../conf:</div><div class="line">2017-01-04 19:20:24,856 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line"></div><div class="line">[zk: localhost:2181(CONNECTED) 0] ls /</div><div class="line">[controller_epoch, brokers, zookeeper, kafka, dubbo, admin, isr_change_notification, consumers, config, sthp]</div><div class="line">[zk: localhost:2181(CONNECTED) 5] ls /kafka/brokers/ids</div><div class="line">[0, 1, 2]</div></pre></td></tr></table></figure>
<p>kafka 三台集群这里可以看到获取到ids。</p>
<h4 id="安全问题"><a href="#安全问题" class="headerlink" title="安全问题"></a>安全问题</h4><p>特别要注意elk所有软件的端口监听,切勿暴露监听到公网上去,另外即便是内网你也得注意配置内网的访问限制。</p>
<h3 id="logstash-客户端安装："><a href="#logstash-客户端安装：" class="headerlink" title="logstash 客户端安装："></a>logstash 客户端安装：</h3><h5 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line">我这里源码包安装</div><div class="line"><span class="comment"># wget https://download.elasticsearch.org/logstash/logstash/logstash-2.4.0.tar.gz</span></div><div class="line"><span class="comment">#curl -O https://download.elastic.co/logstash/logstash/logstash-2.4.0.tar.gz</span></div><div class="line"><span class="comment">#tar -zxvf logstash-2.4.0.tar.gz</span></div><div class="line"><span class="comment">#mv logstash-2.4.0 /usr/local/</span></div><div class="line"><span class="comment">#ln -s /usr/local/logstash-2.4.0/ /usr/local/logstash</span></div><div class="line"></div><div class="line">下载启动脚本</div><div class="line">生产都是运行在后台的，我这里源码安装没有init脚本启动。 去Github下载  https://github.com/benet1006/ELK_config.git</div><div class="line"><span class="comment">#cp logstash.init /etc/init.d/logstash</span></div><div class="line"><span class="comment">#chmod +x /etc/init.d/logstash</span></div><div class="line">这个脚本我做过修改。</div><div class="line"></div><div class="line"><span class="comment">#启动logstash服务</span></div><div class="line">service logstash start</div><div class="line">service logstash status</div><div class="line"></div><div class="line"><span class="comment">#查看5000端口</span></div><div class="line">netstat -nltp</div><div class="line"></div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name</div><div class="line">tcp 0 0 0.0.0.0:9200 0.0.0.0:* LISTEN 1765/java</div><div class="line">tcp 0 0 0.0.0.0:9300 0.0.0.0:* LISTEN 1765/java</div><div class="line">tcp 0 0 0.0.0.0:9301 0.0.0.0:* LISTEN 2309/java</div><div class="line">tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1509/sshd</div><div class="line">tcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 1876/node</div><div class="line">tcp 0 0 0.0.0.0:5000 0.0.0.0:* LISTEN 2309/java</div><div class="line">tcp 0 0 :::22 :::* LISTEN 1509/sshd</div><div class="line"></div><div class="line"></div><div class="line">修改启动脚本</div><div class="line">vim /etc/init.d/logstash </div><div class="line">指定的目录自己源码安装的路径。</div><div class="line"></div><div class="line">name=logstash</div><div class="line">pidfile=<span class="string">"/var/run/<span class="variable">$name</span>.pid"</span></div><div class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/jre/lib/rt.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</div><div class="line">LS_USER=logstash</div><div class="line">LS_GROUP=logstash</div><div class="line">LS_HOME=/usr/<span class="built_in">local</span>/logstash 安装路径</div><div class="line">LS_HEAP_SIZE=<span class="string">"1000m"</span></div><div class="line">LS_JAVA_OPTS=<span class="string">"-Djava.io.tmpdir=<span class="variable">$&#123;LS_HOME&#125;</span>"</span></div><div class="line">LS_LOG_DIR=/usr/<span class="built_in">local</span>/logstash</div><div class="line">LS_LOG_FILE=<span class="string">"<span class="variable">$&#123;LS_LOG_DIR&#125;</span>/<span class="variable">$name</span>.log"</span></div><div class="line">LS_CONF_FILE=/etc/logstash.conf     收集日志的规则conf</div><div class="line">LS_OPEN_FILES=16384</div><div class="line">LS_NICE=19</div><div class="line">LS_OPTS=<span class="string">""</span></div><div class="line"></div><div class="line">https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html</div><div class="line">这个是<span class="built_in">log</span> stash的官方文档的配置说明。</div><div class="line">这个配置说明上面我先修改下我之前的配置文件。</div></pre></td></tr></table></figure>
<h4 id="logstash-agent配置："><a href="#logstash-agent配置：" class="headerlink" title="logstash agent配置："></a>logstash agent配置：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">配置<span class="built_in">log</span> stash－实现系统日志收集input</div><div class="line">file_to_kafka.conf 日志文件读出写入到kafka</div><div class="line">input &#123;</div><div class="line">file &#123;</div><div class="line">path =&gt; <span class="string">"/srv/tomcat/logs/account/logFile.*.log"</span></div><div class="line"><span class="built_in">type</span> =&gt; <span class="string">"tomcat"</span></div><div class="line">discover_interval =&gt; 15 <span class="comment">#logstash</span></div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line"><span class="comment">#stdout &#123; codec =&gt; rubydebug &#125;</span></div><div class="line">kafka&#123;</div><div class="line">bootstrap_servers =&gt; <span class="string">"10.46.72.172:9092,10.47.88.103:9092,10.47.102.137:9092"</span></div><div class="line"><span class="comment">#group_id =&gt; "logstash"</span></div><div class="line">topic_id =&gt; <span class="string">"logstash"</span></div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">2.2 logstash indexer 配置</div><div class="line">kafka_to_es.conf</div><div class="line">input &#123;</div><div class="line">kafka &#123;</div><div class="line">zk_connect =&gt; <span class="string">"10.46.72.172:2181,10.47.88.103:2181,10.47.102.137:2181kafka"</span></div><div class="line">group_id =&gt; <span class="string">"logstash"</span></div><div class="line">topic_id =&gt; <span class="string">"logstash"</span></div><div class="line">reset_beginning =&gt; <span class="literal">false</span> <span class="comment"># boolean (optional)， default: false</span></div><div class="line">consumer_threads =&gt; 2 <span class="comment"># number (optional)， default: 1</span></div><div class="line">decorate_events =&gt; <span class="literal">false</span> <span class="comment"># boolean (optional)， default: false</span></div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line">elasticsearch &#123;</div><div class="line">hosts =&gt; [<span class="string">"10.47.88.206:9200"</span>,<span class="string">"10.47.88.188:9200"</span>]</div><div class="line">index =&gt; <span class="string">"%&#123;host&#125;-%&#123;+YYYY.MM.dd&#125;"</span></div><div class="line">&#125;</div><div class="line"><span class="comment"># stdout &#123; codec =&gt; rubydebug &#125;</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="es安装插件head查看下效果："><a href="#es安装插件head查看下效果：" class="headerlink" title="es安装插件head查看下效果："></a>es安装插件head查看下效果：</h4><p>然后打开网站：<a href="http://elk.ihaozhuo.com/_plugin/head/" target="_blank" rel="external">http://elk.ihaozhuo.com/_plugin/head/</a></p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/es_head.png" alt=""></figure></p>
<p>####kibana网站效果：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/es_kibana.png" alt=""></figure></p>
<h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2016/11/14/日志分析平台/Graylog/graylog 升级2.1 集中日志解决方案/">graylog 升级2.1 集中日志解决方案</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2016-11-14</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Log-Analysis-Platform/">Log Analysis Platform</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Graylog/">Graylog</a>
			</span>
		
	</div>

	

	
		<h1 id="graylog-升级2-1-集中日志解决方案"><a href="#graylog-升级2-1-集中日志解决方案" class="headerlink" title="graylog 升级2.1 集中日志解决方案"></a>graylog 升级2.1 集中日志解决方案</h1><p>参考官网：<a href="http://docs.graylog.org/en/2.1/pages/upgrade/graylog-2.1.html" target="_blank" rel="external">升级到Graylog的2.1.x</a></p>
<h3 id="一：为什么需要集中日志解决方案？"><a href="#一：为什么需要集中日志解决方案？" class="headerlink" title="一：为什么需要集中日志解决方案？"></a>一：为什么需要集中日志解决方案？</h3><p>在公司服务机子部署越来越多的情况下，让我们来想想会遇到的问题：</p>
<ul>
<li>开发人员不能登录线上服务器查看详细日志，经过运维周转费时费力</li>
<li>日志数据分散在多个系统，难以查找</li>
<li>日志数据量大，查询速度慢</li>
<li>一个调用会涉及多个系统，难以在这些系统的日志中快速定位数据</li>
<li>数据不够实时</li>
<li>很难对数据进行挖掘，分析，业务告警，审计</li>
<li>这些问题的存在让开发以及运维人员很是头痛，严重影响效率！</li>
</ul>
<h3 id="二：什么是graylog技术栈？"><a href="#二：什么是graylog技术栈？" class="headerlink" title="二：什么是graylog技术栈？"></a>二：什么是graylog技术栈？</h3><p>为了解决上述问题，我们需要一个日志的集中管理方案，graylog技术栈：</p>
<ul>
<li>java （jdk1.8.0_66/）环境</li>
<li>Collector-sidecar（收集日志）或者syslog</li>
<li>Mongodb（存储日志源文件）</li>
<li>Elasticsearch（提供搜索日志）</li>
<li>Graylog2.1.1（搜索和视图展示日志，告警和权限）</li>
</ul>
<p>有了这些，我们就能把日志先收集起来，进行我们想要的分析之后，web的形式展示出来，提供查询！</p>
<h3 id="三：graylog的安装部署"><a href="#三：graylog的安装部署" class="headerlink" title="三：graylog的安装部署"></a>三：graylog的安装部署</h3><p>安装环境：linux centOS系统安装，已安装JDK1.8版本，安装启动顺序</p>
<p>1.安装部署mongodb<br>2.安装部署elasticsearch<br>3.安装部署graylog<br>4.安装部署Graylog Collector Sidecar</p>
<hr>
<p> <strong>1：安装部署mongodb</strong> <strong>Java也安装</strong>  参考我博文有介绍。 </p>
<p> <strong>2：安装部署elasticsearch</strong></p>
<p><strong>(1)下载jar包</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz</div><div class="line"></div><div class="line">如果报错</div><div class="line">执行</div><div class="line"></div><div class="line">wget --no-check-certificate</div><div class="line"></div><div class="line">https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz</div></pre></td></tr></table></figure>
<p><strong>(2)解压jar包</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tar -zxvf elasticsearch-2.3.5.tar.gz -C /opt/</div></pre></td></tr></table></figure>
<p><strong>(3)修改elasticsearch.yml配置文件</strong></p>
<p> 这里需要修改配置文件：配置前先创建几个目录文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /home/data/es-data -p</div><div class="line">mkdir /home/data/es-work -p</div></pre></td></tr></table></figure>
<p>elasticsearch-2.3.5安装目录conf下执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">vim elasticsearch-2.3.5/config/elasticsearch.yml</div><div class="line"> </div><div class="line">cluster.name: graylog              <span class="comment">#集群名称建议命名graylog，便于识别区分</span></div><div class="line">node.name: elasticsearch-node-1     <span class="comment"># elasticsearch集群节点名称</span></div><div class="line">network.host: 192.168.1.234    <span class="comment"># 绑定节点IP</span></div><div class="line">http.port: 9200         <span class="comment"># 外部访问端口，默认，也可以安全考虑修改</span></div><div class="line">path.logs: /home/data/logs</div><div class="line">path.data: /home/data/es-data</div><div class="line">discovery.zen.ping.multicast.enabled: <span class="literal">false</span>   <span class="comment">#多播发现方式关闭，因为graylog采用单播方式发现elasticsearch集群方式</span></div><div class="line">discovery.zen.ping.unicast.hosts                   <span class="comment">#多个节点用逗号隔开</span></div><div class="line">discovery.zen.minimum_master_nodes: 3    <span class="comment"># elasticsearch集群节点，最少选举数，这个数一定要设置为整个集群节点个数的一半加1，即N/2+1，必须为奇数</span></div></pre></td></tr></table></figure>
<p><strong>(4)启动elasticsearch服务</strong></p>
<p>新建一个elasticsearch用户，出于安全考虑，elasticsearch服务不能使用root用户启动</p>
<p>创建elasticsearch用户组及elasticsearch用户，执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">groupadd elasticsearch</div><div class="line">useradd elasticsearch -g elasticsearch -p elasticsearch</div></pre></td></tr></table></figure>
<p>其中-g使用户属于某个组，-p为新用户使用加密密码）<br>更改elasticsearch-2.3.5文件夹及内部文件的所属用户及组为elasticsearch:elasticsearch</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">chown -R elasticsearch:elasticsearch  /home/data/</div><div class="line">chown -R elasticsearch:elasticsearch  elasticsearch-2.3.5</div></pre></td></tr></table></figure>
<p>切换用户</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">su elasticsearch</div></pre></td></tr></table></figure>
<p>在elasticsearch-2.3.5/bin目录下执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@graylog bin]$ ps -ef | grep elasticsearch</div><div class="line">root     18265 16004  0 14:27 pts/1    00:00:00 su elasticsearch</div><div class="line">502      18405     1 62 14:27 pts/1    00:00:13 /srv/jdk1.8.0_66/bin/java -Xms256m -Xmx1g -Djava.awt.headless=<span class="literal">true</span> -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=<span class="literal">true</span> -Des.path.home=/opt/elasticsearch-2.3.5 -cp /opt/elasticsearch-2.3.5/lib/elasticsearch-2.3.5.jar:/opt/elasticsearch-2.3.5/lib/* org.elasticsearch.bootstrap.Elasticsearch start <span class="_">-d</span></div><div class="line">502      18530 18266  0 14:27 pts/1    00:00:00 grep --color=auto ela</div></pre></td></tr></table></figure>
<p><strong>(5)检查elasticsearch服务状态</strong></p>
<p>执行如下命令测试Elasticsearch是否正常运行：</p>
<p>$ curl -XGET ‘<a href="http://localhost:9200/_cluster/health?pretty=true" target="_blank" rel="external">http://localhost:9200/_cluster/health?pretty=true</a>‘</p>
<p>输出的信息如下表示Elasticsearch安装成功：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">[elasticsearch@graylog bin]$ curl -XGET <span class="string">'http://192.168.1.234:9200/_cluster/health?pretty=true'</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"graylog"</span>,</div><div class="line">  <span class="string">"status"</span> : <span class="string">"yellow"</span>,</div><div class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</div><div class="line">  <span class="string">"number_of_nodes"</span> : 1,</div><div class="line">  <span class="string">"number_of_data_nodes"</span> : 1,</div><div class="line">  <span class="string">"active_primary_shards"</span> : 30,</div><div class="line">  <span class="string">"active_shards"</span> : 30,</div><div class="line">  <span class="string">"relocating_shards"</span> : 0,</div><div class="line">  <span class="string">"initializing_shards"</span> : 0,</div><div class="line">  <span class="string">"unassigned_shards"</span> : 30,</div><div class="line">  <span class="string">"delayed_unassigned_shards"</span> : 0,</div><div class="line">  <span class="string">"number_of_pending_tasks"</span> : 0,</div><div class="line">  <span class="string">"number_of_in_flight_fetch"</span> : 0,</div><div class="line">  <span class="string">"task_max_waiting_in_queue_millis"</span> : 0,</div><div class="line">  <span class="string">"active_shards_percent_as_number"</span> : 50.0</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3：安装部署graylog-2-1版本"><a href="#3：安装部署graylog-2-1版本" class="headerlink" title="3：安装部署graylog-2.1版本"></a>3：安装部署graylog-2.1版本</h3><p><strong>(1)下载安装包</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget https://packages.graylog2.org/releases/graylog/graylog-2.1.1.tgz</div></pre></td></tr></table></figure>
<p><strong>(2)解压安装包</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tar -zxvf graylog-2.1.1.tgz -C /opt/.</div></pre></td></tr></table></figure>
<p><strong>(3)修改配置文件</strong></p>
<p>修改安装目录下 graylog.conf.example 文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim graylog.conf.example</div></pre></td></tr></table></figure>
<p>web_listen_uri 值是graylog启动成功后，web服务访问地址</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">web_listen_uri = http://192.168.1.234:9000/</div></pre></td></tr></table></figure>
<p>rest_listen_uri 的值，是graylog启动成功后，api访问地址</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rest_listen_uri = http://192.168.1.234:9000/api/</div></pre></td></tr></table></figure>
<p>root_timezone = UTC  #设置时区，否则默认使用的是UTC时间也就是世界时间 这里是必须改下的，因为后期收集日志显示时间是有变化的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">root_timezone = Asia/Shanghai</div></pre></td></tr></table></figure>
<p>其中 password_secret 的值用命令生成</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">yum install -y pwgen</div><div class="line">pwgen -N 1 <span class="_">-s</span> 96       H1R5v17kWtyDxj2PzxLMxu41D6HDt9JzhfZcj6QlCURVddgkLAdnUmpkdIscmmu4ELKsTrHwKvPmxFKSYyTn0YlqebbpQqyr</div><div class="line"></div><div class="line">    password_secret = H1R5v17kWtyDxj2PzxLMxu41D6HDt9JzhfZcj6QlCURVddgkLAdnUmpkdIscmmu4ELKsTrHwKvPmxFKSYyTn0YlqebbpQqyr</div></pre></td></tr></table></figure>
<p>其中root_password_sha2 的值使用命令生成</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="built_in">echo</span> -n Ihaozhuo_b313 | sha256sum  (这里对密码123456哈希加密)</div><div class="line"></div><div class="line"><span class="built_in">fc</span>88c28d48b0cb97f3fb5286cc35c520409ef037acd30ec687f0c0bd3d5a5115 </div><div class="line">   </div><div class="line">root_password_sha2 = <span class="built_in">fc</span>88c28d48b0cb97f3fb5286cc35c520409ef037acd30ec687f0c0bd3d5a5115</div></pre></td></tr></table></figure>
<p>elasticsearch_cluster_name 值必须是elasticsearch配置文件中的cluster_name</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">elasticsearch_cluster_name = graylog</div></pre></td></tr></table></figure>
<p>elasticsearch_discovery_zen_ping_unicast_hosts 填写elasticsearch地址，如果是多个，用逗号隔开</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">elasticsearch_discovery_zen_ping_unicast_hosts = 192.168.1.234:9300</div></pre></td></tr></table></figure>
<p>elasticsearch_discovery_zen_ping_multicast_enabled = false 多播模式关闭</p>
<p>由于我们只有一个Elasticsearch shard，需要把elasticsearch_shards参数设置为1：<br>elasticsearch集群分片数量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">elasticsearch_shards = 1</div></pre></td></tr></table></figure>
<p>elasticsearch绑定的节点IP</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">299 elasticsearch_network_host = 192.168.1.234</div><div class="line">300 elasticsearch_network_bind_host = 192.168.1.234</div><div class="line">301 elasticsearch_network_publish_host = 192.168.1.234</div></pre></td></tr></table></figure>
<p>mongodb安装服务的ip地址</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mongodb_uri = mongodb://192.168.1.234/graylog</div></pre></td></tr></table></figure>
<p>这里mongodb是安装在我同一台服务器上面的。如果要把mongodb单独服务器跑连接方式配置文件里面也有例子说明：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mongodb_uri =mongodb://graylog:123456@160.17.2.251:27017/graylog2             <span class="comment">#连接到mongodb的服务器地址为160.17.2.251:27017，账号为graylog，密码为123456 数据库为graylog2</span></div></pre></td></tr></table></figure>
<p> 设置告警邮件发送者信息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># Email transport</span></div><div class="line">transport_email_enabled = <span class="literal">false</span></div><div class="line">transport_email_hostname = smtp.exmail.qq.com</div><div class="line">transport_email_port = 465</div><div class="line">transport_email_use_auth = <span class="literal">true</span></div><div class="line">transport_email_use_tls = <span class="literal">true</span></div><div class="line">transport_email_use_ssl = <span class="literal">true</span></div><div class="line">transport_email_auth_username = chengyangyang@qq.cn</div><div class="line">transport_email_auth_password = beneTqq</div><div class="line">transport_email_subject_prefix = [graylog]</div><div class="line">transport_email_from_email = chengyangyang@qq.cn</div></pre></td></tr></table></figure>
<p><strong>(4)复制配置文件</strong></p>
<p> 因为graylog安装bin目录下，默认启动配置文件</p>
<p>配置文件路径：<code>/etc/graylog/server/server.conf</code></p>
<p>所以需要将<code>graylog.conf.example</code> 复制到<code>/etc/graylog/server/</code>目录下，并且改名 <code>server.conf</code></p>
<p>执行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /etc/graylog/server/ </div><div class="line">cp graylog.conf.example /etc/graylog/server/server.conf</div></pre></td></tr></table></figure>
<p><strong>(5)启动graylog</strong></p>
<p>在graylog安装bin目录下执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./graylogctl start</div></pre></td></tr></table></figure>
<p>查看日志，在graylog安装目录下执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tail -200f /<span class="built_in">log</span>/graylog-server.log</div></pre></td></tr></table></figure>
<p>如果报错：</p>
<p>原因：</p>
<p>在mongodb版本2.6之后，是需要日志journaling设置的，而默认情况下是关闭的</p>
<p>解决办法：</p>
<p>在mongodb启动命令加上 –journal</p>
<p>最后启动命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./mongod --dbpath=/usr/<span class="built_in">local</span>/mongodb/data/ --fork --logpath=/usr/<span class="built_in">local</span>/mongodb/logs --storageEngine=mmapv1 --journal</div></pre></td></tr></table></figure>
<p>重启mongodb后，重启graylog服务即可！</p>
<p><strong>(6)启动graylog(报错二)</strong></p>
<p>查看日志，在graylog安装目录下执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">tail -200f /<span class="built_in">log</span>/graylog-server.log</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123;com.mongodb.MongoSocketOpenException: Exception opening socket&#125;, caused by &#123;java.net.ConnectException: 拒绝连接&#125;&#125;]&#125;. Waiting <span class="keyword">for</span> 30000 ms before timing out</div><div class="line">2016-11-22 15:10:29,355 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Elastic Beats Input 1.1.1 [org.graylog.plugins.beats.BeatsInputPlugin]</div><div class="line">2016-11-22 15:10:29,357 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Collector 1.1.1 [org.graylog.plugins.collector.CollectorPlugin]</div><div class="line">2016-11-22 15:10:29,357 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Enterprise Integration Plugin 1.1.1 [org.graylog.plugins.enterprise_integration.EnterpriseIntegrationPlugin]</div><div class="line">2016-11-22 15:10:29,358 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: MapWidgetPlugin 1.1.1 [org.graylog.plugins.map.MapWidgetPlugin]</div><div class="line">2016-11-22 15:10:29,359 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Pipeline Processor Plugin 1.1.1 [org.graylog.plugins.pipelineprocessor.ProcessorPlugin]</div><div class="line">2016-11-22 15:10:29,359 INFO : org.graylog2.bootstrap.CmdLineTool - Loaded plugin: Anonymous Usage Statistics 2.1.1 [org.graylog.plugins.usagestatistics.UsageStatsPlugin]</div><div class="line">2016-11-22 15:10:29,487 INFO : org.graylog2.bootstrap.CmdLineTool - Running with JVM arguments: -Djava.library.path=./../lib/sigar -Xms1g -Xmx1g -XX:NewRatio=1 -XX:+ResizeTLAB -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseParNewGC -XX:-OmitStackTraceInFastThrow</div></pre></td></tr></table></figure>
<p>提示mongodb 拒绝连接。 这个时候需要看下端口地址是否是IP地址还是127.0.0.1 如果不是需要修改下在重启服务就可以了。</p>
<p><strong>查看启动服务端口：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@graylog graylog-2.1.1]<span class="comment"># netstat -ntulp</span></div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name</div><div class="line">tcp        0      0 127.0.0.1:27017             0.0.0.0:*                   LISTEN      22308/mongod</div><div class="line">tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      1974/sshd</div><div class="line">tcp        0      0 127.0.0.1:25                0.0.0.0:*                   LISTEN      1287/master</div><div class="line">tcp        0      0 ::ffff:192.168.1.234:9350   :::*                        LISTEN      23642/java</div><div class="line">tcp        0      0 ::ffff:192.168.1.234:9000   :::*                        LISTEN      23642/java</div><div class="line">tcp        0      0 ::ffff:192.168.1.234:9200   :::*                        LISTEN      18405/java</div><div class="line">tcp        0      0 ::ffff:192.168.1.234:9300   :::*                        LISTEN      18405/java</div><div class="line">tcp        0      0 :::22                       :::*                        LISTEN      1974/sshd</div><div class="line">tcp        0      0 ::1:25                      :::*                        LISTEN      1287/master</div></pre></td></tr></table></figure>
<p><strong>(6)访问graylog</strong></p>
<p>graylog启动成功后，浏览器访问：<strong>graylog安装IP:9000</strong></p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog10.png" alt=""></figure></p>
<h3 id="四：总结"><a href="#四：总结" class="headerlink" title="四：总结"></a>四：总结</h3><p>到此，基础的集中日志管理graylog安装完毕！，后续将继续介绍：</p>
<p>安装部署Graylog Collector Sidecar 收集应用日志</p>
<p>采用syslog收集日志方式</p>
<p>graylog一些使用，包括日志截取，告警等。</p>
<p>之前也实践过ELK技术栈，后来选型graylog，是基于graylog带有的权限管理，和告警功能比较完善！</p>

	

	

</article>




	<article>
	
		<h1><a href="/2016/11/13/日志分析平台/Graylog/Graylog—日志分析平台完美代替Elasticsearch/">Graylog——日志分析平台完美代替Elasticsearch</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2016-11-13</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Log-Analysis-Platform/">Log Analysis Platform</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-link" href="/tags/Graylog/">Graylog</a>
			</span>
		
	</div>

	

	
		<p>摘要: 提起日志聚合工具，有开源界的ELK，商业界的Splunk，但我要介绍开源的后起之秀Graylog，可以说是龙头老大Splunk的开源版。</p>
<h1 id="Graylog——日志分析平台完美代替Elasticsearch"><a href="#Graylog——日志分析平台完美代替Elasticsearch" class="headerlink" title="Graylog——日志分析平台完美代替Elasticsearch"></a>Graylog——日志分析平台完美代替Elasticsearch</h1><p>先看看 推荐！<a href="https://my.oschina.net/HeAlvin/blog/378262" target="_blank" rel="external">国外程序员整理的系统管理员资源大全</a> 中，国外程序员整理的日志聚合工具的列表：</p>
<p><strong>日志管理工具：收集，解析，可视化</strong></p>
<ul>
<li>Elasticsearch - 一个基于Lucene的文档存储，主要用于日志索引、存储和分析。</li>
<li>Fluentd - 日志收集和发出</li>
<li>Flume -分布式日志收集和聚合系统</li>
<li>Graylog2 -具有报警选项的可插入日志和事件分析服务器</li>
<li>Heka -流处理系统，可用于日志聚合</li>
<li>Kibana - 可视化日志和时间戳数据</li>
<li>Logstash -管理事件和日志的工具</li>
<li>Octopussy -日志管理解决方案（可视化/报警/报告）</li>
</ul>
<p>Graylog与ELK方案的对比</p>
<ul>
<li>ELK： Logstash -&gt; Elasticsearch -&gt; Kibana （使用了一些插件head ，marvel）</li>
<li>Graylog： Graylog Collector -&gt; Graylog Server(封装Elasticsearch) -&gt; Graylog Web</li>
</ul>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Graylog.png" alt=""></figure></p>
<p>做为运维，公司内部使用elk处理日志发现很多问题。</p>
<p>顺便截图了几张：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog-elk.png" alt=""></figure><br><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog-elk1.png" alt=""></figure></p>
<p>之前试过Logstash + Elasticsearch + Kibana的方案，发现有几个缺点：</p>
<ol>
<li>不能处理多行日志，比如Mysql慢查询，Tomcat/Jetty应用的Java异常打印</li>
<li>不能保留原始日志，只能把原始日志分字段保存，这样搜索日志结果是一堆Json格式文本，无法阅读。</li>
<li>不复合正则表达式匹配的日志行，被全部丢弃。</li>
<li>kibana结合使用经常会出现卡死。资源消耗非常大。</li>
</ol>
<p>本着解决以上3个缺点的原则，再次寻找替代方案。 首先找到了商业日志工具Splunk，号称日志界的Google，意思是全文搜索日志的能力，不光能解决以上3个缺点，还提供搜索单词高亮显示，不同错误级别日志标色等吸引人的特性，但是免费版有500M限制，付费版据说要3万美刀，只能放弃，继续寻找。 最后找到了Graylog，第一眼看到Graylog，只是系统日志syslog的采集工具，一点也没吸引到我。但后来深入了解后，才发现Graylog简直就是开源版的Splunk。 我自己总结的Graylog吸引人的地方：</p>
<ol>
<li>一体化方案，安装方便，不像ELK有3个独立系统间的集成问题。</li>
<li>采集原始日志，并可以事后再添加字段，比如http_status_code，response_time等等。</li>
<li>自己开发采集日志的脚本，并用curl/nc发送到Graylog Server，发送格式是自定义的GELF，Flunted和Logstash都有相应的输出GELF消息的插件。自己开发带来很大的自由度。实际上只需要用inotifywait监控日志的modify事件，并把日志的新增行用curl/netcat发送到Graylog Server就可。</li>
<li>搜索结果高亮显示，就像google一样。</li>
<li>搜索语法简单，比如： source:mongo AND reponse_time_ms:&gt;5000，避免直接输入elasticsearch搜索json语法</li>
<li>搜索条件可以导出为elasticsearch的搜索json文本，方便直接开发调用elasticsearch rest api的搜索脚本。</li>
</ol>
<p>Graylog图解</p>
<p>Graylog开源版官网： <a href="https://www.graylog.org/" target="_blank" rel="external">https://www.graylog.org/</a></p>
<p>来几张官网的截图：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog2.png" alt=""></figure></p>
<p><figure class="figure"><img src="https://www.graylog.org/assets/itops_full-87fe2811ab676116142c145affa95b902635b1d6da99718057e8ae9d45c15007.png" alt=""></figure></p>
<p><figure class="figure"><img src="https://www.graylog.org/assets/overview_dashboard-3817012f923d4a00e9bde1c0547796319cf0396149464f434cfc2ae83a9da826.png" alt=""></figure></p>
<p>Graylog是强大的日志管理、分析工具。它基于 Elasticsearch, Java和MongoDB。</p>
<p>Graylog可以收集监控多种不同应用的日志。但是为了示范说明，我只收集syslog。并且，我将会把用到的组件全部安装到一个单独的服务器上。对于大型、生产系统你可以把组件分开安装在不同的服务器上，这样可以提高效率。</p>
<h3 id="Graylog的组件"><a href="#Graylog的组件" class="headerlink" title="Graylog的组件"></a>Graylog的组件</h3><p>Graylog有4个基本组件：</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Graylog <span class="built_in">Server</span>：这个服务负责接收和处理日志/消息，并且和其他组件沟通。</div><div class="line">Elasticsearch：存储所有的日志，它的性能依赖内存和硬盘IO。</div><div class="line">MongoDB：存储元数据，负载不高。</div><div class="line">graylog-Web接口：用户接口。</div></pre></td></tr></table></figure>
<p>下面是Graylog组件之间的关系图：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog3.png" alt=""></figure></p>
<p>下面来自我公司内部分享的PPT拍图：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/Graylog4-ppt.png" alt=""></figure></p>
<p>生产环境 我参考网上一些博客画的图：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog5.png" alt=""></figure></p>
<h3 id="系统要求："><a href="#系统要求：" class="headerlink" title="系统要求："></a>系统要求：</h3><ul>
<li>CentOS 6.7</li>
<li>内存至少2GB</li>
<li>有root权限</li>
<li>服务器ip是192.168.1.234，已安装 1.8.0_77-b03</li>
</ul>
<p>这里我只是举例单一模式跑服务。</p>
<h3 id="安装MongoDB"><a href="#安装MongoDB" class="headerlink" title="安装MongoDB"></a>安装MongoDB</h3><p>MongoDB的安装非常简单，执行如下命令导入MongoDB GPG密钥到rpm：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">vim</span> <span class="string">/etc/yum.repos.d/mongodb-org-3.0.repo</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">[mongodb-org-3.0]</span></div><div class="line"><span class="string">name=MongoDB</span> <span class="string">Repository</span></div><div class="line"><span class="string">baseurl=http://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.0/x86_64/</span></div><div class="line"><span class="string">gpgcheck=0</span></div><div class="line"><span class="string">enabled=1</span></div><div class="line"><span class="meta">---</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">yum</span> <span class="string">install</span> <span class="bullet">-y</span> <span class="string">mongodb-org</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">vi</span> <span class="string">/etc/yum.conf</span></div><div class="line"><span class="string">最后一行添加：</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">exclude=mongodb-org,mongodb-org-server,mongodb-org-shell,mongodb-org-mongos,mongodb-org-tools</span></div><div class="line"><span class="meta">---</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">service</span> <span class="string">mongod</span> <span class="string">start</span></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">chkconfig</span> <span class="string">mongod</span> <span class="string">on</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">yum.repos.d]#</span> <span class="string">vi</span> <span class="string">/etc/security/limits.conf</span></div><div class="line"><span class="string">最后一行添加：</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">*</span>                <span class="string">soft</span>    <span class="string">nproc</span>           <span class="number">65536</span></div><div class="line"><span class="string">*</span>                <span class="string">hard</span>    <span class="string">nproc</span>           <span class="number">65536</span></div><div class="line"><span class="string">mongod</span>           <span class="string">soft</span>    <span class="string">nproc</span>           <span class="number">65536</span></div><div class="line"></div><div class="line"><span class="string">*</span>                <span class="string">soft</span>    <span class="string">nofile</span>          <span class="number">131072</span></div><div class="line"><span class="string">*</span>                <span class="string">hard</span>    <span class="string">nofile</span>          <span class="number">131072</span></div><div class="line"><span class="meta">---</span></div><div class="line"></div><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">vi</span> <span class="string">/etc/init.d/mongod</span></div><div class="line"><span class="string">ulimit</span> <span class="bullet">-f</span> <span class="string">unlimited</span> <span class="string">行前插入：</span></div><div class="line"><span class="meta">---</span></div><div class="line">  <span class="string">if</span> <span class="string">test</span> <span class="bullet">-f</span> <span class="string">/sys/kernel/mm/transparent_hugepage/enabled;</span> <span class="string">then</span></div><div class="line">    <span class="string">echo</span> <span class="string">never</span> <span class="string">&gt; /sys/kernel/mm/transparent_hugepage/enabled</span></div><div class="line">  fi</div><div class="line">  if test -f /sys/kernel/mm/transparent_hugepage/defrag; then</div><div class="line">    echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</div><div class="line">  fi</div><div class="line">---</div><div class="line">[root@graylog ~]# /etc/init.d/mongod restart</div></pre></td></tr></table></figure>
<h3 id="安装Elasticsearch"><a href="#安装Elasticsearch" class="headerlink" title="安装Elasticsearch"></a>安装Elasticsearch</h3><p>Graylog目前为止只能使用Elasticsearch 2.0以前的版本，所以，在这一步中，我将安装Elasticsearch 1.7.x。</p>
<p>添加Elasticsearch GPG密钥：</p>
<pre><code>$ sudo rpm --import http://packages.elastic.co/GPG-KEY-elasticsearch
</code></pre><p>创建Elasticsearch源：</p>
<pre><code>$ sudo vim /etc/yum.repos.d/elasticsearch.repo
</code></pre><p>写入如下内容：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="section">[elasticsearch-1.7]</span></div><div class="line"><span class="attr">name</span>=Elasticsearch repository for <span class="number">1.7</span>.x packages</div><div class="line"><span class="attr">baseurl</span>=http://packages.elastic.co/elasticsearch/<span class="number">1.7</span>/centos</div><div class="line"><span class="attr">gpgcheck</span>=<span class="number">1</span></div><div class="line"><span class="attr">gpgkey</span>=http://packages.elastic.co/GPG-KEY-elasticsearch</div><div class="line"><span class="attr">enabled</span>=<span class="number">1</span></div></pre></td></tr></table></figure>
<p>安装Elasticsearch：</p>
<pre><code>$ sudo yum -y install elasticsearch
</code></pre><p>配置前先创建几个目录文件</p>
<pre><code>mkdir /data/es-data -p
mkdir /data/es-work -p
</code></pre><p>Elasticsearch安装完成之后，编辑配置文件：</p>
<pre><code>sudo vim /etc/elasticsearch/elasticsearch.yml

 node.data: true  # 数据存放true
</code></pre><p>找到cluster.name一行，取消这一行的注释，并把值改为graylog-development：</p>
<pre><code>cluster.name: graylog-development


 path.data: /data/es-data     es数据存放目录  这里需要自己新建目录

 path.work: /data/es-work   
</code></pre><p>你也许想要限制外部访问Elasticsearch（端口9200），这样可以提高系统的安全性。找到network.host一行，取消注释，并把值改为localhost：</p>
<pre><code>network.host: 192.168.1.234
</code></pre><p>保存退出文件。</p>
<p>重启Elasticsearch：</p>
<pre><code>$ service elasticsearch start
</code></pre><p>设置开机启动：</p>
<pre><code>$ chkconfig --add elasticsearch
</code></pre><p>执行如下命令测试Elasticsearch是否正常运行：</p>
<pre><code>$ curl -XGET &apos;http://localhost:9200/_cluster/health?pretty=true&apos;
</code></pre><p>输出的信息如下表示Elasticsearch安装成功：</p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">drwxr-xr-x. <span class="number">2</span> root   root  <span class="number">4096</span> <span class="number">9</span>月  <span class="number">23</span> <span class="number">2011</span> src</div><div class="line">[root<span class="symbol">@ELK</span> <span class="keyword">local</span>]<span class="meta">#  curl -XGET <span class="string">'http://192.168.1.234:9200/_cluster/health?pretty=true'</span></span></div><div class="line">&#123;</div><div class="line">  <span class="string">"cluster_name"</span> : <span class="string">"graylog-development"</span>,</div><div class="line">  <span class="string">"status"</span> : <span class="string">"yellow"</span>,</div><div class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</div><div class="line">  <span class="string">"number_of_nodes"</span> : <span class="number">1</span>,</div><div class="line">  <span class="string">"number_of_data_nodes"</span> : <span class="number">1</span>,</div><div class="line">  <span class="string">"active_primary_shards"</span> : <span class="number">37</span>,</div><div class="line">  <span class="string">"active_shards"</span> : <span class="number">37</span>,</div><div class="line">  <span class="string">"relocating_shards"</span> : <span class="number">0</span>,</div><div class="line">  <span class="string">"initializing_shards"</span> : <span class="number">0</span>,</div><div class="line">  <span class="string">"unassigned_shards"</span> : <span class="number">37</span>,</div><div class="line">  <span class="string">"delayed_unassigned_shards"</span> : <span class="number">0</span>,</div><div class="line">  <span class="string">"number_of_pending_tasks"</span> : <span class="number">0</span>,</div><div class="line">  <span class="string">"number_of_in_flight_fetch"</span> : <span class="number">0</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>安装Graylag</p>
<p>Graylog的最新版是1.1.4，下载链接如下：<br><a href="https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-server-1.1.4-1.noarch.rpm" target="_blank" rel="external">https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-server-1.1.4-1.noarch.rpm</a><br><a href="https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-web-1.1.4-1.noarch.rpm" target="_blank" rel="external">https://packages.graylog2.org/repo/el/6Server/1.1/x86_64/graylog-web-1.1.4-1.noarch.rpm</a></p>
<p>现在Graylog的所有依赖软件安装完成，这一步我们来安装graylog-server。</p>
<p>首先，下载Graylog RPM软件包：</p>
<pre><code>$ cd 
$ sudo rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-1.3-repository-el7_latest.rpm
</code></pre><p>安装graylog-server：</p>
<pre><code>yum -y install graylog-server
</code></pre><p>安装pwgen，我们使用它生成随机密码：</p>
<pre><code>yum -y install epel-release
yum -y install pwgen
</code></pre><p>现在我们来设置Graylog管理员的密钥。配置文件位于/etc/graylog/server/server.conf目录，需要修改password_secret参数：</p>
<pre><code>$ SECRET=$(pwgen -s 96 1)
$ sudo -E sed -i -e &apos;s/password_secret =.*/password_secret = &apos;$SECRET&apos;/&apos; /etc/graylog/server/server.conf
</code></pre><p>执行完上面命令之后，password_secret参数的样子：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog6.png" alt=""></figure></p>
<p>这一步，设置管理员密码。由于密码使用sha哈希算法，我们需要把明文密码转换为hash，然后赋值给root_password_sha2参数。例如，我要设置的管理员密码是 Ihaozhuo_b313，它对应的hash为：</p>
<pre><code>$ echo -n Ihaozhuo_b313 | sha256sum | awk &apos;{print $1}&apos;
</code></pre><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog7.png" alt=""></figure></p>
<p>编辑/etc/graylog/server/server.conf，设置root_password_sha2参数：</p>
<pre><code>$ sudo vim /etc/graylog/server/server.conf
</code></pre><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog8.png" alt=""></figure></p>
<p>现在Graylog管理员密码为Ihaozhuo_b313。</p>
<p>配置rest_transport_uri参数，设置Graylog web接口和服务器的沟通方式。由于我们把所有组件都安装到了单独的一个服务器上，需要把值设置为<strong>127.0.0.1</strong> 或 <strong>localhost</strong>。找到rest_transport_uri一行，取消注释，并把值设置为：</p>
<pre><code>rest_transport_uri = http://192.168.1.234:12900/
</code></pre><p>由于我们只有一个Elasticsearch shard，需要把elasticsearch_shards参数设置为1：</p>
<pre><code>elasticsearch_shards = 1
</code></pre><p>更改elasticsearch_cluster_name参数，应该和前面Elasticsearch的<strong>cluster.name</strong>参数相对应：</p>
<pre><code>elasticsearch_cluster_name = graylog-development
</code></pre><p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog9.png" alt=""></figure></p>
<p>取消下面两行的注释，检测Elasticsearch：</p>
<pre><code>172 elasticsearch_discovery_zen_ping_multicast_enabled = false
173 elasticsearch_discovery_zen_ping_unicast_hosts = 192.168.1.234:9300
</code></pre><p>启动graylog-server：</p>
<pre><code>/etc/init.d/graylog-web restart
</code></pre><h3 id="安装Graylog-Web"><a href="#安装Graylog-Web" class="headerlink" title="安装Graylog Web"></a>安装Graylog Web</h3><p>安装Graylog Web：</p>
<pre><code>$ sudo yum -y install graylog-web
</code></pre><p>安装完成之后配置Graylog Web的密钥，配置文件位于/etc/graylog/web/web.conf，更改application.secret参数：</p>
<pre><code>$ SECRET=$(pwgen -s 96 1)
$ sudo -E sed -i -e &apos;s/application\.secret=&quot;&quot;/application\.secret=&quot;&apos;$SECRET&apos;&quot;/&apos; /etc/graylog/web/web.conf
</code></pre><p>配置graylog2-server.uris参数，它的值应该和Graylog的rest_listen_uri参数相对应：</p>
<pre><code>$ sudo vim /etc/graylog/web/web.conf

graylog2-server.uris=&quot;http://127.0.0.1:12900/&quot;
</code></pre><p>重启graylog-web：</p>
<pre><code>/etc/init.d/graylog-web restart
</code></pre><h3 id="配置Graylog服务器接收其他服务器的syslog日志"><a href="#配置Graylog服务器接收其他服务器的syslog日志" class="headerlink" title="配置Graylog服务器接收其他服务器的syslog日志"></a>配置Graylog服务器接收其他服务器的syslog日志</h3><h4 id="登录Graylog-Web"><a href="#登录Graylog-Web" class="headerlink" title="登录Graylog Web"></a>登录Graylog Web</h4><p>使用浏览器访问Graylog服务器的域名或IP：<strong><a href="http://graylog_public_IP_domain:9000/" target="_blank" rel="external">http://graylog_public_IP_domain:9000/</a></strong>。</p>
<p>你应该能看到一个登录界面，使用admin做为用户名和前面设置的密码登录。</p>
<p>登录之后：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog10.png" alt=""></figure></p>
<p>上面的红数字1是通知（you have a node without any running inputs），下面设置通过UDP接收syslog。</p>
<p>创建syslog UDP输入</p>
<p>添加要接收的其他服务器syslog日志：System-&gt;Inputs-&gt;Syslog UDP-&gt;Launch new input。</p>
<p>在弹出的窗口上输入如下信息：</p>
<ul>
<li>Title: syslog</li>
<li>Port: 8514</li>
<li>Bind address: 这里写Graylog-server服务器主机IP</li>
</ul>
<p>点击Launch</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog11.png" alt=""></figure></p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog12.png" alt=""></figure></p>
<p>如果你需要收集多个服务器的日志，重复上面步骤。</p>
<p>现在，我们的Graylog服务器已经做好了接收其他服务器发来日志的准备。下面我们还需要配置其他服务器，让这些服务器给Graylog服务器发送日志。</p>
<h4 id="配置其他服务器给Graylog服务器发送syslog"><a href="#配置其他服务器给Graylog服务器发送syslog" class="headerlink" title="配置其他服务器给Graylog服务器发送syslog"></a>配置其他服务器给Graylog服务器发送syslog</h4><p>参考官网：<a href="https://marketplace.graylog.org/addons/a47beb3b-0bd9-4792-a56a-33b27b567856" target="_blank" rel="external">从Linux系统日志发送到Graylog</a></p>
<p>SSH登录“其他服务器”，创建rsyslog配置文件90-graylog.conf：</p>
<pre><code>sudo vim /etc/rsyslog.d/90-graylog.conf
</code></pre><p>添加如下代码，把 graylog_server_IP 替换为Graylog服务器ip地址：</p>
<pre><code>$template GRAYLOGRFC5424,&quot;&lt;%pri%&gt;%protocol-version% %timestamp:::date-rfc3339%    %HOSTNAME% %app-name% %procid% %msg%\n&quot;
*.* @graylog_server_IP:8514;GRAYLOGRFC5424
</code></pre><p>重启rsyslog服务使生效：</p>
<pre><code>/etc/init.d/rsyslog restart
</code></pre><p>配置完成之后，回到Graylog Web，点击Sources，查看是否有新添加rsyslog。</p>
<h4 id="Graylog使用http协议发送："><a href="#Graylog使用http协议发送：" class="headerlink" title="Graylog使用http协议发送："></a>Graylog使用http协议发送：</h4><p>添加要接收的其他服务器syslog日志：System-&gt;Inputs-&gt;GELF HTTP-&gt;Launch new input。</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog14.png" alt=""></figure></p>
<p>然后在服务器上面发送下面命令。</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog-development ~]</span># curl -XPOST http://<span class="number">192.168.1.234:12201</span>/gelf -p0 -d '&#123;<span class="string">"short_message"</span>:<span class="string">"Hello there"</span>, <span class="string">"host"</span>:<span class="string">"example.org"</span>, <span class="string">"facility"</span>:<span class="string">"test"</span>, <span class="string">"_foo"</span>:<span class="string">"bar"</span>&#125;'</div><div class="line"><span class="string">[root@graylog-development ~]</span># curl -XPOST http://<span class="number">192.168.1.234:12201</span>/gelf -p0 -d '&#123;<span class="string">"short_message"</span>:<span class="string">"测试"</span>, <span class="string">"host"</span>:<span class="string">"example.org"</span>, <span class="string">"facility"</span>:<span class="string">"test"</span>, <span class="string">"_foo"</span>:<span class="string">"bar"</span>&#125;'</div></pre></td></tr></table></figure>
<p>这里定义192.168.1.234 是graylog-server服务器地址 12201 端口是之前创建好的。</p>
<h3 id="搜素Graylog"><a href="#搜素Graylog" class="headerlink" title="搜素Graylog"></a>搜素Graylog</h3><p>假如你要搜索hello：</p>
<p><figure class="figure"><img src="http://7xrthw.com1.z0.glb.clouddn.com/graylog13.png" alt=""></figure></p>
<p>上面安装配置了基本的Graylog服务器。</p>
<h3 id="时区和高亮设置"><a href="#时区和高亮设置" class="headerlink" title="时区和高亮设置"></a>时区和高亮设置</h3><p>admin帐号的时区：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">vi</span> <span class="string">/etc/graylog/server/server.conf</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="number">30</span> <span class="string">root_timezone</span> <span class="string">=</span> <span class="string">Asia/Shanghai</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">/etc/init.d/graylog-server</span> <span class="string">restart</span></div></pre></td></tr></table></figure>
<p>其他帐号的默认时区：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">vi</span> <span class="string">/etc/graylog/web/web.conf</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="number">18</span> <span class="string">timezone="Asia/Shanghai"</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">/etc/init.d/graylog-web</span> <span class="string">restart</span></div></pre></td></tr></table></figure>
<p>允许查询结果高亮：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">vi</span> <span class="string">/etc/graylog/server/server.conf</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="number">147</span> <span class="string">allow_highlighting</span> <span class="string">=</span> <span class="literal">true</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="string">[root@graylog</span> <span class="string">~]#</span> <span class="string">/etc/init.d/graylog-server</span> <span class="string">restart</span></div></pre></td></tr></table></figure>
<h3 id="移动数据目录"><a href="#移动数据目录" class="headerlink" title="移动数据目录"></a>移动数据目录</h3><figure class="highlight livescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">移动elasticsearch的数据目录</div><div class="line">[root@graylog ~]<span class="comment"># sudo /etc/init.d/elasticsearch stop</span></div><div class="line">[root@graylog ~]<span class="comment"># sudo cp -rp /var/lib/elasticsearch/ /data/</span></div><div class="line">[root@graylog ~]<span class="comment"># sudo vi /etc/sysconfig/elasticsearch</span></div><div class="line">+<span class="number">16</span> DATA_DIR=/data/elasticsearch</div><div class="line">[root@graylog ~]<span class="comment"># sudo /etc/init.d/elasticsearch start</span></div><div class="line"></div><div class="line">移动mongo的数据目录</div><div class="line">[root@graylog ~]<span class="comment"># sudo /etc/init.d/mongod stop</span></div><div class="line">[root@graylog ~]<span class="comment"># sudo cp -rp /var/lib/mongo /data/</span></div><div class="line">[root@graylog ~]<span class="comment"># sudo vi /etc/mongod.conf</span></div><div class="line">---</div><div class="line"><span class="number">13</span> dbpath=/<span class="keyword">var</span>/lib/mongo<span class="function"></span></div><div class="line">-&gt;</div><div class="line"><span class="number">13</span> dbpath=/data/mongo</div><div class="line">---</div><div class="line">[mtagent@access2 ~]$ sudo /etc/init.d/mongod start</div></pre></td></tr></table></figure>
<h4 id="其余参考文档"><a href="#其余参考文档" class="headerlink" title="其余参考文档"></a>其余参考文档</h4><p><a href="http://docs.graylog.org/en/1.3/" target="_blank" rel="external">参考官网</a></p>
<p><a href="http://blog.topspeedsnail.com/archives/4066" target="_blank" rel="external">Centos7 搭建graylog</a></p>
<p><a href="https://my.oschina.net/fitnessefan/blog/464351?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="external">Graylog——日志聚合工具中的后起之秀</a></p>

	

	

</article>




	<article>
	
		<h1><a href="/2016/11/10/运维笔记/如何解决抽奖的性能问题和秒杀的讨论  /">如何解决秒杀的性能问题和微信推广活动的讨论</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2016-11-10</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/运维笔记/">运维笔记</a>
			</span><br />
		
		
	</div>

	

	
		<p>最近业务试水微信抽奖活动，公司现在打算推广下产品。增加些粉丝量 之前经常看到淘宝的同行们讨论秒杀，讨论电商，这次终于轮到我们自己理论结合实际一次了。</p>
<p>ps：进入正文前先说一点个人感受，之前看淘宝的ppt感觉都懂了，等到自己出解决方案的时候发现还是有很多想不到的地方其实都没懂，再次验证了“细节是魔鬼”的理论。并且一个人的能力有限，只有大家一起讨论才能想的更周全，更细致。好了，闲话少说，下面进入正文。</p>
<h4 id="一、抽奖活动带来了什么？"><a href="#一、抽奖活动带来了什么？" class="headerlink" title="一、抽奖活动带来了什么？"></a>一、抽奖活动带来了什么？</h4><p>微信积分兑换现金或抢购抽奖活动一般会经过【关注公众号】【分享活动获得积分】【积分兑换】【抽奖安全环节】这3个大环节，而其中【抢订单】这个环节是最考验业务提供方的抗压能力的。</p>
<p>积分兑换环节一般会带来2个问题：</p>
<ul>
<li>1、高并发</li>
</ul>
<p>　　比较火热的抽奖在线人数都是20w起的，如此之高的在线人数对于网站架构从前到后都是一种考验。</p>
<ul>
<li>2、抽奖人员过多</li>
<li>3、 抢订单</li>
</ul>
<p>　　任何商品都会有数量上限，如何避免成功下订单买到商品的人数不超过商品数量的上限，这是每个抢购活动都要面临的难题。</p>
<h4 id="二、如何解决？"><a href="#二、如何解决？" class="headerlink" title="二、如何解决？"></a>二、如何解决？</h4><p>首先，产品解决方案我们就不予讨论了。我们只讨论技术解决方案</p>
<p><strong>1、前端架构</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">面对高并发的抢购活动，前端常用的三板斧是【扩容】【静态化】【限流】</div><div class="line"></div><div class="line">　　A：扩容</div><div class="line">　　</div><div class="line">　加机器，这是最简单的方法，通过增加前端池的整体承载量来抗峰值。 </div><div class="line">　这里我们都是多台主从集群去跑。</div><div class="line"></div><div class="line">　　B：静态化</div><div class="line"></div><div class="line">　　将活动页面上的所有可以静态的元素全部静态化，并尽量减少动态元素。通过CDN来抗峰值。</div><div class="line">　　这里我们都是把图片静态放在又拍云和阿里云上面。</div><div class="line">　</div><div class="line">　　C：限流</div><div class="line"></div><div class="line">　　一般都会采用IP级别的限流，即针对某一个IP，限制单位时间内发起请求数量。</div><div class="line">　　或者活动入口的时候增加游戏或者问题环节进行消峰操作。</div><div class="line">　　我们采用Nginx前面做了反向代理后面优化nginx。</div><div class="line">　　</div><div class="line">　　D：有损服务</div><div class="line"></div><div class="line">　　最后一招，在接近前端池承载能力的水位上限的时候，随机拒绝部分请求来保护活动整体的可用性。</div></pre></td></tr></table></figure>
<p><strong>2、后端架构</strong></p>
<p>那么后端的数据库在高并发和超卖下会遇到什么问题呢？主要会有如下3个问题：（主要讨论写的问题，读的问题通过增加cache可以很容易的解决）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">　　I：　首先MySQL自身对于高并发的处理性能就会出现问题，一般来说，MySQL的处理性能会随着并发thread上升而上升，但是到了一定的并发度之后会出现明显的拐点，之后一路下降，最终甚至会比单thread的性能还要差。</div><div class="line"></div><div class="line">　　II： 其次，超卖的根结在于减库存操作是一个事务操作，需要先<span class="keyword">select</span>，然后<span class="keyword">insert</span>，最后<span class="keyword">update</span> <span class="number">-1</span>。最后这个<span class="number">-1</span>操作是不能出现负数的，但是当多用户在有库存的情况下并发操作，出现负数这是无法避免的。</div><div class="line"></div><div class="line">　　III：最后，当减库存和高并发碰到一起的时候，由于操作的库存数目在同一行，就会出现争抢<span class="keyword">InnoDB</span>行锁的问题，导致出现互相等待甚至死锁，从而大大降低MySQL的处理性能，最终导致前端页面出现超时异常。</div><div class="line">　　</div><div class="line">　　这个有遇到过的，就是锁表导致访问出现网络异常。</div></pre></td></tr></table></figure>
<p>针对上述问题，如何解决呢？ 我们先看眼淘宝的高大上解决方案：</p>
<figure class="highlight axapta"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">I：关闭死锁检测，提高并发处理性能。</div><div class="line">II：修改源代码，将排队提到进入引擎层前，降低引擎层面的并发度。</div><div class="line">III：组提交，降低<span class="keyword">server</span>和引擎的交互次数，降低IO消耗。</div></pre></td></tr></table></figure>
<h3 id="解决方案1："><a href="#解决方案1：" class="headerlink" title="解决方案1："></a>解决方案1：</h3><p>将存库从MySQL前移到Redis中，所有的写操作放到内存中，由于Redis中不存在锁故不会出现互相等待，并且由于Redis的写性能和读性能都远高于MySQL，这就解决了高并发下的性能问题。然后通过队列等异步手段，将变化的数据异步写入到DB中。</p>
<p>优点：解决性能问题<br>缺点：没有解决超卖问题，同时由于异步写入DB，存在某一时刻DB和Redis中数据不一致的风险。</p>
<h3 id="解决方案2："><a href="#解决方案2：" class="headerlink" title="解决方案2："></a>解决方案2：</h3><p>引入队列，然后将所有写DB操作在单队列中排队，完全串行处理。当达到库存阀值的时候就不在消费队列，并关闭购买功能。这就解决了超卖问题。</p>
<p>优点：解决超卖问题，略微提升性能。<br>缺点：性能受限于队列处理机处理性能和DB的写入性能中最短的那个，另外多商品同时抢购的时候需要准备多条队列。</p>
<h3 id="解决方案3："><a href="#解决方案3：" class="headerlink" title="解决方案3："></a>解决方案3：</h3><p>将写操作前移到MC中，同时利用MC的轻量级的锁机制CAS来实现减库存操作。<br>优点：读写在内存中，操作性能快，引入轻量级锁之后可以保证同一时刻只有一个写入成功，解决减库存问题。<br>缺点：没有实测，基于CAS的特性不知道高并发下是否会出现大量更新失败？不过加锁之后肯定对并发性能会有影响。</p>
<h3 id="解决方案4："><a href="#解决方案4：" class="headerlink" title="解决方案4："></a>解决方案4：</h3><p>将提交操作变成两段式，先申请后确认。然后利用Redis的原子自增操作（相比较MySQL的自增来说没有空洞），同时利用Redis的事务特性来发号，保证拿到小于等于库存阀值的号的人都可以成功提交订单。然后数据异步更新到DB中。</p>
<p>优点：解决超卖问题，库存读写都在内存中，故同时解决性能问题。<br>缺点：由于异步写入DB，可能存在数据不一致。另可能存在少买，也就是如果拿到号的人不真正下订单，可能库存减为0，但是订单数并没有达到库存阀值。</p>
<p> 三、总结</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">1</span>、前端三板斧【扩容】【限流】【静态化】</div><div class="line"><span class="number">2</span>、后端两条路【内存】+【排队】</div></pre></td></tr></table></figure>
<h4 id="测试总结经验"><a href="#测试总结经验" class="headerlink" title="测试总结经验"></a>测试总结经验</h4><p>第一次做抽奖活动，测试那边测试出很多问题。 下面是遇到的问题。</p>
<figure class="highlight x86asm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">测试第一次: 访问压测出现了访问白屏。定位：代码写的有问题。 tomcat需要做优化。 我们一般用户量粉丝在100W</div><div class="line"></div><div class="line">测试第二次： 出现数据库<span class="meta">CPU</span>瓶颈过高。主从同步，然后做了数据库读写分离， redis集群缓存。 最主要是数据库SQL语句做了优化。优化了提升了很大一部分。</div><div class="line"></div><div class="line">测试第三次：还有出现了<span class="number">503</span>访问受限了。 后面做了nginx代理 防止同一个<span class="built_in">IP</span>同一个时间超过多少次去请求，会被受限。 不过这个做DDOS攻击 有很大一部分影响用户名体验度。</div></pre></td></tr></table></figure>
<h4 id="四、非技术感想"><a href="#四、非技术感想" class="headerlink" title="四、非技术感想"></a>四、非技术感想</h4><p>1、团队的力量是无穷的，各种各样的解决方案（先不谈可行性）都是在小伙伴们七嘴八舌中讨论出来的。我们需要让所有人都发出自己的声音，不要着急去否定。<br>2、优化需要从整体层面去思考，不要只纠结于自己负责的部分，如果只盯着一个点思考，最后很可能就走进死胡同中了。<br>3、有很多东西以为读过了就懂了，其实不然。依然还是需要实践，否则别人的知识永远不可能变成自己的。<br>4、多思考为什么，会发生什么，不要想当然。只有这样才能深入进去，而不是留在表面。</p>
<h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>




	<article>
	
		<h1><a href="/2016/10/10/运维笔记/个人总结：为什要学习docker，如何学习Docker/">个人总结：为什要学习docker，如何学习Docker</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2016-10-10</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/运维笔记/">运维笔记</a>
			</span><br />
		
		
	</div>

	

	
		<p>学习任何一个开源新技术，首先问自己几个问题：</p>
<ol>
<li>为什要学习它？</li>
<li>学习它需要了解哪些相关知识点？</li>
<li>如何快速学习？</li>
<li>该技术的使用场景是什么？</li>
</ol>
<p>拿我个人的学习经验来举例（本人之前比较了解KVM，ESxi,OpenStack）</p>
<h4 id="为什要学习docker？"><a href="#为什要学习docker？" class="headerlink" title="为什要学习docker？"></a>为什要学习docker？</h4><p>个人回答：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">docker是轻量级虚拟化技术，docker使linux容器技术的应用更加简单和标准化</div><div class="line">docker的速度很快,容器启动时毫秒级的</div><div class="line">docker将开发和运维职责分清</div><div class="line">docker解决了依赖地狱问题</div><div class="line">docker支持几乎所有操作系统</div><div class="line">docker有着飞速发展的生态圈</div><div class="line">很多IT巨头逐渐加入和支持</div></pre></td></tr></table></figure>
<p>学习它需要了解哪些相关知识点？</p>
<p>回答：</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">云计算概念相关（restapi, 微服务，OpenStack）</div><div class="line"><span class="symbol">Linux</span> 系统管理（软件包管理，用户管理，进程管理等）</div><div class="line"><span class="symbol">Linux</span> 内核相关（Cgroup, namespace 等）</div><div class="line"><span class="symbol">Linux</span> 文件系统和存储相关（AUFS，<span class="keyword">BRFS,devicemapper </span>等）</div><div class="line"><span class="symbol">Linux</span> 网络（网桥，veth,iptables等）</div><div class="line"><span class="symbol">Linux</span>安全相关（Appmor,<span class="keyword">Selinux </span>等）</div><div class="line"><span class="symbol">Linux</span>进程管理（Supervisord,Systemd etc)</div><div class="line"><span class="symbol">Linux</span>容器技术（LXC等）</div><div class="line">开发语言（Python, GO,Shell 等）</div></pre></td></tr></table></figure>
<p>3.如何快速学习？</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">回答：个人体会最好有一个实际的需求或项目来边实践边学习，入门可以参考</div><div class="line">（第一本docker书）写的不错，非常适合入门。</div><div class="line"></div><div class="line">除此之外，阅读牛人的blog比如官方blog <span class="string">http:</span><span class="comment">//blog.docker.com/</span></div></pre></td></tr></table></figure>
<p>最后，参与社区互动也是很好的学习方式。</p>
<p>该技术的使用场景是什么？<br>回答：docker非常适用于dev/test CI/CD 场景，用完就扔。还有就是PasS了。</p>
<h4 id="为什要学习Docker"><a href="#为什要学习Docker" class="headerlink" title="为什要学习Docker"></a>为什要学习Docker</h4><p>1.学习Docker，如果没有云计算的基本知识，以及内核的基本知识，那么学习并理解起来会稍吃力。作为容器，Docker容器的优势在哪，不足在哪，最好了解容器的实现是怎样的（简单了解）；拥有镜像管理，Docker又该如何体现软件开发，集成，部署，发布，再迭代的软件生命周期管理优势。以上两点我认为最为关键，有这两方面的认识势必会对之后的工作帮助巨大。</p>
<p>2.关于学习资源，起码的硬件设施总是要有的。Docker及其生态的发展很快，不使用纯理论肯定收效甚微。另外，资源还包括Docker官方，各大电子媒体平台，技术论坛，开源社区等，往往大拿的观点能点破自己的困惑，或者让自己知道哪方面的认识还很欠缺，以及让自己少走很多的弯路。</p>
<p>3.个人兴趣的话，归结为强扭的瓜不甜。起码应该认同Docker的设计价值，以及Docker的未来潜力，当然有依据的批判Docker并带动大家的思考，也是深切关注的表现。</p>
<p>4.个人发展方向，我认为如果需要把Docker当作软件生命周期管理工具的话，那用好Docker最为重要，API及命令的理解与使用是必需的。如果专注系统设计方面，那么除Docker以上的知识与经验之外，若有Docker源码的学习与理解，那么这些肯定会让你的Docker水平提高一个层次。</p>
<p>学习Docker，最大的好处是跟进新技术发展方向。我觉得在校生应该没有多少硬性需求在Docker的研究上，这也是为什么学校没做具体应用要求的原因。最实际的做法是看一些Docker使用案例，自己实践出一些经验应该会再以后的社会实践中起到作用。</p>
<p>研究docker的源代码，应该到你下定决心从事云计算方面的事业或者研究，那么你就需要以研究者的身份去做仔细的源码分析的工作。</p>
<h4 id="Docker火起来的真正原因是什么？"><a href="#Docker火起来的真正原因是什么？" class="headerlink" title="Docker火起来的真正原因是什么？"></a>Docker火起来的真正原因是什么？</h4><p>我认为有这么几点，</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">普通开发者有机会接触容器技术，享受开发发布一体化的便利。</div><div class="line">容器云是区别于传统IaaS服务的另一种更快速更高效的轻量级选择。</div><div class="line">基于镜像的第二个GitHub、<span class="keyword">CI</span>/<span class="keyword">CD</span>服务、代码发布、在线debug。</div></pre></td></tr></table></figure>
<h3 id="交流学习："><a href="#交流学习：" class="headerlink" title="交流学习："></a>交流学习：</h3><p>🐧  Linux shell_高级运维派: <code>459096184</code>    圈子 (系统运维-应用运维-自动化运维-虚拟化技术研究欢迎加入)<br>🐧  BigData-Exchange School : <code>521621407</code>  圈子（大数据运维)（Hadoop开发人员)（大数据研究爱好者) 欢迎加入</p>
<p>相应Bidata有内部微信交流群互相学习，加入QQ群有链接。</p>

	

	

</article>





	<span class="different-posts">📖 <a href="/page/5">more posts</a> 📖</span>



	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<h5>Hi there,</h5>
	<p style="text-align:justify;">my name is Yancy, they know me as Radiant🐑🐑. This blog is about Bi-data and my live.<br> I like 🌳🌳🌳</p>
</div>


	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2017 Yancy | Powered by <a href="http://blog.yancy.cc/">Yancy</a> | Theme <a href="https://github.com/yangcvo">Yancy_GitHub</a></span>
		</div>

	</div>


</footer>



</body>

</html>
